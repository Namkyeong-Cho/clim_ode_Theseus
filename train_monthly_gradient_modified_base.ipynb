{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f46e158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:44.200735Z",
     "start_time": "2024-10-14T07:36:41.647096Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as Fin\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint as odeint\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.empty_cache() \n",
    "import torch.optim as optim\n",
    "import random\n",
    "import logging\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "import sys\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "from torchcubicspline import(natural_cubic_spline_coeffs, \n",
    "                             NaturalCubicSpline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189e4fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:44.214884Z",
     "start_time": "2024-10-14T07:36:44.204384Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_gradient(data, dim) :\n",
    "    # 10, 5, 32, 64\n",
    "    # 10, 1, 5, 32, 64\n",
    "    squeeze=False\n",
    "    if data.dim() == 5:\n",
    "        squeeze=True\n",
    "        data = data.squeeze(1)\n",
    "        dim = dim - 1\n",
    "        \n",
    "    if dim==2 :\n",
    "        pad = nn.CircularPad2d((0,0,2,2))\n",
    "    if dim==3 :\n",
    "        pad = nn.CircularPad2d((2,2,0,0))\n",
    "        \n",
    "    out = pad(data)\n",
    "    out = torch.gradient(out, dim=dim)[0]\n",
    "\n",
    "    if dim==2 :\n",
    "        out = out[:,:,2:-2,:]\n",
    "    if dim==3 :\n",
    "        out = out[:,:,:,2:-2]\n",
    "    if squeeze :\n",
    "        out = out.unsqueeze(1)\n",
    "        \n",
    "    return out, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c37df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:44.234810Z",
     "start_time": "2024-10-14T07:36:44.216424Z"
    }
   },
   "outputs": [],
   "source": [
    "# from model_function import *\n",
    "# from model_utils import *\n",
    "# from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58139880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:44.258718Z",
     "start_time": "2024-10-14T07:36:44.238393Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "def get_train_test_data_without_scales_batched_monthly(data_path,train_time_scale,val_time_scale,test_time_scale,lev,spectral):\n",
    "    data = xr.open_mfdataset(data_path, combine='by_coords')\n",
    "    if lev in [\"v\",\"u\",\"r\",\"q\",\"tisr\"]:\n",
    "        data = data.sel(level=500)\n",
    "    data = data.resample(time=\"6H\").nearest(tolerance=\"1H\") # Setting data to be 6-hour cycles\n",
    "    data = data.resample(time=\"MS\").mean()\n",
    "\n",
    "    data_train = data.sel(time=train_time_scale).load()\n",
    "    data_val = data.sel(time=val_time_scale).load()\n",
    "    data_test = data.sel(time=test_time_scale).load()\n",
    "    data_global = data.sel(time=slice('2006','2018')).load()\n",
    "\n",
    "    max_val = data_global.max()[lev].values.tolist()\n",
    "    min_val = data_global.min()[lev].values.tolist()\n",
    "\n",
    "    data_train_final = (data_train - min_val)/ (max_val - min_val)\n",
    "    data_val_final = (data_val - min_val)/ (max_val - min_val)\n",
    "    data_test_final = (data_test - min_val)/ (max_val - min_val)\n",
    "\n",
    "    time_vals = data_test_final.time.values\n",
    "    train_times = [i for i in range(2006,2016)]\n",
    "    test_times = [2017,2018]\n",
    "    val_times = [2016]\n",
    "\n",
    "    train_data = get_batched_monthly(train_times,data_train_final,lev)\n",
    "    test_data = get_batched_monthly(test_times,data_test_final,lev)\n",
    "    val_data = get_batched_monthly(val_times,data_val_final,lev)\n",
    "\n",
    "\n",
    "    t = [i for i in range(12)]\n",
    "    time_steps = torch.tensor(t).view(-1,1)\n",
    "\n",
    "    return train_data,val_data,test_data,time_steps,data.lat.values,data.lon.values,max_val,min_val,time_vals\n",
    "\n",
    "\n",
    "def get_batched_monthly(train_times,data_train_final,lev):\n",
    "    for idx,year in enumerate(train_times):\n",
    "        data_per_year = data_train_final.sel(time=slice(str(year),str(year))).load()\n",
    "        data_values = data_per_year[lev].values\n",
    "        t_data = torch.from_numpy(data_values).reshape(-1,1,1,data_values.shape[-2],data_values.shape[-1])\n",
    "        if idx ==0:\n",
    "            train_data = t_data\n",
    "        else:\n",
    "            train_data = torch.cat([train_data,t_data],dim=1)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "def add_constant_info(path):\n",
    "    print(path)\n",
    "    data = xr.open_mfdataset(path, combine='by_coords')\n",
    "    for idx,var in enumerate(['orography','lsm']):\n",
    "        var_value = torch.from_numpy(data[var].values).view(1,1,32,64)\n",
    "        if idx ==0: final_var = var_value\n",
    "        else:\n",
    "            final_var = torch.cat([final_var,var_value],dim=1)\n",
    "\n",
    "    return final_var,torch.from_numpy(data['lat2d'].values),torch.from_numpy(data['lon2d'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e31dd32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:44.277506Z",
     "start_time": "2024-10-14T07:36:44.259916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "cwd = os.getcwd()\n",
    "#data_path = {'z500':str(cwd) + '/era5_data/geopotential_500/*.nc','t850':str(cwd) + '/era5_data/temperature_850/*.nc'}\n",
    "SOLVERS = [\"dopri8\",\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams', 'fixed_adams',\"adaptive_heun\",\"euler\"]\n",
    "parser = argparse.ArgumentParser('ClimODE')\n",
    "parser.add_argument('--solver', type=str, default=\"euler\", choices=SOLVERS)\n",
    "parser.add_argument('--atol', type=float, default=5e-3)\n",
    "parser.add_argument('--rtol', type=float, default=5e-3)\n",
    "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
    "parser.add_argument('--niters', type=int, default=2000)\n",
    "parser.add_argument('--teacher', type=int, default=1,choices=[0,1])\n",
    "parser.add_argument('--scale', type=int, default=0)\n",
    "parser.add_argument('--days', type=int, default=3)\n",
    "parser.add_argument('--batch_size', type=int, default=3)\n",
    "parser.add_argument('--spectral', type=int, default=0,choices=[0,1])\n",
    "parser.add_argument('--lr', type=float, default=0.0005)\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5)\n",
    "parser.add_argument('--loss_type', type=int, default=0,choices=[0,1])\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd0f723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:46.034376Z",
     "start_time": "2024-10-14T07:36:46.012727Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_time_scale= slice('2006','2016')\n",
    "val_time_scale = slice('2016','2016')\n",
    "test_time_scale = slice('2017','2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acee6282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:57.236443Z",
     "start_time": "2024-10-14T07:36:46.476681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ Data is loading ###########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data 12\n",
      "Length of validation data 12\n",
      "Length of testing data 12\n",
      "['./era5_data/constants/constants_5.625deg.nc']\n"
     ]
    }
   ],
   "source": [
    "paths_to_data = [str(cwd) + '/era5_data/geopotential_500/*.nc',str(cwd) + '/era5_data/temperature_850/*.nc',str(cwd) + '/era5_data/2m_temperature/*.nc',str(cwd) + '/era5_data/10m_u_component_of_wind/*.nc',str(cwd) + '/era5_data/10m_v_component_of_wind/*.nc']\n",
    "const_info_path = ['./era5_data/constants/constants_5.625deg.nc']#/scratch/project_2006852/PDE_climate\n",
    "levels = [\"z\",\"t\",\"t2m\",\"u10\",\"v10\",\"v\",\"u\",\"r\",\"q\"]\n",
    "paths_to_data = paths_to_data[0:5]\n",
    "levels = levels[0:5]\n",
    "assert len(paths_to_data) == len(levels), \"Paths to different type of data must be same as number of types of observations\"\n",
    "print(\"############################ Data is loading ###########################\")\n",
    "Final_train_data = 0\n",
    "Final_val_data = 0\n",
    "Final_test_data = 0\n",
    "max_lev = []\n",
    "min_lev = []\n",
    "for idx,data in enumerate(paths_to_data):\n",
    "    Train_data,Val_data,Test_data,time_steps,lat,lon,mean,std,time_stamp = get_train_test_data_without_scales_batched_monthly(data,train_time_scale,val_time_scale,test_time_scale,levels[idx],args.spectral)  \n",
    "    max_lev.append(mean)\n",
    "    min_lev.append(std)\n",
    "    if idx==0: \n",
    "        Final_train_data = Train_data\n",
    "        Final_val_data = Val_data\n",
    "        Final_test_data = Test_data\n",
    "    else:\n",
    "        Final_train_data = torch.cat([Final_train_data,Train_data],dim=2)\n",
    "        Final_val_data = torch.cat([Final_val_data,Val_data],dim=2)\n",
    "        Final_test_data = torch.cat([Final_test_data,Test_data],dim=2)\n",
    "\n",
    "print(\"Length of training data\",len(Final_train_data))\n",
    "print(\"Length of validation data\",len(Final_val_data))\n",
    "print(\"Length of testing data\",len(Final_test_data))\n",
    "const_channels_info,lat_map,lon_map = add_constant_info(const_info_path)\n",
    "if args.spectral == 1: print(\"############## Running the Model in Spectral Domain ####################\")\n",
    "H,W = Train_data.shape[3],Train_data.shape[4]\n",
    "Train_loader = DataLoader(Final_train_data[2:],batch_size=args.batch_size,shuffle=False,pin_memory=False)\n",
    "Val_loader = DataLoader(Final_val_data[2:],batch_size=args.batch_size,shuffle=False,pin_memory=False)\n",
    "Test_loader = DataLoader(Final_test_data[2:],batch_size=args.batch_size,shuffle=False,pin_memory=False)\n",
    "time_loader = DataLoader(time_steps[2:],batch_size=args.batch_size,shuffle=False,pin_memory=False)\n",
    "time_idx_steps = torch.tensor([i for i in range(12)]).view(-1,1)\n",
    "time_idx = DataLoader(time_idx_steps[2:],batch_size=args.batch_size,shuffle=False,pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfc804",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58c945a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:36:57.259336Z",
     "start_time": "2024-10-14T07:36:57.237997Z"
    }
   },
   "outputs": [],
   "source": [
    "class Climate_encoder_free_uncertain_monthly(nn.Module): \n",
    "    \n",
    "    def __init__(self,num_channels,const_channels,out_types,method,use_att,use_err,use_pos):\n",
    "        super().__init__()\n",
    "        self.layers = [4,3,2]\n",
    "        self.hidden = [128,64,2*out_types]\n",
    "        input_channels = 50 \n",
    "        self.vel_f = Climate_ResNet_2D(input_channels,self.layers,self.hidden)\n",
    "\n",
    "        if use_att: \n",
    "            self.vel_att = Self_attn_conv(input_channels,10)\n",
    "            self.gamma = nn.Parameter(torch.tensor([0.1]))\n",
    "\n",
    "        self.scales = num_channels\n",
    "        self.const_channel = const_channels\n",
    "        \n",
    "        self.out_ch = out_types\n",
    "        self.past_samples = 0\n",
    "        self.const_info = 0\n",
    "        self.lat_map = 0\n",
    "        self.lon_map = 0\n",
    "        self.elev = 0\n",
    "        self.pos_emb = 0\n",
    "        self.elev_info_grad_x = 0\n",
    "        self.elev_info_grad_y = 0\n",
    "        self.method = method\n",
    "        err_in = 29\n",
    "        if use_err: self.noise_net = Climate_ResNet_2D(err_in,[3,2,2],[128,64,2*out_types])\n",
    "        if use_pos: self.pos_enc = Climate_ResNet_2D(4,[2,1,1],[32,16,out_types])\n",
    "        self.att = use_att\n",
    "        self.err = use_err\n",
    "        self.pos = use_pos\n",
    "        self.pos_feat = 0\n",
    "        self.lsm =0 \n",
    "        self.oro =0 \n",
    "\n",
    "\n",
    "    def update_param(self, params):\n",
    "        self.past_samples = params[0]\n",
    "        self.const_info = params[1]\n",
    "        self.lat_map = params[2]\n",
    "        self.lon_map = params[3]\n",
    "\n",
    "    def pde(self,t,vs):\n",
    "\n",
    "        ds = vs[:,-self.out_ch:,:,:].float().view(-1,self.out_ch,vs.shape[2],vs.shape[3]).float()\n",
    "        v = vs[:,:2*self.out_ch,:,:].float().view(-1,2*self.out_ch,vs.shape[2],vs.shape[3]).float()\n",
    "\n",
    "        t_emb = ((t*100)%6).view(1,1,1,1).expand(ds.shape[0],1,ds.shape[2],ds.shape[3])\n",
    "        sin_t_emb = torch.sin(torch.pi*t_emb/(12*6) - torch.pi/2)\n",
    "        cos_t_emb = torch.cos(torch.pi*t_emb/(12*6)- torch.pi/2)\n",
    "        \n",
    "\n",
    "        day_emb = torch.cat([sin_t_emb,cos_t_emb],dim=1)\n",
    "\n",
    "        ds_grad_x = my_gradient(ds,dim=3)[0]\n",
    "        ds_grad_y = my_gradient(ds,dim=2)[0]\n",
    "        nabla_u = torch.cat([ds_grad_x,ds_grad_y],dim=1)\n",
    "\n",
    "        if self.pos:\n",
    "            comb_rep = torch.cat([t_emb/6,day_emb,nabla_u,v,ds,self.pos_feat],dim=1)\n",
    "        else:\n",
    "            cos_lat_map,sin_lat_map = torch.cos(self.new_lat_map),torch.sin(self.new_lat_map)\n",
    "            cos_lon_map,sin_lon_map = torch.cos(self.new_lon_map),torch.sin(self.new_lon_map)\n",
    "            t_cyc_emb = day_emb\n",
    "            pos_feats = torch.cat([cos_lat_map,cos_lon_map,sin_lat_map,sin_lon_map,sin_lat_map*cos_lon_map,sin_lat_map*sin_lon_map],dim=1)\n",
    "            pos_time_ft = self.get_time_pos_embedding(t_cyc_emb,pos_feats)\n",
    "            comb_rep = torch.cat([t_emb/6,day_emb,nabla_u,v,ds,self.new_lat_map,self.new_lon_map,self.lsm,self.oro,pos_feats,pos_time_ft],dim=1)\n",
    "\n",
    "        if self.att: dv = self.vel_f(comb_rep) + self.gamma*self.vel_att(comb_rep)\n",
    "        else: dv = self.vel_f(comb_rep)\n",
    "        v_x = v[:,:self.out_ch,:,:].float().view(-1,self.out_ch,vs.shape[2],vs.shape[3]).float()\n",
    "        v_y = v[:,-self.out_ch:,:,:].float().view(-1,self.out_ch,vs.shape[2],vs.shape[3]).float()\n",
    "\n",
    "        adv1 = v_x*ds_grad_x + v_y*ds_grad_y\n",
    "        adv2 = ds*(my_gradient(v_x,dim=3)[0] + my_gradient(v_y,dim=2)[0] )\n",
    "\n",
    "        ds = adv1 + adv2\n",
    "\n",
    "        dvs = torch.cat([dv,ds],1)\n",
    "        return dvs\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_time_pos_embedding(self,time_feats,pos_feats):\n",
    "        for idx in range(time_feats.shape[1]):\n",
    "            tf = time_feats[:,idx].unsqueeze(dim=1)*pos_feats\n",
    "            if idx == 0:\n",
    "                final_out = tf\n",
    "            else:\n",
    "                final_out = torch.cat([final_out,tf],dim=1)\n",
    "\n",
    "        return final_out\n",
    "\n",
    "    def noise_net_contrib(self,t,pos_enc,s_final,noise_net,H,W):\n",
    "\n",
    "        t_emb = (t%6).view(-1,1,1,1,1)\n",
    "        sin_t_emb = torch.sin(torch.pi*t_emb/(12*6) - torch.pi/2).expand(len(s_final),s_final.shape[1],1,H,W)\n",
    "        cos_t_emb = torch.cos(torch.pi*t_emb/(12*6) - torch.pi/2).expand(len(s_final),s_final.shape[1],1,H,W)\n",
    "        \n",
    "\n",
    "        pos_enc = pos_enc.expand(len(s_final),s_final.shape[1],-1,H,W).flatten(start_dim=0,end_dim=1)\n",
    "        t_cyc_emb = torch.cat([sin_t_emb,cos_t_emb],dim=2).flatten(start_dim=0,end_dim=1)\n",
    "\n",
    "        pos_time_ft = self.get_time_pos_embedding(t_cyc_emb,pos_enc[:,2:-2])\n",
    "\n",
    "        comb_rep = torch.cat([t_cyc_emb,s_final.flatten(start_dim=0,end_dim=1),pos_enc,pos_time_ft],dim=1)\n",
    "        final_out = noise_net(comb_rep).view(len(t),-1,2*self.out_ch,H,W)\n",
    "\n",
    "        mean = s_final + final_out[:,:,:self.out_ch]\n",
    "        std = nn.Softplus()(final_out[:,:,self.out_ch:])\n",
    "        \n",
    "        return mean,std\n",
    "\n",
    "\n",
    "    def forward(self,T,data,atol=0.1,rtol=0.1):\n",
    "        H,W = self.past_samples.shape[2],self.past_samples.shape[3]\n",
    "        final_data = torch.cat([self.past_samples ,data.float().view(-1,self.out_ch,H,W)],1)\n",
    "        init_time = T[0].item()*6\n",
    "        final_time = T[-1].item()*6\n",
    "        steps_val = final_time - init_time\n",
    "        \n",
    "        #breakpoint()\n",
    "\n",
    "        if self.pos:\n",
    "            lat_map = self.lat_map.unsqueeze(dim=0)*torch.pi/180\n",
    "            lon_map = self.lon_map.unsqueeze(dim=0)*torch.pi/180\n",
    "            pos_rep = torch.cat([lat_map.unsqueeze(dim=0),lon_map.unsqueeze(dim=0),self.const_info],dim=1)\n",
    "            self.pos_feat = self.pos_enc(pos_rep).expand(data.shape[0],-1,data.shape[3],data.shape[4])\n",
    "            final_pos_enc = self.pos_feat\n",
    "        \n",
    "        else:\n",
    "            self.oro,self.lsm = self.const_info[0,0],self.const_info[0,1]\n",
    "            self.lsm = self.lsm.unsqueeze(dim=0).expand(data.shape[0],-1,data.shape[3],data.shape[4])\n",
    "            self.oro  = F.normalize(self.const_info[0,0]).unsqueeze(dim=0).expand(data.shape[0],-1,data.shape[3],data.shape[4])\n",
    "            self.new_lat_map = self.lat_map.expand(data.shape[0],1,data.shape[3],data.shape[4])*torch.pi/180 # Converting to radians\n",
    "            self.new_lon_map = self.lon_map.expand(data.shape[0],1,data.shape[3],data.shape[4])*torch.pi/180\n",
    "            cos_lat_map,sin_lat_map = torch.cos(self.new_lat_map),torch.sin(self.new_lat_map)\n",
    "            cos_lon_map,sin_lon_map = torch.cos(self.new_lon_map),torch.sin(self.new_lon_map)\n",
    "            pos_feats = torch.cat([cos_lat_map,cos_lon_map,sin_lat_map,sin_lon_map,sin_lat_map*cos_lon_map,sin_lat_map*sin_lon_map],dim=1)\n",
    "            final_pos_enc = torch.cat([self.new_lat_map,self.new_lon_map,pos_feats,self.lsm,self.oro],dim=1)\n",
    "\n",
    "\n",
    "        new_time_steps = torch.linspace(init_time,final_time,steps=int(steps_val)+1).to(data.device)\n",
    "        t = 0.01*new_time_steps.float().to(data.device).flatten().float()\n",
    "        pde_rhs  = lambda t,vs: self.pde(t,vs) # make the ODE forward function\n",
    "        final_result = odeint(pde_rhs,final_data,t,method=self.method,atol=atol,rtol=rtol)\n",
    "        s_final = final_result[:,:,-self.out_ch:,:,:].view(len(t),-1,self.out_ch,H,W)\n",
    "\n",
    "        if self.err:\n",
    "            mean,std = self.noise_net_contrib(T,final_pos_enc,s_final[0:len(s_final):6],self.noise_net,H,W)\n",
    "\n",
    "        else:\n",
    "            s_final = s_final[0:len(s_final):6]\n",
    "\n",
    "        return mean,std,s_final[0:len(s_final):6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de451ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:09.871695Z",
     "start_time": "2024-10-14T07:37:09.835835Z"
    }
   },
   "outputs": [],
   "source": [
    "class Climate_ResNet_2D(nn.Module): \n",
    "    \n",
    "    def __init__(self,num_channels,layers,hidden_size):\n",
    "        super().__init__()\n",
    "        layers_cnn = []\n",
    "        activation_fns = []\n",
    "        self.block = ResidualBlock\n",
    "        self.inplanes = num_channels\n",
    "\n",
    "        for idx in range(len(layers)):\n",
    "            if idx ==0:\n",
    "                layers_cnn.append(self.make_layer(self.block,num_channels,hidden_size[idx],layers[idx]))\n",
    "            else:\n",
    "                layers_cnn.append(self.make_layer(self.block,hidden_size[idx-1],hidden_size[idx],layers[idx]))\n",
    "        \n",
    "        self.layer_cnn = nn.ModuleList(layers_cnn)\n",
    "        self.activation_cnn = nn.ModuleList(activation_fns)\n",
    "\n",
    "    def make_layer(self,block,in_channels,out_channels,reps):\n",
    "        layers = []\n",
    "        layers.append(block(in_channels,out_channels))\n",
    "        self.inplanes = out_channels\n",
    "        for i in range(1, reps):  \n",
    "              layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,data):\n",
    "        dx_final = data.float()\n",
    "        for l,layer in enumerate(self.layer_cnn):\n",
    "            dx_final = layer(dx_final)\n",
    "\n",
    "                \n",
    "        return dx_final\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "        n_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = nn.LeakyReLU(0.3)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if norm:\n",
    "            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First convolution layer\n",
    "        x_mod = F.pad(F.pad(x,(0,0,1,1),'reflect'),(1,1,0,0),'circular')\n",
    "        h = self.activation(self.bn1(self.conv1(self.norm1(x_mod))))\n",
    "        # Second convolution layer\n",
    "        h = F.pad(F.pad(h,(0,0,1,1),'reflect'),(1,1,0,0),'circular')\n",
    "        h = self.activation(self.bn2(self.conv2(self.norm2(h))))\n",
    "        h = self.drop(h)\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "class Self_attn_conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels,out_channels):\n",
    "        super(Self_attn_conv, self).__init__()\n",
    "        self.query = self._conv(in_channels,in_channels//8,stride=1)\n",
    "        self.key = self.key_conv(in_channels,in_channels//8,stride=2)\n",
    "        self.value = self.key_conv(in_channels,out_channels,stride=2)\n",
    "        self.post_map = nn.Sequential(nn.Conv2d(out_channels,out_channels,kernel_size=(1,1),stride=1,padding=0))\n",
    "        self.out_ch = out_channels\n",
    "\n",
    "    def _conv(self,n_in,n_out,stride):\n",
    "        return nn.Sequential(boundarypad(),nn.Conv2d(n_in,n_in//2,kernel_size=(3,3),stride=stride,padding=0),nn.LeakyReLU(0.3),boundarypad(),nn.Conv2d(n_in//2,n_out,kernel_size=(3,3),stride=stride,padding=0),nn.LeakyReLU(0.3),boundarypad(),nn.Conv2d(n_out,n_out,kernel_size=(3,3),stride=stride,padding=0))\n",
    "    \n",
    "    def key_conv(self,n_in,n_out,stride):\n",
    "        return nn.Sequential(nn.Conv2d(n_in,n_in//2,kernel_size=(3,3),stride=stride,padding=0),nn.LeakyReLU(0.3),nn.Conv2d(n_in//2,n_out,kernel_size=(3,3),stride=stride,padding=0),nn.LeakyReLU(0.3),nn.Conv2d(n_out,n_out,kernel_size=(3,3),stride=1,padding=0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size = x.size()\n",
    "        x = x.float()\n",
    "        q,k,v = self.query(x).flatten(-2,-1),self.key(x).flatten(-2,-1),self.value(x).flatten(-2,-1)\n",
    "        beta = F.softmax(torch.bmm(q.transpose(1,2), k), dim=1)\n",
    "        o = torch.bmm(v, beta.transpose(1,2))\n",
    "        o = self.post_map(o.view(-1,self.out_ch,size[-2],size[-1]).contiguous())\n",
    "        return o\n",
    "\n",
    "class boundarypad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.pad(F.pad(input,(0,0,1,1),'reflect'),(1,1,0,0),'circular')\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_gauss_kernel(shape,lat,lon):\n",
    "    cwd = os.getcwd()\n",
    "    rows,columns  = shape\n",
    "    kernel = torch.zeros(shape[0]*shape[1],shape[0]*shape[1])\n",
    "    pos = []\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            pos.append([lat[i],lon[j]])\n",
    "\n",
    "    for i in range(rows*columns):\n",
    "        for j in range(rows*columns):\n",
    "            dist = torch.sum((torch.tensor(pos[i]) - torch.tensor(pos[j]))**2)\n",
    "            kernel[i][j] = torch.exp(-dist/(2*1*1))\n",
    "\n",
    "    kernel_inv = torch.linalg.inv(kernel).numpy()\n",
    "    np.save(str(cwd) +\"/kernel.npy\",kernel_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84979f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:22.118522Z",
     "start_time": "2024-10-14T07:37:21.377876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ Data is loaded, Fitting the velocity #########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "num_years = len(range(2006,2016))\n",
    "model = Climate_encoder_free_uncertain_monthly(len(paths_to_data),2,out_types=len(paths_to_data),method=args.solver,use_att=True,use_err=True,use_pos=False).to(device)\n",
    "param = count_parameters(model)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.niters)\n",
    "\n",
    "best_loss = float('inf')\n",
    "train_best_loss = float('inf')\n",
    "best_epoch = float('inf')\n",
    "print(\"############################ Data is loaded, Fitting the velocity #########################\")\n",
    "\n",
    "#get_gauss_kernel((32,64),lat,lon)\n",
    "kernel = torch.from_numpy(np.load(str(cwd) +\"/kernel.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cc0ef",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a89d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:23.782796Z",
     "start_time": "2024-10-14T07:37:23.774016Z"
    }
   },
   "outputs": [],
   "source": [
    "class Optim_velocity(nn.Module):\n",
    "    def __init__(self,num_years,H,W):\n",
    "        super(Optim_velocity,self).__init__()\n",
    "        self.v_x = torch.nn.Parameter(torch.randn(num_years,1,5,H,W))\n",
    "        self.v_y = torch.nn.Parameter(torch.randn(num_years,1,5,H,W))\n",
    "\n",
    "    def forward(self,data):\n",
    "        #print('data size: ', data.size())\n",
    "        u_y = my_gradient(data,dim=3)[0] # (H,W) --> (y,x)\n",
    "        u_x = my_gradient(data,dim=4)[0]\n",
    "        #print('self.v_y size: ', self.v_y.size())\n",
    "        adv = self.v_x*u_x + self.v_y*u_y + data*(my_gradient(self.v_y,dim=3)[0] + my_gradient(self.v_x,dim=4)[0])\n",
    "        out = adv\n",
    "        return out,self.v_x,self.v_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d8edb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:33.878298Z",
     "start_time": "2024-10-14T07:37:33.844770Z"
    }
   },
   "outputs": [],
   "source": [
    "def nll(mean,std,truth,lat,var_coeff):\n",
    "    normal_lkl = torch.distributions.normal.Normal(mean, 1e-3 + std)\n",
    "    lkl = - normal_lkl.log_prob(truth)\n",
    "    loss_val = lkl.mean() + var_coeff*(std**2).sum()\n",
    "    #loss_val = torch.mean(lkl,dim=(0,1,3,4))\n",
    "    return loss_val\n",
    "\n",
    "def get_delta_u(u_vel,t_steps):\n",
    "    levels = [\"z\",\"t\",\"t2m\",\"u10\",\"v10\",\"tisr\",\"v\",\"u\",\"r\",\"q\"]\n",
    "    t = t_steps.flatten().float()*6\n",
    "    title = {\"z\":\"Geopotential\",\"v10\": \"v component of wind at 10m\",\"u10\": \"u component of wind at 10m\",\"t2m\": \"Temperature at 2m\",\"t\": \"Temperature at 850hPa pressure\"}\n",
    "    input_u_vel = u_vel.view(u_vel.shape[0],u_vel.shape[1],-1)\n",
    "    coeffs = natural_cubic_spline_coeffs(t, input_u_vel)\n",
    "    spline = NaturalCubicSpline(coeffs)\n",
    "    point = t[-1]\n",
    "    out = spline.derivative(point).view(-1,u_vel.shape[2],u_vel.shape[3],u_vel.shape[4])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def optimize_vel(num,data,delta_u,vel_model,kernel,H,W,steps=200):\n",
    "    model = vel_model(num,H,W)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=2)\n",
    "    best_loss = float('inf')\n",
    "    loss_step = []\n",
    "    out,v_x,v_y = model(data)\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        out,v_x,v_y = model(data)\n",
    "        kernel_v_x = v_x.view(num,5,-1,1)\n",
    "        kernel_v_y = v_y.view(num,5,-1,1)\n",
    "        kernel_expand = kernel.expand(num,5,kernel.shape[0],kernel.shape[1])\n",
    "        v_x_kernel  = torch.matmul(kernel_v_x.transpose(2,3), kernel_expand)\n",
    "        final_x = torch.matmul(v_x_kernel, kernel_v_x).mean()\n",
    "        v_y_kernel  = torch.matmul(kernel_v_y.transpose(2,3), kernel_expand)\n",
    "        final_y = torch.matmul(v_y_kernel, kernel_v_y).mean() \n",
    "        vel_loss = nn.MSELoss()(delta_u,out.squeeze(dim=1)) + 0.0000001*(final_x + final_y)\n",
    "        loss_step.append(vel_loss.item())\n",
    "        if vel_loss.item() < best_loss:\n",
    "            best_loss = vel_loss.item()\n",
    "            final_vx = v_x\n",
    "            final_vy = v_y\n",
    "            final_out = out\n",
    "        vel_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(best_loss)\n",
    "    return final_vx,final_vy,loss_step,final_out\n",
    "\n",
    "\n",
    "\n",
    "def fit_velocity(time_idx,time_loader,Final_train_data,data_loader,device,num_years,paths_to_data,scale,H,W,types,vel_model,kernel,lat,lon):\n",
    "    num =0\n",
    "    cwd = os.getcwd() \n",
    "    for idx_steps,time_steps,batch in zip(time_idx,time_loader,data_loader):\n",
    "        pst = [time_steps[0].item()-i for i in range(3)]\n",
    "        pst.reverse()\n",
    "        pst_idx = [idx_steps[0].item()-i for i in range(3)]\n",
    "        pst_idx.reverse()\n",
    "        past_time = torch.tensor(pst).to(device)\n",
    "        data = batch[0].to(device).view(num_years,1,len(paths_to_data)*(scale+1),H,W)\n",
    "        past_sample = [Final_train_data[j].view(num_years,-1,len(paths_to_data)*(scale+1),H,W) for j in pst_idx]\n",
    "        past_sample = torch.stack(past_sample).view(num_years,3,-1,H,W).to(device)\n",
    "        delta_u = get_delta_u(past_sample,past_time)\n",
    "        v_x,v_y,loss_terms,out = optimize_vel(num_years,data,delta_u,vel_model,kernel,H,W)\n",
    "        final_v = torch.cat([v_x,v_y],dim=1).unsqueeze(dim=0)\n",
    "        if num == 0:\n",
    "            Final_v = final_v\n",
    "        else:\n",
    "            Final_v = torch.cat([Final_v,final_v],dim=0)\n",
    "        num = num+1\n",
    "\n",
    "    if os.path.exists(str(cwd) +\"/\" + types + \"_vel_gradient_modified_pad2.npy\"):\n",
    "        os.remove(str(cwd) +\"/\" + types + \"_vel_gradient_modified_pad2.npy\")\n",
    "\n",
    "    np.save(str(cwd) +\"/\" + types + \"_vel_gradient_modified_pad2.npy\",Final_v.detach().numpy())\n",
    "    \n",
    "def load_velocity(types):\n",
    "    cwd = os.getcwd()\n",
    "    vel = []\n",
    "    for file in types:\n",
    "        vel.append(np.load(str(cwd) + \"/\" + file + \"_vel_gradient_modified_pad2.npy\"))\n",
    "\n",
    "    return (torch.from_numpy(v) for v in vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2b5c81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:38:24.778885Z",
     "start_time": "2024-10-14T07:37:36.516157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010879666660912335\n",
      "0.0001236649986822158\n",
      "0.00012661542859859765\n",
      "0.0001080036599887535\n",
      "0.00011096575326519087\n",
      "0.00012157019227743149\n",
      "0.00013229127216618508\n",
      "0.00011100822302978486\n",
      "0.00011746177915483713\n",
      "0.00012462225276976824\n",
      "0.00012113949924241751\n",
      "0.00011078453826485202\n",
      "############################ Velocity loaded, Model starts to train #########################\n",
      "Climate_encoder_free_uncertain_monthly(\n",
      "  (vel_f): Climate_ResNet_2D(\n",
      "    (layer_cnn): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(50, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(50, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (2): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (3): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (2): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (activation_cnn): ModuleList()\n",
      "  )\n",
      "  (vel_att): Self_attn_conv(\n",
      "    (query): Sequential(\n",
      "      (0): boundarypad()\n",
      "      (1): Conv2d(50, 25, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (2): LeakyReLU(negative_slope=0.3)\n",
      "      (3): boundarypad()\n",
      "      (4): Conv2d(25, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): LeakyReLU(negative_slope=0.3)\n",
      "      (6): boundarypad()\n",
      "      (7): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (key): Sequential(\n",
      "      (0): Conv2d(50, 25, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): LeakyReLU(negative_slope=0.3)\n",
      "      (2): Conv2d(25, 6, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): LeakyReLU(negative_slope=0.3)\n",
      "      (4): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (value): Sequential(\n",
      "      (0): Conv2d(50, 25, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): LeakyReLU(negative_slope=0.3)\n",
      "      (2): Conv2d(25, 10, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): LeakyReLU(negative_slope=0.3)\n",
      "      (4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (post_map): Sequential(\n",
      "      (0): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (noise_net): Climate_ResNet_2D(\n",
      "    (layer_cnn): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(29, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(29, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (2): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (activation_cnn): ModuleList()\n",
      "  )\n",
      ")\n",
      "####################### Total Parameters 2395962 ################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Climate_encoder_free_uncertain_monthly(\n",
       "  (vel_f): Climate_ResNet_2D(\n",
       "    (layer_cnn): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(50, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Conv2d(50, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (2): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (3): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (2): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(64, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (activation_cnn): ModuleList()\n",
       "  )\n",
       "  (vel_att): Self_attn_conv(\n",
       "    (query): Sequential(\n",
       "      (0): boundarypad()\n",
       "      (1): Conv2d(50, 25, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.3)\n",
       "      (3): boundarypad()\n",
       "      (4): Conv2d(25, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.3)\n",
       "      (6): boundarypad()\n",
       "      (7): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (key): Sequential(\n",
       "      (0): Conv2d(50, 25, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.3)\n",
       "      (2): Conv2d(25, 6, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (3): LeakyReLU(negative_slope=0.3)\n",
       "      (4): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (value): Sequential(\n",
       "      (0): Conv2d(50, 25, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.3)\n",
       "      (2): Conv2d(25, 10, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (3): LeakyReLU(negative_slope=0.3)\n",
       "      (4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (post_map): Sequential(\n",
       "      (0): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (noise_net): Climate_ResNet_2D(\n",
       "    (layer_cnn): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(29, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Conv2d(29, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (2): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(64, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (activation): LeakyReLU(negative_slope=0.3)\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (shortcut): Identity()\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (activation_cnn): ModuleList()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_velocity(time_idx,time_loader,Final_train_data,Train_loader,torch.device('cpu'),num_years,paths_to_data,args.scale,H,W,types='train_monthly',vel_model=Optim_velocity,kernel=kernel,lat=lat,lon=lon)\n",
    "fit_velocity(time_idx,time_loader,Final_val_data,Val_loader,torch.device('cpu'),1,paths_to_data,args.scale,H,W,types='val_monthly',vel_model=Optim_velocity,kernel=kernel,lat=lat,lon=lon)\n",
    "fit_velocity(time_idx,time_loader,Final_test_data,Test_loader,torch.device('cpu'),2,paths_to_data,args.scale,H,W,types='test_monthly',vel_model=Optim_velocity,kernel=kernel,lat=lat,lon=lon)\n",
    "\n",
    "vel_train,vel_val = load_velocity(['train_monthly','val_monthly'])\n",
    "print(\"############################ Velocity loaded, Model starts to train #########################\")\n",
    "print(model)\n",
    "print(\"####################### Total Parameters\",param ,\"################################\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8cae096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:38:24.897426Z",
     "start_time": "2024-10-14T07:38:24.780035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(vel_train[0,0,0,0,8,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77db69f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:27:42.117683Z",
     "start_time": "2024-10-14T07:38:24.898378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  702.4989013671875\n",
      "Loss for batch is  747.1881713867188\n",
      "Loss for batch is  599.2195434570312\n",
      "Loss for batch is  140.92962646484375\n",
      "|Iter  0  | Total Train Loss  2189.8362426757812 |\n",
      "Val Loss for batch is  39.056488037109375\n",
      "Val Loss for batch is  38.678863525390625\n",
      "Val Loss for batch is  39.2478141784668\n",
      "Val Loss for batch is  15.922685623168945\n",
      "|Iter  0  | Total Val Loss  132.90585136413574 |\n",
      "Loss for batch is  351.5543518066406\n",
      "Loss for batch is  296.7303466796875\n",
      "Loss for batch is  252.26998901367188\n",
      "Loss for batch is  78.32789611816406\n",
      "|Iter  1  | Total Train Loss  978.8825836181641 |\n",
      "Val Loss for batch is  23.983177185058594\n",
      "Val Loss for batch is  23.85585594177246\n",
      "Val Loss for batch is  24.08226203918457\n",
      "Val Loss for batch is  11.620111465454102\n",
      "|Iter  1  | Total Val Loss  83.54140663146973 |\n",
      "Loss for batch is  195.46849060058594\n",
      "Loss for batch is  173.11239624023438\n",
      "Loss for batch is  152.2373809814453\n",
      "Loss for batch is  52.128055572509766\n",
      "|Iter  2  | Total Train Loss  572.9463233947754 |\n",
      "Val Loss for batch is  19.768600463867188\n",
      "Val Loss for batch is  19.645694732666016\n",
      "Val Loss for batch is  19.408496856689453\n",
      "Val Loss for batch is  13.040021896362305\n",
      "|Iter  2  | Total Val Loss  71.86281394958496 |\n",
      "Loss for batch is  119.11756134033203\n",
      "Loss for batch is  106.95015716552734\n",
      "Loss for batch is  96.59955596923828\n",
      "Loss for batch is  38.889705657958984\n",
      "|Iter  3  | Total Train Loss  361.55698013305664 |\n",
      "Val Loss for batch is  18.01453971862793\n",
      "Val Loss for batch is  17.92843246459961\n",
      "Val Loss for batch is  18.212535858154297\n",
      "Val Loss for batch is  13.992242813110352\n",
      "|Iter  3  | Total Val Loss  68.14775085449219 |\n",
      "Loss for batch is  80.58573913574219\n",
      "Loss for batch is  74.64348602294922\n",
      "Loss for batch is  69.35137176513672\n",
      "Loss for batch is  31.977142333984375\n",
      "|Iter  4  | Total Train Loss  256.5577392578125 |\n",
      "Val Loss for batch is  17.3275146484375\n",
      "Val Loss for batch is  16.981229782104492\n",
      "Val Loss for batch is  16.727462768554688\n",
      "Val Loss for batch is  14.978171348571777\n",
      "|Iter  4  | Total Val Loss  66.01437854766846 |\n",
      "Loss for batch is  60.85205841064453\n",
      "Loss for batch is  57.569454193115234\n",
      "Loss for batch is  54.79739761352539\n",
      "Loss for batch is  29.130659103393555\n",
      "|Iter  5  | Total Train Loss  202.3495693206787 |\n",
      "Val Loss for batch is  18.279573440551758\n",
      "Val Loss for batch is  17.776126861572266\n",
      "Val Loss for batch is  18.02003288269043\n",
      "Val Loss for batch is  16.036657333374023\n",
      "|Iter  5  | Total Val Loss  70.11239051818848 |\n",
      "Loss for batch is  49.85329055786133\n",
      "Loss for batch is  48.24784851074219\n",
      "Loss for batch is  46.68296813964844\n",
      "Loss for batch is  28.01062774658203\n",
      "|Iter  6  | Total Train Loss  172.79473495483398 |\n",
      "Val Loss for batch is  19.109895706176758\n",
      "Val Loss for batch is  18.216663360595703\n",
      "Val Loss for batch is  19.234514236450195\n",
      "Val Loss for batch is  16.330955505371094\n",
      "|Iter  6  | Total Val Loss  72.89202880859375 |\n",
      "Loss for batch is  43.9958610534668\n",
      "Loss for batch is  42.59096908569336\n",
      "Loss for batch is  41.683555603027344\n",
      "Loss for batch is  25.808338165283203\n",
      "|Iter  7  | Total Train Loss  154.0787239074707 |\n",
      "Val Loss for batch is  16.839981079101562\n",
      "Val Loss for batch is  17.384584426879883\n",
      "Val Loss for batch is  17.9062557220459\n",
      "Val Loss for batch is  15.202154159545898\n",
      "|Iter  7  | Total Val Loss  67.33297538757324 |\n",
      "Loss for batch is  38.79262924194336\n",
      "Loss for batch is  37.76355743408203\n",
      "Loss for batch is  36.7513427734375\n",
      "Loss for batch is  22.981081008911133\n",
      "|Iter  8  | Total Train Loss  136.28861045837402 |\n",
      "Val Loss for batch is  14.088665962219238\n",
      "Val Loss for batch is  14.184942245483398\n",
      "Val Loss for batch is  15.251411437988281\n",
      "Val Loss for batch is  13.024619102478027\n",
      "|Iter  8  | Total Val Loss  56.549638748168945 |\n",
      "Loss for batch is  34.59307861328125\n",
      "Loss for batch is  33.77302169799805\n",
      "Loss for batch is  33.09044647216797\n",
      "Loss for batch is  20.442913055419922\n",
      "|Iter  9  | Total Train Loss  121.89945983886719 |\n",
      "Val Loss for batch is  12.397645950317383\n",
      "Val Loss for batch is  12.405305862426758\n",
      "Val Loss for batch is  12.464951515197754\n",
      "Val Loss for batch is  11.21260929107666\n",
      "|Iter  9  | Total Val Loss  48.480512619018555 |\n",
      "Loss for batch is  31.044273376464844\n",
      "Loss for batch is  30.317903518676758\n",
      "Loss for batch is  29.54986572265625\n",
      "Loss for batch is  17.91153907775879\n",
      "|Iter  10  | Total Train Loss  108.82358169555664 |\n",
      "Val Loss for batch is  10.595666885375977\n",
      "Val Loss for batch is  10.327497482299805\n",
      "Val Loss for batch is  10.498716354370117\n",
      "Val Loss for batch is  9.628067970275879\n",
      "|Iter  10  | Total Val Loss  41.04994869232178 |\n",
      "Loss for batch is  28.03946304321289\n",
      "Loss for batch is  27.68728256225586\n",
      "Loss for batch is  27.112491607666016\n",
      "Loss for batch is  16.24313735961914\n",
      "|Iter  11  | Total Train Loss  99.0823745727539 |\n",
      "Val Loss for batch is  8.941507339477539\n",
      "Val Loss for batch is  9.157793045043945\n",
      "Val Loss for batch is  9.084300994873047\n",
      "Val Loss for batch is  8.14250659942627\n",
      "|Iter  11  | Total Val Loss  35.3261079788208 |\n",
      "Loss for batch is  26.091232299804688\n",
      "Loss for batch is  25.604185104370117\n",
      "Loss for batch is  25.221378326416016\n",
      "Loss for batch is  14.990638732910156\n",
      "|Iter  12  | Total Train Loss  91.90743446350098 |\n",
      "Val Loss for batch is  8.64712905883789\n",
      "Val Loss for batch is  8.432454109191895\n",
      "Val Loss for batch is  8.29030704498291\n",
      "Val Loss for batch is  6.820189476013184\n",
      "|Iter  12  | Total Val Loss  32.19007968902588 |\n",
      "Loss for batch is  24.257097244262695\n",
      "Loss for batch is  24.02135467529297\n",
      "Loss for batch is  23.53691864013672\n",
      "Loss for batch is  14.070283889770508\n",
      "|Iter  13  | Total Train Loss  85.88565444946289 |\n",
      "Val Loss for batch is  7.664328575134277\n",
      "Val Loss for batch is  7.398887634277344\n",
      "Val Loss for batch is  7.78965950012207\n",
      "Val Loss for batch is  7.185595989227295\n",
      "|Iter  13  | Total Val Loss  30.038471698760986 |\n",
      "Loss for batch is  22.946121215820312\n",
      "Loss for batch is  22.34966278076172\n",
      "Loss for batch is  22.34049415588379\n",
      "Loss for batch is  13.288908004760742\n",
      "|Iter  14  | Total Train Loss  80.92518615722656 |\n",
      "Val Loss for batch is  7.252536296844482\n",
      "Val Loss for batch is  7.21526575088501\n",
      "Val Loss for batch is  7.337095260620117\n",
      "Val Loss for batch is  6.488375186920166\n",
      "|Iter  14  | Total Val Loss  28.293272495269775 |\n",
      "Loss for batch is  21.47203254699707\n",
      "Loss for batch is  21.209457397460938\n",
      "Loss for batch is  20.942598342895508\n",
      "Loss for batch is  12.944278717041016\n",
      "|Iter  15  | Total Train Loss  76.56836700439453 |\n",
      "Val Loss for batch is  6.868896007537842\n",
      "Val Loss for batch is  6.9134674072265625\n",
      "Val Loss for batch is  7.017361640930176\n",
      "Val Loss for batch is  6.075281620025635\n",
      "|Iter  15  | Total Val Loss  26.875006675720215 |\n",
      "Loss for batch is  20.58061981201172\n",
      "Loss for batch is  20.133468627929688\n",
      "Loss for batch is  19.99909782409668\n",
      "Loss for batch is  12.558065414428711\n",
      "|Iter  16  | Total Train Loss  73.2712516784668 |\n",
      "Val Loss for batch is  6.772009372711182\n",
      "Val Loss for batch is  7.212940216064453\n",
      "Val Loss for batch is  6.73245096206665\n",
      "Val Loss for batch is  6.144827842712402\n",
      "|Iter  16  | Total Val Loss  26.862228393554688 |\n",
      "Loss for batch is  19.594839096069336\n",
      "Loss for batch is  19.438369750976562\n",
      "Loss for batch is  19.231355667114258\n",
      "Loss for batch is  12.027912139892578\n",
      "|Iter  17  | Total Train Loss  70.29247665405273 |\n",
      "Val Loss for batch is  6.360128402709961\n",
      "Val Loss for batch is  6.559998512268066\n",
      "Val Loss for batch is  6.565967559814453\n",
      "Val Loss for batch is  6.037543773651123\n",
      "|Iter  17  | Total Val Loss  25.523638248443604 |\n",
      "Loss for batch is  18.829145431518555\n",
      "Loss for batch is  18.683013916015625\n",
      "Loss for batch is  18.48230743408203\n",
      "Loss for batch is  11.982867240905762\n",
      "|Iter  18  | Total Train Loss  67.97733402252197 |\n",
      "Val Loss for batch is  6.593801975250244\n",
      "Val Loss for batch is  6.481595516204834\n",
      "Val Loss for batch is  6.544734954833984\n",
      "Val Loss for batch is  5.758236885070801\n",
      "|Iter  18  | Total Val Loss  25.378369331359863 |\n",
      "Loss for batch is  18.16764259338379\n",
      "Loss for batch is  18.06245231628418\n",
      "Loss for batch is  18.07379150390625\n",
      "Loss for batch is  11.652130126953125\n",
      "|Iter  19  | Total Train Loss  65.95601654052734 |\n",
      "Val Loss for batch is  6.526185989379883\n",
      "Val Loss for batch is  6.160778045654297\n",
      "Val Loss for batch is  6.214837551116943\n",
      "Val Loss for batch is  4.980807304382324\n",
      "|Iter  19  | Total Val Loss  23.882608890533447 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  17.705585479736328\n",
      "Loss for batch is  17.596820831298828\n",
      "Loss for batch is  17.524003982543945\n",
      "Loss for batch is  11.304322242736816\n",
      "|Iter  20  | Total Train Loss  64.13073253631592 |\n",
      "Val Loss for batch is  6.18039083480835\n",
      "Val Loss for batch is  6.414510726928711\n",
      "Val Loss for batch is  6.496344089508057\n",
      "Val Loss for batch is  5.7970123291015625\n",
      "|Iter  20  | Total Val Loss  24.88825798034668 |\n",
      "Loss for batch is  17.196680068969727\n",
      "Loss for batch is  17.044090270996094\n",
      "Loss for batch is  17.101207733154297\n",
      "Loss for batch is  11.076870918273926\n",
      "|Iter  21  | Total Train Loss  62.41884899139404 |\n",
      "Val Loss for batch is  6.028713226318359\n",
      "Val Loss for batch is  6.020188808441162\n",
      "Val Loss for batch is  5.998263835906982\n",
      "Val Loss for batch is  5.4288482666015625\n",
      "|Iter  21  | Total Val Loss  23.476014137268066 |\n",
      "Loss for batch is  16.859704971313477\n",
      "Loss for batch is  16.757240295410156\n",
      "Loss for batch is  16.724166870117188\n",
      "Loss for batch is  10.941854476928711\n",
      "|Iter  22  | Total Train Loss  61.28296661376953 |\n",
      "Val Loss for batch is  6.153315544128418\n",
      "Val Loss for batch is  5.902349472045898\n",
      "Val Loss for batch is  6.26617431640625\n",
      "Val Loss for batch is  5.667306423187256\n",
      "|Iter  22  | Total Val Loss  23.989145755767822 |\n",
      "Loss for batch is  16.494230270385742\n",
      "Loss for batch is  16.405832290649414\n",
      "Loss for batch is  16.403730392456055\n",
      "Loss for batch is  10.908116340637207\n",
      "|Iter  23  | Total Train Loss  60.21190929412842 |\n",
      "Val Loss for batch is  5.968727111816406\n",
      "Val Loss for batch is  5.900190353393555\n",
      "Val Loss for batch is  5.864302635192871\n",
      "Val Loss for batch is  4.96614933013916\n",
      "|Iter  23  | Total Val Loss  22.699369430541992 |\n",
      "Loss for batch is  16.128822326660156\n",
      "Loss for batch is  15.966764450073242\n",
      "Loss for batch is  16.018840789794922\n",
      "Loss for batch is  10.570255279541016\n",
      "|Iter  24  | Total Train Loss  58.684682846069336 |\n",
      "Val Loss for batch is  6.027037143707275\n",
      "Val Loss for batch is  6.096764087677002\n",
      "Val Loss for batch is  5.843637943267822\n",
      "Val Loss for batch is  5.1908087730407715\n",
      "|Iter  24  | Total Val Loss  23.15824794769287 |\n",
      "Loss for batch is  15.930377960205078\n",
      "Loss for batch is  15.903373718261719\n",
      "Loss for batch is  15.623407363891602\n",
      "Loss for batch is  10.543517112731934\n",
      "|Iter  25  | Total Train Loss  58.00067615509033 |\n",
      "Val Loss for batch is  5.79231071472168\n",
      "Val Loss for batch is  5.526556015014648\n",
      "Val Loss for batch is  5.855583190917969\n",
      "Val Loss for batch is  5.321201324462891\n",
      "|Iter  25  | Total Val Loss  22.495651245117188 |\n",
      "Loss for batch is  15.676946640014648\n",
      "Loss for batch is  15.47589111328125\n",
      "Loss for batch is  15.443607330322266\n",
      "Loss for batch is  10.213332176208496\n",
      "|Iter  26  | Total Train Loss  56.80977725982666 |\n",
      "Val Loss for batch is  5.78988790512085\n",
      "Val Loss for batch is  5.617482662200928\n",
      "Val Loss for batch is  5.736515998840332\n",
      "Val Loss for batch is  5.2440104484558105\n",
      "|Iter  26  | Total Val Loss  22.38789701461792 |\n",
      "Loss for batch is  15.493432998657227\n",
      "Loss for batch is  15.282437324523926\n",
      "Loss for batch is  15.379219055175781\n",
      "Loss for batch is  10.158876419067383\n",
      "|Iter  27  | Total Train Loss  56.313965797424316 |\n",
      "Val Loss for batch is  5.504516124725342\n",
      "Val Loss for batch is  5.50833797454834\n",
      "Val Loss for batch is  5.613400459289551\n",
      "Val Loss for batch is  4.945721626281738\n",
      "|Iter  27  | Total Val Loss  21.57197618484497 |\n",
      "Loss for batch is  15.140482902526855\n",
      "Loss for batch is  15.07298755645752\n",
      "Loss for batch is  15.141708374023438\n",
      "Loss for batch is  9.95026969909668\n",
      "|Iter  28  | Total Train Loss  55.30544853210449 |\n",
      "Val Loss for batch is  5.470030784606934\n",
      "Val Loss for batch is  5.64111852645874\n",
      "Val Loss for batch is  5.381317138671875\n",
      "Val Loss for batch is  4.735729694366455\n",
      "|Iter  28  | Total Val Loss  21.228196144104004 |\n",
      "Loss for batch is  15.009207725524902\n",
      "Loss for batch is  14.808910369873047\n",
      "Loss for batch is  14.860424041748047\n",
      "Loss for batch is  9.960395812988281\n",
      "|Iter  29  | Total Train Loss  54.63893795013428 |\n",
      "Val Loss for batch is  5.318626880645752\n",
      "Val Loss for batch is  5.1794962882995605\n",
      "Val Loss for batch is  5.46579122543335\n",
      "Val Loss for batch is  4.603779315948486\n",
      "|Iter  29  | Total Val Loss  20.56769371032715 |\n",
      "Loss for batch is  14.69692325592041\n",
      "Loss for batch is  14.724028587341309\n",
      "Loss for batch is  14.792685508728027\n",
      "Loss for batch is  9.768473625183105\n",
      "|Iter  30  | Total Train Loss  53.98211097717285 |\n",
      "Val Loss for batch is  5.559088706970215\n",
      "Val Loss for batch is  5.533753395080566\n",
      "Val Loss for batch is  5.3959808349609375\n",
      "Val Loss for batch is  4.169690132141113\n",
      "|Iter  30  | Total Val Loss  20.658513069152832 |\n",
      "Loss for batch is  14.612955093383789\n",
      "Loss for batch is  14.506654739379883\n",
      "Loss for batch is  14.5372953414917\n",
      "Loss for batch is  9.769639015197754\n",
      "|Iter  31  | Total Train Loss  53.426544189453125 |\n",
      "Val Loss for batch is  5.162472248077393\n",
      "Val Loss for batch is  5.09477424621582\n",
      "Val Loss for batch is  5.237013816833496\n",
      "Val Loss for batch is  4.492328643798828\n",
      "|Iter  31  | Total Val Loss  19.986588954925537 |\n",
      "Loss for batch is  14.373621940612793\n",
      "Loss for batch is  14.384651184082031\n",
      "Loss for batch is  14.339313507080078\n",
      "Loss for batch is  9.585344314575195\n",
      "|Iter  32  | Total Train Loss  52.6829309463501 |\n",
      "Val Loss for batch is  4.9060869216918945\n",
      "Val Loss for batch is  5.13308572769165\n",
      "Val Loss for batch is  5.30274772644043\n",
      "Val Loss for batch is  4.955284118652344\n",
      "|Iter  32  | Total Val Loss  20.29720449447632 |\n",
      "Loss for batch is  14.2853422164917\n",
      "Loss for batch is  14.203991889953613\n",
      "Loss for batch is  14.179740905761719\n",
      "Loss for batch is  9.428472518920898\n",
      "|Iter  33  | Total Train Loss  52.09754753112793 |\n",
      "Val Loss for batch is  5.098453521728516\n",
      "Val Loss for batch is  4.946617603302002\n",
      "Val Loss for batch is  5.237758636474609\n",
      "Val Loss for batch is  4.38530969619751\n",
      "|Iter  33  | Total Val Loss  19.668139457702637 |\n",
      "Loss for batch is  14.045473098754883\n",
      "Loss for batch is  14.072559356689453\n",
      "Loss for batch is  13.921841621398926\n",
      "Loss for batch is  9.391733169555664\n",
      "|Iter  34  | Total Train Loss  51.431607246398926 |\n",
      "Val Loss for batch is  5.192926406860352\n",
      "Val Loss for batch is  5.2512922286987305\n",
      "Val Loss for batch is  5.186987400054932\n",
      "Val Loss for batch is  4.457873821258545\n",
      "|Iter  34  | Total Val Loss  20.08907985687256 |\n",
      "Loss for batch is  13.884099960327148\n",
      "Loss for batch is  13.817747116088867\n",
      "Loss for batch is  13.810030937194824\n",
      "Loss for batch is  9.11490535736084\n",
      "|Iter  35  | Total Train Loss  50.62678337097168 |\n",
      "Val Loss for batch is  5.189619541168213\n",
      "Val Loss for batch is  4.992635726928711\n",
      "Val Loss for batch is  5.1133270263671875\n",
      "Val Loss for batch is  4.186973571777344\n",
      "|Iter  35  | Total Val Loss  19.482555866241455 |\n",
      "Loss for batch is  13.796611785888672\n",
      "Loss for batch is  13.625845909118652\n",
      "Loss for batch is  13.728547096252441\n",
      "Loss for batch is  9.196433067321777\n",
      "|Iter  36  | Total Train Loss  50.34743785858154 |\n",
      "Val Loss for batch is  4.661615371704102\n",
      "Val Loss for batch is  4.784487724304199\n",
      "Val Loss for batch is  5.031215667724609\n",
      "Val Loss for batch is  4.320947170257568\n",
      "|Iter  36  | Total Val Loss  18.79826593399048 |\n",
      "Loss for batch is  13.61171817779541\n",
      "Loss for batch is  13.573419570922852\n",
      "Loss for batch is  13.540848731994629\n",
      "Loss for batch is  9.049348831176758\n",
      "|Iter  37  | Total Train Loss  49.77533531188965 |\n",
      "Val Loss for batch is  4.928269386291504\n",
      "Val Loss for batch is  4.684484481811523\n",
      "Val Loss for batch is  4.5577898025512695\n",
      "Val Loss for batch is  4.268491268157959\n",
      "|Iter  37  | Total Val Loss  18.439034938812256 |\n",
      "Loss for batch is  13.571127891540527\n",
      "Loss for batch is  13.364320755004883\n",
      "Loss for batch is  13.403365135192871\n",
      "Loss for batch is  8.926373481750488\n",
      "|Iter  38  | Total Train Loss  49.26518726348877 |\n",
      "Val Loss for batch is  4.607003211975098\n",
      "Val Loss for batch is  4.454014778137207\n",
      "Val Loss for batch is  4.6904425621032715\n",
      "Val Loss for batch is  3.7864043712615967\n",
      "|Iter  38  | Total Val Loss  17.537864923477173 |\n",
      "Loss for batch is  13.336828231811523\n",
      "Loss for batch is  13.258674621582031\n",
      "Loss for batch is  13.28941535949707\n",
      "Loss for batch is  8.927419662475586\n",
      "|Iter  39  | Total Train Loss  48.81233787536621 |\n",
      "Val Loss for batch is  4.764974594116211\n",
      "Val Loss for batch is  4.463498592376709\n",
      "Val Loss for batch is  4.560811996459961\n",
      "Val Loss for batch is  4.326932430267334\n",
      "|Iter  39  | Total Val Loss  18.116217613220215 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  13.315813064575195\n",
      "Loss for batch is  13.074831008911133\n",
      "Loss for batch is  13.194337844848633\n",
      "Loss for batch is  8.758953094482422\n",
      "|Iter  40  | Total Train Loss  48.34393501281738 |\n",
      "Val Loss for batch is  4.462689399719238\n",
      "Val Loss for batch is  4.432924270629883\n",
      "Val Loss for batch is  4.616924285888672\n",
      "Val Loss for batch is  4.38262939453125\n",
      "|Iter  40  | Total Val Loss  17.895167350769043 |\n",
      "Loss for batch is  13.14543342590332\n",
      "Loss for batch is  12.983929634094238\n",
      "Loss for batch is  13.043366432189941\n",
      "Loss for batch is  8.639445304870605\n",
      "|Iter  41  | Total Train Loss  47.812174797058105 |\n",
      "Val Loss for batch is  4.475697040557861\n",
      "Val Loss for batch is  4.393117427825928\n",
      "Val Loss for batch is  4.315075874328613\n",
      "Val Loss for batch is  3.936729907989502\n",
      "|Iter  41  | Total Val Loss  17.120620250701904 |\n",
      "Loss for batch is  12.919729232788086\n",
      "Loss for batch is  12.828116416931152\n",
      "Loss for batch is  12.811442375183105\n",
      "Loss for batch is  8.588340759277344\n",
      "|Iter  42  | Total Train Loss  47.14762878417969 |\n",
      "Val Loss for batch is  4.269853591918945\n",
      "Val Loss for batch is  4.15325927734375\n",
      "Val Loss for batch is  4.796413898468018\n",
      "Val Loss for batch is  3.9031195640563965\n",
      "|Iter  42  | Total Val Loss  17.12264633178711 |\n",
      "Loss for batch is  12.806191444396973\n",
      "Loss for batch is  12.690797805786133\n",
      "Loss for batch is  12.762996673583984\n",
      "Loss for batch is  8.636551856994629\n",
      "|Iter  43  | Total Train Loss  46.89653778076172 |\n",
      "Val Loss for batch is  4.486969470977783\n",
      "Val Loss for batch is  4.512709617614746\n",
      "Val Loss for batch is  4.472931385040283\n",
      "Val Loss for batch is  3.754178285598755\n",
      "|Iter  43  | Total Val Loss  17.226788759231567 |\n",
      "Loss for batch is  12.674692153930664\n",
      "Loss for batch is  12.549996376037598\n",
      "Loss for batch is  12.642745971679688\n",
      "Loss for batch is  8.510936737060547\n",
      "|Iter  44  | Total Train Loss  46.378371238708496 |\n",
      "Val Loss for batch is  4.672033786773682\n",
      "Val Loss for batch is  4.169164657592773\n",
      "Val Loss for batch is  4.411774635314941\n",
      "Val Loss for batch is  3.6053481101989746\n",
      "|Iter  44  | Total Val Loss  16.85832118988037 |\n",
      "Loss for batch is  12.515522956848145\n",
      "Loss for batch is  12.435789108276367\n",
      "Loss for batch is  12.465459823608398\n",
      "Loss for batch is  8.216987609863281\n",
      "|Iter  45  | Total Train Loss  45.63375949859619 |\n",
      "Val Loss for batch is  4.497654914855957\n",
      "Val Loss for batch is  4.131806373596191\n",
      "Val Loss for batch is  4.406599998474121\n",
      "Val Loss for batch is  3.581331253051758\n",
      "|Iter  45  | Total Val Loss  16.617392539978027 |\n",
      "Loss for batch is  12.452588081359863\n",
      "Loss for batch is  12.378437995910645\n",
      "Loss for batch is  12.412829399108887\n",
      "Loss for batch is  8.272736549377441\n",
      "|Iter  46  | Total Train Loss  45.516592025756836 |\n",
      "Val Loss for batch is  4.372890472412109\n",
      "Val Loss for batch is  4.403528213500977\n",
      "Val Loss for batch is  4.327245712280273\n",
      "Val Loss for batch is  3.6573262214660645\n",
      "|Iter  46  | Total Val Loss  16.760990619659424 |\n",
      "Loss for batch is  12.313889503479004\n",
      "Loss for batch is  12.168941497802734\n",
      "Loss for batch is  12.274454116821289\n",
      "Loss for batch is  8.189559936523438\n",
      "|Iter  47  | Total Train Loss  44.946845054626465 |\n",
      "Val Loss for batch is  4.002264976501465\n",
      "Val Loss for batch is  4.1842780113220215\n",
      "Val Loss for batch is  4.212831020355225\n",
      "Val Loss for batch is  3.6420106887817383\n",
      "|Iter  47  | Total Val Loss  16.04138469696045 |\n",
      "Loss for batch is  12.173343658447266\n",
      "Loss for batch is  12.141647338867188\n",
      "Loss for batch is  12.154244422912598\n",
      "Loss for batch is  8.244847297668457\n",
      "|Iter  48  | Total Train Loss  44.71408271789551 |\n",
      "Val Loss for batch is  4.095039367675781\n",
      "Val Loss for batch is  3.9443798065185547\n",
      "Val Loss for batch is  4.267933368682861\n",
      "Val Loss for batch is  3.626288890838623\n",
      "|Iter  48  | Total Val Loss  15.93364143371582 |\n",
      "Loss for batch is  12.077276229858398\n",
      "Loss for batch is  11.944355010986328\n",
      "Loss for batch is  11.976812362670898\n",
      "Loss for batch is  8.116847038269043\n",
      "|Iter  49  | Total Train Loss  44.11529064178467 |\n",
      "Val Loss for batch is  4.157783031463623\n",
      "Val Loss for batch is  3.978348970413208\n",
      "Val Loss for batch is  4.277212142944336\n",
      "Val Loss for batch is  3.321690082550049\n",
      "|Iter  49  | Total Val Loss  15.735034227371216 |\n",
      "Loss for batch is  11.999759674072266\n",
      "Loss for batch is  11.90573501586914\n",
      "Loss for batch is  11.955808639526367\n",
      "Loss for batch is  7.971301078796387\n",
      "|Iter  50  | Total Train Loss  43.83260440826416 |\n",
      "Val Loss for batch is  4.045016765594482\n",
      "Val Loss for batch is  3.960275888442993\n",
      "Val Loss for batch is  4.2265777587890625\n",
      "Val Loss for batch is  3.5884060859680176\n",
      "|Iter  50  | Total Val Loss  15.820276498794556 |\n",
      "Loss for batch is  11.912712097167969\n",
      "Loss for batch is  11.792330741882324\n",
      "Loss for batch is  11.756793975830078\n",
      "Loss for batch is  7.916281223297119\n",
      "|Iter  51  | Total Train Loss  43.37811803817749 |\n",
      "Val Loss for batch is  3.974794864654541\n",
      "Val Loss for batch is  4.131621360778809\n",
      "Val Loss for batch is  4.008059978485107\n",
      "Val Loss for batch is  3.460709571838379\n",
      "|Iter  51  | Total Val Loss  15.575185775756836 |\n",
      "Loss for batch is  11.738024711608887\n",
      "Loss for batch is  11.633329391479492\n",
      "Loss for batch is  11.631475448608398\n",
      "Loss for batch is  7.96484375\n",
      "|Iter  52  | Total Train Loss  42.96767330169678 |\n",
      "Val Loss for batch is  3.775106191635132\n",
      "Val Loss for batch is  3.7798824310302734\n",
      "Val Loss for batch is  4.114152908325195\n",
      "Val Loss for batch is  3.5304853916168213\n",
      "|Iter  52  | Total Val Loss  15.199626922607422 |\n",
      "Loss for batch is  11.685534477233887\n",
      "Loss for batch is  11.565406799316406\n",
      "Loss for batch is  11.667760848999023\n",
      "Loss for batch is  7.740054130554199\n",
      "|Iter  53  | Total Train Loss  42.658756256103516 |\n",
      "Val Loss for batch is  3.838848829269409\n",
      "Val Loss for batch is  3.9522526264190674\n",
      "Val Loss for batch is  4.224444389343262\n",
      "Val Loss for batch is  3.263260841369629\n",
      "|Iter  53  | Total Val Loss  15.278806686401367 |\n",
      "Loss for batch is  11.58704662322998\n",
      "Loss for batch is  11.474514961242676\n",
      "Loss for batch is  11.49356460571289\n",
      "Loss for batch is  7.687867641448975\n",
      "|Iter  54  | Total Train Loss  42.24299383163452 |\n",
      "Val Loss for batch is  3.9447593688964844\n",
      "Val Loss for batch is  3.8666045665740967\n",
      "Val Loss for batch is  4.121109962463379\n",
      "Val Loss for batch is  3.298752546310425\n",
      "|Iter  54  | Total Val Loss  15.231226444244385 |\n",
      "Loss for batch is  11.447433471679688\n",
      "Loss for batch is  11.353099822998047\n",
      "Loss for batch is  11.43515396118164\n",
      "Loss for batch is  7.65109920501709\n",
      "|Iter  55  | Total Train Loss  41.886786460876465 |\n",
      "Val Loss for batch is  3.6392626762390137\n",
      "Val Loss for batch is  3.8586907386779785\n",
      "Val Loss for batch is  3.6546499729156494\n",
      "Val Loss for batch is  3.2258059978485107\n",
      "|Iter  55  | Total Val Loss  14.378409385681152 |\n",
      "Loss for batch is  11.40855598449707\n",
      "Loss for batch is  11.230320930480957\n",
      "Loss for batch is  11.275307655334473\n",
      "Loss for batch is  7.5834550857543945\n",
      "|Iter  56  | Total Train Loss  41.497639656066895 |\n",
      "Val Loss for batch is  3.8605711460113525\n",
      "Val Loss for batch is  3.721897840499878\n",
      "Val Loss for batch is  3.912642478942871\n",
      "Val Loss for batch is  3.1243653297424316\n",
      "|Iter  56  | Total Val Loss  14.619476795196533 |\n",
      "Loss for batch is  11.22718620300293\n",
      "Loss for batch is  11.187541961669922\n",
      "Loss for batch is  11.207174301147461\n",
      "Loss for batch is  7.681839466094971\n",
      "|Iter  57  | Total Train Loss  41.30374193191528 |\n",
      "Val Loss for batch is  3.732675790786743\n",
      "Val Loss for batch is  3.7816927433013916\n",
      "Val Loss for batch is  3.644747257232666\n",
      "Val Loss for batch is  2.9222021102905273\n",
      "|Iter  57  | Total Val Loss  14.081317901611328 |\n",
      "Loss for batch is  11.192580223083496\n",
      "Loss for batch is  10.995743751525879\n",
      "Loss for batch is  11.08868408203125\n",
      "Loss for batch is  7.415143966674805\n",
      "|Iter  58  | Total Train Loss  40.69215202331543 |\n",
      "Val Loss for batch is  3.6231396198272705\n",
      "Val Loss for batch is  3.7066071033477783\n",
      "Val Loss for batch is  3.596911668777466\n",
      "Val Loss for batch is  2.9889297485351562\n",
      "|Iter  58  | Total Val Loss  13.915588140487671 |\n",
      "Loss for batch is  11.060911178588867\n",
      "Loss for batch is  10.910107612609863\n",
      "Loss for batch is  10.975020408630371\n",
      "Loss for batch is  7.383557319641113\n",
      "|Iter  59  | Total Train Loss  40.329596519470215 |\n",
      "Val Loss for batch is  3.780625343322754\n",
      "Val Loss for batch is  3.4828813076019287\n",
      "Val Loss for batch is  3.9266984462738037\n",
      "Val Loss for batch is  3.039062023162842\n",
      "|Iter  59  | Total Val Loss  14.229267120361328 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  10.934136390686035\n",
      "Loss for batch is  10.840278625488281\n",
      "Loss for batch is  10.887079238891602\n",
      "Loss for batch is  7.390399932861328\n",
      "|Iter  60  | Total Train Loss  40.051894187927246 |\n",
      "Val Loss for batch is  3.5042574405670166\n",
      "Val Loss for batch is  3.5870370864868164\n",
      "Val Loss for batch is  3.6513023376464844\n",
      "Val Loss for batch is  2.963245153427124\n",
      "|Iter  60  | Total Val Loss  13.705842018127441 |\n",
      "Loss for batch is  10.87303352355957\n",
      "Loss for batch is  10.756378173828125\n",
      "Loss for batch is  10.832026481628418\n",
      "Loss for batch is  7.2176513671875\n",
      "|Iter  61  | Total Train Loss  39.67908954620361 |\n",
      "Val Loss for batch is  3.5756280422210693\n",
      "Val Loss for batch is  3.5597400665283203\n",
      "Val Loss for batch is  3.5778136253356934\n",
      "Val Loss for batch is  2.89555287361145\n",
      "|Iter  61  | Total Val Loss  13.608734607696533 |\n",
      "Loss for batch is  10.767465591430664\n",
      "Loss for batch is  10.607254028320312\n",
      "Loss for batch is  10.726641654968262\n",
      "Loss for batch is  7.187183380126953\n",
      "|Iter  62  | Total Train Loss  39.28854465484619 |\n",
      "Val Loss for batch is  3.5246734619140625\n",
      "Val Loss for batch is  3.4232797622680664\n",
      "Val Loss for batch is  3.5670719146728516\n",
      "Val Loss for batch is  2.9090805053710938\n",
      "|Iter  62  | Total Val Loss  13.424105644226074 |\n",
      "Loss for batch is  10.600821495056152\n",
      "Loss for batch is  10.516136169433594\n",
      "Loss for batch is  10.668065071105957\n",
      "Loss for batch is  7.141084671020508\n",
      "|Iter  63  | Total Train Loss  38.92610740661621 |\n",
      "Val Loss for batch is  3.5058786869049072\n",
      "Val Loss for batch is  3.2338294982910156\n",
      "Val Loss for batch is  3.387791395187378\n",
      "Val Loss for batch is  2.728283166885376\n",
      "|Iter  63  | Total Val Loss  12.855782747268677 |\n",
      "Loss for batch is  10.57646656036377\n",
      "Loss for batch is  10.466166496276855\n",
      "Loss for batch is  10.505151748657227\n",
      "Loss for batch is  6.985781192779541\n",
      "|Iter  64  | Total Train Loss  38.53356599807739 |\n",
      "Val Loss for batch is  3.3067500591278076\n",
      "Val Loss for batch is  3.2832844257354736\n",
      "Val Loss for batch is  3.4288957118988037\n",
      "Val Loss for batch is  2.823394298553467\n",
      "|Iter  64  | Total Val Loss  12.842324495315552 |\n",
      "Loss for batch is  10.504293441772461\n",
      "Loss for batch is  10.362298011779785\n",
      "Loss for batch is  10.469655990600586\n",
      "Loss for batch is  7.008856296539307\n",
      "|Iter  65  | Total Train Loss  38.34510374069214 |\n",
      "Val Loss for batch is  3.447120189666748\n",
      "Val Loss for batch is  3.249326705932617\n",
      "Val Loss for batch is  3.5480949878692627\n",
      "Val Loss for batch is  2.8548741340637207\n",
      "|Iter  65  | Total Val Loss  13.099416017532349 |\n",
      "Loss for batch is  10.4154691696167\n",
      "Loss for batch is  10.26397705078125\n",
      "Loss for batch is  10.383176803588867\n",
      "Loss for batch is  6.911109447479248\n",
      "|Iter  66  | Total Train Loss  37.973732471466064 |\n",
      "Val Loss for batch is  3.1998484134674072\n",
      "Val Loss for batch is  3.135082960128784\n",
      "Val Loss for batch is  3.3737635612487793\n",
      "Val Loss for batch is  2.9713518619537354\n",
      "|Iter  66  | Total Val Loss  12.680046796798706 |\n",
      "Loss for batch is  10.268889427185059\n",
      "Loss for batch is  10.211641311645508\n",
      "Loss for batch is  10.293798446655273\n",
      "Loss for batch is  6.913893699645996\n",
      "|Iter  67  | Total Train Loss  37.688222885131836 |\n",
      "Val Loss for batch is  3.4221272468566895\n",
      "Val Loss for batch is  3.032097816467285\n",
      "Val Loss for batch is  3.2947683334350586\n",
      "Val Loss for batch is  2.524862289428711\n",
      "|Iter  67  | Total Val Loss  12.273855686187744 |\n",
      "Loss for batch is  10.239457130432129\n",
      "Loss for batch is  10.113091468811035\n",
      "Loss for batch is  10.224260330200195\n",
      "Loss for batch is  6.820713043212891\n",
      "|Iter  68  | Total Train Loss  37.39752197265625 |\n",
      "Val Loss for batch is  3.325082302093506\n",
      "Val Loss for batch is  3.0260376930236816\n",
      "Val Loss for batch is  3.398491144180298\n",
      "Val Loss for batch is  2.6375668048858643\n",
      "|Iter  68  | Total Val Loss  12.38717794418335 |\n",
      "Loss for batch is  10.074906349182129\n",
      "Loss for batch is  10.014023780822754\n",
      "Loss for batch is  10.104660987854004\n",
      "Loss for batch is  6.767309665679932\n",
      "|Iter  69  | Total Train Loss  36.96090078353882 |\n",
      "Val Loss for batch is  3.1666269302368164\n",
      "Val Loss for batch is  3.127488136291504\n",
      "Val Loss for batch is  3.1463606357574463\n",
      "Val Loss for batch is  2.9527840614318848\n",
      "|Iter  69  | Total Val Loss  12.393259763717651 |\n",
      "Loss for batch is  10.029375076293945\n",
      "Loss for batch is  9.960948944091797\n",
      "Loss for batch is  10.02111530303955\n",
      "Loss for batch is  6.628440856933594\n",
      "|Iter  70  | Total Train Loss  36.63988018035889 |\n",
      "Val Loss for batch is  3.347228765487671\n",
      "Val Loss for batch is  3.162198781967163\n",
      "Val Loss for batch is  3.343360424041748\n",
      "Val Loss for batch is  2.6157023906707764\n",
      "|Iter  70  | Total Val Loss  12.468490362167358 |\n",
      "Loss for batch is  9.923816680908203\n",
      "Loss for batch is  9.8199462890625\n",
      "Loss for batch is  9.95146369934082\n",
      "Loss for batch is  6.753274440765381\n",
      "|Iter  71  | Total Train Loss  36.448501110076904 |\n",
      "Val Loss for batch is  3.0767946243286133\n",
      "Val Loss for batch is  2.9435694217681885\n",
      "Val Loss for batch is  3.1194751262664795\n",
      "Val Loss for batch is  2.7326152324676514\n",
      "|Iter  71  | Total Val Loss  11.872454404830933 |\n",
      "Loss for batch is  9.8316011428833\n",
      "Loss for batch is  9.771004676818848\n",
      "Loss for batch is  9.866418838500977\n",
      "Loss for batch is  6.511068344116211\n",
      "|Iter  72  | Total Train Loss  35.980093002319336 |\n",
      "Val Loss for batch is  3.0801258087158203\n",
      "Val Loss for batch is  2.869900703430176\n",
      "Val Loss for batch is  3.2148678302764893\n",
      "Val Loss for batch is  2.5850830078125\n",
      "|Iter  72  | Total Val Loss  11.749977350234985 |\n",
      "Loss for batch is  9.780430793762207\n",
      "Loss for batch is  9.728808403015137\n",
      "Loss for batch is  9.769394874572754\n",
      "Loss for batch is  6.582523822784424\n",
      "|Iter  73  | Total Train Loss  35.86115789413452 |\n",
      "Val Loss for batch is  3.0692861080169678\n",
      "Val Loss for batch is  2.847946882247925\n",
      "Val Loss for batch is  3.2320542335510254\n",
      "Val Loss for batch is  2.3861002922058105\n",
      "|Iter  73  | Total Val Loss  11.535387516021729 |\n",
      "Loss for batch is  9.70814037322998\n",
      "Loss for batch is  9.581864356994629\n",
      "Loss for batch is  9.722583770751953\n",
      "Loss for batch is  6.477861404418945\n",
      "|Iter  74  | Total Train Loss  35.49044990539551 |\n",
      "Val Loss for batch is  2.8535776138305664\n",
      "Val Loss for batch is  2.967683792114258\n",
      "Val Loss for batch is  3.0477678775787354\n",
      "Val Loss for batch is  2.3539645671844482\n",
      "|Iter  74  | Total Val Loss  11.222993850708008 |\n",
      "Loss for batch is  9.646211624145508\n",
      "Loss for batch is  9.49921989440918\n",
      "Loss for batch is  9.58607006072998\n",
      "Loss for batch is  6.454783916473389\n",
      "|Iter  75  | Total Train Loss  35.18628549575806 |\n",
      "Val Loss for batch is  2.8000168800354004\n",
      "Val Loss for batch is  2.90301513671875\n",
      "Val Loss for batch is  3.0436720848083496\n",
      "Val Loss for batch is  2.3911211490631104\n",
      "|Iter  75  | Total Val Loss  11.13782525062561 |\n",
      "Loss for batch is  9.534388542175293\n",
      "Loss for batch is  9.42344856262207\n",
      "Loss for batch is  9.533252716064453\n",
      "Loss for batch is  6.427705764770508\n",
      "|Iter  76  | Total Train Loss  34.918795585632324 |\n",
      "Val Loss for batch is  3.129229784011841\n",
      "Val Loss for batch is  2.9605605602264404\n",
      "Val Loss for batch is  2.924689292907715\n",
      "Val Loss for batch is  2.507338285446167\n",
      "|Iter  76  | Total Val Loss  11.521817922592163 |\n",
      "Loss for batch is  9.50318431854248\n",
      "Loss for batch is  9.353522300720215\n",
      "Loss for batch is  9.478446960449219\n",
      "Loss for batch is  6.196094989776611\n",
      "|Iter  77  | Total Train Loss  34.531248569488525 |\n",
      "Val Loss for batch is  2.8453407287597656\n",
      "Val Loss for batch is  2.7373204231262207\n",
      "Val Loss for batch is  2.8737118244171143\n",
      "Val Loss for batch is  1.996774435043335\n",
      "|Iter  77  | Total Val Loss  10.453147411346436 |\n",
      "Loss for batch is  9.426939964294434\n",
      "Loss for batch is  9.288728713989258\n",
      "Loss for batch is  9.36057186126709\n",
      "Loss for batch is  6.308262825012207\n",
      "|Iter  78  | Total Train Loss  34.38450336456299 |\n",
      "Val Loss for batch is  2.937084913253784\n",
      "Val Loss for batch is  2.833430290222168\n",
      "Val Loss for batch is  2.895866870880127\n",
      "Val Loss for batch is  2.299916982650757\n",
      "|Iter  78  | Total Val Loss  10.966299057006836 |\n",
      "Loss for batch is  9.306541442871094\n",
      "Loss for batch is  9.184697151184082\n",
      "Loss for batch is  9.325569152832031\n",
      "Loss for batch is  6.273894786834717\n",
      "|Iter  79  | Total Train Loss  34.090702533721924 |\n",
      "Val Loss for batch is  2.7252488136291504\n",
      "Val Loss for batch is  2.703932046890259\n",
      "Val Loss for batch is  2.8898532390594482\n",
      "Val Loss for batch is  2.1584410667419434\n",
      "|Iter  79  | Total Val Loss  10.4774751663208 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  9.227691650390625\n",
      "Loss for batch is  9.149624824523926\n",
      "Loss for batch is  9.196882247924805\n",
      "Loss for batch is  6.135437965393066\n",
      "|Iter  80  | Total Train Loss  33.70963668823242 |\n",
      "Val Loss for batch is  2.776137351989746\n",
      "Val Loss for batch is  2.78269362449646\n",
      "Val Loss for batch is  2.802603006362915\n",
      "Val Loss for batch is  2.1493778228759766\n",
      "|Iter  80  | Total Val Loss  10.510811805725098 |\n",
      "Loss for batch is  9.141353607177734\n",
      "Loss for batch is  9.030829429626465\n",
      "Loss for batch is  9.161503791809082\n",
      "Loss for batch is  6.200193405151367\n",
      "|Iter  81  | Total Train Loss  33.53388023376465 |\n",
      "Val Loss for batch is  2.741969585418701\n",
      "Val Loss for batch is  2.451768636703491\n",
      "Val Loss for batch is  2.865966796875\n",
      "Val Loss for batch is  2.026749610900879\n",
      "|Iter  81  | Total Val Loss  10.086454629898071 |\n",
      "Loss for batch is  9.119762420654297\n",
      "Loss for batch is  8.972496032714844\n",
      "Loss for batch is  9.117570877075195\n",
      "Loss for batch is  6.085235595703125\n",
      "|Iter  82  | Total Train Loss  33.29506492614746 |\n",
      "Val Loss for batch is  2.5831079483032227\n",
      "Val Loss for batch is  2.670058250427246\n",
      "Val Loss for batch is  2.7695164680480957\n",
      "Val Loss for batch is  2.026628017425537\n",
      "|Iter  82  | Total Val Loss  10.049310684204102 |\n",
      "Loss for batch is  8.998562812805176\n",
      "Loss for batch is  8.78972053527832\n",
      "Loss for batch is  9.05298137664795\n",
      "Loss for batch is  6.051311492919922\n",
      "|Iter  83  | Total Train Loss  32.89257621765137 |\n",
      "Val Loss for batch is  2.616447925567627\n",
      "Val Loss for batch is  2.6204915046691895\n",
      "Val Loss for batch is  2.736732244491577\n",
      "Val Loss for batch is  2.0026237964630127\n",
      "|Iter  83  | Total Val Loss  9.976295471191406 |\n",
      "Loss for batch is  8.856293678283691\n",
      "Loss for batch is  8.804616928100586\n",
      "Loss for batch is  8.944950103759766\n",
      "Loss for batch is  5.950353622436523\n",
      "|Iter  84  | Total Train Loss  32.556214332580566 |\n",
      "Val Loss for batch is  2.5957679748535156\n",
      "Val Loss for batch is  2.4370627403259277\n",
      "Val Loss for batch is  2.7364628314971924\n",
      "Val Loss for batch is  2.23294734954834\n",
      "|Iter  84  | Total Val Loss  10.002240896224976 |\n",
      "Loss for batch is  8.852180480957031\n",
      "Loss for batch is  8.696925163269043\n",
      "Loss for batch is  8.854435920715332\n",
      "Loss for batch is  5.926626205444336\n",
      "|Iter  85  | Total Train Loss  32.33016777038574 |\n",
      "Val Loss for batch is  2.736321449279785\n",
      "Val Loss for batch is  2.436150550842285\n",
      "Val Loss for batch is  2.636514663696289\n",
      "Val Loss for batch is  2.042328357696533\n",
      "|Iter  85  | Total Val Loss  9.851315021514893 |\n",
      "Loss for batch is  8.770318984985352\n",
      "Loss for batch is  8.650594711303711\n",
      "Loss for batch is  8.717122077941895\n",
      "Loss for batch is  5.777627468109131\n",
      "|Iter  86  | Total Train Loss  31.915663242340088 |\n",
      "Val Loss for batch is  2.649509906768799\n",
      "Val Loss for batch is  2.5911686420440674\n",
      "Val Loss for batch is  2.6495630741119385\n",
      "Val Loss for batch is  1.9975923299789429\n",
      "|Iter  86  | Total Val Loss  9.887833952903748 |\n",
      "Loss for batch is  8.635526657104492\n",
      "Loss for batch is  8.506521224975586\n",
      "Loss for batch is  8.744916915893555\n",
      "Loss for batch is  5.763042449951172\n",
      "|Iter  87  | Total Train Loss  31.650007247924805 |\n",
      "Val Loss for batch is  2.349184036254883\n",
      "Val Loss for batch is  2.6289570331573486\n",
      "Val Loss for batch is  2.6583621501922607\n",
      "Val Loss for batch is  2.036346435546875\n",
      "|Iter  87  | Total Val Loss  9.672849655151367 |\n",
      "Loss for batch is  8.68658447265625\n",
      "Loss for batch is  8.53503704071045\n",
      "Loss for batch is  8.61937141418457\n",
      "Loss for batch is  5.654472827911377\n",
      "|Iter  88  | Total Train Loss  31.495465755462646 |\n",
      "Val Loss for batch is  2.533959150314331\n",
      "Val Loss for batch is  2.5658507347106934\n",
      "Val Loss for batch is  2.5352914333343506\n",
      "Val Loss for batch is  2.0275967121124268\n",
      "|Iter  88  | Total Val Loss  9.662698030471802 |\n",
      "Loss for batch is  8.558097839355469\n",
      "Loss for batch is  8.462268829345703\n",
      "Loss for batch is  8.605523109436035\n",
      "Loss for batch is  5.643628120422363\n",
      "|Iter  89  | Total Train Loss  31.26951789855957 |\n",
      "Val Loss for batch is  2.4599921703338623\n",
      "Val Loss for batch is  2.499281406402588\n",
      "Val Loss for batch is  2.6051104068756104\n",
      "Val Loss for batch is  2.041016101837158\n",
      "|Iter  89  | Total Val Loss  9.605400085449219 |\n",
      "Loss for batch is  8.452864646911621\n",
      "Loss for batch is  8.373397827148438\n",
      "Loss for batch is  8.537210464477539\n",
      "Loss for batch is  5.624378204345703\n",
      "|Iter  90  | Total Train Loss  30.9878511428833 |\n",
      "Val Loss for batch is  2.515702962875366\n",
      "Val Loss for batch is  2.3324122428894043\n",
      "Val Loss for batch is  2.532733678817749\n",
      "Val Loss for batch is  1.8208069801330566\n",
      "|Iter  90  | Total Val Loss  9.201655864715576 |\n",
      "Loss for batch is  8.447027206420898\n",
      "Loss for batch is  8.319114685058594\n",
      "Loss for batch is  8.47271728515625\n",
      "Loss for batch is  5.456049919128418\n",
      "|Iter  91  | Total Train Loss  30.69490909576416 |\n",
      "Val Loss for batch is  2.3572916984558105\n",
      "Val Loss for batch is  2.165215015411377\n",
      "Val Loss for batch is  2.4889869689941406\n",
      "Val Loss for batch is  1.7040772438049316\n",
      "|Iter  91  | Total Val Loss  8.71557092666626 |\n",
      "Loss for batch is  8.425835609436035\n",
      "Loss for batch is  8.248102188110352\n",
      "Loss for batch is  8.376069068908691\n",
      "Loss for batch is  5.5389556884765625\n",
      "|Iter  92  | Total Train Loss  30.58896255493164 |\n",
      "Val Loss for batch is  2.3364481925964355\n",
      "Val Loss for batch is  2.324251174926758\n",
      "Val Loss for batch is  2.4847640991210938\n",
      "Val Loss for batch is  1.874814748764038\n",
      "|Iter  92  | Total Val Loss  9.020278215408325 |\n",
      "Loss for batch is  8.305255889892578\n",
      "Loss for batch is  8.130078315734863\n",
      "Loss for batch is  8.327949523925781\n",
      "Loss for batch is  5.470477104187012\n",
      "|Iter  93  | Total Train Loss  30.233760833740234 |\n",
      "Val Loss for batch is  2.60680890083313\n",
      "Val Loss for batch is  2.2135603427886963\n",
      "Val Loss for batch is  2.371638059616089\n",
      "Val Loss for batch is  1.7035424709320068\n",
      "|Iter  93  | Total Val Loss  8.895549774169922 |\n",
      "Loss for batch is  8.200288772583008\n",
      "Loss for batch is  8.050518989562988\n",
      "Loss for batch is  8.282088279724121\n",
      "Loss for batch is  5.437225341796875\n",
      "|Iter  94  | Total Train Loss  29.970121383666992 |\n",
      "Val Loss for batch is  2.114253520965576\n",
      "Val Loss for batch is  2.319319248199463\n",
      "Val Loss for batch is  2.492539405822754\n",
      "Val Loss for batch is  1.8133165836334229\n",
      "|Iter  94  | Total Val Loss  8.739428758621216 |\n",
      "Loss for batch is  8.081460952758789\n",
      "Loss for batch is  7.985335826873779\n",
      "Loss for batch is  8.156087875366211\n",
      "Loss for batch is  5.288377285003662\n",
      "|Iter  95  | Total Train Loss  29.51126194000244 |\n",
      "Val Loss for batch is  2.2394471168518066\n",
      "Val Loss for batch is  2.1467044353485107\n",
      "Val Loss for batch is  2.428142547607422\n",
      "Val Loss for batch is  1.737839698791504\n",
      "|Iter  95  | Total Val Loss  8.552133798599243 |\n",
      "Loss for batch is  8.096481323242188\n",
      "Loss for batch is  7.951406955718994\n",
      "Loss for batch is  8.106427192687988\n",
      "Loss for batch is  5.275444984436035\n",
      "|Iter  96  | Total Train Loss  29.429760456085205 |\n",
      "Val Loss for batch is  2.2512097358703613\n",
      "Val Loss for batch is  2.262216806411743\n",
      "Val Loss for batch is  2.2788245677948\n",
      "Val Loss for batch is  1.5408179759979248\n",
      "|Iter  96  | Total Val Loss  8.333069086074829 |\n",
      "Loss for batch is  7.999876022338867\n",
      "Loss for batch is  7.901120185852051\n",
      "Loss for batch is  8.018362045288086\n",
      "Loss for batch is  5.273101806640625\n",
      "|Iter  97  | Total Train Loss  29.19246006011963 |\n",
      "Val Loss for batch is  2.2195675373077393\n",
      "Val Loss for batch is  2.0292117595672607\n",
      "Val Loss for batch is  2.31750750541687\n",
      "Val Loss for batch is  1.7896090745925903\n",
      "|Iter  97  | Total Val Loss  8.35589587688446 |\n",
      "Loss for batch is  7.977520942687988\n",
      "Loss for batch is  7.840115547180176\n",
      "Loss for batch is  7.961676597595215\n",
      "Loss for batch is  5.291442394256592\n",
      "|Iter  98  | Total Train Loss  29.07075548171997 |\n",
      "Val Loss for batch is  2.24977707862854\n",
      "Val Loss for batch is  1.9803814888000488\n",
      "Val Loss for batch is  2.2551956176757812\n",
      "Val Loss for batch is  1.5660158395767212\n",
      "|Iter  98  | Total Val Loss  8.051370024681091 |\n",
      "Loss for batch is  7.893019676208496\n",
      "Loss for batch is  7.805784225463867\n",
      "Loss for batch is  7.892159461975098\n",
      "Loss for batch is  5.186147689819336\n",
      "|Iter  99  | Total Train Loss  28.777111053466797 |\n",
      "Val Loss for batch is  2.2839760780334473\n",
      "Val Loss for batch is  2.046604633331299\n",
      "Val Loss for batch is  2.0769948959350586\n",
      "Val Loss for batch is  1.6014609336853027\n",
      "|Iter  99  | Total Val Loss  8.009036540985107 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  7.800950050354004\n",
      "Loss for batch is  7.709844589233398\n",
      "Loss for batch is  7.852319717407227\n",
      "Loss for batch is  5.1302995681762695\n",
      "|Iter  100  | Total Train Loss  28.4934139251709 |\n",
      "Val Loss for batch is  1.9288867712020874\n",
      "Val Loss for batch is  1.952347755432129\n",
      "Val Loss for batch is  2.276841163635254\n",
      "Val Loss for batch is  1.5660676956176758\n",
      "|Iter  100  | Total Val Loss  7.724143385887146 |\n",
      "Loss for batch is  7.78079891204834\n",
      "Loss for batch is  7.592223644256592\n",
      "Loss for batch is  7.818648815155029\n",
      "Loss for batch is  5.073623180389404\n",
      "|Iter  101  | Total Train Loss  28.265294551849365 |\n",
      "Val Loss for batch is  2.0524911880493164\n",
      "Val Loss for batch is  1.8711209297180176\n",
      "Val Loss for batch is  2.0563340187072754\n",
      "Val Loss for batch is  1.6086194515228271\n",
      "|Iter  101  | Total Val Loss  7.5885655879974365 |\n",
      "Loss for batch is  7.736187934875488\n",
      "Loss for batch is  7.5218424797058105\n",
      "Loss for batch is  7.722125053405762\n",
      "Loss for batch is  4.999155521392822\n",
      "|Iter  102  | Total Train Loss  27.979310989379883 |\n",
      "Val Loss for batch is  1.9376612901687622\n",
      "Val Loss for batch is  1.8911209106445312\n",
      "Val Loss for batch is  2.232617139816284\n",
      "Val Loss for batch is  1.7461485862731934\n",
      "|Iter  102  | Total Val Loss  7.807547926902771 |\n",
      "Loss for batch is  7.632668495178223\n",
      "Loss for batch is  7.498833656311035\n",
      "Loss for batch is  7.627922058105469\n",
      "Loss for batch is  5.070162773132324\n",
      "|Iter  103  | Total Train Loss  27.82958698272705 |\n",
      "Val Loss for batch is  2.1350224018096924\n",
      "Val Loss for batch is  1.958561658859253\n",
      "Val Loss for batch is  2.100386381149292\n",
      "Val Loss for batch is  1.3562535047531128\n",
      "|Iter  103  | Total Val Loss  7.55022394657135 |\n",
      "Loss for batch is  7.527729034423828\n",
      "Loss for batch is  7.406943321228027\n",
      "Loss for batch is  7.603786468505859\n",
      "Loss for batch is  4.955189228057861\n",
      "|Iter  104  | Total Train Loss  27.493648052215576 |\n",
      "Val Loss for batch is  1.9798643589019775\n",
      "Val Loss for batch is  1.8494288921356201\n",
      "Val Loss for batch is  1.9948625564575195\n",
      "Val Loss for batch is  1.2796647548675537\n",
      "|Iter  104  | Total Val Loss  7.103820562362671 |\n",
      "Loss for batch is  7.531768798828125\n",
      "Loss for batch is  7.435490608215332\n",
      "Loss for batch is  7.579773426055908\n",
      "Loss for batch is  4.889236927032471\n",
      "|Iter  105  | Total Train Loss  27.436269760131836 |\n",
      "Val Loss for batch is  1.92250394821167\n",
      "Val Loss for batch is  1.8814549446105957\n",
      "Val Loss for batch is  2.0140554904937744\n",
      "Val Loss for batch is  1.3895460367202759\n",
      "|Iter  105  | Total Val Loss  7.207560420036316 |\n",
      "Loss for batch is  7.456430435180664\n",
      "Loss for batch is  7.306820869445801\n",
      "Loss for batch is  7.492790222167969\n",
      "Loss for batch is  4.899021148681641\n",
      "|Iter  106  | Total Train Loss  27.155062675476074 |\n",
      "Val Loss for batch is  1.9583909511566162\n",
      "Val Loss for batch is  1.8563024997711182\n",
      "Val Loss for batch is  2.141094446182251\n",
      "Val Loss for batch is  1.3898158073425293\n",
      "|Iter  106  | Total Val Loss  7.345603704452515 |\n",
      "Loss for batch is  7.389834403991699\n",
      "Loss for batch is  7.226207733154297\n",
      "Loss for batch is  7.425259590148926\n",
      "Loss for batch is  4.851643085479736\n",
      "|Iter  107  | Total Train Loss  26.892944812774658 |\n",
      "Val Loss for batch is  1.8205186128616333\n",
      "Val Loss for batch is  1.8742624521255493\n",
      "Val Loss for batch is  1.9857382774353027\n",
      "Val Loss for batch is  1.3472352027893066\n",
      "|Iter  107  | Total Val Loss  7.027754545211792 |\n",
      "Loss for batch is  7.330263137817383\n",
      "Loss for batch is  7.216592788696289\n",
      "Loss for batch is  7.439870834350586\n",
      "Loss for batch is  4.806791305541992\n",
      "|Iter  108  | Total Train Loss  26.79351806640625 |\n",
      "Val Loss for batch is  1.9095203876495361\n",
      "Val Loss for batch is  1.8008592128753662\n",
      "Val Loss for batch is  2.1148743629455566\n",
      "Val Loss for batch is  1.191245675086975\n",
      "|Iter  108  | Total Val Loss  7.016499638557434 |\n",
      "Loss for batch is  7.29410457611084\n",
      "Loss for batch is  7.115588188171387\n",
      "Loss for batch is  7.368047714233398\n",
      "Loss for batch is  4.801065444946289\n",
      "|Iter  109  | Total Train Loss  26.578805923461914 |\n",
      "Val Loss for batch is  1.962099552154541\n",
      "Val Loss for batch is  1.7054851055145264\n",
      "Val Loss for batch is  1.8670732975006104\n",
      "Val Loss for batch is  1.6734845638275146\n",
      "|Iter  109  | Total Val Loss  7.208142518997192 |\n",
      "Loss for batch is  7.2316155433654785\n",
      "Loss for batch is  7.098494529724121\n",
      "Loss for batch is  7.292295932769775\n",
      "Loss for batch is  4.694195747375488\n",
      "|Iter  110  | Total Train Loss  26.316601753234863 |\n",
      "Val Loss for batch is  1.8007956743240356\n",
      "Val Loss for batch is  1.7645339965820312\n",
      "Val Loss for batch is  2.0490825176239014\n",
      "Val Loss for batch is  1.4173088073730469\n",
      "|Iter  110  | Total Val Loss  7.031720995903015 |\n",
      "Loss for batch is  7.173525810241699\n",
      "Loss for batch is  7.019596576690674\n",
      "Loss for batch is  7.238002777099609\n",
      "Loss for batch is  4.753396987915039\n",
      "|Iter  111  | Total Train Loss  26.18452215194702 |\n",
      "Val Loss for batch is  1.7602925300598145\n",
      "Val Loss for batch is  1.6394602060317993\n",
      "Val Loss for batch is  1.8169012069702148\n",
      "Val Loss for batch is  1.2333067655563354\n",
      "|Iter  111  | Total Val Loss  6.449960708618164 |\n",
      "Loss for batch is  7.130215644836426\n",
      "Loss for batch is  6.957709312438965\n",
      "Loss for batch is  7.134382247924805\n",
      "Loss for batch is  4.663783073425293\n",
      "|Iter  112  | Total Train Loss  25.88609027862549 |\n",
      "Val Loss for batch is  1.8960754871368408\n",
      "Val Loss for batch is  1.6034972667694092\n",
      "Val Loss for batch is  1.8217350244522095\n",
      "Val Loss for batch is  1.2646523714065552\n",
      "|Iter  112  | Total Val Loss  6.585960149765015 |\n",
      "Loss for batch is  7.045882701873779\n",
      "Loss for batch is  6.909489631652832\n",
      "Loss for batch is  7.063987731933594\n",
      "Loss for batch is  4.715788841247559\n",
      "|Iter  113  | Total Train Loss  25.735148906707764 |\n",
      "Val Loss for batch is  1.6664899587631226\n",
      "Val Loss for batch is  1.5540891885757446\n",
      "Val Loss for batch is  1.9951063394546509\n",
      "Val Loss for batch is  1.1382042169570923\n",
      "|Iter  113  | Total Val Loss  6.35388970375061 |\n",
      "Loss for batch is  6.991065502166748\n",
      "Loss for batch is  6.845047473907471\n",
      "Loss for batch is  7.094752788543701\n",
      "Loss for batch is  4.553215980529785\n",
      "|Iter  114  | Total Train Loss  25.484081745147705 |\n",
      "Val Loss for batch is  1.6597261428833008\n",
      "Val Loss for batch is  1.5804864168167114\n",
      "Val Loss for batch is  1.7256577014923096\n",
      "Val Loss for batch is  1.167283058166504\n",
      "|Iter  114  | Total Val Loss  6.133153319358826 |\n",
      "Loss for batch is  6.979863166809082\n",
      "Loss for batch is  6.827902793884277\n",
      "Loss for batch is  6.984317779541016\n",
      "Loss for batch is  4.518634796142578\n",
      "|Iter  115  | Total Train Loss  25.310718536376953 |\n",
      "Val Loss for batch is  1.517613172531128\n",
      "Val Loss for batch is  1.7097474336624146\n",
      "Val Loss for batch is  1.7519166469573975\n",
      "Val Loss for batch is  1.0801935195922852\n",
      "|Iter  115  | Total Val Loss  6.059470772743225 |\n",
      "Loss for batch is  6.881775856018066\n",
      "Loss for batch is  6.658802032470703\n",
      "Loss for batch is  6.9554924964904785\n",
      "Loss for batch is  4.494494438171387\n",
      "|Iter  116  | Total Train Loss  24.990564823150635 |\n",
      "Val Loss for batch is  1.657517671585083\n",
      "Val Loss for batch is  1.4653408527374268\n",
      "Val Loss for batch is  1.5917433500289917\n",
      "Val Loss for batch is  1.14122474193573\n",
      "|Iter  116  | Total Val Loss  5.8558266162872314 |\n",
      "Loss for batch is  6.850644588470459\n",
      "Loss for batch is  6.702301025390625\n",
      "Loss for batch is  6.889464378356934\n",
      "Loss for batch is  4.420604228973389\n",
      "|Iter  117  | Total Train Loss  24.863014221191406 |\n",
      "Val Loss for batch is  1.6278434991836548\n",
      "Val Loss for batch is  1.5900704860687256\n",
      "Val Loss for batch is  1.9984936714172363\n",
      "Val Loss for batch is  0.8865838050842285\n",
      "|Iter  117  | Total Val Loss  6.102991461753845 |\n",
      "Loss for batch is  6.780885696411133\n",
      "Loss for batch is  6.658716201782227\n",
      "Loss for batch is  6.818103790283203\n",
      "Loss for batch is  4.398458003997803\n",
      "|Iter  118  | Total Train Loss  24.656163692474365 |\n",
      "Val Loss for batch is  1.6268543004989624\n",
      "Val Loss for batch is  1.577600121498108\n",
      "Val Loss for batch is  1.7878903150558472\n",
      "Val Loss for batch is  0.8835501670837402\n",
      "|Iter  118  | Total Val Loss  5.875894904136658 |\n",
      "Loss for batch is  6.689241886138916\n",
      "Loss for batch is  6.5230889320373535\n",
      "Loss for batch is  6.8243794441223145\n",
      "Loss for batch is  4.353125095367432\n",
      "|Iter  119  | Total Train Loss  24.389835357666016 |\n",
      "Val Loss for batch is  1.6140201091766357\n",
      "Val Loss for batch is  1.4449219703674316\n",
      "Val Loss for batch is  1.6191558837890625\n",
      "Val Loss for batch is  1.150242805480957\n",
      "|Iter  119  | Total Val Loss  5.828340768814087 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  6.659824371337891\n",
      "Loss for batch is  6.505184650421143\n",
      "Loss for batch is  6.740224838256836\n",
      "Loss for batch is  4.308574199676514\n",
      "|Iter  120  | Total Train Loss  24.213808059692383 |\n",
      "Val Loss for batch is  1.4194225072860718\n",
      "Val Loss for batch is  1.4993973970413208\n",
      "Val Loss for batch is  1.6671326160430908\n",
      "Val Loss for batch is  0.7822785377502441\n",
      "|Iter  120  | Total Val Loss  5.3682310581207275 |\n",
      "Loss for batch is  6.676740646362305\n",
      "Loss for batch is  6.5212602615356445\n",
      "Loss for batch is  6.687427520751953\n",
      "Loss for batch is  4.296574592590332\n",
      "|Iter  121  | Total Train Loss  24.182003021240234 |\n",
      "Val Loss for batch is  1.4472837448120117\n",
      "Val Loss for batch is  1.3686895370483398\n",
      "Val Loss for batch is  1.5620602369308472\n",
      "Val Loss for batch is  1.0037819147109985\n",
      "|Iter  121  | Total Val Loss  5.381815433502197 |\n",
      "Loss for batch is  6.625997543334961\n",
      "Loss for batch is  6.435692310333252\n",
      "Loss for batch is  6.643115043640137\n",
      "Loss for batch is  4.3441362380981445\n",
      "|Iter  122  | Total Train Loss  24.048941135406494 |\n",
      "Val Loss for batch is  1.6188526153564453\n",
      "Val Loss for batch is  1.3408167362213135\n",
      "Val Loss for batch is  1.5138496160507202\n",
      "Val Loss for batch is  1.0201451778411865\n",
      "|Iter  122  | Total Val Loss  5.4936641454696655 |\n",
      "Loss for batch is  6.549134254455566\n",
      "Loss for batch is  6.385371685028076\n",
      "Loss for batch is  6.605888366699219\n",
      "Loss for batch is  4.223067283630371\n",
      "|Iter  123  | Total Train Loss  23.763461589813232 |\n",
      "Val Loss for batch is  1.446331262588501\n",
      "Val Loss for batch is  1.3736516237258911\n",
      "Val Loss for batch is  1.5554050207138062\n",
      "Val Loss for batch is  0.8938270807266235\n",
      "|Iter  123  | Total Val Loss  5.269214987754822 |\n",
      "Loss for batch is  6.48374080657959\n",
      "Loss for batch is  6.31223201751709\n",
      "Loss for batch is  6.538936614990234\n",
      "Loss for batch is  4.176788330078125\n",
      "|Iter  124  | Total Train Loss  23.51169776916504 |\n",
      "Val Loss for batch is  1.4676398038864136\n",
      "Val Loss for batch is  1.544467806816101\n",
      "Val Loss for batch is  1.6352993249893188\n",
      "Val Loss for batch is  1.0357918739318848\n",
      "|Iter  124  | Total Val Loss  5.683198809623718 |\n",
      "Loss for batch is  6.48057746887207\n",
      "Loss for batch is  6.268204689025879\n",
      "Loss for batch is  6.512809753417969\n",
      "Loss for batch is  4.080181121826172\n",
      "|Iter  125  | Total Train Loss  23.34177303314209 |\n",
      "Val Loss for batch is  1.328542947769165\n",
      "Val Loss for batch is  1.309657096862793\n",
      "Val Loss for batch is  1.5336400270462036\n",
      "Val Loss for batch is  1.0228163003921509\n",
      "|Iter  125  | Total Val Loss  5.1946563720703125 |\n",
      "Loss for batch is  6.428436279296875\n",
      "Loss for batch is  6.245804786682129\n",
      "Loss for batch is  6.507159233093262\n",
      "Loss for batch is  4.052994728088379\n",
      "|Iter  126  | Total Train Loss  23.234395027160645 |\n",
      "Val Loss for batch is  1.4821704626083374\n",
      "Val Loss for batch is  1.3686449527740479\n",
      "Val Loss for batch is  1.6036838293075562\n",
      "Val Loss for batch is  0.959287166595459\n",
      "|Iter  126  | Total Val Loss  5.4137864112854 |\n",
      "Loss for batch is  6.29482364654541\n",
      "Loss for batch is  6.193168640136719\n",
      "Loss for batch is  6.354869842529297\n",
      "Loss for batch is  4.125652313232422\n",
      "|Iter  127  | Total Train Loss  22.968514442443848 |\n",
      "Val Loss for batch is  1.3857730627059937\n",
      "Val Loss for batch is  1.3398805856704712\n",
      "Val Loss for batch is  1.4303176403045654\n",
      "Val Loss for batch is  0.7800547480583191\n",
      "|Iter  127  | Total Val Loss  4.936026036739349 |\n",
      "Loss for batch is  6.282094955444336\n",
      "Loss for batch is  6.137733459472656\n",
      "Loss for batch is  6.419537544250488\n",
      "Loss for batch is  4.077241897583008\n",
      "|Iter  128  | Total Train Loss  22.91660785675049 |\n",
      "Val Loss for batch is  1.409811019897461\n",
      "Val Loss for batch is  1.1655776500701904\n",
      "Val Loss for batch is  1.6658135652542114\n",
      "Val Loss for batch is  0.9661034345626831\n",
      "|Iter  128  | Total Val Loss  5.207305669784546 |\n",
      "Loss for batch is  6.286919593811035\n",
      "Loss for batch is  6.107629776000977\n",
      "Loss for batch is  6.307012557983398\n",
      "Loss for batch is  3.9330215454101562\n",
      "|Iter  129  | Total Train Loss  22.634583473205566 |\n",
      "Val Loss for batch is  1.3819706439971924\n",
      "Val Loss for batch is  1.2232756614685059\n",
      "Val Loss for batch is  1.6366952657699585\n",
      "Val Loss for batch is  0.7896106243133545\n",
      "|Iter  129  | Total Val Loss  5.031552195549011 |\n",
      "Loss for batch is  6.249059200286865\n",
      "Loss for batch is  6.045229911804199\n",
      "Loss for batch is  6.336282253265381\n",
      "Loss for batch is  4.02459192276001\n",
      "|Iter  130  | Total Train Loss  22.655163288116455 |\n",
      "Val Loss for batch is  1.350249171257019\n",
      "Val Loss for batch is  1.1777563095092773\n",
      "Val Loss for batch is  1.4206589460372925\n",
      "Val Loss for batch is  0.6438975930213928\n",
      "|Iter  130  | Total Val Loss  4.592562019824982 |\n",
      "Loss for batch is  6.201920509338379\n",
      "Loss for batch is  5.964200496673584\n",
      "Loss for batch is  6.220790863037109\n",
      "Loss for batch is  3.9670190811157227\n",
      "|Iter  131  | Total Train Loss  22.353930950164795 |\n",
      "Val Loss for batch is  1.3458013534545898\n",
      "Val Loss for batch is  1.3767316341400146\n",
      "Val Loss for batch is  1.50717031955719\n",
      "Val Loss for batch is  0.7879284620285034\n",
      "|Iter  131  | Total Val Loss  5.017631769180298 |\n",
      "Loss for batch is  6.13893985748291\n",
      "Loss for batch is  5.946877479553223\n",
      "Loss for batch is  6.195611953735352\n",
      "Loss for batch is  3.892418384552002\n",
      "|Iter  132  | Total Train Loss  22.173847675323486 |\n",
      "Val Loss for batch is  1.2720537185668945\n",
      "Val Loss for batch is  1.1477415561676025\n",
      "Val Loss for batch is  1.4258575439453125\n",
      "Val Loss for batch is  0.7454068064689636\n",
      "|Iter  132  | Total Val Loss  4.591059625148773 |\n",
      "Loss for batch is  6.013123512268066\n",
      "Loss for batch is  5.866521835327148\n",
      "Loss for batch is  6.158092498779297\n",
      "Loss for batch is  3.9271960258483887\n",
      "|Iter  133  | Total Train Loss  21.9649338722229 |\n",
      "Val Loss for batch is  1.2150602340698242\n",
      "Val Loss for batch is  1.2340271472930908\n",
      "Val Loss for batch is  1.458809494972229\n",
      "Val Loss for batch is  0.7728503346443176\n",
      "|Iter  133  | Total Val Loss  4.680747210979462 |\n",
      "Loss for batch is  6.073223114013672\n",
      "Loss for batch is  5.848384380340576\n",
      "Loss for batch is  6.151479721069336\n",
      "Loss for batch is  3.8656630516052246\n",
      "|Iter  134  | Total Train Loss  21.93875026702881 |\n",
      "Val Loss for batch is  1.3719855546951294\n",
      "Val Loss for batch is  1.1130400896072388\n",
      "Val Loss for batch is  1.300775170326233\n",
      "Val Loss for batch is  0.7436322569847107\n",
      "|Iter  134  | Total Val Loss  4.529433071613312 |\n",
      "Loss for batch is  5.980683326721191\n",
      "Loss for batch is  5.8397722244262695\n",
      "Loss for batch is  6.0385637283325195\n",
      "Loss for batch is  3.832716941833496\n",
      "|Iter  135  | Total Train Loss  21.691736221313477 |\n",
      "Val Loss for batch is  1.175944209098816\n",
      "Val Loss for batch is  1.008886694908142\n",
      "Val Loss for batch is  1.3846311569213867\n",
      "Val Loss for batch is  0.5430991053581238\n",
      "|Iter  135  | Total Val Loss  4.1125611662864685 |\n",
      "Loss for batch is  5.9534196853637695\n",
      "Loss for batch is  5.794094562530518\n",
      "Loss for batch is  6.029264450073242\n",
      "Loss for batch is  3.83435320854187\n",
      "|Iter  136  | Total Train Loss  21.6111319065094 |\n",
      "Val Loss for batch is  1.057673692703247\n",
      "Val Loss for batch is  1.0080711841583252\n",
      "Val Loss for batch is  1.4395710229873657\n",
      "Val Loss for batch is  0.6606168150901794\n",
      "|Iter  136  | Total Val Loss  4.165932714939117 |\n",
      "Loss for batch is  5.91220235824585\n",
      "Loss for batch is  5.70322322845459\n",
      "Loss for batch is  6.005884170532227\n",
      "Loss for batch is  3.7505505084991455\n",
      "|Iter  137  | Total Train Loss  21.37186026573181 |\n",
      "Val Loss for batch is  1.1287833452224731\n",
      "Val Loss for batch is  1.0544893741607666\n",
      "Val Loss for batch is  1.3392207622528076\n",
      "Val Loss for batch is  0.5921565890312195\n",
      "|Iter  137  | Total Val Loss  4.114650070667267 |\n",
      "Loss for batch is  5.869263648986816\n",
      "Loss for batch is  5.717354774475098\n",
      "Loss for batch is  5.928234100341797\n",
      "Loss for batch is  3.690279960632324\n",
      "|Iter  138  | Total Train Loss  21.205132484436035 |\n",
      "Val Loss for batch is  1.2627577781677246\n",
      "Val Loss for batch is  1.0584721565246582\n",
      "Val Loss for batch is  1.2601391077041626\n",
      "Val Loss for batch is  0.43514207005500793\n",
      "|Iter  138  | Total Val Loss  4.016511112451553 |\n",
      "Loss for batch is  5.848920822143555\n",
      "Loss for batch is  5.601637840270996\n",
      "Loss for batch is  5.935441017150879\n",
      "Loss for batch is  3.6979422569274902\n",
      "|Iter  139  | Total Train Loss  21.08394193649292 |\n",
      "Val Loss for batch is  1.1929854154586792\n",
      "Val Loss for batch is  0.9071965217590332\n",
      "Val Loss for batch is  1.2687242031097412\n",
      "Val Loss for batch is  0.5910459160804749\n",
      "|Iter  139  | Total Val Loss  3.9599520564079285 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  5.801176071166992\n",
      "Loss for batch is  5.584389686584473\n",
      "Loss for batch is  5.866555690765381\n",
      "Loss for batch is  3.612152099609375\n",
      "|Iter  140  | Total Train Loss  20.86427354812622 |\n",
      "Val Loss for batch is  1.3204067945480347\n",
      "Val Loss for batch is  0.9322136640548706\n",
      "Val Loss for batch is  1.2704015970230103\n",
      "Val Loss for batch is  0.6859095692634583\n",
      "|Iter  140  | Total Val Loss  4.208931624889374 |\n",
      "Loss for batch is  5.760184288024902\n",
      "Loss for batch is  5.533174991607666\n",
      "Loss for batch is  5.830362319946289\n",
      "Loss for batch is  3.7247872352600098\n",
      "|Iter  141  | Total Train Loss  20.848508834838867 |\n",
      "Val Loss for batch is  1.3420485258102417\n",
      "Val Loss for batch is  0.9012719392776489\n",
      "Val Loss for batch is  1.1302757263183594\n",
      "Val Loss for batch is  0.5700554847717285\n",
      "|Iter  141  | Total Val Loss  3.9436516761779785 |\n",
      "Loss for batch is  5.749566078186035\n",
      "Loss for batch is  5.539388656616211\n",
      "Loss for batch is  5.755152702331543\n",
      "Loss for batch is  3.58302640914917\n",
      "|Iter  142  | Total Train Loss  20.62713384628296 |\n",
      "Val Loss for batch is  1.2694947719573975\n",
      "Val Loss for batch is  1.0016692876815796\n",
      "Val Loss for batch is  1.1584608554840088\n",
      "Val Loss for batch is  0.37757620215415955\n",
      "|Iter  142  | Total Val Loss  3.8072011172771454 |\n",
      "Loss for batch is  5.683866500854492\n",
      "Loss for batch is  5.4653120040893555\n",
      "Loss for batch is  5.756599426269531\n",
      "Loss for batch is  3.608473300933838\n",
      "|Iter  143  | Total Train Loss  20.514251232147217 |\n",
      "Val Loss for batch is  1.0670595169067383\n",
      "Val Loss for batch is  0.8905233144760132\n",
      "Val Loss for batch is  1.2066128253936768\n",
      "Val Loss for batch is  0.3942323625087738\n",
      "|Iter  143  | Total Val Loss  3.558428019285202 |\n",
      "Loss for batch is  5.599120140075684\n",
      "Loss for batch is  5.465760231018066\n",
      "Loss for batch is  5.734071731567383\n",
      "Loss for batch is  3.476685047149658\n",
      "|Iter  144  | Total Train Loss  20.27563714981079 |\n",
      "Val Loss for batch is  1.0550193786621094\n",
      "Val Loss for batch is  0.9961873888969421\n",
      "Val Loss for batch is  1.176612377166748\n",
      "Val Loss for batch is  0.4526773691177368\n",
      "|Iter  144  | Total Val Loss  3.6804965138435364 |\n",
      "Loss for batch is  5.550567150115967\n",
      "Loss for batch is  5.388334274291992\n",
      "Loss for batch is  5.677679061889648\n",
      "Loss for batch is  3.5176382064819336\n",
      "|Iter  145  | Total Train Loss  20.13421869277954 |\n",
      "Val Loss for batch is  0.9818713665008545\n",
      "Val Loss for batch is  0.8523181676864624\n",
      "Val Loss for batch is  1.1387330293655396\n",
      "Val Loss for batch is  0.4517707824707031\n",
      "|Iter  145  | Total Val Loss  3.4246933460235596 |\n",
      "Loss for batch is  5.586124897003174\n",
      "Loss for batch is  5.36796760559082\n",
      "Loss for batch is  5.6055097579956055\n",
      "Loss for batch is  3.4502463340759277\n",
      "|Iter  146  | Total Train Loss  20.009848594665527 |\n",
      "Val Loss for batch is  1.0259654521942139\n",
      "Val Loss for batch is  0.8618342280387878\n",
      "Val Loss for batch is  1.0414817333221436\n",
      "Val Loss for batch is  0.6416719555854797\n",
      "|Iter  146  | Total Val Loss  3.570953369140625 |\n",
      "Loss for batch is  5.498660087585449\n",
      "Loss for batch is  5.27159309387207\n",
      "Loss for batch is  5.564252853393555\n",
      "Loss for batch is  3.5289628505706787\n",
      "|Iter  147  | Total Train Loss  19.863468885421753 |\n",
      "Val Loss for batch is  0.9652758240699768\n",
      "Val Loss for batch is  0.8943306803703308\n",
      "Val Loss for batch is  1.1590484380722046\n",
      "Val Loss for batch is  0.43395179510116577\n",
      "|Iter  147  | Total Val Loss  3.452606737613678 |\n",
      "Loss for batch is  5.498898983001709\n",
      "Loss for batch is  5.27577018737793\n",
      "Loss for batch is  5.504232406616211\n",
      "Loss for batch is  3.438969135284424\n",
      "|Iter  148  | Total Train Loss  19.717870712280273 |\n",
      "Val Loss for batch is  0.921587347984314\n",
      "Val Loss for batch is  0.7688175439834595\n",
      "Val Loss for batch is  1.137056589126587\n",
      "Val Loss for batch is  0.45725923776626587\n",
      "|Iter  148  | Total Val Loss  3.284720718860626 |\n",
      "Loss for batch is  5.500432014465332\n",
      "Loss for batch is  5.302061080932617\n",
      "Loss for batch is  5.5106635093688965\n",
      "Loss for batch is  3.370574474334717\n",
      "|Iter  149  | Total Train Loss  19.683731079101562 |\n",
      "Val Loss for batch is  0.9232602119445801\n",
      "Val Loss for batch is  0.9773375391960144\n",
      "Val Loss for batch is  1.1964137554168701\n",
      "Val Loss for batch is  0.522455096244812\n",
      "|Iter  149  | Total Val Loss  3.6194666028022766 |\n",
      "Loss for batch is  5.349333763122559\n",
      "Loss for batch is  5.179830551147461\n",
      "Loss for batch is  5.459547519683838\n",
      "Loss for batch is  3.4135873317718506\n",
      "|Iter  150  | Total Train Loss  19.402299165725708 |\n",
      "Val Loss for batch is  0.9360127449035645\n",
      "Val Loss for batch is  0.7233870029449463\n",
      "Val Loss for batch is  1.0618833303451538\n",
      "Val Loss for batch is  0.3324439525604248\n",
      "|Iter  150  | Total Val Loss  3.0537270307540894 |\n",
      "Loss for batch is  5.3662109375\n",
      "Loss for batch is  5.2064714431762695\n",
      "Loss for batch is  5.41195011138916\n",
      "Loss for batch is  3.3359198570251465\n",
      "|Iter  151  | Total Train Loss  19.320552349090576 |\n",
      "Val Loss for batch is  0.8745000958442688\n",
      "Val Loss for batch is  0.7803683280944824\n",
      "Val Loss for batch is  1.1050714254379272\n",
      "Val Loss for batch is  0.3550031781196594\n",
      "|Iter  151  | Total Val Loss  3.114943027496338 |\n",
      "Loss for batch is  5.367476940155029\n",
      "Loss for batch is  5.201831817626953\n",
      "Loss for batch is  5.404852867126465\n",
      "Loss for batch is  3.2504138946533203\n",
      "|Iter  152  | Total Train Loss  19.224575519561768 |\n",
      "Val Loss for batch is  0.9102204442024231\n",
      "Val Loss for batch is  0.6102819442749023\n",
      "Val Loss for batch is  1.0177786350250244\n",
      "Val Loss for batch is  0.23701724410057068\n",
      "|Iter  152  | Total Val Loss  2.7752982676029205 |\n",
      "Loss for batch is  5.321617603302002\n",
      "Loss for batch is  5.091392517089844\n",
      "Loss for batch is  5.3277587890625\n",
      "Loss for batch is  3.3733274936676025\n",
      "|Iter  153  | Total Train Loss  19.11409640312195 |\n",
      "Val Loss for batch is  0.9379566311836243\n",
      "Val Loss for batch is  0.7014965415000916\n",
      "Val Loss for batch is  1.0809175968170166\n",
      "Val Loss for batch is  0.3857649862766266\n",
      "|Iter  153  | Total Val Loss  3.106135755777359 |\n",
      "Loss for batch is  5.238386154174805\n",
      "Loss for batch is  5.04566764831543\n",
      "Loss for batch is  5.341536521911621\n",
      "Loss for batch is  3.3279054164886475\n",
      "|Iter  154  | Total Train Loss  18.953495740890503 |\n",
      "Val Loss for batch is  0.8690983653068542\n",
      "Val Loss for batch is  0.709359347820282\n",
      "Val Loss for batch is  0.9716756343841553\n",
      "Val Loss for batch is  0.14084023237228394\n",
      "|Iter  154  | Total Val Loss  2.6909735798835754 |\n",
      "Loss for batch is  5.247117042541504\n",
      "Loss for batch is  5.037206649780273\n",
      "Loss for batch is  5.330894470214844\n",
      "Loss for batch is  3.2358322143554688\n",
      "|Iter  155  | Total Train Loss  18.85105037689209 |\n",
      "Val Loss for batch is  0.9371416568756104\n",
      "Val Loss for batch is  0.6788074374198914\n",
      "Val Loss for batch is  0.9517216086387634\n",
      "Val Loss for batch is  0.5025981068611145\n",
      "|Iter  155  | Total Val Loss  3.0702688097953796 |\n",
      "Loss for batch is  5.228051662445068\n",
      "Loss for batch is  4.982921600341797\n",
      "Loss for batch is  5.219964027404785\n",
      "Loss for batch is  3.2443864345550537\n",
      "|Iter  156  | Total Train Loss  18.675323724746704 |\n",
      "Val Loss for batch is  1.0552778244018555\n",
      "Val Loss for batch is  0.6337087154388428\n",
      "Val Loss for batch is  0.9319116473197937\n",
      "Val Loss for batch is  0.21265432238578796\n",
      "|Iter  156  | Total Val Loss  2.83355250954628 |\n",
      "Loss for batch is  5.203103542327881\n",
      "Loss for batch is  4.940826416015625\n",
      "Loss for batch is  5.252643585205078\n",
      "Loss for batch is  3.185164451599121\n",
      "|Iter  157  | Total Train Loss  18.581737995147705 |\n",
      "Val Loss for batch is  0.9919471144676208\n",
      "Val Loss for batch is  0.6673691272735596\n",
      "Val Loss for batch is  0.901990532875061\n",
      "Val Loss for batch is  0.18189530074596405\n",
      "|Iter  157  | Total Val Loss  2.7432020753622055 |\n",
      "Loss for batch is  5.1611199378967285\n",
      "Loss for batch is  4.927529335021973\n",
      "Loss for batch is  5.1694111824035645\n",
      "Loss for batch is  3.1228365898132324\n",
      "|Iter  158  | Total Train Loss  18.380897045135498 |\n",
      "Val Loss for batch is  0.8548038005828857\n",
      "Val Loss for batch is  0.6712646484375\n",
      "Val Loss for batch is  1.1172451972961426\n",
      "Val Loss for batch is  0.21058663725852966\n",
      "|Iter  158  | Total Val Loss  2.853900283575058 |\n",
      "Loss for batch is  5.123455047607422\n",
      "Loss for batch is  4.890803337097168\n",
      "Loss for batch is  5.185940742492676\n",
      "Loss for batch is  3.1825778484344482\n",
      "|Iter  159  | Total Train Loss  18.382776975631714 |\n",
      "Val Loss for batch is  0.732118546962738\n",
      "Val Loss for batch is  0.6384826898574829\n",
      "Val Loss for batch is  0.9255678653717041\n",
      "Val Loss for batch is  0.07902240753173828\n",
      "|Iter  159  | Total Val Loss  2.3751915097236633 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  5.063943386077881\n",
      "Loss for batch is  4.822944164276123\n",
      "Loss for batch is  5.161386966705322\n",
      "Loss for batch is  3.172696590423584\n",
      "|Iter  160  | Total Train Loss  18.22097110748291 |\n",
      "Val Loss for batch is  0.8204108476638794\n",
      "Val Loss for batch is  0.749550461769104\n",
      "Val Loss for batch is  0.7758907675743103\n",
      "Val Loss for batch is  0.13038203120231628\n",
      "|Iter  160  | Total Val Loss  2.47623410820961 |\n",
      "Loss for batch is  5.057182312011719\n",
      "Loss for batch is  4.827999114990234\n",
      "Loss for batch is  5.064977645874023\n",
      "Loss for batch is  3.0735068321228027\n",
      "|Iter  161  | Total Train Loss  18.02366590499878 |\n",
      "Val Loss for batch is  0.853176474571228\n",
      "Val Loss for batch is  0.8647613525390625\n",
      "Val Loss for batch is  1.0853700637817383\n",
      "Val Loss for batch is  0.23463746905326843\n",
      "|Iter  161  | Total Val Loss  3.0379453599452972 |\n",
      "Loss for batch is  4.9672112464904785\n",
      "Loss for batch is  4.784194469451904\n",
      "Loss for batch is  5.095773696899414\n",
      "Loss for batch is  3.066650390625\n",
      "|Iter  162  | Total Train Loss  17.913829803466797 |\n",
      "Val Loss for batch is  0.6259331107139587\n",
      "Val Loss for batch is  0.612707793712616\n",
      "Val Loss for batch is  0.8750444054603577\n",
      "Val Loss for batch is  0.11260853707790375\n",
      "|Iter  162  | Total Val Loss  2.226293846964836 |\n",
      "Loss for batch is  5.02016544342041\n",
      "Loss for batch is  4.748681545257568\n",
      "Loss for batch is  5.044649124145508\n",
      "Loss for batch is  2.990997552871704\n",
      "|Iter  163  | Total Train Loss  17.80449366569519 |\n",
      "Val Loss for batch is  0.7121307253837585\n",
      "Val Loss for batch is  0.524730920791626\n",
      "Val Loss for batch is  0.9147289991378784\n",
      "Val Loss for batch is  0.3351943790912628\n",
      "|Iter  163  | Total Val Loss  2.4867850244045258 |\n",
      "Loss for batch is  4.9303765296936035\n",
      "Loss for batch is  4.761277675628662\n",
      "Loss for batch is  5.035111427307129\n",
      "Loss for batch is  3.020749807357788\n",
      "|Iter  164  | Total Train Loss  17.747515439987183 |\n",
      "Val Loss for batch is  0.8173934817314148\n",
      "Val Loss for batch is  0.5784429311752319\n",
      "Val Loss for batch is  0.8855710625648499\n",
      "Val Loss for batch is  0.12061430513858795\n",
      "|Iter  164  | Total Val Loss  2.4020217806100845 |\n",
      "Loss for batch is  4.932541847229004\n",
      "Loss for batch is  4.669199466705322\n",
      "Loss for batch is  4.988924980163574\n",
      "Loss for batch is  3.012242317199707\n",
      "|Iter  165  | Total Train Loss  17.602908611297607 |\n",
      "Val Loss for batch is  0.7526549100875854\n",
      "Val Loss for batch is  0.5518345236778259\n",
      "Val Loss for batch is  0.7883143424987793\n",
      "Val Loss for batch is  0.03235705569386482\n",
      "|Iter  165  | Total Val Loss  2.1251608319580555 |\n",
      "Loss for batch is  4.867554664611816\n",
      "Loss for batch is  4.6345014572143555\n",
      "Loss for batch is  4.925204277038574\n",
      "Loss for batch is  2.944828748703003\n",
      "|Iter  166  | Total Train Loss  17.37208914756775 |\n",
      "Val Loss for batch is  0.7295852899551392\n",
      "Val Loss for batch is  0.5533910989761353\n",
      "Val Loss for batch is  0.8032751083374023\n",
      "Val Loss for batch is  0.11151982843875885\n",
      "|Iter  166  | Total Val Loss  2.1977713257074356 |\n",
      "Loss for batch is  4.847933769226074\n",
      "Loss for batch is  4.629557132720947\n",
      "Loss for batch is  4.920202255249023\n",
      "Loss for batch is  2.9688098430633545\n",
      "|Iter  167  | Total Train Loss  17.3665030002594 |\n",
      "Val Loss for batch is  0.6011053323745728\n",
      "Val Loss for batch is  0.4697961211204529\n",
      "Val Loss for batch is  0.7780992984771729\n",
      "Val Loss for batch is  0.14813363552093506\n",
      "|Iter  167  | Total Val Loss  1.9971343874931335 |\n",
      "Loss for batch is  4.8000168800354\n",
      "Loss for batch is  4.567557334899902\n",
      "Loss for batch is  4.895546913146973\n",
      "Loss for batch is  2.9258646965026855\n",
      "|Iter  168  | Total Train Loss  17.18898582458496 |\n",
      "Val Loss for batch is  0.6930960416793823\n",
      "Val Loss for batch is  0.5581499338150024\n",
      "Val Loss for batch is  0.7851179838180542\n",
      "Val Loss for batch is  -0.1679716855287552\n",
      "|Iter  168  | Total Val Loss  1.8683922737836838 |\n",
      "Loss for batch is  4.800778388977051\n",
      "Loss for batch is  4.60164737701416\n",
      "Loss for batch is  4.8233537673950195\n",
      "Loss for batch is  2.9200775623321533\n",
      "|Iter  169  | Total Train Loss  17.145857095718384 |\n",
      "Val Loss for batch is  0.7899202108383179\n",
      "Val Loss for batch is  0.534553050994873\n",
      "Val Loss for batch is  0.7958041429519653\n",
      "Val Loss for batch is  0.15717539191246033\n",
      "|Iter  169  | Total Val Loss  2.2774527966976166 |\n",
      "Loss for batch is  4.741125106811523\n",
      "Loss for batch is  4.489901542663574\n",
      "Loss for batch is  4.858335018157959\n",
      "Loss for batch is  2.8677563667297363\n",
      "|Iter  170  | Total Train Loss  16.957118034362793 |\n",
      "Val Loss for batch is  0.690817654132843\n",
      "Val Loss for batch is  0.4901106357574463\n",
      "Val Loss for batch is  0.8018643856048584\n",
      "Val Loss for batch is  0.07945890724658966\n",
      "|Iter  170  | Total Val Loss  2.0622515827417374 |\n",
      "Loss for batch is  4.726157188415527\n",
      "Loss for batch is  4.496221542358398\n",
      "Loss for batch is  4.827773094177246\n",
      "Loss for batch is  2.8476743698120117\n",
      "|Iter  171  | Total Train Loss  16.897826194763184 |\n",
      "Val Loss for batch is  0.567544162273407\n",
      "Val Loss for batch is  0.4100629687309265\n",
      "Val Loss for batch is  0.7636816501617432\n",
      "Val Loss for batch is  -0.014793843030929565\n",
      "|Iter  171  | Total Val Loss  1.726494938135147 |\n",
      "Loss for batch is  4.662763595581055\n",
      "Loss for batch is  4.462829113006592\n",
      "Loss for batch is  4.76686429977417\n",
      "Loss for batch is  2.827165365219116\n",
      "|Iter  172  | Total Train Loss  16.719622373580933 |\n",
      "Val Loss for batch is  0.5211026668548584\n",
      "Val Loss for batch is  0.37168920040130615\n",
      "Val Loss for batch is  0.64858478307724\n",
      "Val Loss for batch is  -0.02667444944381714\n",
      "|Iter  172  | Total Val Loss  1.5147022008895874 |\n",
      "Loss for batch is  4.6986188888549805\n",
      "Loss for batch is  4.452764511108398\n",
      "Loss for batch is  4.805253028869629\n",
      "Loss for batch is  2.8706369400024414\n",
      "|Iter  173  | Total Train Loss  16.82727336883545 |\n",
      "Val Loss for batch is  0.5245914459228516\n",
      "Val Loss for batch is  0.31694066524505615\n",
      "Val Loss for batch is  0.805580735206604\n",
      "Val Loss for batch is  -0.005600683391094208\n",
      "|Iter  173  | Total Val Loss  1.6415121629834175 |\n",
      "Loss for batch is  4.641928672790527\n",
      "Loss for batch is  4.4933061599731445\n",
      "Loss for batch is  4.799328327178955\n",
      "Loss for batch is  2.769153594970703\n",
      "|Iter  174  | Total Train Loss  16.70371675491333 |\n",
      "Val Loss for batch is  0.5432841777801514\n",
      "Val Loss for batch is  0.4020923376083374\n",
      "Val Loss for batch is  0.7364856004714966\n",
      "Val Loss for batch is  -0.21946346759796143\n",
      "|Iter  174  | Total Val Loss  1.462398648262024 |\n",
      "Loss for batch is  4.651063919067383\n",
      "Loss for batch is  4.357537269592285\n",
      "Loss for batch is  4.653851509094238\n",
      "Loss for batch is  2.8136260509490967\n",
      "|Iter  175  | Total Train Loss  16.476078748703003 |\n",
      "Val Loss for batch is  0.7305791974067688\n",
      "Val Loss for batch is  0.5298427939414978\n",
      "Val Loss for batch is  0.6927013397216797\n",
      "Val Loss for batch is  0.14658470451831818\n",
      "|Iter  175  | Total Val Loss  2.0997080355882645 |\n",
      "Loss for batch is  4.603044509887695\n",
      "Loss for batch is  4.3714799880981445\n",
      "Loss for batch is  4.667294025421143\n",
      "Loss for batch is  2.7550575733184814\n",
      "|Iter  176  | Total Train Loss  16.396876096725464 |\n",
      "Val Loss for batch is  0.5287270545959473\n",
      "Val Loss for batch is  0.30712851881980896\n",
      "Val Loss for batch is  0.5942728519439697\n",
      "Val Loss for batch is  -0.04260330647230148\n",
      "|Iter  176  | Total Val Loss  1.3875251188874245 |\n",
      "Loss for batch is  4.668230056762695\n",
      "Loss for batch is  4.347084999084473\n",
      "Loss for batch is  4.656540393829346\n",
      "Loss for batch is  2.7110719680786133\n",
      "|Iter  177  | Total Train Loss  16.382927417755127 |\n",
      "Val Loss for batch is  0.6649540662765503\n",
      "Val Loss for batch is  0.5051138401031494\n",
      "Val Loss for batch is  0.6294705867767334\n",
      "Val Loss for batch is  -0.1230543926358223\n",
      "|Iter  177  | Total Val Loss  1.6764841005206108 |\n",
      "Loss for batch is  4.561891555786133\n",
      "Loss for batch is  4.3025312423706055\n",
      "Loss for batch is  4.60385274887085\n",
      "Loss for batch is  2.7447071075439453\n",
      "|Iter  178  | Total Train Loss  16.212982654571533 |\n",
      "Val Loss for batch is  0.6009906530380249\n",
      "Val Loss for batch is  0.2488967627286911\n",
      "Val Loss for batch is  0.6291455626487732\n",
      "Val Loss for batch is  -0.16710206866264343\n",
      "|Iter  178  | Total Val Loss  1.3119309097528458 |\n",
      "Loss for batch is  4.530198097229004\n",
      "Loss for batch is  4.2636213302612305\n",
      "Loss for batch is  4.538012504577637\n",
      "Loss for batch is  2.6697607040405273\n",
      "|Iter  179  | Total Train Loss  16.0015926361084 |\n",
      "Val Loss for batch is  0.4945041537284851\n",
      "Val Loss for batch is  0.3529217839241028\n",
      "Val Loss for batch is  0.7387647032737732\n",
      "Val Loss for batch is  -0.07141163945198059\n",
      "|Iter  179  | Total Val Loss  1.5147790014743805 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  4.49539852142334\n",
      "Loss for batch is  4.3106794357299805\n",
      "Loss for batch is  4.510981559753418\n",
      "Loss for batch is  2.6215500831604004\n",
      "|Iter  180  | Total Train Loss  15.938609600067139 |\n",
      "Val Loss for batch is  0.4439508020877838\n",
      "Val Loss for batch is  0.43294283747673035\n",
      "Val Loss for batch is  0.6266899108886719\n",
      "Val Loss for batch is  -0.2769893407821655\n",
      "|Iter  180  | Total Val Loss  1.2265942096710205 |\n",
      "Loss for batch is  4.490847587585449\n",
      "Loss for batch is  4.2532453536987305\n",
      "Loss for batch is  4.529280662536621\n",
      "Loss for batch is  2.598018169403076\n",
      "|Iter  181  | Total Train Loss  15.871391773223877 |\n",
      "Val Loss for batch is  0.49223190546035767\n",
      "Val Loss for batch is  0.27274906635284424\n",
      "Val Loss for batch is  0.6774308681488037\n",
      "Val Loss for batch is  -0.18995070457458496\n",
      "|Iter  181  | Total Val Loss  1.2524611353874207 |\n",
      "Loss for batch is  4.376522064208984\n",
      "Loss for batch is  4.181889533996582\n",
      "Loss for batch is  4.523647308349609\n",
      "Loss for batch is  2.5939290523529053\n",
      "|Iter  182  | Total Train Loss  15.675987958908081 |\n",
      "Val Loss for batch is  0.3908507823944092\n",
      "Val Loss for batch is  0.3273424208164215\n",
      "Val Loss for batch is  0.539664089679718\n",
      "Val Loss for batch is  -0.3475553095340729\n",
      "|Iter  182  | Total Val Loss  0.9103019833564758 |\n",
      "Loss for batch is  4.398011207580566\n",
      "Loss for batch is  4.188864707946777\n",
      "Loss for batch is  4.464798927307129\n",
      "Loss for batch is  2.616614818572998\n",
      "|Iter  183  | Total Train Loss  15.66828966140747 |\n",
      "Val Loss for batch is  0.3918381929397583\n",
      "Val Loss for batch is  0.2702745199203491\n",
      "Val Loss for batch is  0.6041339635848999\n",
      "Val Loss for batch is  -0.25625428557395935\n",
      "|Iter  183  | Total Val Loss  1.009992390871048 |\n",
      "Loss for batch is  4.38222599029541\n",
      "Loss for batch is  4.160850524902344\n",
      "Loss for batch is  4.44065523147583\n",
      "Loss for batch is  2.566854476928711\n",
      "|Iter  184  | Total Train Loss  15.550586223602295 |\n",
      "Val Loss for batch is  0.5147973895072937\n",
      "Val Loss for batch is  0.20537106692790985\n",
      "Val Loss for batch is  0.5221747756004333\n",
      "Val Loss for batch is  -0.20188257098197937\n",
      "|Iter  184  | Total Val Loss  1.0404606610536575 |\n",
      "Loss for batch is  4.311488151550293\n",
      "Loss for batch is  4.12043571472168\n",
      "Loss for batch is  4.412585735321045\n",
      "Loss for batch is  2.575437545776367\n",
      "|Iter  185  | Total Train Loss  15.419947147369385 |\n",
      "Val Loss for batch is  0.4552127420902252\n",
      "Val Loss for batch is  0.31106066703796387\n",
      "Val Loss for batch is  0.4768768548965454\n",
      "Val Loss for batch is  -0.25799888372421265\n",
      "|Iter  185  | Total Val Loss  0.9851513803005219 |\n",
      "Loss for batch is  4.309568405151367\n",
      "Loss for batch is  4.139578342437744\n",
      "Loss for batch is  4.411699295043945\n",
      "Loss for batch is  2.5489742755889893\n",
      "|Iter  186  | Total Train Loss  15.409820318222046 |\n",
      "Val Loss for batch is  0.3809683620929718\n",
      "Val Loss for batch is  0.17535661160945892\n",
      "Val Loss for batch is  0.5597904324531555\n",
      "Val Loss for batch is  -0.15174302458763123\n",
      "|Iter  186  | Total Val Loss  0.964372381567955 |\n",
      "Loss for batch is  4.284827709197998\n",
      "Loss for batch is  4.061310768127441\n",
      "Loss for batch is  4.408499717712402\n",
      "Loss for batch is  2.5333433151245117\n",
      "|Iter  187  | Total Train Loss  15.287981510162354 |\n",
      "Val Loss for batch is  0.48024266958236694\n",
      "Val Loss for batch is  0.3740409016609192\n",
      "Val Loss for batch is  0.5565676689147949\n",
      "Val Loss for batch is  -0.3233546018600464\n",
      "|Iter  187  | Total Val Loss  1.0874966382980347 |\n",
      "Loss for batch is  4.280004978179932\n",
      "Loss for batch is  4.0345377922058105\n",
      "Loss for batch is  4.324082374572754\n",
      "Loss for batch is  2.489736318588257\n",
      "|Iter  188  | Total Train Loss  15.128361463546753 |\n",
      "Val Loss for batch is  0.33273592591285706\n",
      "Val Loss for batch is  0.19087417423725128\n",
      "Val Loss for batch is  0.4601422846317291\n",
      "Val Loss for batch is  -0.16001582145690918\n",
      "|Iter  188  | Total Val Loss  0.8237365633249283 |\n",
      "Loss for batch is  4.294694900512695\n",
      "Loss for batch is  3.9941699504852295\n",
      "Loss for batch is  4.286846160888672\n",
      "Loss for batch is  2.5512335300445557\n",
      "|Iter  189  | Total Train Loss  15.126944541931152 |\n",
      "Val Loss for batch is  0.3345206379890442\n",
      "Val Loss for batch is  0.14282655715942383\n",
      "Val Loss for batch is  0.485312819480896\n",
      "Val Loss for batch is  -0.2811043858528137\n",
      "|Iter  189  | Total Val Loss  0.6815556287765503 |\n",
      "Loss for batch is  4.183441162109375\n",
      "Loss for batch is  4.0083394050598145\n",
      "Loss for batch is  4.3081183433532715\n",
      "Loss for batch is  2.4137566089630127\n",
      "|Iter  190  | Total Train Loss  14.913655519485474 |\n",
      "Val Loss for batch is  0.3732112646102905\n",
      "Val Loss for batch is  0.22150462865829468\n",
      "Val Loss for batch is  0.4094265103340149\n",
      "Val Loss for batch is  -0.28699207305908203\n",
      "|Iter  190  | Total Val Loss  0.7171503305435181 |\n",
      "Loss for batch is  4.248122215270996\n",
      "Loss for batch is  3.968329906463623\n",
      "Loss for batch is  4.278945446014404\n",
      "Loss for batch is  2.514845848083496\n",
      "|Iter  191  | Total Train Loss  15.01024341583252 |\n",
      "Val Loss for batch is  0.4411700367927551\n",
      "Val Loss for batch is  0.07780451327562332\n",
      "Val Loss for batch is  0.5617638826370239\n",
      "Val Loss for batch is  -0.3473796248435974\n",
      "|Iter  191  | Total Val Loss  0.733358807861805 |\n",
      "Loss for batch is  4.180555820465088\n",
      "Loss for batch is  3.9539170265197754\n",
      "Loss for batch is  4.26068639755249\n",
      "Loss for batch is  2.39778208732605\n",
      "|Iter  192  | Total Train Loss  14.792941331863403 |\n",
      "Val Loss for batch is  0.3235473334789276\n",
      "Val Loss for batch is  0.04230642318725586\n",
      "Val Loss for batch is  0.4421646296977997\n",
      "Val Loss for batch is  -0.3346502184867859\n",
      "|Iter  192  | Total Val Loss  0.47336816787719727 |\n",
      "Loss for batch is  4.168155670166016\n",
      "Loss for batch is  3.910733222961426\n",
      "Loss for batch is  4.213568687438965\n",
      "Loss for batch is  2.40718150138855\n",
      "|Iter  193  | Total Train Loss  14.699639081954956 |\n",
      "Val Loss for batch is  0.325251042842865\n",
      "Val Loss for batch is  0.1027195006608963\n",
      "Val Loss for batch is  0.30961522459983826\n",
      "Val Loss for batch is  -0.2949478030204773\n",
      "|Iter  193  | Total Val Loss  0.44263796508312225 |\n",
      "Loss for batch is  4.1541290283203125\n",
      "Loss for batch is  3.9211206436157227\n",
      "Loss for batch is  4.207102298736572\n",
      "Loss for batch is  2.3838624954223633\n",
      "|Iter  194  | Total Train Loss  14.66621446609497 |\n",
      "Val Loss for batch is  0.2754930853843689\n",
      "Val Loss for batch is  0.09475269168615341\n",
      "Val Loss for batch is  0.4442974328994751\n",
      "Val Loss for batch is  -0.26804885268211365\n",
      "|Iter  194  | Total Val Loss  0.5464943572878838 |\n",
      "Loss for batch is  4.092833995819092\n",
      "Loss for batch is  3.876594305038452\n",
      "Loss for batch is  4.158674716949463\n",
      "Loss for batch is  2.3404674530029297\n",
      "|Iter  195  | Total Train Loss  14.468570470809937 |\n",
      "Val Loss for batch is  0.33142584562301636\n",
      "Val Loss for batch is  0.20458924770355225\n",
      "Val Loss for batch is  0.38111090660095215\n",
      "Val Loss for batch is  -0.18240174651145935\n",
      "|Iter  195  | Total Val Loss  0.7347242534160614 |\n",
      "Loss for batch is  4.04231595993042\n",
      "Loss for batch is  3.8978424072265625\n",
      "Loss for batch is  4.162775039672852\n",
      "Loss for batch is  2.3302001953125\n",
      "|Iter  196  | Total Train Loss  14.433133602142334 |\n",
      "Val Loss for batch is  0.3566630482673645\n",
      "Val Loss for batch is  0.1354658603668213\n",
      "Val Loss for batch is  0.38518428802490234\n",
      "Val Loss for batch is  -0.3285476267337799\n",
      "|Iter  196  | Total Val Loss  0.5487655699253082 |\n",
      "Loss for batch is  4.157402038574219\n",
      "Loss for batch is  3.83267879486084\n",
      "Loss for batch is  4.146749019622803\n",
      "Loss for batch is  2.400688886642456\n",
      "|Iter  197  | Total Train Loss  14.537518739700317 |\n",
      "Val Loss for batch is  0.3625226318836212\n",
      "Val Loss for batch is  0.1100408211350441\n",
      "Val Loss for batch is  0.561656653881073\n",
      "Val Loss for batch is  -0.24145011603832245\n",
      "|Iter  197  | Total Val Loss  0.7927699908614159 |\n",
      "Loss for batch is  4.054546356201172\n",
      "Loss for batch is  3.8484983444213867\n",
      "Loss for batch is  4.142164707183838\n",
      "Loss for batch is  2.264097213745117\n",
      "|Iter  198  | Total Train Loss  14.309306621551514 |\n",
      "Val Loss for batch is  0.18798261880874634\n",
      "Val Loss for batch is  0.11000458896160126\n",
      "Val Loss for batch is  0.37068188190460205\n",
      "Val Loss for batch is  -0.3751460015773773\n",
      "|Iter  198  | Total Val Loss  0.2935230880975723 |\n",
      "Loss for batch is  4.108553886413574\n",
      "Loss for batch is  3.807659387588501\n",
      "Loss for batch is  4.076725959777832\n",
      "Loss for batch is  2.374140501022339\n",
      "|Iter  199  | Total Train Loss  14.367079734802246 |\n",
      "Val Loss for batch is  0.3218221366405487\n",
      "Val Loss for batch is  0.43443453311920166\n",
      "Val Loss for batch is  0.42126959562301636\n",
      "Val Loss for batch is  -0.34818118810653687\n",
      "|Iter  199  | Total Val Loss  0.8293450772762299 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  3.9509499073028564\n",
      "Loss for batch is  3.759659767150879\n",
      "Loss for batch is  4.0525360107421875\n",
      "Loss for batch is  2.251450777053833\n",
      "|Iter  200  | Total Train Loss  14.014596462249756 |\n",
      "Val Loss for batch is  0.28355973958969116\n",
      "Val Loss for batch is  0.047105491161346436\n",
      "Val Loss for batch is  0.26357579231262207\n",
      "Val Loss for batch is  -0.4279201030731201\n",
      "|Iter  200  | Total Val Loss  0.16632091999053955 |\n",
      "Loss for batch is  3.976781129837036\n",
      "Loss for batch is  3.7482872009277344\n",
      "Loss for batch is  4.030467987060547\n",
      "Loss for batch is  2.2550528049468994\n",
      "|Iter  201  | Total Train Loss  14.010589122772217 |\n",
      "Val Loss for batch is  0.2676018476486206\n",
      "Val Loss for batch is  0.053431540727615356\n",
      "Val Loss for batch is  0.5373984575271606\n",
      "Val Loss for batch is  -0.30655884742736816\n",
      "|Iter  201  | Total Val Loss  0.5518729984760284 |\n",
      "Loss for batch is  3.865668296813965\n",
      "Loss for batch is  3.69496488571167\n",
      "Loss for batch is  4.023112773895264\n",
      "Loss for batch is  2.2704756259918213\n",
      "|Iter  202  | Total Train Loss  13.85422158241272 |\n",
      "Val Loss for batch is  0.29351183772087097\n",
      "Val Loss for batch is  0.11847073584794998\n",
      "Val Loss for batch is  0.31377488374710083\n",
      "Val Loss for batch is  -0.2941719591617584\n",
      "|Iter  202  | Total Val Loss  0.43158549815416336 |\n",
      "Loss for batch is  3.9631128311157227\n",
      "Loss for batch is  3.6967086791992188\n",
      "Loss for batch is  3.9619359970092773\n",
      "Loss for batch is  2.2115275859832764\n",
      "|Iter  203  | Total Train Loss  13.833285093307495 |\n",
      "Val Loss for batch is  0.16941305994987488\n",
      "Val Loss for batch is  0.07589723914861679\n",
      "Val Loss for batch is  0.30556756258010864\n",
      "Val Loss for batch is  -0.2878261208534241\n",
      "|Iter  203  | Total Val Loss  0.26305174082517624 |\n",
      "Loss for batch is  3.909980297088623\n",
      "Loss for batch is  3.612091064453125\n",
      "Loss for batch is  3.9649720191955566\n",
      "Loss for batch is  2.219149112701416\n",
      "|Iter  204  | Total Train Loss  13.70619249343872 |\n",
      "Val Loss for batch is  0.33816760778427124\n",
      "Val Loss for batch is  0.06853040307760239\n",
      "Val Loss for batch is  0.24909409880638123\n",
      "Val Loss for batch is  -0.4533032774925232\n",
      "|Iter  204  | Total Val Loss  0.20248883217573166 |\n",
      "Loss for batch is  3.8760085105895996\n",
      "Loss for batch is  3.631155252456665\n",
      "Loss for batch is  4.001506805419922\n",
      "Loss for batch is  2.169579029083252\n",
      "|Iter  205  | Total Train Loss  13.678249597549438 |\n",
      "Val Loss for batch is  0.1465717852115631\n",
      "Val Loss for batch is  0.1513746678829193\n",
      "Val Loss for batch is  0.33723652362823486\n",
      "Val Loss for batch is  -0.4506571292877197\n",
      "|Iter  205  | Total Val Loss  0.18452584743499756 |\n",
      "Loss for batch is  3.894740581512451\n",
      "Loss for batch is  3.5976645946502686\n",
      "Loss for batch is  3.934211254119873\n",
      "Loss for batch is  2.2159531116485596\n",
      "|Iter  206  | Total Train Loss  13.642569541931152 |\n",
      "Val Loss for batch is  0.263409286737442\n",
      "Val Loss for batch is  0.04552076756954193\n",
      "Val Loss for batch is  0.35026633739471436\n",
      "Val Loss for batch is  -0.5347118377685547\n",
      "|Iter  206  | Total Val Loss  0.12448455393314362 |\n",
      "Loss for batch is  3.867990255355835\n",
      "Loss for batch is  3.643134832382202\n",
      "Loss for batch is  3.929072856903076\n",
      "Loss for batch is  2.0987071990966797\n",
      "|Iter  207  | Total Train Loss  13.538905143737793 |\n",
      "Val Loss for batch is  0.20952332019805908\n",
      "Val Loss for batch is  -0.045004189014434814\n",
      "Val Loss for batch is  0.3428386151790619\n",
      "Val Loss for batch is  -0.5346704721450806\n",
      "|Iter  207  | Total Val Loss  -0.02731272578239441 |\n",
      "Loss for batch is  3.804226875305176\n",
      "Loss for batch is  3.582951545715332\n",
      "Loss for batch is  3.9229955673217773\n",
      "Loss for batch is  2.1306028366088867\n",
      "|Iter  208  | Total Train Loss  13.440776824951172 |\n",
      "Val Loss for batch is  0.18098926544189453\n",
      "Val Loss for batch is  -0.021798238158226013\n",
      "Val Loss for batch is  0.31630364060401917\n",
      "Val Loss for batch is  -0.4464748501777649\n",
      "|Iter  208  | Total Val Loss  0.02901981770992279 |\n",
      "Loss for batch is  3.8525567054748535\n",
      "Loss for batch is  3.5870752334594727\n",
      "Loss for batch is  3.899336576461792\n",
      "Loss for batch is  2.163520097732544\n",
      "|Iter  209  | Total Train Loss  13.502488613128662 |\n",
      "Val Loss for batch is  0.07669252157211304\n",
      "Val Loss for batch is  -0.04746268689632416\n",
      "Val Loss for batch is  0.22743366658687592\n",
      "Val Loss for batch is  -0.626457929611206\n",
      "|Iter  209  | Total Val Loss  -0.36979442834854126 |\n",
      "Loss for batch is  3.790062189102173\n",
      "Loss for batch is  3.5288782119750977\n",
      "Loss for batch is  3.8296780586242676\n",
      "Loss for batch is  2.165189743041992\n",
      "|Iter  210  | Total Train Loss  13.31380820274353 |\n",
      "Val Loss for batch is  0.10044185817241669\n",
      "Val Loss for batch is  -0.016766294836997986\n",
      "Val Loss for batch is  0.3240554928779602\n",
      "Val Loss for batch is  -0.3532889485359192\n",
      "|Iter  210  | Total Val Loss  0.05444210767745972 |\n",
      "Loss for batch is  3.7438321113586426\n",
      "Loss for batch is  3.5399558544158936\n",
      "Loss for batch is  3.800206184387207\n",
      "Loss for batch is  2.0556066036224365\n",
      "|Iter  211  | Total Train Loss  13.13960075378418 |\n",
      "Val Loss for batch is  0.15173321962356567\n",
      "Val Loss for batch is  -0.0570315420627594\n",
      "Val Loss for batch is  0.2439444661140442\n",
      "Val Loss for batch is  -0.49822482466697693\n",
      "|Iter  211  | Total Val Loss  -0.15957868099212646 |\n",
      "Loss for batch is  3.7153944969177246\n",
      "Loss for batch is  3.4930338859558105\n",
      "Loss for batch is  3.8129472732543945\n",
      "Loss for batch is  2.0868735313415527\n",
      "|Iter  212  | Total Train Loss  13.108249187469482 |\n",
      "Val Loss for batch is  0.1867021918296814\n",
      "Val Loss for batch is  0.07317323982715607\n",
      "Val Loss for batch is  0.22675299644470215\n",
      "Val Loss for batch is  -0.5162185430526733\n",
      "|Iter  212  | Total Val Loss  -0.029590114951133728 |\n",
      "Loss for batch is  3.694112777709961\n",
      "Loss for batch is  3.4848270416259766\n",
      "Loss for batch is  3.7752938270568848\n",
      "Loss for batch is  2.1109769344329834\n",
      "|Iter  213  | Total Train Loss  13.065210580825806 |\n",
      "Val Loss for batch is  0.04948925971984863\n",
      "Val Loss for batch is  -0.05857096612453461\n",
      "Val Loss for batch is  0.26629161834716797\n",
      "Val Loss for batch is  -0.6501007676124573\n",
      "|Iter  213  | Total Val Loss  -0.3928908556699753 |\n",
      "Loss for batch is  3.689927339553833\n",
      "Loss for batch is  3.467869758605957\n",
      "Loss for batch is  3.7703089714050293\n",
      "Loss for batch is  2.0960733890533447\n",
      "|Iter  214  | Total Train Loss  13.024179458618164 |\n",
      "Val Loss for batch is  0.010710537433624268\n",
      "Val Loss for batch is  0.011013567447662354\n",
      "Val Loss for batch is  0.4293759763240814\n",
      "Val Loss for batch is  -0.4392956793308258\n",
      "|Iter  214  | Total Val Loss  0.011804401874542236 |\n",
      "Loss for batch is  3.611072301864624\n",
      "Loss for batch is  3.452510356903076\n",
      "Loss for batch is  3.7880938053131104\n",
      "Loss for batch is  1.9865578413009644\n",
      "|Iter  215  | Total Train Loss  12.838234305381775 |\n",
      "Val Loss for batch is  0.03759826719760895\n",
      "Val Loss for batch is  -0.04150192439556122\n",
      "Val Loss for batch is  0.19329966604709625\n",
      "Val Loss for batch is  -0.5340003371238708\n",
      "|Iter  215  | Total Val Loss  -0.34460432827472687 |\n",
      "Loss for batch is  3.615680694580078\n",
      "Loss for batch is  3.4813084602355957\n",
      "Loss for batch is  3.734005928039551\n",
      "Loss for batch is  2.0014901161193848\n",
      "|Iter  216  | Total Train Loss  12.83248519897461 |\n",
      "Val Loss for batch is  0.13892245292663574\n",
      "Val Loss for batch is  -0.05714017152786255\n",
      "Val Loss for batch is  0.1483958661556244\n",
      "Val Loss for batch is  -0.6108300089836121\n",
      "|Iter  216  | Total Val Loss  -0.3806518614292145 |\n",
      "Loss for batch is  3.676331043243408\n",
      "Loss for batch is  3.41892147064209\n",
      "Loss for batch is  3.7231502532958984\n",
      "Loss for batch is  2.020608901977539\n",
      "|Iter  217  | Total Train Loss  12.839011669158936 |\n",
      "Val Loss for batch is  0.08189775049686432\n",
      "Val Loss for batch is  -0.14752857387065887\n",
      "Val Loss for batch is  0.1850549578666687\n",
      "Val Loss for batch is  -0.6478948593139648\n",
      "|Iter  217  | Total Val Loss  -0.5284707248210907 |\n",
      "Loss for batch is  3.577859878540039\n",
      "Loss for batch is  3.378387451171875\n",
      "Loss for batch is  3.695524215698242\n",
      "Loss for batch is  1.9721167087554932\n",
      "|Iter  218  | Total Train Loss  12.62388825416565 |\n",
      "Val Loss for batch is  0.2349780797958374\n",
      "Val Loss for batch is  0.0417308583855629\n",
      "Val Loss for batch is  0.341499000787735\n",
      "Val Loss for batch is  -0.6160807609558105\n",
      "|Iter  218  | Total Val Loss  0.0021271780133247375 |\n",
      "Loss for batch is  3.5915982723236084\n",
      "Loss for batch is  3.4112682342529297\n",
      "Loss for batch is  3.7298383712768555\n",
      "Loss for batch is  1.9356859922409058\n",
      "|Iter  219  | Total Train Loss  12.6683908700943 |\n",
      "Val Loss for batch is  0.06468062102794647\n",
      "Val Loss for batch is  -0.07487218081951141\n",
      "Val Loss for batch is  0.21264876425266266\n",
      "Val Loss for batch is  -0.5832248330116272\n",
      "|Iter  219  | Total Val Loss  -0.3807676285505295 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  3.6307148933410645\n",
      "Loss for batch is  3.344773530960083\n",
      "Loss for batch is  3.621187925338745\n",
      "Loss for batch is  1.9728422164916992\n",
      "|Iter  220  | Total Train Loss  12.569518566131592 |\n",
      "Val Loss for batch is  0.037253908812999725\n",
      "Val Loss for batch is  -0.08152583241462708\n",
      "Val Loss for batch is  0.24845072627067566\n",
      "Val Loss for batch is  -0.4517303705215454\n",
      "|Iter  220  | Total Val Loss  -0.2475515678524971 |\n",
      "Loss for batch is  3.581770181655884\n",
      "Loss for batch is  3.315199613571167\n",
      "Loss for batch is  3.651644229888916\n",
      "Loss for batch is  1.942225456237793\n",
      "|Iter  221  | Total Train Loss  12.49083948135376 |\n",
      "Val Loss for batch is  0.03736752271652222\n",
      "Val Loss for batch is  -0.10466079413890839\n",
      "Val Loss for batch is  0.17053180932998657\n",
      "Val Loss for batch is  -0.7311304211616516\n",
      "|Iter  221  | Total Val Loss  -0.6278918832540512 |\n",
      "Loss for batch is  3.538090705871582\n",
      "Loss for batch is  3.311570644378662\n",
      "Loss for batch is  3.5886337757110596\n",
      "Loss for batch is  1.934194803237915\n",
      "|Iter  222  | Total Train Loss  12.372489929199219 |\n",
      "Val Loss for batch is  0.0024257302284240723\n",
      "Val Loss for batch is  -0.033079639077186584\n",
      "Val Loss for batch is  0.16508497297763824\n",
      "Val Loss for batch is  -0.5143623948097229\n",
      "|Iter  222  | Total Val Loss  -0.37993133068084717 |\n",
      "Loss for batch is  3.4980661869049072\n",
      "Loss for batch is  3.30546236038208\n",
      "Loss for batch is  3.592386245727539\n",
      "Loss for batch is  1.9157416820526123\n",
      "|Iter  223  | Total Train Loss  12.311656475067139 |\n",
      "Val Loss for batch is  0.15281476080417633\n",
      "Val Loss for batch is  -0.17789342999458313\n",
      "Val Loss for batch is  0.15840086340904236\n",
      "Val Loss for batch is  -0.7034158706665039\n",
      "|Iter  223  | Total Val Loss  -0.5700936764478683 |\n",
      "Loss for batch is  3.516526222229004\n",
      "Loss for batch is  3.3329429626464844\n",
      "Loss for batch is  3.572208881378174\n",
      "Loss for batch is  1.9056706428527832\n",
      "|Iter  224  | Total Train Loss  12.327348709106445 |\n",
      "Val Loss for batch is  0.15805156528949738\n",
      "Val Loss for batch is  -0.1951068490743637\n",
      "Val Loss for batch is  0.15509125590324402\n",
      "Val Loss for batch is  -0.6217676401138306\n",
      "|Iter  224  | Total Val Loss  -0.5037316679954529 |\n",
      "Loss for batch is  3.5122628211975098\n",
      "Loss for batch is  3.294104814529419\n",
      "Loss for batch is  3.638618230819702\n",
      "Loss for batch is  1.826451301574707\n",
      "|Iter  225  | Total Train Loss  12.271437168121338 |\n",
      "Val Loss for batch is  0.032576605677604675\n",
      "Val Loss for batch is  -0.17905502021312714\n",
      "Val Loss for batch is  0.18049895763397217\n",
      "Val Loss for batch is  -0.639580249786377\n",
      "|Iter  225  | Total Val Loss  -0.6055597066879272 |\n",
      "Loss for batch is  3.6060023307800293\n",
      "Loss for batch is  3.2848639488220215\n",
      "Loss for batch is  3.5586071014404297\n",
      "Loss for batch is  1.8841592073440552\n",
      "|Iter  226  | Total Train Loss  12.333632588386536 |\n",
      "Val Loss for batch is  0.09523257613182068\n",
      "Val Loss for batch is  -0.10704988241195679\n",
      "Val Loss for batch is  0.19045406579971313\n",
      "Val Loss for batch is  -0.5727353692054749\n",
      "|Iter  226  | Total Val Loss  -0.3940986096858978 |\n",
      "Loss for batch is  3.5272083282470703\n",
      "Loss for batch is  3.2248897552490234\n",
      "Loss for batch is  3.620096206665039\n",
      "Loss for batch is  1.8675142526626587\n",
      "|Iter  227  | Total Train Loss  12.239708542823792 |\n",
      "Val Loss for batch is  -0.06137189269065857\n",
      "Val Loss for batch is  -0.1649722009897232\n",
      "Val Loss for batch is  0.2143429070711136\n",
      "Val Loss for batch is  -0.7050920724868774\n",
      "|Iter  227  | Total Val Loss  -0.7170932590961456 |\n",
      "Loss for batch is  3.5035133361816406\n",
      "Loss for batch is  3.2058334350585938\n",
      "Loss for batch is  3.5795435905456543\n",
      "Loss for batch is  1.819934368133545\n",
      "|Iter  228  | Total Train Loss  12.108824729919434 |\n",
      "Val Loss for batch is  -0.00551779568195343\n",
      "Val Loss for batch is  -0.07322266697883606\n",
      "Val Loss for batch is  0.180939719080925\n",
      "Val Loss for batch is  -0.6652579307556152\n",
      "|Iter  228  | Total Val Loss  -0.5630586743354797 |\n",
      "Loss for batch is  3.466165542602539\n",
      "Loss for batch is  3.157207489013672\n",
      "Loss for batch is  3.5175552368164062\n",
      "Loss for batch is  1.8315255641937256\n",
      "|Iter  229  | Total Train Loss  11.972453832626343 |\n",
      "Val Loss for batch is  -0.02584785223007202\n",
      "Val Loss for batch is  -0.22030426561832428\n",
      "Val Loss for batch is  0.034029074013233185\n",
      "Val Loss for batch is  -0.6700847148895264\n",
      "|Iter  229  | Total Val Loss  -0.8822077587246895 |\n",
      "Loss for batch is  3.4102747440338135\n",
      "Loss for batch is  3.186879873275757\n",
      "Loss for batch is  3.511820077896118\n",
      "Loss for batch is  1.8097202777862549\n",
      "|Iter  230  | Total Train Loss  11.918694972991943 |\n",
      "Val Loss for batch is  -0.009214326739311218\n",
      "Val Loss for batch is  -0.11613212525844574\n",
      "Val Loss for batch is  0.10029865056276321\n",
      "Val Loss for batch is  -0.7028242349624634\n",
      "|Iter  230  | Total Val Loss  -0.7278720363974571 |\n",
      "Loss for batch is  3.399599313735962\n",
      "Loss for batch is  3.1668243408203125\n",
      "Loss for batch is  3.439908504486084\n",
      "Loss for batch is  1.7916226387023926\n",
      "|Iter  231  | Total Train Loss  11.797954797744751 |\n",
      "Val Loss for batch is  -0.06268559396266937\n",
      "Val Loss for batch is  -0.1646047830581665\n",
      "Val Loss for batch is  0.11593162268400192\n",
      "Val Loss for batch is  -0.7289525866508484\n",
      "|Iter  231  | Total Val Loss  -0.8403113409876823 |\n",
      "Loss for batch is  3.3821144104003906\n",
      "Loss for batch is  3.166186809539795\n",
      "Loss for batch is  3.4224648475646973\n",
      "Loss for batch is  1.7797019481658936\n",
      "|Iter  232  | Total Train Loss  11.750468015670776 |\n",
      "Val Loss for batch is  0.0769919753074646\n",
      "Val Loss for batch is  -0.2956122159957886\n",
      "Val Loss for batch is  0.11723484098911285\n",
      "Val Loss for batch is  -0.6630728244781494\n",
      "|Iter  232  | Total Val Loss  -0.7644582241773605 |\n",
      "Loss for batch is  3.355225086212158\n",
      "Loss for batch is  3.13025164604187\n",
      "Loss for batch is  3.424009323120117\n",
      "Loss for batch is  1.7228913307189941\n",
      "|Iter  233  | Total Train Loss  11.63237738609314 |\n",
      "Val Loss for batch is  -0.09749859571456909\n",
      "Val Loss for batch is  -0.06322655081748962\n",
      "Val Loss for batch is  0.11808918416500092\n",
      "Val Loss for batch is  -0.7641013860702515\n",
      "|Iter  233  | Total Val Loss  -0.8067373484373093 |\n",
      "Loss for batch is  3.3876492977142334\n",
      "Loss for batch is  3.094007968902588\n",
      "Loss for batch is  3.435455322265625\n",
      "Loss for batch is  1.8049894571304321\n",
      "|Iter  234  | Total Train Loss  11.722102046012878 |\n",
      "Val Loss for batch is  -0.06830057501792908\n",
      "Val Loss for batch is  -0.16002504527568817\n",
      "Val Loss for batch is  0.14800558984279633\n",
      "Val Loss for batch is  -0.7335286140441895\n",
      "|Iter  234  | Total Val Loss  -0.8138486444950104 |\n",
      "Loss for batch is  3.2540841102600098\n",
      "Loss for batch is  3.1379404067993164\n",
      "Loss for batch is  3.367811441421509\n",
      "Loss for batch is  1.696875810623169\n",
      "|Iter  235  | Total Train Loss  11.456711769104004 |\n",
      "Val Loss for batch is  -0.039179518818855286\n",
      "Val Loss for batch is  -0.31554949283599854\n",
      "Val Loss for batch is  0.0876394659280777\n",
      "Val Loss for batch is  -0.8096482753753662\n",
      "|Iter  235  | Total Val Loss  -1.0767378211021423 |\n",
      "Loss for batch is  3.3033862113952637\n",
      "Loss for batch is  3.0854134559631348\n",
      "Loss for batch is  3.3895034790039062\n",
      "Loss for batch is  1.7271018028259277\n",
      "|Iter  236  | Total Train Loss  11.505404949188232 |\n",
      "Val Loss for batch is  0.029005765914916992\n",
      "Val Loss for batch is  -0.2637908458709717\n",
      "Val Loss for batch is  0.13229651749134064\n",
      "Val Loss for batch is  -0.6851546764373779\n",
      "|Iter  236  | Total Val Loss  -0.787643238902092 |\n",
      "Loss for batch is  3.3096671104431152\n",
      "Loss for batch is  3.047154426574707\n",
      "Loss for batch is  3.324122905731201\n",
      "Loss for batch is  1.7018797397613525\n",
      "|Iter  237  | Total Train Loss  11.382824182510376 |\n",
      "Val Loss for batch is  -0.02758444845676422\n",
      "Val Loss for batch is  -0.2479834258556366\n",
      "Val Loss for batch is  0.15024985373020172\n",
      "Val Loss for batch is  -0.6844224333763123\n",
      "|Iter  237  | Total Val Loss  -0.8097404539585114 |\n",
      "Loss for batch is  3.2530107498168945\n",
      "Loss for batch is  3.046909809112549\n",
      "Loss for batch is  3.378525733947754\n",
      "Loss for batch is  1.6847400665283203\n",
      "|Iter  238  | Total Train Loss  11.363186359405518 |\n",
      "Val Loss for batch is  -0.11624060571193695\n",
      "Val Loss for batch is  -0.22560299932956696\n",
      "Val Loss for batch is  0.08632408827543259\n",
      "Val Loss for batch is  -0.7746434211730957\n",
      "|Iter  238  | Total Val Loss  -1.030162937939167 |\n",
      "Loss for batch is  3.245063543319702\n",
      "Loss for batch is  3.0171191692352295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  3.330965518951416\n",
      "Loss for batch is  1.7024602890014648\n",
      "|Iter  239  | Total Train Loss  11.295608520507812 |\n",
      "Val Loss for batch is  -0.032931044697761536\n",
      "Val Loss for batch is  -0.2018698900938034\n",
      "Val Loss for batch is  0.08442463725805283\n",
      "Val Loss for batch is  -0.7650729417800903\n",
      "|Iter  239  | Total Val Loss  -0.9154492393136024 |\n",
      "Loss for batch is  3.285086154937744\n",
      "Loss for batch is  3.015172004699707\n",
      "Loss for batch is  3.4302124977111816\n",
      "Loss for batch is  1.671515941619873\n",
      "|Iter  240  | Total Train Loss  11.401986598968506 |\n",
      "Val Loss for batch is  -0.06314510107040405\n",
      "Val Loss for batch is  -0.2403944581747055\n",
      "Val Loss for batch is  0.09149734675884247\n",
      "Val Loss for batch is  -0.7405505180358887\n",
      "|Iter  240  | Total Val Loss  -0.9525927305221558 |\n",
      "Loss for batch is  3.2889132499694824\n",
      "Loss for batch is  3.0132687091827393\n",
      "Loss for batch is  3.3599348068237305\n",
      "Loss for batch is  1.647803544998169\n",
      "|Iter  241  | Total Train Loss  11.309920310974121 |\n",
      "Val Loss for batch is  -0.057927146553993225\n",
      "Val Loss for batch is  -0.15916161239147186\n",
      "Val Loss for batch is  0.08072518557310104\n",
      "Val Loss for batch is  -0.8784654140472412\n",
      "|Iter  241  | Total Val Loss  -1.0148289874196053 |\n",
      "Loss for batch is  3.2621078491210938\n",
      "Loss for batch is  2.9256386756896973\n",
      "Loss for batch is  3.3518319129943848\n",
      "Loss for batch is  1.6550928354263306\n",
      "|Iter  242  | Total Train Loss  11.194671273231506 |\n",
      "Val Loss for batch is  -0.04723702371120453\n",
      "Val Loss for batch is  -0.257567822933197\n",
      "Val Loss for batch is  0.019975945353507996\n",
      "Val Loss for batch is  -0.8234682679176331\n",
      "|Iter  242  | Total Val Loss  -1.1082971692085266 |\n",
      "Loss for batch is  3.243957996368408\n",
      "Loss for batch is  2.995164155960083\n",
      "Loss for batch is  3.306093692779541\n",
      "Loss for batch is  1.7142856121063232\n",
      "|Iter  243  | Total Train Loss  11.259501457214355 |\n",
      "Val Loss for batch is  -0.12263770401477814\n",
      "Val Loss for batch is  -0.11527930200099945\n",
      "Val Loss for batch is  -0.034867361187934875\n",
      "Val Loss for batch is  -0.8662024736404419\n",
      "|Iter  243  | Total Val Loss  -1.1389868408441544 |\n",
      "Loss for batch is  3.1820783615112305\n",
      "Loss for batch is  2.897268056869507\n",
      "Loss for batch is  3.2860665321350098\n",
      "Loss for batch is  1.6173229217529297\n",
      "|Iter  244  | Total Train Loss  10.982735872268677 |\n",
      "Val Loss for batch is  0.03946841508150101\n",
      "Val Loss for batch is  -0.22408118844032288\n",
      "Val Loss for batch is  0.08967771381139755\n",
      "Val Loss for batch is  -0.611311137676239\n",
      "|Iter  244  | Total Val Loss  -0.7062461972236633 |\n",
      "Loss for batch is  3.171231508255005\n",
      "Loss for batch is  2.937101364135742\n",
      "Loss for batch is  3.2896852493286133\n",
      "Loss for batch is  1.6224465370178223\n",
      "|Iter  245  | Total Train Loss  11.020464658737183 |\n",
      "Val Loss for batch is  -0.2153652310371399\n",
      "Val Loss for batch is  -0.3579368591308594\n",
      "Val Loss for batch is  0.037688709795475006\n",
      "Val Loss for batch is  -0.8271080255508423\n",
      "|Iter  245  | Total Val Loss  -1.3627214059233665 |\n",
      "Loss for batch is  3.1777405738830566\n",
      "Loss for batch is  2.929976224899292\n",
      "Loss for batch is  3.2559447288513184\n",
      "Loss for batch is  1.6236770153045654\n",
      "|Iter  246  | Total Train Loss  10.987338542938232 |\n",
      "Val Loss for batch is  -0.16766200959682465\n",
      "Val Loss for batch is  -0.15417248010635376\n",
      "Val Loss for batch is  0.04875326156616211\n",
      "Val Loss for batch is  -0.6648703813552856\n",
      "|Iter  246  | Total Val Loss  -0.9379516094923019 |\n",
      "Loss for batch is  3.1207799911499023\n",
      "Loss for batch is  2.884611129760742\n",
      "Loss for batch is  3.2415294647216797\n",
      "Loss for batch is  1.5811374187469482\n",
      "|Iter  247  | Total Train Loss  10.828058004379272 |\n",
      "Val Loss for batch is  -0.1194617748260498\n",
      "Val Loss for batch is  -0.341352254152298\n",
      "Val Loss for batch is  -0.0021783560514450073\n",
      "Val Loss for batch is  -0.8345999717712402\n",
      "|Iter  247  | Total Val Loss  -1.297592356801033 |\n",
      "Loss for batch is  3.1279420852661133\n",
      "Loss for batch is  2.8622305393218994\n",
      "Loss for batch is  3.18628191947937\n",
      "Loss for batch is  1.6126666069030762\n",
      "|Iter  248  | Total Train Loss  10.789121150970459 |\n",
      "Val Loss for batch is  -0.12037500739097595\n",
      "Val Loss for batch is  -0.33401161432266235\n",
      "Val Loss for batch is  0.07106583565473557\n",
      "Val Loss for batch is  -0.8617591857910156\n",
      "|Iter  248  | Total Val Loss  -1.2450799718499184 |\n",
      "Loss for batch is  3.047236442565918\n",
      "Loss for batch is  2.8600921630859375\n",
      "Loss for batch is  3.1894824504852295\n",
      "Loss for batch is  1.5597997903823853\n",
      "|Iter  249  | Total Train Loss  10.65661084651947 |\n",
      "Val Loss for batch is  -0.22771835327148438\n",
      "Val Loss for batch is  -0.34184563159942627\n",
      "Val Loss for batch is  -0.1149158924818039\n",
      "Val Loss for batch is  -0.9256958961486816\n",
      "|Iter  249  | Total Val Loss  -1.6101757735013962 |\n",
      "Loss for batch is  3.130679130554199\n",
      "Loss for batch is  2.854928731918335\n",
      "Loss for batch is  3.11914324760437\n",
      "Loss for batch is  1.5756160020828247\n",
      "|Iter  250  | Total Train Loss  10.680367112159729 |\n",
      "Val Loss for batch is  -0.2125636786222458\n",
      "Val Loss for batch is  -0.25284528732299805\n",
      "Val Loss for batch is  0.005731791257858276\n",
      "Val Loss for batch is  -0.7199172377586365\n",
      "|Iter  250  | Total Val Loss  -1.179594412446022 |\n",
      "Loss for batch is  3.0200743675231934\n",
      "Loss for batch is  2.8029208183288574\n",
      "Loss for batch is  3.1383156776428223\n",
      "Loss for batch is  1.5183953046798706\n",
      "|Iter  251  | Total Train Loss  10.479706168174744 |\n",
      "Val Loss for batch is  -0.2223033756017685\n",
      "Val Loss for batch is  -0.22980333864688873\n",
      "Val Loss for batch is  -0.07631975412368774\n",
      "Val Loss for batch is  -0.8599084615707397\n",
      "|Iter  251  | Total Val Loss  -1.3883349299430847 |\n",
      "Loss for batch is  3.0652472972869873\n",
      "Loss for batch is  2.840573310852051\n",
      "Loss for batch is  3.114352226257324\n",
      "Loss for batch is  1.5564686059951782\n",
      "|Iter  252  | Total Train Loss  10.57664144039154 |\n",
      "Val Loss for batch is  -0.2012150138616562\n",
      "Val Loss for batch is  -0.3705846667289734\n",
      "Val Loss for batch is  0.03016749769449234\n",
      "Val Loss for batch is  -0.7850470542907715\n",
      "|Iter  252  | Total Val Loss  -1.3266792371869087 |\n",
      "Loss for batch is  2.973651170730591\n",
      "Loss for batch is  2.8040289878845215\n",
      "Loss for batch is  3.114443302154541\n",
      "Loss for batch is  1.5158780813217163\n",
      "|Iter  253  | Total Train Loss  10.40800154209137 |\n",
      "Val Loss for batch is  -0.10589313507080078\n",
      "Val Loss for batch is  -0.35440748929977417\n",
      "Val Loss for batch is  -0.051216840744018555\n",
      "Val Loss for batch is  -0.9083946943283081\n",
      "|Iter  253  | Total Val Loss  -1.4199121594429016 |\n",
      "Loss for batch is  3.04141902923584\n",
      "Loss for batch is  2.7717316150665283\n",
      "Loss for batch is  3.1157755851745605\n",
      "Loss for batch is  1.5306217670440674\n",
      "|Iter  254  | Total Train Loss  10.459547996520996 |\n",
      "Val Loss for batch is  -0.19739072024822235\n",
      "Val Loss for batch is  -0.28229820728302\n",
      "Val Loss for batch is  -0.027670875191688538\n",
      "Val Loss for batch is  -0.8404963612556458\n",
      "|Iter  254  | Total Val Loss  -1.3478561639785767 |\n",
      "Loss for batch is  3.0379271507263184\n",
      "Loss for batch is  2.824369192123413\n",
      "Loss for batch is  3.121739149093628\n",
      "Loss for batch is  1.517139196395874\n",
      "|Iter  255  | Total Train Loss  10.501174688339233 |\n",
      "Val Loss for batch is  -0.16699054837226868\n",
      "Val Loss for batch is  -0.3737293779850006\n",
      "Val Loss for batch is  -0.043567582964897156\n",
      "Val Loss for batch is  -0.8747099041938782\n",
      "|Iter  255  | Total Val Loss  -1.4589974135160446 |\n",
      "Loss for batch is  2.98453426361084\n",
      "Loss for batch is  2.7183914184570312\n",
      "Loss for batch is  3.07371187210083\n",
      "Loss for batch is  1.4785043001174927\n",
      "|Iter  256  | Total Train Loss  10.255141854286194 |\n",
      "Val Loss for batch is  -0.10023297369480133\n",
      "Val Loss for batch is  -0.33072564005851746\n",
      "Val Loss for batch is  -0.001732170581817627\n",
      "Val Loss for batch is  -1.0196278095245361\n",
      "|Iter  256  | Total Val Loss  -1.4523185938596725 |\n",
      "Loss for batch is  2.993621349334717\n",
      "Loss for batch is  2.7661261558532715\n",
      "Loss for batch is  3.0477733612060547\n",
      "Loss for batch is  1.4754302501678467\n",
      "|Iter  257  | Total Train Loss  10.28295111656189 |\n",
      "Val Loss for batch is  -0.2293979376554489\n",
      "Val Loss for batch is  -0.3777562975883484\n",
      "Val Loss for batch is  -0.08236846327781677\n",
      "Val Loss for batch is  -0.9447389841079712\n",
      "|Iter  257  | Total Val Loss  -1.6342616826295853 |\n",
      "Loss for batch is  2.9803595542907715\n",
      "Loss for batch is  2.7394261360168457\n",
      "Loss for batch is  3.0478339195251465\n",
      "Loss for batch is  1.4840617179870605\n",
      "|Iter  258  | Total Train Loss  10.251681327819824 |\n",
      "Val Loss for batch is  -0.16760912537574768\n",
      "Val Loss for batch is  -0.36810874938964844\n",
      "Val Loss for batch is  -0.09409096837043762\n",
      "Val Loss for batch is  -0.985879123210907\n",
      "|Iter  258  | Total Val Loss  -1.6156879663467407 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  2.934413194656372\n",
      "Loss for batch is  2.7236239910125732\n",
      "Loss for batch is  3.047713279724121\n",
      "Loss for batch is  1.4359647035598755\n",
      "|Iter  259  | Total Train Loss  10.141715168952942 |\n",
      "Val Loss for batch is  -0.27173173427581787\n",
      "Val Loss for batch is  -0.33118385076522827\n",
      "Val Loss for batch is  -0.08405019342899323\n",
      "Val Loss for batch is  -0.949266254901886\n",
      "|Iter  259  | Total Val Loss  -1.6362320333719254 |\n",
      "Loss for batch is  2.9365718364715576\n",
      "Loss for batch is  2.7284374237060547\n",
      "Loss for batch is  3.029694080352783\n",
      "Loss for batch is  1.3751187324523926\n",
      "|Iter  260  | Total Train Loss  10.069822072982788 |\n",
      "Val Loss for batch is  -0.27697694301605225\n",
      "Val Loss for batch is  -0.43327295780181885\n",
      "Val Loss for batch is  -0.19781433045864105\n",
      "Val Loss for batch is  -0.9948769807815552\n",
      "|Iter  260  | Total Val Loss  -1.9029412120580673 |\n",
      "Loss for batch is  2.922102928161621\n",
      "Loss for batch is  2.720583915710449\n",
      "Loss for batch is  3.040755271911621\n",
      "Loss for batch is  1.4235339164733887\n",
      "|Iter  261  | Total Train Loss  10.10697603225708 |\n",
      "Val Loss for batch is  -0.2692086696624756\n",
      "Val Loss for batch is  -0.25286129117012024\n",
      "Val Loss for batch is  -0.20769470930099487\n",
      "Val Loss for batch is  -0.9708276391029358\n",
      "|Iter  261  | Total Val Loss  -1.7005923092365265 |\n",
      "Loss for batch is  2.85446834564209\n",
      "Loss for batch is  2.6698405742645264\n",
      "Loss for batch is  2.99251651763916\n",
      "Loss for batch is  1.3701260089874268\n",
      "|Iter  262  | Total Train Loss  9.886951446533203 |\n",
      "Val Loss for batch is  -0.28872668743133545\n",
      "Val Loss for batch is  -0.38446009159088135\n",
      "Val Loss for batch is  -0.02350004017353058\n",
      "Val Loss for batch is  -0.993634045124054\n",
      "|Iter  262  | Total Val Loss  -1.6903208643198013 |\n",
      "Loss for batch is  2.8704991340637207\n",
      "Loss for batch is  2.6897659301757812\n",
      "Loss for batch is  2.9833431243896484\n",
      "Loss for batch is  1.3941856622695923\n",
      "|Iter  263  | Total Train Loss  9.937793850898743 |\n",
      "Val Loss for batch is  -0.2708529531955719\n",
      "Val Loss for batch is  -0.3023509681224823\n",
      "Val Loss for batch is  -0.1996404081583023\n",
      "Val Loss for batch is  -0.9778590202331543\n",
      "|Iter  263  | Total Val Loss  -1.7507033497095108 |\n",
      "Loss for batch is  2.8863577842712402\n",
      "Loss for batch is  2.6703732013702393\n",
      "Loss for batch is  2.959601402282715\n",
      "Loss for batch is  1.363450288772583\n",
      "|Iter  264  | Total Train Loss  9.879782676696777 |\n",
      "Val Loss for batch is  -0.20302847027778625\n",
      "Val Loss for batch is  -0.37447482347488403\n",
      "Val Loss for batch is  -0.17909552156925201\n",
      "Val Loss for batch is  -1.0518763065338135\n",
      "|Iter  264  | Total Val Loss  -1.8084751218557358 |\n",
      "Loss for batch is  2.8625638484954834\n",
      "Loss for batch is  2.6510698795318604\n",
      "Loss for batch is  2.9868717193603516\n",
      "Loss for batch is  1.3690128326416016\n",
      "|Iter  265  | Total Train Loss  9.869518280029297 |\n",
      "Val Loss for batch is  -0.2922270894050598\n",
      "Val Loss for batch is  -0.424389123916626\n",
      "Val Loss for batch is  -0.14143973588943481\n",
      "Val Loss for batch is  -0.9737771153450012\n",
      "|Iter  265  | Total Val Loss  -1.8318330645561218 |\n",
      "Loss for batch is  2.8529274463653564\n",
      "Loss for batch is  2.637500286102295\n",
      "Loss for batch is  2.941544532775879\n",
      "Loss for batch is  1.3948471546173096\n",
      "|Iter  266  | Total Train Loss  9.82681941986084 |\n",
      "Val Loss for batch is  -0.27614328265190125\n",
      "Val Loss for batch is  -0.40814563632011414\n",
      "Val Loss for batch is  -0.04284893721342087\n",
      "Val Loss for batch is  -1.0462493896484375\n",
      "|Iter  266  | Total Val Loss  -1.7733872458338737 |\n",
      "Loss for batch is  2.856235980987549\n",
      "Loss for batch is  2.6623146533966064\n",
      "Loss for batch is  2.9196934700012207\n",
      "Loss for batch is  1.326981782913208\n",
      "|Iter  267  | Total Train Loss  9.765225887298584 |\n",
      "Val Loss for batch is  -0.31030410528182983\n",
      "Val Loss for batch is  -0.4949660301208496\n",
      "Val Loss for batch is  -0.17639601230621338\n",
      "Val Loss for batch is  -0.931327760219574\n",
      "|Iter  267  | Total Val Loss  -1.9129939079284668 |\n",
      "Loss for batch is  2.813772201538086\n",
      "Loss for batch is  2.654629945755005\n",
      "Loss for batch is  2.889981508255005\n",
      "Loss for batch is  1.3150973320007324\n",
      "|Iter  268  | Total Train Loss  9.673480987548828 |\n",
      "Val Loss for batch is  -0.20387782156467438\n",
      "Val Loss for batch is  -0.4981932044029236\n",
      "Val Loss for batch is  -0.1131741851568222\n",
      "Val Loss for batch is  -1.1877540349960327\n",
      "|Iter  268  | Total Val Loss  -2.002999246120453 |\n",
      "Loss for batch is  2.8124818801879883\n",
      "Loss for batch is  2.5551886558532715\n",
      "Loss for batch is  2.9015798568725586\n",
      "Loss for batch is  1.3517346382141113\n",
      "|Iter  269  | Total Train Loss  9.62098503112793 |\n",
      "Val Loss for batch is  -0.22360806167125702\n",
      "Val Loss for batch is  -0.426568865776062\n",
      "Val Loss for batch is  -0.27128756046295166\n",
      "Val Loss for batch is  -1.104393482208252\n",
      "|Iter  269  | Total Val Loss  -2.0258579701185226 |\n",
      "Loss for batch is  2.778660535812378\n",
      "Loss for batch is  2.561788558959961\n",
      "Loss for batch is  2.9007081985473633\n",
      "Loss for batch is  1.3002192974090576\n",
      "|Iter  270  | Total Train Loss  9.54137659072876 |\n",
      "Val Loss for batch is  -0.2673797309398651\n",
      "Val Loss for batch is  -0.4526025652885437\n",
      "Val Loss for batch is  -0.1776120960712433\n",
      "Val Loss for batch is  -1.1153438091278076\n",
      "|Iter  270  | Total Val Loss  -2.0129382014274597 |\n",
      "Loss for batch is  2.8402724266052246\n",
      "Loss for batch is  2.579721212387085\n",
      "Loss for batch is  2.8931374549865723\n",
      "Loss for batch is  1.3478829860687256\n",
      "|Iter  271  | Total Train Loss  9.661014080047607 |\n",
      "Val Loss for batch is  -0.19394712150096893\n",
      "Val Loss for batch is  -0.35497766733169556\n",
      "Val Loss for batch is  -0.23599955439567566\n",
      "Val Loss for batch is  -1.0688295364379883\n",
      "|Iter  271  | Total Val Loss  -1.8537538796663284 |\n",
      "Loss for batch is  2.9056406021118164\n",
      "Loss for batch is  2.5942044258117676\n",
      "Loss for batch is  3.0638370513916016\n",
      "Loss for batch is  1.2936458587646484\n",
      "|Iter  272  | Total Train Loss  9.857327938079834 |\n",
      "Val Loss for batch is  -0.16199442744255066\n",
      "Val Loss for batch is  -0.33193114399909973\n",
      "Val Loss for batch is  0.005233272910118103\n",
      "Val Loss for batch is  -1.021972894668579\n",
      "|Iter  272  | Total Val Loss  -1.5106651932001114 |\n",
      "Loss for batch is  2.890066146850586\n",
      "Loss for batch is  2.5832083225250244\n",
      "Loss for batch is  3.0373549461364746\n",
      "Loss for batch is  1.2918142080307007\n",
      "|Iter  273  | Total Train Loss  9.802443623542786 |\n",
      "Val Loss for batch is  -0.1957446038722992\n",
      "Val Loss for batch is  -0.4713492691516876\n",
      "Val Loss for batch is  -0.09855954349040985\n",
      "Val Loss for batch is  -1.0930688381195068\n",
      "|Iter  273  | Total Val Loss  -1.8587222546339035 |\n",
      "Loss for batch is  2.9372782707214355\n",
      "Loss for batch is  2.5329113006591797\n",
      "Loss for batch is  2.995290756225586\n",
      "Loss for batch is  1.2711131572723389\n",
      "|Iter  274  | Total Train Loss  9.73659348487854 |\n",
      "Val Loss for batch is  -0.2445889413356781\n",
      "Val Loss for batch is  -0.40580350160598755\n",
      "Val Loss for batch is  -0.10861138999462128\n",
      "Val Loss for batch is  -1.1333348751068115\n",
      "|Iter  274  | Total Val Loss  -1.8923387080430984 |\n",
      "Loss for batch is  2.839411973953247\n",
      "Loss for batch is  2.535804510116577\n",
      "Loss for batch is  3.0121047496795654\n",
      "Loss for batch is  1.2659351825714111\n",
      "|Iter  275  | Total Train Loss  9.6532564163208 |\n",
      "Val Loss for batch is  -0.296845942735672\n",
      "Val Loss for batch is  -0.4938164949417114\n",
      "Val Loss for batch is  -0.1257069706916809\n",
      "Val Loss for batch is  -1.0468339920043945\n",
      "|Iter  275  | Total Val Loss  -1.9632034003734589 |\n",
      "Loss for batch is  2.8526840209960938\n",
      "Loss for batch is  2.473111152648926\n",
      "Loss for batch is  2.931283712387085\n",
      "Loss for batch is  1.272670030593872\n",
      "|Iter  276  | Total Train Loss  9.529748916625977 |\n",
      "Val Loss for batch is  -0.35408759117126465\n",
      "Val Loss for batch is  -0.3884199559688568\n",
      "Val Loss for batch is  -0.12860888242721558\n",
      "Val Loss for batch is  -1.086767554283142\n",
      "|Iter  276  | Total Val Loss  -1.9578839838504791 |\n",
      "Loss for batch is  2.818676710128784\n",
      "Loss for batch is  2.5366172790527344\n",
      "Loss for batch is  2.8443076610565186\n",
      "Loss for batch is  1.2913705110549927\n",
      "|Iter  277  | Total Train Loss  9.49097216129303 |\n",
      "Val Loss for batch is  -0.31553488969802856\n",
      "Val Loss for batch is  -0.5905558466911316\n",
      "Val Loss for batch is  -0.19431692361831665\n",
      "Val Loss for batch is  -0.9459770321846008\n",
      "|Iter  277  | Total Val Loss  -2.0463846921920776 |\n",
      "Loss for batch is  2.7682290077209473\n",
      "Loss for batch is  2.505995750427246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  2.872565269470215\n",
      "Loss for batch is  1.2300257682800293\n",
      "|Iter  278  | Total Train Loss  9.376815795898438 |\n",
      "Val Loss for batch is  -0.2660784125328064\n",
      "Val Loss for batch is  -0.4190947413444519\n",
      "Val Loss for batch is  -0.12288857996463776\n",
      "Val Loss for batch is  -1.0022342205047607\n",
      "|Iter  278  | Total Val Loss  -1.8102959543466568 |\n",
      "Loss for batch is  2.743529796600342\n",
      "Loss for batch is  2.536435604095459\n",
      "Loss for batch is  2.851609230041504\n",
      "Loss for batch is  1.3103971481323242\n",
      "|Iter  279  | Total Train Loss  9.441971778869629 |\n",
      "Val Loss for batch is  -0.34405380487442017\n",
      "Val Loss for batch is  -0.5621267557144165\n",
      "Val Loss for batch is  -0.22827847301959991\n",
      "Val Loss for batch is  -1.1786547899246216\n",
      "|Iter  279  | Total Val Loss  -2.313113823533058 |\n",
      "Loss for batch is  2.7287516593933105\n",
      "Loss for batch is  2.472703456878662\n",
      "Loss for batch is  2.828641891479492\n",
      "Loss for batch is  1.2482876777648926\n",
      "|Iter  280  | Total Train Loss  9.278384685516357 |\n",
      "Val Loss for batch is  -0.30949482321739197\n",
      "Val Loss for batch is  -0.2748279273509979\n",
      "Val Loss for batch is  -0.21625705063343048\n",
      "Val Loss for batch is  -1.1193976402282715\n",
      "|Iter  280  | Total Val Loss  -1.9199774414300919 |\n",
      "Loss for batch is  2.6971497535705566\n",
      "Loss for batch is  2.4653313159942627\n",
      "Loss for batch is  2.830923557281494\n",
      "Loss for batch is  1.199483036994934\n",
      "|Iter  281  | Total Train Loss  9.192887663841248 |\n",
      "Val Loss for batch is  -0.30194389820098877\n",
      "Val Loss for batch is  -0.6194590926170349\n",
      "Val Loss for batch is  -0.26199233531951904\n",
      "Val Loss for batch is  -1.1421645879745483\n",
      "|Iter  281  | Total Val Loss  -2.325559914112091 |\n",
      "Loss for batch is  2.683027744293213\n",
      "Loss for batch is  2.4671359062194824\n",
      "Loss for batch is  2.7680823802948\n",
      "Loss for batch is  1.2246344089508057\n",
      "|Iter  282  | Total Train Loss  9.1428804397583 |\n",
      "Val Loss for batch is  -0.2102307230234146\n",
      "Val Loss for batch is  -0.37098851799964905\n",
      "Val Loss for batch is  -0.1634194701910019\n",
      "Val Loss for batch is  -1.0651366710662842\n",
      "|Iter  282  | Total Val Loss  -1.8097753822803497 |\n",
      "Loss for batch is  2.6909356117248535\n",
      "Loss for batch is  2.4646689891815186\n",
      "Loss for batch is  2.7494964599609375\n",
      "Loss for batch is  1.2051414251327515\n",
      "|Iter  283  | Total Train Loss  9.110242486000061 |\n",
      "Val Loss for batch is  -0.3222825527191162\n",
      "Val Loss for batch is  -0.5901392102241516\n",
      "Val Loss for batch is  -0.2951136529445648\n",
      "Val Loss for batch is  -0.9889098405838013\n",
      "|Iter  283  | Total Val Loss  -2.196445256471634 |\n",
      "Loss for batch is  2.6482467651367188\n",
      "Loss for batch is  2.4496445655822754\n",
      "Loss for batch is  2.7752318382263184\n",
      "Loss for batch is  1.2358238697052002\n",
      "|Iter  284  | Total Train Loss  9.108947038650513 |\n",
      "Val Loss for batch is  -0.3708399534225464\n",
      "Val Loss for batch is  -0.4852559566497803\n",
      "Val Loss for batch is  -0.2595055103302002\n",
      "Val Loss for batch is  -1.140499234199524\n",
      "|Iter  284  | Total Val Loss  -2.256100654602051 |\n",
      "Loss for batch is  2.679382085800171\n",
      "Loss for batch is  2.4342525005340576\n",
      "Loss for batch is  2.735774040222168\n",
      "Loss for batch is  1.1805691719055176\n",
      "|Iter  285  | Total Train Loss  9.029977798461914 |\n",
      "Val Loss for batch is  -0.32408609986305237\n",
      "Val Loss for batch is  -0.5035673975944519\n",
      "Val Loss for batch is  -0.26072582602500916\n",
      "Val Loss for batch is  -1.1741092205047607\n",
      "|Iter  285  | Total Val Loss  -2.262488543987274 |\n",
      "Loss for batch is  2.6441338062286377\n",
      "Loss for batch is  2.4105658531188965\n",
      "Loss for batch is  2.7429347038269043\n",
      "Loss for batch is  1.1688518524169922\n",
      "|Iter  286  | Total Train Loss  8.96648621559143 |\n",
      "Val Loss for batch is  -0.40610644221305847\n",
      "Val Loss for batch is  -0.40136581659317017\n",
      "Val Loss for batch is  -0.22760915756225586\n",
      "Val Loss for batch is  -1.14536452293396\n",
      "|Iter  286  | Total Val Loss  -2.1804459393024445 |\n",
      "Loss for batch is  2.6591038703918457\n",
      "Loss for batch is  2.4297189712524414\n",
      "Loss for batch is  2.696939468383789\n",
      "Loss for batch is  1.132914662361145\n",
      "|Iter  287  | Total Train Loss  8.918676972389221 |\n",
      "Val Loss for batch is  -0.31707829236984253\n",
      "Val Loss for batch is  -0.5516796112060547\n",
      "Val Loss for batch is  -0.1553489863872528\n",
      "Val Loss for batch is  -1.1283220052719116\n",
      "|Iter  287  | Total Val Loss  -2.1524288952350616 |\n",
      "Loss for batch is  2.5834991931915283\n",
      "Loss for batch is  2.399176597595215\n",
      "Loss for batch is  2.696716547012329\n",
      "Loss for batch is  1.1504210233688354\n",
      "|Iter  288  | Total Train Loss  8.829813361167908 |\n",
      "Val Loss for batch is  -0.4705020785331726\n",
      "Val Loss for batch is  -0.4592210650444031\n",
      "Val Loss for batch is  -0.17943175137043\n",
      "Val Loss for batch is  -1.0478324890136719\n",
      "|Iter  288  | Total Val Loss  -2.1569873839616776 |\n",
      "Loss for batch is  2.648817539215088\n",
      "Loss for batch is  2.356104850769043\n",
      "Loss for batch is  2.684567928314209\n",
      "Loss for batch is  1.1646816730499268\n",
      "|Iter  289  | Total Train Loss  8.854171991348267 |\n",
      "Val Loss for batch is  -0.435427725315094\n",
      "Val Loss for batch is  -0.5711300373077393\n",
      "Val Loss for batch is  -0.13892218470573425\n",
      "Val Loss for batch is  -1.20122230052948\n",
      "|Iter  289  | Total Val Loss  -2.3467022478580475 |\n",
      "Loss for batch is  2.5727121829986572\n",
      "Loss for batch is  2.35162091255188\n",
      "Loss for batch is  2.708780288696289\n",
      "Loss for batch is  1.1300181150436401\n",
      "|Iter  290  | Total Train Loss  8.763131499290466 |\n",
      "Val Loss for batch is  -0.44124898314476013\n",
      "Val Loss for batch is  -0.47550326585769653\n",
      "Val Loss for batch is  -0.2837636470794678\n",
      "Val Loss for batch is  -1.2266836166381836\n",
      "|Iter  290  | Total Val Loss  -2.427199512720108 |\n",
      "Loss for batch is  2.586005687713623\n",
      "Loss for batch is  2.289370059967041\n",
      "Loss for batch is  2.6882760524749756\n",
      "Loss for batch is  1.1444921493530273\n",
      "|Iter  291  | Total Train Loss  8.708143949508667 |\n",
      "Val Loss for batch is  -0.3569222688674927\n",
      "Val Loss for batch is  -0.5366498231887817\n",
      "Val Loss for batch is  -0.3539123833179474\n",
      "Val Loss for batch is  -1.1430829763412476\n",
      "|Iter  291  | Total Val Loss  -2.3905674517154694 |\n",
      "Loss for batch is  2.538695812225342\n",
      "Loss for batch is  2.3747010231018066\n",
      "Loss for batch is  2.700378894805908\n",
      "Loss for batch is  1.1546112298965454\n",
      "|Iter  292  | Total Train Loss  8.768386960029602 |\n",
      "Val Loss for batch is  -0.39013221859931946\n",
      "Val Loss for batch is  -0.5496633052825928\n",
      "Val Loss for batch is  -0.30843105912208557\n",
      "Val Loss for batch is  -1.2547712326049805\n",
      "|Iter  292  | Total Val Loss  -2.5029978156089783 |\n",
      "Loss for batch is  2.6051971912384033\n",
      "Loss for batch is  2.365889310836792\n",
      "Loss for batch is  2.6504147052764893\n",
      "Loss for batch is  1.0687305927276611\n",
      "|Iter  293  | Total Train Loss  8.690231800079346 |\n",
      "Val Loss for batch is  -0.13769084215164185\n",
      "Val Loss for batch is  -0.4089060425758362\n",
      "Val Loss for batch is  -0.200602188706398\n",
      "Val Loss for batch is  -1.246658205986023\n",
      "|Iter  293  | Total Val Loss  -1.993857279419899 |\n",
      "Loss for batch is  2.5572144985198975\n",
      "Loss for batch is  2.3696858882904053\n",
      "Loss for batch is  2.63370418548584\n",
      "Loss for batch is  1.0594635009765625\n",
      "|Iter  294  | Total Train Loss  8.620068073272705 |\n",
      "Val Loss for batch is  -0.39096319675445557\n",
      "Val Loss for batch is  -0.6257200241088867\n",
      "Val Loss for batch is  -0.34011244773864746\n",
      "Val Loss for batch is  -1.1792869567871094\n",
      "|Iter  294  | Total Val Loss  -2.536082625389099 |\n",
      "Loss for batch is  2.550652027130127\n",
      "Loss for batch is  2.3401198387145996\n",
      "Loss for batch is  2.648912191390991\n",
      "Loss for batch is  1.092710018157959\n",
      "|Iter  295  | Total Train Loss  8.632394075393677 |\n",
      "Val Loss for batch is  -0.38662445545196533\n",
      "Val Loss for batch is  -0.5763223171234131\n",
      "Val Loss for batch is  -0.1850198209285736\n",
      "Val Loss for batch is  -1.1900432109832764\n",
      "|Iter  295  | Total Val Loss  -2.3380098044872284 |\n",
      "Loss for batch is  2.5098822116851807\n",
      "Loss for batch is  2.293134927749634\n",
      "Loss for batch is  2.6193013191223145\n",
      "Loss for batch is  1.1033895015716553\n",
      "|Iter  296  | Total Train Loss  8.525707960128784 |\n",
      "Val Loss for batch is  -0.4667421579360962\n",
      "Val Loss for batch is  -0.6110569834709167\n",
      "Val Loss for batch is  -0.33399343490600586\n",
      "Val Loss for batch is  -1.2705074548721313\n",
      "|Iter  296  | Total Val Loss  -2.68230003118515 |\n",
      "Loss for batch is  2.5398833751678467\n",
      "Loss for batch is  2.3097095489501953\n",
      "Loss for batch is  2.6088545322418213\n",
      "Loss for batch is  1.0543243885040283\n",
      "|Iter  297  | Total Train Loss  8.512771844863892 |\n",
      "Val Loss for batch is  -0.4395563304424286\n",
      "Val Loss for batch is  -0.43740612268447876\n",
      "Val Loss for batch is  -0.25390270352363586\n",
      "Val Loss for batch is  -1.045943260192871\n",
      "|Iter  297  | Total Val Loss  -2.1768084168434143 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  2.4759888648986816\n",
      "Loss for batch is  2.291393756866455\n",
      "Loss for batch is  2.5877315998077393\n",
      "Loss for batch is  1.0489685535430908\n",
      "|Iter  298  | Total Train Loss  8.404082775115967 |\n",
      "Val Loss for batch is  -0.4119085669517517\n",
      "Val Loss for batch is  -0.5942001938819885\n",
      "Val Loss for batch is  -0.33596542477607727\n",
      "Val Loss for batch is  -1.2901986837387085\n",
      "|Iter  298  | Total Val Loss  -2.632272869348526 |\n",
      "Loss for batch is  2.535376787185669\n",
      "Loss for batch is  2.257493019104004\n",
      "Loss for batch is  2.5427563190460205\n",
      "Loss for batch is  1.0985859632492065\n",
      "|Iter  299  | Total Train Loss  8.4342120885849 |\n",
      "Val Loss for batch is  -0.4466122090816498\n",
      "Val Loss for batch is  -0.6336898803710938\n",
      "Val Loss for batch is  -0.2304363250732422\n",
      "Val Loss for batch is  -1.2191557884216309\n",
      "|Iter  299  | Total Val Loss  -2.5298942029476166 |\n",
      "Loss for batch is  2.4753189086914062\n",
      "Loss for batch is  2.256542444229126\n",
      "Loss for batch is  2.5248098373413086\n",
      "Loss for batch is  1.0637325048446655\n",
      "|Iter  300  | Total Train Loss  8.320403695106506 |\n",
      "Val Loss for batch is  -0.4844796657562256\n",
      "Val Loss for batch is  -0.6845025420188904\n",
      "Val Loss for batch is  -0.26516541838645935\n",
      "Val Loss for batch is  -1.2555394172668457\n",
      "|Iter  300  | Total Val Loss  -2.689687043428421 |\n",
      "Loss for batch is  2.4249167442321777\n",
      "Loss for batch is  2.2394497394561768\n",
      "Loss for batch is  2.5437402725219727\n",
      "Loss for batch is  1.0636132955551147\n",
      "|Iter  301  | Total Train Loss  8.271720051765442 |\n",
      "Val Loss for batch is  -0.4969826340675354\n",
      "Val Loss for batch is  -0.5540484189987183\n",
      "Val Loss for batch is  -0.1863797903060913\n",
      "Val Loss for batch is  -1.2830393314361572\n",
      "|Iter  301  | Total Val Loss  -2.520450174808502 |\n",
      "Loss for batch is  2.4372379779815674\n",
      "Loss for batch is  2.251138925552368\n",
      "Loss for batch is  2.519786834716797\n",
      "Loss for batch is  0.9973478317260742\n",
      "|Iter  302  | Total Train Loss  8.205511569976807 |\n",
      "Val Loss for batch is  -0.33721792697906494\n",
      "Val Loss for batch is  -0.5537316799163818\n",
      "Val Loss for batch is  -0.22499194741249084\n",
      "Val Loss for batch is  -1.1984314918518066\n",
      "|Iter  302  | Total Val Loss  -2.3143730461597443 |\n",
      "Loss for batch is  2.399284839630127\n",
      "Loss for batch is  2.2144722938537598\n",
      "Loss for batch is  2.548867702484131\n",
      "Loss for batch is  0.9852837324142456\n",
      "|Iter  303  | Total Train Loss  8.147908568382263 |\n",
      "Val Loss for batch is  -0.38688310980796814\n",
      "Val Loss for batch is  -0.5896478891372681\n",
      "Val Loss for batch is  -0.4150616526603699\n",
      "Val Loss for batch is  -1.2163894176483154\n",
      "|Iter  303  | Total Val Loss  -2.6079820692539215 |\n",
      "Loss for batch is  2.4779937267303467\n",
      "Loss for batch is  2.2143237590789795\n",
      "Loss for batch is  2.4885294437408447\n",
      "Loss for batch is  1.0452715158462524\n",
      "|Iter  304  | Total Train Loss  8.226118445396423 |\n",
      "Val Loss for batch is  -0.49946606159210205\n",
      "Val Loss for batch is  -0.7004571557044983\n",
      "Val Loss for batch is  -0.3095955550670624\n",
      "Val Loss for batch is  -1.158908486366272\n",
      "|Iter  304  | Total Val Loss  -2.6684272587299347 |\n",
      "Loss for batch is  2.391103982925415\n",
      "Loss for batch is  2.2133123874664307\n",
      "Loss for batch is  2.5273046493530273\n",
      "Loss for batch is  1.0339207649230957\n",
      "|Iter  305  | Total Train Loss  8.165641784667969 |\n",
      "Val Loss for batch is  -0.49687469005584717\n",
      "Val Loss for batch is  -0.672351598739624\n",
      "Val Loss for batch is  -0.3424708843231201\n",
      "Val Loss for batch is  -1.2608411312103271\n",
      "|Iter  305  | Total Val Loss  -2.7725383043289185 |\n",
      "Loss for batch is  2.406470537185669\n",
      "Loss for batch is  2.2096784114837646\n",
      "Loss for batch is  2.464468002319336\n",
      "Loss for batch is  0.9674813747406006\n",
      "|Iter  306  | Total Train Loss  8.04809832572937 |\n",
      "Val Loss for batch is  -0.43014252185821533\n",
      "Val Loss for batch is  -0.47890394926071167\n",
      "Val Loss for batch is  -0.38351747393608093\n",
      "Val Loss for batch is  -1.3117940425872803\n",
      "|Iter  306  | Total Val Loss  -2.604357987642288 |\n",
      "Loss for batch is  2.3842368125915527\n",
      "Loss for batch is  2.190408706665039\n",
      "Loss for batch is  2.479951858520508\n",
      "Loss for batch is  0.989759087562561\n",
      "|Iter  307  | Total Train Loss  8.04435646533966 |\n",
      "Val Loss for batch is  -0.5550704002380371\n",
      "Val Loss for batch is  -0.7082067131996155\n",
      "Val Loss for batch is  -0.31370675563812256\n",
      "Val Loss for batch is  -1.315352439880371\n",
      "|Iter  307  | Total Val Loss  -2.8923363089561462 |\n",
      "Loss for batch is  2.370192527770996\n",
      "Loss for batch is  2.1352319717407227\n",
      "Loss for batch is  2.4659109115600586\n",
      "Loss for batch is  0.9911985993385315\n",
      "|Iter  308  | Total Train Loss  7.962534010410309 |\n",
      "Val Loss for batch is  -0.4314383566379547\n",
      "Val Loss for batch is  -0.5738760232925415\n",
      "Val Loss for batch is  -0.24286606907844543\n",
      "Val Loss for batch is  -1.1738908290863037\n",
      "|Iter  308  | Total Val Loss  -2.4220712780952454 |\n",
      "Loss for batch is  2.3585097789764404\n",
      "Loss for batch is  2.240922451019287\n",
      "Loss for batch is  2.4724364280700684\n",
      "Loss for batch is  0.9534019231796265\n",
      "|Iter  309  | Total Train Loss  8.025270581245422 |\n",
      "Val Loss for batch is  -0.5246897339820862\n",
      "Val Loss for batch is  -0.5533576011657715\n",
      "Val Loss for batch is  -0.3456744849681854\n",
      "Val Loss for batch is  -1.3479658365249634\n",
      "|Iter  309  | Total Val Loss  -2.7716876566410065 |\n",
      "Loss for batch is  2.4432950019836426\n",
      "Loss for batch is  2.1427810192108154\n",
      "Loss for batch is  2.470196008682251\n",
      "Loss for batch is  0.9277988076210022\n",
      "|Iter  310  | Total Train Loss  7.984070837497711 |\n",
      "Val Loss for batch is  -0.46334946155548096\n",
      "Val Loss for batch is  -0.6147643327713013\n",
      "Val Loss for batch is  -0.34150421619415283\n",
      "Val Loss for batch is  -1.211467981338501\n",
      "|Iter  310  | Total Val Loss  -2.631085991859436 |\n",
      "Loss for batch is  2.3569321632385254\n",
      "Loss for batch is  2.160942554473877\n",
      "Loss for batch is  2.4726879596710205\n",
      "Loss for batch is  0.948852002620697\n",
      "|Iter  311  | Total Train Loss  7.93941468000412 |\n",
      "Val Loss for batch is  -0.5157899856567383\n",
      "Val Loss for batch is  -0.5463902354240417\n",
      "Val Loss for batch is  -0.41438573598861694\n",
      "Val Loss for batch is  -1.2988766431808472\n",
      "|Iter  311  | Total Val Loss  -2.775442600250244 |\n",
      "Loss for batch is  2.422276496887207\n",
      "Loss for batch is  2.181736946105957\n",
      "Loss for batch is  2.467256546020508\n",
      "Loss for batch is  0.9318886399269104\n",
      "|Iter  312  | Total Train Loss  8.003158628940582 |\n",
      "Val Loss for batch is  -0.34848707914352417\n",
      "Val Loss for batch is  -0.4572838246822357\n",
      "Val Loss for batch is  -0.28261685371398926\n",
      "Val Loss for batch is  -1.2093156576156616\n",
      "|Iter  312  | Total Val Loss  -2.2977034151554108 |\n",
      "Loss for batch is  2.426576852798462\n",
      "Loss for batch is  2.2312254905700684\n",
      "Loss for batch is  2.4921038150787354\n",
      "Loss for batch is  0.903755247592926\n",
      "|Iter  313  | Total Train Loss  8.053661406040192 |\n",
      "Val Loss for batch is  -0.5041492581367493\n",
      "Val Loss for batch is  -0.7211123108863831\n",
      "Val Loss for batch is  -0.3273262679576874\n",
      "Val Loss for batch is  -1.3821218013763428\n",
      "|Iter  313  | Total Val Loss  -2.9347096383571625 |\n",
      "Loss for batch is  2.413383722305298\n",
      "Loss for batch is  2.125598907470703\n",
      "Loss for batch is  2.550060272216797\n",
      "Loss for batch is  0.960214376449585\n",
      "|Iter  314  | Total Train Loss  8.049257278442383 |\n",
      "Val Loss for batch is  -0.47773098945617676\n",
      "Val Loss for batch is  -0.6899771690368652\n",
      "Val Loss for batch is  -0.3431243896484375\n",
      "Val Loss for batch is  -1.3039915561676025\n",
      "|Iter  314  | Total Val Loss  -2.814824104309082 |\n",
      "Loss for batch is  2.4127566814422607\n",
      "Loss for batch is  2.137385368347168\n",
      "Loss for batch is  2.5512051582336426\n",
      "Loss for batch is  0.9119647145271301\n",
      "|Iter  315  | Total Train Loss  8.013311922550201 |\n",
      "Val Loss for batch is  -0.5013309121131897\n",
      "Val Loss for batch is  -0.7226853966712952\n",
      "Val Loss for batch is  -0.316722571849823\n",
      "Val Loss for batch is  -1.3065265417099\n",
      "|Iter  315  | Total Val Loss  -2.8472654223442078 |\n",
      "Loss for batch is  2.410292148590088\n",
      "Loss for batch is  2.1365082263946533\n",
      "Loss for batch is  2.5034337043762207\n",
      "Loss for batch is  0.9718356132507324\n",
      "|Iter  316  | Total Train Loss  8.022069692611694 |\n",
      "Val Loss for batch is  -0.48677176237106323\n",
      "Val Loss for batch is  -0.6618034243583679\n",
      "Val Loss for batch is  -0.257352739572525\n",
      "Val Loss for batch is  -1.3071472644805908\n",
      "|Iter  316  | Total Val Loss  -2.713075190782547 |\n",
      "Loss for batch is  2.3244333267211914\n",
      "Loss for batch is  2.099062204360962\n",
      "Loss for batch is  2.480433225631714\n",
      "Loss for batch is  0.9224584102630615\n",
      "|Iter  317  | Total Train Loss  7.826387166976929 |\n",
      "Val Loss for batch is  -0.6259768009185791\n",
      "Val Loss for batch is  -0.7595125436782837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss for batch is  -0.4282102882862091\n",
      "Val Loss for batch is  -1.4277251958847046\n",
      "|Iter  317  | Total Val Loss  -3.2414248287677765 |\n",
      "Loss for batch is  2.382031202316284\n",
      "Loss for batch is  2.1457290649414062\n",
      "Loss for batch is  2.4600138664245605\n",
      "Loss for batch is  0.9220788478851318\n",
      "|Iter  318  | Total Train Loss  7.909852981567383 |\n",
      "Val Loss for batch is  -0.33445364236831665\n",
      "Val Loss for batch is  -0.6501140594482422\n",
      "Val Loss for batch is  -0.27177566289901733\n",
      "Val Loss for batch is  -1.1926097869873047\n",
      "|Iter  318  | Total Val Loss  -2.448953151702881 |\n",
      "Loss for batch is  2.392193555831909\n",
      "Loss for batch is  2.1170177459716797\n",
      "Loss for batch is  2.508626937866211\n",
      "Loss for batch is  0.863329291343689\n",
      "|Iter  319  | Total Train Loss  7.881167531013489 |\n",
      "Val Loss for batch is  -0.5210549831390381\n",
      "Val Loss for batch is  -0.7211577892303467\n",
      "Val Loss for batch is  -0.41531941294670105\n",
      "Val Loss for batch is  -1.3403338193893433\n",
      "|Iter  319  | Total Val Loss  -2.997866004705429 |\n",
      "Loss for batch is  2.364802360534668\n",
      "Loss for batch is  2.1055917739868164\n",
      "Loss for batch is  2.5286645889282227\n",
      "Loss for batch is  0.9364316463470459\n",
      "|Iter  320  | Total Train Loss  7.935490369796753 |\n",
      "Val Loss for batch is  -0.35401690006256104\n",
      "Val Loss for batch is  -0.6204864382743835\n",
      "Val Loss for batch is  -0.24649032950401306\n",
      "Val Loss for batch is  -1.16921865940094\n",
      "|Iter  320  | Total Val Loss  -2.3902123272418976 |\n",
      "Loss for batch is  2.472763776779175\n",
      "Loss for batch is  2.0724244117736816\n",
      "Loss for batch is  2.513812303543091\n",
      "Loss for batch is  0.899134635925293\n",
      "|Iter  321  | Total Train Loss  7.95813512802124 |\n",
      "Val Loss for batch is  -0.5472873449325562\n",
      "Val Loss for batch is  -0.6804244518280029\n",
      "Val Loss for batch is  -0.24986715614795685\n",
      "Val Loss for batch is  -1.4370108842849731\n",
      "|Iter  321  | Total Val Loss  -2.914589837193489 |\n",
      "Loss for batch is  2.4102022647857666\n",
      "Loss for batch is  2.105116844177246\n",
      "Loss for batch is  2.5070533752441406\n",
      "Loss for batch is  0.9224703311920166\n",
      "|Iter  322  | Total Train Loss  7.94484281539917 |\n",
      "Val Loss for batch is  -0.4952591061592102\n",
      "Val Loss for batch is  -0.5597615242004395\n",
      "Val Loss for batch is  -0.32161885499954224\n",
      "Val Loss for batch is  -1.3241615295410156\n",
      "|Iter  322  | Total Val Loss  -2.7008010149002075 |\n",
      "Loss for batch is  2.399134397506714\n",
      "Loss for batch is  2.0453548431396484\n",
      "Loss for batch is  2.4880356788635254\n",
      "Loss for batch is  0.8524597883224487\n",
      "|Iter  323  | Total Train Loss  7.784984707832336 |\n",
      "Val Loss for batch is  -0.45470622181892395\n",
      "Val Loss for batch is  -0.6876335740089417\n",
      "Val Loss for batch is  -0.31586354970932007\n",
      "Val Loss for batch is  -1.4172744750976562\n",
      "|Iter  323  | Total Val Loss  -2.875477820634842 |\n",
      "Loss for batch is  2.3552892208099365\n",
      "Loss for batch is  2.0284109115600586\n",
      "Loss for batch is  2.406449317932129\n",
      "Loss for batch is  0.8735884428024292\n",
      "|Iter  324  | Total Train Loss  7.663737893104553 |\n",
      "Val Loss for batch is  -0.5435084700584412\n",
      "Val Loss for batch is  -0.6393919587135315\n",
      "Val Loss for batch is  -0.3715454041957855\n",
      "Val Loss for batch is  -1.3753098249435425\n",
      "|Iter  324  | Total Val Loss  -2.9297556579113007 |\n",
      "Loss for batch is  2.3303799629211426\n",
      "Loss for batch is  2.059274196624756\n",
      "Loss for batch is  2.3932387828826904\n",
      "Loss for batch is  0.8848751187324524\n",
      "|Iter  325  | Total Train Loss  7.667768061161041 |\n",
      "Val Loss for batch is  -0.5992156267166138\n",
      "Val Loss for batch is  -0.6941915154457092\n",
      "Val Loss for batch is  -0.43249303102493286\n",
      "Val Loss for batch is  -1.0848848819732666\n",
      "|Iter  325  | Total Val Loss  -2.8107850551605225 |\n",
      "Loss for batch is  2.287991523742676\n",
      "Loss for batch is  2.029411792755127\n",
      "Loss for batch is  2.361361026763916\n",
      "Loss for batch is  0.906692624092102\n",
      "|Iter  326  | Total Train Loss  7.585456967353821 |\n",
      "Val Loss for batch is  -0.5602644681930542\n",
      "Val Loss for batch is  -0.6721160411834717\n",
      "Val Loss for batch is  -0.3224029839038849\n",
      "Val Loss for batch is  -1.4304039478302002\n",
      "|Iter  326  | Total Val Loss  -2.985187441110611 |\n",
      "Loss for batch is  2.2944931983947754\n",
      "Loss for batch is  2.0182554721832275\n",
      "Loss for batch is  2.354515552520752\n",
      "Loss for batch is  0.8109363913536072\n",
      "|Iter  327  | Total Train Loss  7.478200614452362 |\n",
      "Val Loss for batch is  -0.5511986017227173\n",
      "Val Loss for batch is  -0.7032994031906128\n",
      "Val Loss for batch is  -0.33712559938430786\n",
      "Val Loss for batch is  -1.3658334016799927\n",
      "|Iter  327  | Total Val Loss  -2.9574570059776306 |\n",
      "Loss for batch is  2.2767386436462402\n",
      "Loss for batch is  1.9862821102142334\n",
      "Loss for batch is  2.308084487915039\n",
      "Loss for batch is  0.8728475570678711\n",
      "|Iter  328  | Total Train Loss  7.443952798843384 |\n",
      "Val Loss for batch is  -0.57196044921875\n",
      "Val Loss for batch is  -0.7439210414886475\n",
      "Val Loss for batch is  -0.4286400079727173\n",
      "Val Loss for batch is  -1.3032042980194092\n",
      "|Iter  328  | Total Val Loss  -3.047725796699524 |\n",
      "Loss for batch is  2.2444815635681152\n",
      "Loss for batch is  2.0098071098327637\n",
      "Loss for batch is  2.3007941246032715\n",
      "Loss for batch is  0.8449453115463257\n",
      "|Iter  329  | Total Train Loss  7.400028109550476 |\n",
      "Val Loss for batch is  -0.523662269115448\n",
      "Val Loss for batch is  -0.7094317674636841\n",
      "Val Loss for batch is  -0.4174030125141144\n",
      "Val Loss for batch is  -1.4092018604278564\n",
      "|Iter  329  | Total Val Loss  -3.059698909521103 |\n",
      "Loss for batch is  2.2270774841308594\n",
      "Loss for batch is  2.0142526626586914\n",
      "Loss for batch is  2.3298885822296143\n",
      "Loss for batch is  0.7888967990875244\n",
      "|Iter  330  | Total Train Loss  7.3601155281066895 |\n",
      "Val Loss for batch is  -0.5616898536682129\n",
      "Val Loss for batch is  -0.7626086473464966\n",
      "Val Loss for batch is  -0.3948352336883545\n",
      "Val Loss for batch is  -1.4410419464111328\n",
      "|Iter  330  | Total Val Loss  -3.1601756811141968 |\n",
      "Loss for batch is  2.2254509925842285\n",
      "Loss for batch is  1.967059850692749\n",
      "Loss for batch is  2.2901973724365234\n",
      "Loss for batch is  0.8328126072883606\n",
      "|Iter  331  | Total Train Loss  7.315520823001862 |\n",
      "Val Loss for batch is  -0.5318120121955872\n",
      "Val Loss for batch is  -0.652873158454895\n",
      "Val Loss for batch is  -0.44978243112564087\n",
      "Val Loss for batch is  -1.3736459016799927\n",
      "|Iter  331  | Total Val Loss  -3.0081135034561157 |\n",
      "Loss for batch is  2.1719961166381836\n",
      "Loss for batch is  1.9486093521118164\n",
      "Loss for batch is  2.2610397338867188\n",
      "Loss for batch is  0.7731677293777466\n",
      "|Iter  332  | Total Train Loss  7.154812932014465 |\n",
      "Val Loss for batch is  -0.6527562141418457\n",
      "Val Loss for batch is  -0.7541691064834595\n",
      "Val Loss for batch is  -0.44698238372802734\n",
      "Val Loss for batch is  -1.3300772905349731\n",
      "|Iter  332  | Total Val Loss  -3.1839849948883057 |\n",
      "Loss for batch is  2.1960246562957764\n",
      "Loss for batch is  1.950303077697754\n",
      "Loss for batch is  2.2361814975738525\n",
      "Loss for batch is  0.7947293519973755\n",
      "|Iter  333  | Total Train Loss  7.177238583564758 |\n",
      "Val Loss for batch is  -0.6052941083908081\n",
      "Val Loss for batch is  -0.704006552696228\n",
      "Val Loss for batch is  -0.39106273651123047\n",
      "Val Loss for batch is  -1.3666921854019165\n",
      "|Iter  333  | Total Val Loss  -3.067055583000183 |\n",
      "Loss for batch is  2.206779956817627\n",
      "Loss for batch is  1.9371845722198486\n",
      "Loss for batch is  2.288173198699951\n",
      "Loss for batch is  0.798407793045044\n",
      "|Iter  334  | Total Train Loss  7.230545520782471 |\n",
      "Val Loss for batch is  -0.6219972372055054\n",
      "Val Loss for batch is  -0.8105195164680481\n",
      "Val Loss for batch is  -0.45447948575019836\n",
      "Val Loss for batch is  -1.4378972053527832\n",
      "|Iter  334  | Total Val Loss  -3.324893444776535 |\n",
      "Loss for batch is  2.155344009399414\n",
      "Loss for batch is  1.9383738040924072\n",
      "Loss for batch is  2.2603726387023926\n",
      "Loss for batch is  0.7849583625793457\n",
      "|Iter  335  | Total Train Loss  7.13904881477356 |\n",
      "Val Loss for batch is  -0.5889759063720703\n",
      "Val Loss for batch is  -0.7566437721252441\n",
      "Val Loss for batch is  -0.45947733521461487\n",
      "Val Loss for batch is  -1.4926115274429321\n",
      "|Iter  335  | Total Val Loss  -3.2977085411548615 |\n",
      "Loss for batch is  2.147078037261963\n",
      "Loss for batch is  1.911763310432434\n",
      "Loss for batch is  2.250495195388794\n",
      "Loss for batch is  0.742274284362793\n",
      "|Iter  336  | Total Train Loss  7.051610827445984 |\n",
      "Val Loss for batch is  -0.5409278869628906\n",
      "Val Loss for batch is  -0.7303040623664856\n",
      "Val Loss for batch is  -0.42316409945487976\n",
      "Val Loss for batch is  -1.442408800125122\n",
      "|Iter  336  | Total Val Loss  -3.136804848909378 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  2.141730308532715\n",
      "Loss for batch is  1.9482393264770508\n",
      "Loss for batch is  2.2395124435424805\n",
      "Loss for batch is  0.7484961748123169\n",
      "|Iter  337  | Total Train Loss  7.077978253364563 |\n",
      "Val Loss for batch is  -0.6051225662231445\n",
      "Val Loss for batch is  -0.5945282578468323\n",
      "Val Loss for batch is  -0.44116854667663574\n",
      "Val Loss for batch is  -1.4604535102844238\n",
      "|Iter  337  | Total Val Loss  -3.1012728810310364 |\n",
      "Loss for batch is  2.135462999343872\n",
      "Loss for batch is  1.908905029296875\n",
      "Loss for batch is  2.2557942867279053\n",
      "Loss for batch is  0.7490320205688477\n",
      "|Iter  338  | Total Train Loss  7.0491943359375 |\n",
      "Val Loss for batch is  -0.6796263456344604\n",
      "Val Loss for batch is  -0.7714431881904602\n",
      "Val Loss for batch is  -0.3627163767814636\n",
      "Val Loss for batch is  -1.4137917757034302\n",
      "|Iter  338  | Total Val Loss  -3.2275776863098145 |\n",
      "Loss for batch is  2.10621976852417\n",
      "Loss for batch is  1.8992047309875488\n",
      "Loss for batch is  2.24362850189209\n",
      "Loss for batch is  0.7311463356018066\n",
      "|Iter  339  | Total Train Loss  6.980199337005615 |\n",
      "Val Loss for batch is  -0.5424708127975464\n",
      "Val Loss for batch is  -0.8001131415367126\n",
      "Val Loss for batch is  -0.49501240253448486\n",
      "Val Loss for batch is  -1.4934511184692383\n",
      "|Iter  339  | Total Val Loss  -3.331047475337982 |\n",
      "Loss for batch is  2.1104929447174072\n",
      "Loss for batch is  1.9326421022415161\n",
      "Loss for batch is  2.195343017578125\n",
      "Loss for batch is  0.6974496841430664\n",
      "|Iter  340  | Total Train Loss  6.935927748680115 |\n",
      "Val Loss for batch is  -0.6790684461593628\n",
      "Val Loss for batch is  -0.7227281332015991\n",
      "Val Loss for batch is  -0.49626341462135315\n",
      "Val Loss for batch is  -1.437104344367981\n",
      "|Iter  340  | Total Val Loss  -3.335164338350296 |\n",
      "Loss for batch is  2.1118874549865723\n",
      "Loss for batch is  1.8736217021942139\n",
      "Loss for batch is  2.141662836074829\n",
      "Loss for batch is  0.7288501262664795\n",
      "|Iter  341  | Total Train Loss  6.856022119522095 |\n",
      "Val Loss for batch is  -0.6285178661346436\n",
      "Val Loss for batch is  -0.7230478525161743\n",
      "Val Loss for batch is  -0.4646906852722168\n",
      "Val Loss for batch is  -1.4159414768218994\n",
      "|Iter  341  | Total Val Loss  -3.232197880744934 |\n",
      "Loss for batch is  2.0424745082855225\n",
      "Loss for batch is  1.8274004459381104\n",
      "Loss for batch is  2.164285898208618\n",
      "Loss for batch is  0.6820031404495239\n",
      "|Iter  342  | Total Train Loss  6.716163992881775 |\n",
      "Val Loss for batch is  -0.5773629546165466\n",
      "Val Loss for batch is  -0.748436450958252\n",
      "Val Loss for batch is  -0.5113693475723267\n",
      "Val Loss for batch is  -1.4570846557617188\n",
      "|Iter  342  | Total Val Loss  -3.294253408908844 |\n",
      "Loss for batch is  2.136565923690796\n",
      "Loss for batch is  1.8417959213256836\n",
      "Loss for batch is  2.159067153930664\n",
      "Loss for batch is  0.6878829002380371\n",
      "|Iter  343  | Total Train Loss  6.825311899185181 |\n",
      "Val Loss for batch is  -0.6056562662124634\n",
      "Val Loss for batch is  -0.6773448586463928\n",
      "Val Loss for batch is  -0.40292617678642273\n",
      "Val Loss for batch is  -1.425477147102356\n",
      "|Iter  343  | Total Val Loss  -3.111404448747635 |\n",
      "Loss for batch is  2.0813238620758057\n",
      "Loss for batch is  1.872753381729126\n",
      "Loss for batch is  2.207629442214966\n",
      "Loss for batch is  0.7052514553070068\n",
      "|Iter  344  | Total Train Loss  6.866958141326904 |\n",
      "Val Loss for batch is  -0.63526850938797\n",
      "Val Loss for batch is  -0.8402624726295471\n",
      "Val Loss for batch is  -0.5060631036758423\n",
      "Val Loss for batch is  -1.477184534072876\n",
      "|Iter  344  | Total Val Loss  -3.4587786197662354 |\n",
      "Loss for batch is  2.067222833633423\n",
      "Loss for batch is  1.8695722818374634\n",
      "Loss for batch is  2.1846561431884766\n",
      "Loss for batch is  0.7154636383056641\n",
      "|Iter  345  | Total Train Loss  6.836914896965027 |\n",
      "Val Loss for batch is  -0.5112606883049011\n",
      "Val Loss for batch is  -0.7114230394363403\n",
      "Val Loss for batch is  -0.40789276361465454\n",
      "Val Loss for batch is  -1.3690170049667358\n",
      "|Iter  345  | Total Val Loss  -2.999593496322632 |\n",
      "Loss for batch is  2.104933738708496\n",
      "Loss for batch is  1.8678052425384521\n",
      "Loss for batch is  2.244978904724121\n",
      "Loss for batch is  0.6761597394943237\n",
      "|Iter  346  | Total Train Loss  6.893877625465393 |\n",
      "Val Loss for batch is  -0.6199453473091125\n",
      "Val Loss for batch is  -0.7926090359687805\n",
      "Val Loss for batch is  -0.48202750086784363\n",
      "Val Loss for batch is  -1.4679203033447266\n",
      "|Iter  346  | Total Val Loss  -3.3625021874904633 |\n",
      "Loss for batch is  2.061239242553711\n",
      "Loss for batch is  1.9072089195251465\n",
      "Loss for batch is  2.1443750858306885\n",
      "Loss for batch is  0.6882747411727905\n",
      "|Iter  347  | Total Train Loss  6.801097989082336 |\n",
      "Val Loss for batch is  -0.6457439064979553\n",
      "Val Loss for batch is  -0.8743126392364502\n",
      "Val Loss for batch is  -0.5464695692062378\n",
      "Val Loss for batch is  -1.4732635021209717\n",
      "|Iter  347  | Total Val Loss  -3.539789617061615 |\n",
      "Loss for batch is  2.038487195968628\n",
      "Loss for batch is  1.8248343467712402\n",
      "Loss for batch is  2.148538827896118\n",
      "Loss for batch is  0.6664367914199829\n",
      "|Iter  348  | Total Train Loss  6.678297162055969 |\n",
      "Val Loss for batch is  -0.668171226978302\n",
      "Val Loss for batch is  -0.7530447244644165\n",
      "Val Loss for batch is  -0.5627392530441284\n",
      "Val Loss for batch is  -1.3862203359603882\n",
      "|Iter  348  | Total Val Loss  -3.370175540447235 |\n",
      "Loss for batch is  2.0411903858184814\n",
      "Loss for batch is  1.8371626138687134\n",
      "Loss for batch is  2.1047418117523193\n",
      "Loss for batch is  0.6413346529006958\n",
      "|Iter  349  | Total Train Loss  6.62442946434021 |\n",
      "Val Loss for batch is  -0.6109026670455933\n",
      "Val Loss for batch is  -0.7994473576545715\n",
      "Val Loss for batch is  -0.47107797861099243\n",
      "Val Loss for batch is  -1.524124264717102\n",
      "|Iter  349  | Total Val Loss  -3.4055522680282593 |\n",
      "Loss for batch is  2.041008234024048\n",
      "Loss for batch is  1.8182096481323242\n",
      "Loss for batch is  2.1537301540374756\n",
      "Loss for batch is  0.6321244239807129\n",
      "|Iter  350  | Total Train Loss  6.6450724601745605 |\n",
      "Val Loss for batch is  -0.6593817472457886\n",
      "Val Loss for batch is  -0.7137823700904846\n",
      "Val Loss for batch is  -0.43573468923568726\n",
      "Val Loss for batch is  -1.4703952074050903\n",
      "|Iter  350  | Total Val Loss  -3.279294013977051 |\n",
      "Loss for batch is  2.0378127098083496\n",
      "Loss for batch is  1.8893167972564697\n",
      "Loss for batch is  2.165451765060425\n",
      "Loss for batch is  0.6350218057632446\n",
      "|Iter  351  | Total Train Loss  6.727603077888489 |\n",
      "Val Loss for batch is  -0.614936351776123\n",
      "Val Loss for batch is  -0.7770284414291382\n",
      "Val Loss for batch is  -0.4789830446243286\n",
      "Val Loss for batch is  -1.5371956825256348\n",
      "|Iter  351  | Total Val Loss  -3.4081435203552246 |\n",
      "Loss for batch is  2.0754687786102295\n",
      "Loss for batch is  1.836310625076294\n",
      "Loss for batch is  2.218318462371826\n",
      "Loss for batch is  0.686464786529541\n",
      "|Iter  352  | Total Train Loss  6.816562652587891 |\n",
      "Val Loss for batch is  -0.2705332040786743\n",
      "Val Loss for batch is  -0.737693727016449\n",
      "Val Loss for batch is  -0.35153689980506897\n",
      "Val Loss for batch is  -1.4927403926849365\n",
      "|Iter  352  | Total Val Loss  -2.852504223585129 |\n",
      "Loss for batch is  2.2615344524383545\n",
      "Loss for batch is  1.8512613773345947\n",
      "Loss for batch is  2.2490901947021484\n",
      "Loss for batch is  0.6413986682891846\n",
      "|Iter  353  | Total Train Loss  7.003284692764282 |\n",
      "Val Loss for batch is  -0.4627518057823181\n",
      "Val Loss for batch is  -0.8921892642974854\n",
      "Val Loss for batch is  -0.48585838079452515\n",
      "Val Loss for batch is  -1.234556794166565\n",
      "|Iter  353  | Total Val Loss  -3.0753562450408936 |\n",
      "Loss for batch is  2.2388529777526855\n",
      "Loss for batch is  1.7989767789840698\n",
      "Loss for batch is  2.21201229095459\n",
      "Loss for batch is  0.6781004667282104\n",
      "|Iter  354  | Total Train Loss  6.927942514419556 |\n",
      "Val Loss for batch is  -0.492246150970459\n",
      "Val Loss for batch is  -0.7520236968994141\n",
      "Val Loss for batch is  -0.488457053899765\n",
      "Val Loss for batch is  -1.3989810943603516\n",
      "|Iter  354  | Total Val Loss  -3.1317079961299896 |\n",
      "Loss for batch is  2.1559367179870605\n",
      "Loss for batch is  1.7962602376937866\n",
      "Loss for batch is  2.205388069152832\n",
      "Loss for batch is  0.638979434967041\n",
      "|Iter  355  | Total Train Loss  6.79656445980072 |\n",
      "Val Loss for batch is  -0.6147375106811523\n",
      "Val Loss for batch is  -0.7899065017700195\n",
      "Val Loss for batch is  -0.5096518397331238\n",
      "Val Loss for batch is  -1.3991692066192627\n",
      "|Iter  355  | Total Val Loss  -3.3134650588035583 |\n",
      "Loss for batch is  2.1357884407043457\n",
      "Loss for batch is  1.7950918674468994\n",
      "Loss for batch is  2.1845107078552246\n",
      "Loss for batch is  0.6418452262878418\n",
      "|Iter  356  | Total Train Loss  6.7572362422943115 |\n",
      "Val Loss for batch is  -0.6399142742156982\n",
      "Val Loss for batch is  -0.8483757376670837\n",
      "Val Loss for batch is  -0.5083105564117432\n",
      "Val Loss for batch is  -1.40506112575531\n",
      "|Iter  356  | Total Val Loss  -3.401661694049835 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  2.071143388748169\n",
      "Loss for batch is  1.7562260627746582\n",
      "Loss for batch is  2.1335904598236084\n",
      "Loss for batch is  0.7014514207839966\n",
      "|Iter  357  | Total Train Loss  6.662411332130432 |\n",
      "Val Loss for batch is  -0.6759510040283203\n",
      "Val Loss for batch is  -0.9218853116035461\n",
      "Val Loss for batch is  -0.550629198551178\n",
      "Val Loss for batch is  -1.5916285514831543\n",
      "|Iter  357  | Total Val Loss  -3.7400940656661987 |\n",
      "Loss for batch is  2.0396525859832764\n",
      "Loss for batch is  1.7681264877319336\n",
      "Loss for batch is  2.1277315616607666\n",
      "Loss for batch is  0.6198046207427979\n",
      "|Iter  358  | Total Train Loss  6.555315256118774 |\n",
      "Val Loss for batch is  -0.6490951776504517\n",
      "Val Loss for batch is  -0.8522834181785583\n",
      "Val Loss for batch is  -0.546191930770874\n",
      "Val Loss for batch is  -1.5261980295181274\n",
      "|Iter  358  | Total Val Loss  -3.5737685561180115 |\n",
      "Loss for batch is  2.0342535972595215\n",
      "Loss for batch is  1.7618917226791382\n",
      "Loss for batch is  2.0748777389526367\n",
      "Loss for batch is  0.6141098737716675\n",
      "|Iter  359  | Total Train Loss  6.485132932662964 |\n",
      "Val Loss for batch is  -0.7076491117477417\n",
      "Val Loss for batch is  -0.7520878314971924\n",
      "Val Loss for batch is  -0.4938505291938782\n",
      "Val Loss for batch is  -1.4732064008712769\n",
      "|Iter  359  | Total Val Loss  -3.426793873310089 |\n",
      "Loss for batch is  1.9954257011413574\n",
      "Loss for batch is  1.7057561874389648\n",
      "Loss for batch is  2.022782802581787\n",
      "Loss for batch is  0.6338064670562744\n",
      "|Iter  360  | Total Train Loss  6.357771158218384 |\n",
      "Val Loss for batch is  -0.6209173798561096\n",
      "Val Loss for batch is  -0.8517852425575256\n",
      "Val Loss for batch is  -0.517216682434082\n",
      "Val Loss for batch is  -1.5495792627334595\n",
      "|Iter  360  | Total Val Loss  -3.5394985675811768 |\n",
      "Loss for batch is  2.0156593322753906\n",
      "Loss for batch is  1.7713520526885986\n",
      "Loss for batch is  2.0511586666107178\n",
      "Loss for batch is  0.6119487285614014\n",
      "|Iter  361  | Total Train Loss  6.450118780136108 |\n",
      "Val Loss for batch is  -0.6591872572898865\n",
      "Val Loss for batch is  -0.8436928391456604\n",
      "Val Loss for batch is  -0.5475952625274658\n",
      "Val Loss for batch is  -1.524031162261963\n",
      "|Iter  361  | Total Val Loss  -3.5745065212249756 |\n",
      "Loss for batch is  1.9450616836547852\n",
      "Loss for batch is  1.6880066394805908\n",
      "Loss for batch is  2.0662715435028076\n",
      "Loss for batch is  0.6045047044754028\n",
      "|Iter  362  | Total Train Loss  6.303844571113586 |\n",
      "Val Loss for batch is  -0.7442187666893005\n",
      "Val Loss for batch is  -0.813311755657196\n",
      "Val Loss for batch is  -0.5785984396934509\n",
      "Val Loss for batch is  -1.5570160150527954\n",
      "|Iter  362  | Total Val Loss  -3.693144977092743 |\n",
      "Loss for batch is  1.9695262908935547\n",
      "Loss for batch is  1.719191074371338\n",
      "Loss for batch is  2.1009020805358887\n",
      "Loss for batch is  0.5536297559738159\n",
      "|Iter  363  | Total Train Loss  6.343249201774597 |\n",
      "Val Loss for batch is  -0.35512208938598633\n",
      "Val Loss for batch is  -0.7214949131011963\n",
      "Val Loss for batch is  -0.4936417043209076\n",
      "Val Loss for batch is  -1.5672906637191772\n",
      "|Iter  363  | Total Val Loss  -3.1375493705272675 |\n",
      "Loss for batch is  2.2318899631500244\n",
      "Loss for batch is  1.7873761653900146\n",
      "Loss for batch is  2.1104354858398438\n",
      "Loss for batch is  0.5948736667633057\n",
      "|Iter  364  | Total Train Loss  6.7245752811431885 |\n",
      "Val Loss for batch is  -0.6512184739112854\n",
      "Val Loss for batch is  -0.9263688921928406\n",
      "Val Loss for batch is  -0.5534796714782715\n",
      "Val Loss for batch is  -1.542820692062378\n",
      "|Iter  364  | Total Val Loss  -3.6738877296447754 |\n",
      "Loss for batch is  2.0716235637664795\n",
      "Loss for batch is  1.7227513790130615\n",
      "Loss for batch is  2.101722240447998\n",
      "Loss for batch is  0.5765924453735352\n",
      "|Iter  365  | Total Train Loss  6.472689628601074 |\n",
      "Val Loss for batch is  -0.5968086123466492\n",
      "Val Loss for batch is  -0.7705617547035217\n",
      "Val Loss for batch is  -0.5218368172645569\n",
      "Val Loss for batch is  -1.5435651540756226\n",
      "|Iter  365  | Total Val Loss  -3.4327723383903503 |\n",
      "Loss for batch is  2.007190704345703\n",
      "Loss for batch is  1.736412763595581\n",
      "Loss for batch is  2.0722618103027344\n",
      "Loss for batch is  0.5699412822723389\n",
      "|Iter  366  | Total Train Loss  6.385806560516357 |\n",
      "Val Loss for batch is  -0.6735756397247314\n",
      "Val Loss for batch is  -0.8137699365615845\n",
      "Val Loss for batch is  -0.532545268535614\n",
      "Val Loss for batch is  -1.5366902351379395\n",
      "|Iter  366  | Total Val Loss  -3.5565810799598694 |\n",
      "Loss for batch is  2.001722574234009\n",
      "Loss for batch is  1.7026069164276123\n",
      "Loss for batch is  2.006861925125122\n",
      "Loss for batch is  0.5676872730255127\n",
      "|Iter  367  | Total Train Loss  6.278878688812256 |\n",
      "Val Loss for batch is  -0.5995936393737793\n",
      "Val Loss for batch is  -0.7739842534065247\n",
      "Val Loss for batch is  -0.4605892598628998\n",
      "Val Loss for batch is  -1.540424108505249\n",
      "|Iter  367  | Total Val Loss  -3.3745912611484528 |\n",
      "Loss for batch is  1.9505078792572021\n",
      "Loss for batch is  1.7287366390228271\n",
      "Loss for batch is  2.05875301361084\n",
      "Loss for batch is  0.5363506078720093\n",
      "|Iter  368  | Total Train Loss  6.274348139762878 |\n",
      "Val Loss for batch is  -0.6333770751953125\n",
      "Val Loss for batch is  -0.8647153377532959\n",
      "Val Loss for batch is  -0.6040095686912537\n",
      "Val Loss for batch is  -1.619950532913208\n",
      "|Iter  368  | Total Val Loss  -3.72205251455307 |\n",
      "Loss for batch is  1.9759601354599\n",
      "Loss for batch is  1.6958348751068115\n",
      "Loss for batch is  2.035374641418457\n",
      "Loss for batch is  0.5588736534118652\n",
      "|Iter  369  | Total Train Loss  6.266043305397034 |\n",
      "Val Loss for batch is  -0.6315236687660217\n",
      "Val Loss for batch is  -0.7434797883033752\n",
      "Val Loss for batch is  -0.5121345520019531\n",
      "Val Loss for batch is  -1.4485564231872559\n",
      "|Iter  369  | Total Val Loss  -3.335694432258606 |\n",
      "Loss for batch is  1.9152237176895142\n",
      "Loss for batch is  1.7164306640625\n",
      "Loss for batch is  2.007817506790161\n",
      "Loss for batch is  0.541412353515625\n",
      "|Iter  370  | Total Train Loss  6.1808842420578 |\n",
      "Val Loss for batch is  -0.6947077512741089\n",
      "Val Loss for batch is  -0.8678948283195496\n",
      "Val Loss for batch is  -0.5715709924697876\n",
      "Val Loss for batch is  -1.6559455394744873\n",
      "|Iter  370  | Total Val Loss  -3.7901191115379333 |\n",
      "Loss for batch is  1.9373314380645752\n",
      "Loss for batch is  1.6907117366790771\n",
      "Loss for batch is  1.9801104068756104\n",
      "Loss for batch is  0.5707830190658569\n",
      "|Iter  371  | Total Train Loss  6.17893660068512 |\n",
      "Val Loss for batch is  -0.6758593916893005\n",
      "Val Loss for batch is  -0.8676912188529968\n",
      "Val Loss for batch is  -0.47037237882614136\n",
      "Val Loss for batch is  -1.576926350593567\n",
      "|Iter  371  | Total Val Loss  -3.5908493399620056 |\n",
      "Loss for batch is  1.910271167755127\n",
      "Loss for batch is  1.6953155994415283\n",
      "Loss for batch is  2.00738263130188\n",
      "Loss for batch is  0.561474084854126\n",
      "|Iter  372  | Total Train Loss  6.174443483352661 |\n",
      "Val Loss for batch is  -0.6559629440307617\n",
      "Val Loss for batch is  -0.8201769590377808\n",
      "Val Loss for batch is  -0.4970867931842804\n",
      "Val Loss for batch is  -1.514595866203308\n",
      "|Iter  372  | Total Val Loss  -3.487822562456131 |\n",
      "Loss for batch is  1.8902982473373413\n",
      "Loss for batch is  1.67225182056427\n",
      "Loss for batch is  1.9434130191802979\n",
      "Loss for batch is  0.5288999080657959\n",
      "|Iter  373  | Total Train Loss  6.034862995147705 |\n",
      "Val Loss for batch is  -0.6766362190246582\n",
      "Val Loss for batch is  -0.9358018040657043\n",
      "Val Loss for batch is  -0.5360066890716553\n",
      "Val Loss for batch is  -1.6307010650634766\n",
      "|Iter  373  | Total Val Loss  -3.7791457772254944 |\n",
      "Loss for batch is  1.8600759506225586\n",
      "Loss for batch is  1.665623426437378\n",
      "Loss for batch is  1.9648680686950684\n",
      "Loss for batch is  0.5281827449798584\n",
      "|Iter  374  | Total Train Loss  6.018750190734863 |\n",
      "Val Loss for batch is  -0.6854584217071533\n",
      "Val Loss for batch is  -0.8926444053649902\n",
      "Val Loss for batch is  -0.5121290683746338\n",
      "Val Loss for batch is  -1.6362152099609375\n",
      "|Iter  374  | Total Val Loss  -3.726447105407715 |\n",
      "Loss for batch is  1.8755741119384766\n",
      "Loss for batch is  1.6144782304763794\n",
      "Loss for batch is  1.9651371240615845\n",
      "Loss for batch is  0.5090214014053345\n",
      "|Iter  375  | Total Train Loss  5.964210867881775 |\n",
      "Val Loss for batch is  -0.7154412865638733\n",
      "Val Loss for batch is  -0.7781727910041809\n",
      "Val Loss for batch is  -0.44427788257598877\n",
      "Val Loss for batch is  -1.582000494003296\n",
      "|Iter  375  | Total Val Loss  -3.519892454147339 |\n",
      "Loss for batch is  1.8738092184066772\n",
      "Loss for batch is  1.6337299346923828\n",
      "Loss for batch is  1.968810796737671\n",
      "Loss for batch is  0.5043221712112427\n",
      "|Iter  376  | Total Train Loss  5.980672121047974 |\n",
      "Val Loss for batch is  -0.7383806705474854\n",
      "Val Loss for batch is  -0.9105334281921387\n",
      "Val Loss for batch is  -0.6199599504470825\n",
      "Val Loss for batch is  -1.6005403995513916\n",
      "|Iter  376  | Total Val Loss  -3.869414448738098 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.9178218841552734\n",
      "Loss for batch is  1.6728272438049316\n",
      "Loss for batch is  1.9191608428955078\n",
      "Loss for batch is  0.4955625534057617\n",
      "|Iter  377  | Total Train Loss  6.005372524261475 |\n",
      "Val Loss for batch is  -0.6569328308105469\n",
      "Val Loss for batch is  -0.8490152359008789\n",
      "Val Loss for batch is  -0.5724531412124634\n",
      "Val Loss for batch is  -1.557080626487732\n",
      "|Iter  377  | Total Val Loss  -3.635481834411621 |\n",
      "Loss for batch is  1.837451696395874\n",
      "Loss for batch is  1.622501254081726\n",
      "Loss for batch is  1.9566477537155151\n",
      "Loss for batch is  0.4946054220199585\n",
      "|Iter  378  | Total Train Loss  5.911206126213074 |\n",
      "Val Loss for batch is  -0.7182636260986328\n",
      "Val Loss for batch is  -0.854019284248352\n",
      "Val Loss for batch is  -0.6432492136955261\n",
      "Val Loss for batch is  -1.6379469633102417\n",
      "|Iter  378  | Total Val Loss  -3.8534790873527527 |\n",
      "Loss for batch is  1.8875600099563599\n",
      "Loss for batch is  1.633193850517273\n",
      "Loss for batch is  1.912008285522461\n",
      "Loss for batch is  0.5027436017990112\n",
      "|Iter  379  | Total Train Loss  5.935505747795105 |\n",
      "Val Loss for batch is  -0.6280282735824585\n",
      "Val Loss for batch is  -0.864136278629303\n",
      "Val Loss for batch is  -0.5417196750640869\n",
      "Val Loss for batch is  -1.6439136266708374\n",
      "|Iter  379  | Total Val Loss  -3.677797853946686 |\n",
      "Loss for batch is  1.9007978439331055\n",
      "Loss for batch is  1.6666858196258545\n",
      "Loss for batch is  1.969090223312378\n",
      "Loss for batch is  0.4852433204650879\n",
      "|Iter  380  | Total Train Loss  6.021817207336426 |\n",
      "Val Loss for batch is  -0.7129310965538025\n",
      "Val Loss for batch is  -0.8240960836410522\n",
      "Val Loss for batch is  -0.5712634325027466\n",
      "Val Loss for batch is  -1.4866747856140137\n",
      "|Iter  380  | Total Val Loss  -3.594965398311615 |\n",
      "Loss for batch is  1.859250545501709\n",
      "Loss for batch is  1.5843944549560547\n",
      "Loss for batch is  1.9139745235443115\n",
      "Loss for batch is  0.47979748249053955\n",
      "|Iter  381  | Total Train Loss  5.837417006492615 |\n",
      "Val Loss for batch is  -0.7437388300895691\n",
      "Val Loss for batch is  -0.9682310819625854\n",
      "Val Loss for batch is  -0.6380399465560913\n",
      "Val Loss for batch is  -1.5812031030654907\n",
      "|Iter  381  | Total Val Loss  -3.9312129616737366 |\n",
      "Loss for batch is  1.7865301370620728\n",
      "Loss for batch is  1.5739322900772095\n",
      "Loss for batch is  1.8578200340270996\n",
      "Loss for batch is  0.4679802656173706\n",
      "|Iter  382  | Total Train Loss  5.686262726783752 |\n",
      "Val Loss for batch is  -0.759903073310852\n",
      "Val Loss for batch is  -0.8455161452293396\n",
      "Val Loss for batch is  -0.5790344476699829\n",
      "Val Loss for batch is  -1.587550163269043\n",
      "|Iter  382  | Total Val Loss  -3.7720038294792175 |\n",
      "Loss for batch is  1.792227029800415\n",
      "Loss for batch is  1.5844886302947998\n",
      "Loss for batch is  1.9059014320373535\n",
      "Loss for batch is  0.43743467330932617\n",
      "|Iter  383  | Total Train Loss  5.7200517654418945 |\n",
      "Val Loss for batch is  -0.7262948751449585\n",
      "Val Loss for batch is  -0.9097069501876831\n",
      "Val Loss for batch is  -0.6458026170730591\n",
      "Val Loss for batch is  -1.6075263023376465\n",
      "|Iter  383  | Total Val Loss  -3.889330744743347 |\n",
      "Loss for batch is  1.8185561895370483\n",
      "Loss for batch is  1.5683187246322632\n",
      "Loss for batch is  1.8515909910202026\n",
      "Loss for batch is  0.4450263977050781\n",
      "|Iter  384  | Total Train Loss  5.683492302894592 |\n",
      "Val Loss for batch is  -0.658930242061615\n",
      "Val Loss for batch is  -0.790936291217804\n",
      "Val Loss for batch is  -0.632983386516571\n",
      "Val Loss for batch is  -1.5510343313217163\n",
      "|Iter  384  | Total Val Loss  -3.6338842511177063 |\n",
      "Loss for batch is  1.7658182382583618\n",
      "Loss for batch is  1.5964114665985107\n",
      "Loss for batch is  1.8540759086608887\n",
      "Loss for batch is  0.43386948108673096\n",
      "|Iter  385  | Total Train Loss  5.650175094604492 |\n",
      "Val Loss for batch is  -0.7556380033493042\n",
      "Val Loss for batch is  -0.8989022970199585\n",
      "Val Loss for batch is  -0.5564907193183899\n",
      "Val Loss for batch is  -1.6072156429290771\n",
      "|Iter  385  | Total Val Loss  -3.8182466626167297 |\n",
      "Loss for batch is  1.8148163557052612\n",
      "Loss for batch is  1.5720956325531006\n",
      "Loss for batch is  1.8498456478118896\n",
      "Loss for batch is  0.4464796781539917\n",
      "|Iter  386  | Total Train Loss  5.683237314224243 |\n",
      "Val Loss for batch is  -0.7633127570152283\n",
      "Val Loss for batch is  -0.8593944311141968\n",
      "Val Loss for batch is  -0.6361856460571289\n",
      "Val Loss for batch is  -1.659927248954773\n",
      "|Iter  386  | Total Val Loss  -3.918820083141327 |\n",
      "Loss for batch is  1.8027253150939941\n",
      "Loss for batch is  1.5820496082305908\n",
      "Loss for batch is  1.8402314186096191\n",
      "Loss for batch is  0.4643183946609497\n",
      "|Iter  387  | Total Train Loss  5.689324736595154 |\n",
      "Val Loss for batch is  -0.7665502429008484\n",
      "Val Loss for batch is  -0.9885875582695007\n",
      "Val Loss for batch is  -0.6988230347633362\n",
      "Val Loss for batch is  -1.7098112106323242\n",
      "|Iter  387  | Total Val Loss  -4.1637720465660095 |\n",
      "Loss for batch is  1.788327932357788\n",
      "Loss for batch is  1.5438714027404785\n",
      "Loss for batch is  1.8433640003204346\n",
      "Loss for batch is  0.4170231819152832\n",
      "|Iter  388  | Total Train Loss  5.592586517333984 |\n",
      "Val Loss for batch is  -0.7491810321807861\n",
      "Val Loss for batch is  -0.880642294883728\n",
      "Val Loss for batch is  -0.532179594039917\n",
      "Val Loss for batch is  -1.63278067111969\n",
      "|Iter  388  | Total Val Loss  -3.794783592224121 |\n",
      "Loss for batch is  1.770587682723999\n",
      "Loss for batch is  1.5176111459732056\n",
      "Loss for batch is  1.8600234985351562\n",
      "Loss for batch is  0.4748079776763916\n",
      "|Iter  389  | Total Train Loss  5.623030304908752 |\n",
      "Val Loss for batch is  -0.7679410576820374\n",
      "Val Loss for batch is  -0.9752067923545837\n",
      "Val Loss for batch is  -0.644646942615509\n",
      "Val Loss for batch is  -1.5930739641189575\n",
      "|Iter  389  | Total Val Loss  -3.9808687567710876 |\n",
      "Loss for batch is  1.7837920188903809\n",
      "Loss for batch is  1.5791685581207275\n",
      "Loss for batch is  1.8392293453216553\n",
      "Loss for batch is  0.4467886686325073\n",
      "|Iter  390  | Total Train Loss  5.648978590965271 |\n",
      "Val Loss for batch is  -0.7616372108459473\n",
      "Val Loss for batch is  -0.84907066822052\n",
      "Val Loss for batch is  -0.5706716775894165\n",
      "Val Loss for batch is  -1.525834560394287\n",
      "|Iter  390  | Total Val Loss  -3.707214117050171 |\n",
      "Loss for batch is  1.7533457279205322\n",
      "Loss for batch is  1.537611961364746\n",
      "Loss for batch is  1.781796932220459\n",
      "Loss for batch is  0.39968252182006836\n",
      "|Iter  391  | Total Train Loss  5.472437143325806 |\n",
      "Val Loss for batch is  -0.7129102945327759\n",
      "Val Loss for batch is  -0.927634596824646\n",
      "Val Loss for batch is  -0.6197613477706909\n",
      "Val Loss for batch is  -1.6802610158920288\n",
      "|Iter  391  | Total Val Loss  -3.9405672550201416 |\n",
      "Loss for batch is  1.7309236526489258\n",
      "Loss for batch is  1.5326869487762451\n",
      "Loss for batch is  1.876682162284851\n",
      "Loss for batch is  0.41101157665252686\n",
      "|Iter  392  | Total Train Loss  5.551304340362549 |\n",
      "Val Loss for batch is  -0.7861591577529907\n",
      "Val Loss for batch is  -1.0002516508102417\n",
      "Val Loss for batch is  -0.6179176568984985\n",
      "Val Loss for batch is  -1.6118777990341187\n",
      "|Iter  392  | Total Val Loss  -4.01620626449585 |\n",
      "Loss for batch is  1.7519041299819946\n",
      "Loss for batch is  1.5390946865081787\n",
      "Loss for batch is  1.7992513179779053\n",
      "Loss for batch is  0.4193466901779175\n",
      "|Iter  393  | Total Train Loss  5.509596824645996 |\n",
      "Val Loss for batch is  -0.7945343255996704\n",
      "Val Loss for batch is  -0.9111208915710449\n",
      "Val Loss for batch is  -0.679854154586792\n",
      "Val Loss for batch is  -1.641728401184082\n",
      "|Iter  393  | Total Val Loss  -4.027237772941589 |\n",
      "Loss for batch is  1.7510063648223877\n",
      "Loss for batch is  1.532669186592102\n",
      "Loss for batch is  1.8011583089828491\n",
      "Loss for batch is  0.39788782596588135\n",
      "|Iter  394  | Total Train Loss  5.48272168636322 |\n",
      "Val Loss for batch is  -0.8095720410346985\n",
      "Val Loss for batch is  -0.8842969536781311\n",
      "Val Loss for batch is  -0.7506142854690552\n",
      "Val Loss for batch is  -1.6556917428970337\n",
      "|Iter  394  | Total Val Loss  -4.1001750230789185 |\n",
      "Loss for batch is  1.7455687522888184\n",
      "Loss for batch is  1.5751209259033203\n",
      "Loss for batch is  1.7657990455627441\n",
      "Loss for batch is  0.3940788507461548\n",
      "|Iter  395  | Total Train Loss  5.480567574501038 |\n",
      "Val Loss for batch is  -0.8149172067642212\n",
      "Val Loss for batch is  -0.8081969618797302\n",
      "Val Loss for batch is  -0.6779621243476868\n",
      "Val Loss for batch is  -1.6363093852996826\n",
      "|Iter  395  | Total Val Loss  -3.937385678291321 |\n",
      "Loss for batch is  1.7251758575439453\n",
      "Loss for batch is  1.49141526222229\n",
      "Loss for batch is  1.8264111280441284\n",
      "Loss for batch is  0.34949707984924316\n",
      "|Iter  396  | Total Train Loss  5.392499327659607 |\n",
      "Val Loss for batch is  -0.7191988229751587\n",
      "Val Loss for batch is  -0.9575866460800171\n",
      "Val Loss for batch is  -0.6824424862861633\n",
      "Val Loss for batch is  -1.6404953002929688\n",
      "|Iter  396  | Total Val Loss  -3.999723255634308 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.7626111507415771\n",
      "Loss for batch is  1.491726040840149\n",
      "Loss for batch is  1.8315306901931763\n",
      "Loss for batch is  0.3796870708465576\n",
      "|Iter  397  | Total Train Loss  5.46555495262146 |\n",
      "Val Loss for batch is  -0.7642140984535217\n",
      "Val Loss for batch is  -0.9363438487052917\n",
      "Val Loss for batch is  -0.7038077116012573\n",
      "Val Loss for batch is  -1.5815749168395996\n",
      "|Iter  397  | Total Val Loss  -3.9859405755996704 |\n",
      "Loss for batch is  1.7542471885681152\n",
      "Loss for batch is  1.5134367942810059\n",
      "Loss for batch is  1.7916159629821777\n",
      "Loss for batch is  0.43182599544525146\n",
      "|Iter  398  | Total Train Loss  5.49112594127655 |\n",
      "Val Loss for batch is  -0.8020215034484863\n",
      "Val Loss for batch is  -0.8774192333221436\n",
      "Val Loss for batch is  -0.5684889554977417\n",
      "Val Loss for batch is  -1.6763148307800293\n",
      "|Iter  398  | Total Val Loss  -3.924244523048401 |\n",
      "Loss for batch is  1.7272441387176514\n",
      "Loss for batch is  1.4755700826644897\n",
      "Loss for batch is  1.7879570722579956\n",
      "Loss for batch is  0.3927948474884033\n",
      "|Iter  399  | Total Train Loss  5.38356614112854 |\n",
      "Val Loss for batch is  -0.8330913782119751\n",
      "Val Loss for batch is  -0.9365897178649902\n",
      "Val Loss for batch is  -0.6208431720733643\n",
      "Val Loss for batch is  -1.707387089729309\n",
      "|Iter  399  | Total Val Loss  -4.097911357879639 |\n",
      "Loss for batch is  1.6992652416229248\n",
      "Loss for batch is  1.488580346107483\n",
      "Loss for batch is  1.7751309871673584\n",
      "Loss for batch is  0.3719480037689209\n",
      "|Iter  400  | Total Train Loss  5.334924578666687 |\n",
      "Val Loss for batch is  -0.7387282848358154\n",
      "Val Loss for batch is  -0.9302012920379639\n",
      "Val Loss for batch is  -0.6898804903030396\n",
      "Val Loss for batch is  -1.7356579303741455\n",
      "|Iter  400  | Total Val Loss  -4.094467997550964 |\n",
      "Loss for batch is  1.7163739204406738\n",
      "Loss for batch is  1.4592111110687256\n",
      "Loss for batch is  1.7386929988861084\n",
      "Loss for batch is  0.37578046321868896\n",
      "|Iter  401  | Total Train Loss  5.290058493614197 |\n",
      "Val Loss for batch is  -0.7897446155548096\n",
      "Val Loss for batch is  -0.9399070739746094\n",
      "Val Loss for batch is  -0.6496520638465881\n",
      "Val Loss for batch is  -1.677722454071045\n",
      "|Iter  401  | Total Val Loss  -4.057026207447052 |\n",
      "Loss for batch is  1.676810622215271\n",
      "Loss for batch is  1.4525436162948608\n",
      "Loss for batch is  1.729013442993164\n",
      "Loss for batch is  0.37543225288391113\n",
      "|Iter  402  | Total Train Loss  5.233799934387207 |\n",
      "Val Loss for batch is  -0.8679823279380798\n",
      "Val Loss for batch is  -1.050368070602417\n",
      "Val Loss for batch is  -0.7059332728385925\n",
      "Val Loss for batch is  -1.6895040273666382\n",
      "|Iter  402  | Total Val Loss  -4.3137876987457275 |\n",
      "Loss for batch is  1.671501874923706\n",
      "Loss for batch is  1.4511573314666748\n",
      "Loss for batch is  1.719496488571167\n",
      "Loss for batch is  0.34318363666534424\n",
      "|Iter  403  | Total Train Loss  5.185339331626892 |\n",
      "Val Loss for batch is  -0.8084323406219482\n",
      "Val Loss for batch is  -0.9741540551185608\n",
      "Val Loss for batch is  -0.6870254278182983\n",
      "Val Loss for batch is  -1.630082607269287\n",
      "|Iter  403  | Total Val Loss  -4.0996944308280945 |\n",
      "Loss for batch is  1.6726521253585815\n",
      "Loss for batch is  1.4105784893035889\n",
      "Loss for batch is  1.6892876625061035\n",
      "Loss for batch is  0.3334735631942749\n",
      "|Iter  404  | Total Train Loss  5.105991840362549 |\n",
      "Val Loss for batch is  -0.8131964206695557\n",
      "Val Loss for batch is  -0.9670904874801636\n",
      "Val Loss for batch is  -0.7577533721923828\n",
      "Val Loss for batch is  -1.7135789394378662\n",
      "|Iter  404  | Total Val Loss  -4.251619219779968 |\n",
      "Loss for batch is  1.6789052486419678\n",
      "Loss for batch is  1.422064185142517\n",
      "Loss for batch is  1.6718106269836426\n",
      "Loss for batch is  0.36005687713623047\n",
      "|Iter  405  | Total Train Loss  5.132836937904358 |\n",
      "Val Loss for batch is  -0.7961169481277466\n",
      "Val Loss for batch is  -0.8960778117179871\n",
      "Val Loss for batch is  -0.6912119388580322\n",
      "Val Loss for batch is  -1.7141854763031006\n",
      "|Iter  405  | Total Val Loss  -4.0975921750068665 |\n",
      "Loss for batch is  1.642531394958496\n",
      "Loss for batch is  1.4253740310668945\n",
      "Loss for batch is  1.756333589553833\n",
      "Loss for batch is  0.3466411828994751\n",
      "|Iter  406  | Total Train Loss  5.170880198478699 |\n",
      "Val Loss for batch is  -0.8062982559204102\n",
      "Val Loss for batch is  -0.9022301435470581\n",
      "Val Loss for batch is  -0.6424752473831177\n",
      "Val Loss for batch is  -1.74339759349823\n",
      "|Iter  406  | Total Val Loss  -4.094401240348816 |\n",
      "Loss for batch is  1.6276509761810303\n",
      "Loss for batch is  1.541776180267334\n",
      "Loss for batch is  1.6985279321670532\n",
      "Loss for batch is  0.3200650215148926\n",
      "|Iter  407  | Total Train Loss  5.18802011013031 |\n",
      "Val Loss for batch is  -0.7551741003990173\n",
      "Val Loss for batch is  -0.9045259952545166\n",
      "Val Loss for batch is  -0.5760485529899597\n",
      "Val Loss for batch is  -1.7638609409332275\n",
      "|Iter  407  | Total Val Loss  -3.999609589576721 |\n",
      "Loss for batch is  1.638852596282959\n",
      "Loss for batch is  1.418553113937378\n",
      "Loss for batch is  1.7461652755737305\n",
      "Loss for batch is  0.29449141025543213\n",
      "|Iter  408  | Total Train Loss  5.0980623960494995 |\n",
      "Val Loss for batch is  -0.7883334755897522\n",
      "Val Loss for batch is  -0.9865483641624451\n",
      "Val Loss for batch is  -0.6081960201263428\n",
      "Val Loss for batch is  -1.6985048055648804\n",
      "|Iter  408  | Total Val Loss  -4.08158266544342 |\n",
      "Loss for batch is  1.6644892692565918\n",
      "Loss for batch is  1.4458236694335938\n",
      "Loss for batch is  1.6609784364700317\n",
      "Loss for batch is  0.3099493980407715\n",
      "|Iter  409  | Total Train Loss  5.081240773200989 |\n",
      "Val Loss for batch is  -0.6586741209030151\n",
      "Val Loss for batch is  -0.9341362118721008\n",
      "Val Loss for batch is  -0.7224206328392029\n",
      "Val Loss for batch is  -1.7311841249465942\n",
      "|Iter  409  | Total Val Loss  -4.046415090560913 |\n",
      "Loss for batch is  1.6626065969467163\n",
      "Loss for batch is  1.4016145467758179\n",
      "Loss for batch is  1.7085676193237305\n",
      "Loss for batch is  0.2915818691253662\n",
      "|Iter  410  | Total Train Loss  5.064370632171631 |\n",
      "Val Loss for batch is  -0.8251522183418274\n",
      "Val Loss for batch is  -0.97492915391922\n",
      "Val Loss for batch is  -0.7014740109443665\n",
      "Val Loss for batch is  -1.6948890686035156\n",
      "|Iter  410  | Total Val Loss  -4.196444451808929 |\n",
      "Loss for batch is  1.6408047676086426\n",
      "Loss for batch is  1.412907361984253\n",
      "Loss for batch is  1.6700220108032227\n",
      "Loss for batch is  0.3242521286010742\n",
      "|Iter  411  | Total Train Loss  5.047986268997192 |\n",
      "Val Loss for batch is  -0.8390694856643677\n",
      "Val Loss for batch is  -0.9685660004615784\n",
      "Val Loss for batch is  -0.7291654348373413\n",
      "Val Loss for batch is  -1.7005126476287842\n",
      "|Iter  411  | Total Val Loss  -4.2373135685920715 |\n",
      "Loss for batch is  1.5953655242919922\n",
      "Loss for batch is  1.379792332649231\n",
      "Loss for batch is  1.6726789474487305\n",
      "Loss for batch is  0.29564452171325684\n",
      "|Iter  412  | Total Train Loss  4.9434813261032104 |\n",
      "Val Loss for batch is  -0.8194196224212646\n",
      "Val Loss for batch is  -0.9431920647621155\n",
      "Val Loss for batch is  -0.7744423747062683\n",
      "Val Loss for batch is  -1.6810886859893799\n",
      "|Iter  412  | Total Val Loss  -4.218142747879028 |\n",
      "Loss for batch is  1.5852571725845337\n",
      "Loss for batch is  1.4232176542282104\n",
      "Loss for batch is  1.6634972095489502\n",
      "Loss for batch is  0.29734742641448975\n",
      "|Iter  413  | Total Train Loss  4.969319462776184 |\n",
      "Val Loss for batch is  -0.8931235671043396\n",
      "Val Loss for batch is  -1.0792778730392456\n",
      "Val Loss for batch is  -0.7898416519165039\n",
      "Val Loss for batch is  -1.7961902618408203\n",
      "|Iter  413  | Total Val Loss  -4.558433353900909 |\n",
      "Loss for batch is  1.6450779438018799\n",
      "Loss for batch is  1.3841533660888672\n",
      "Loss for batch is  1.6642571687698364\n",
      "Loss for batch is  0.3224450349807739\n",
      "|Iter  414  | Total Train Loss  5.015933513641357 |\n",
      "Val Loss for batch is  -0.79190593957901\n",
      "Val Loss for batch is  -0.9791989922523499\n",
      "Val Loss for batch is  -0.6882320046424866\n",
      "Val Loss for batch is  -1.717797040939331\n",
      "|Iter  414  | Total Val Loss  -4.1771339774131775 |\n",
      "Loss for batch is  1.5398313999176025\n",
      "Loss for batch is  1.3583412170410156\n",
      "Loss for batch is  1.6695759296417236\n",
      "Loss for batch is  0.2780132293701172\n",
      "|Iter  415  | Total Train Loss  4.845761775970459 |\n",
      "Val Loss for batch is  -0.8217292428016663\n",
      "Val Loss for batch is  -0.9872180223464966\n",
      "Val Loss for batch is  -0.6563552618026733\n",
      "Val Loss for batch is  -1.6806761026382446\n",
      "|Iter  415  | Total Val Loss  -4.145978629589081 |\n",
      "Loss for batch is  1.589254379272461\n",
      "Loss for batch is  1.337656021118164\n",
      "Loss for batch is  1.6149165630340576\n",
      "Loss for batch is  0.28685855865478516\n",
      "|Iter  416  | Total Train Loss  4.828685522079468 |\n",
      "Val Loss for batch is  -0.801530122756958\n",
      "Val Loss for batch is  -0.9713819026947021\n",
      "Val Loss for batch is  -0.7248647212982178\n",
      "Val Loss for batch is  -1.718901515007019\n",
      "|Iter  416  | Total Val Loss  -4.216678261756897 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.5883495807647705\n",
      "Loss for batch is  1.3606919050216675\n",
      "Loss for batch is  1.6637437343597412\n",
      "Loss for batch is  0.29338371753692627\n",
      "|Iter  417  | Total Train Loss  4.9061689376831055 |\n",
      "Val Loss for batch is  -0.7915021181106567\n",
      "Val Loss for batch is  -0.8887805938720703\n",
      "Val Loss for batch is  -0.6656097173690796\n",
      "Val Loss for batch is  -1.704209566116333\n",
      "|Iter  417  | Total Val Loss  -4.05010199546814 |\n",
      "Loss for batch is  1.582611322402954\n",
      "Loss for batch is  1.3426759243011475\n",
      "Loss for batch is  1.6345293521881104\n",
      "Loss for batch is  0.26312172412872314\n",
      "|Iter  418  | Total Train Loss  4.822938323020935 |\n",
      "Val Loss for batch is  -0.8212217688560486\n",
      "Val Loss for batch is  -1.0438634157180786\n",
      "Val Loss for batch is  -0.7622022032737732\n",
      "Val Loss for batch is  -1.785544753074646\n",
      "|Iter  418  | Total Val Loss  -4.412832140922546 |\n",
      "Loss for batch is  1.59639573097229\n",
      "Loss for batch is  1.3834550380706787\n",
      "Loss for batch is  1.5940371751785278\n",
      "Loss for batch is  0.27806174755096436\n",
      "|Iter  419  | Total Train Loss  4.851949691772461 |\n",
      "Val Loss for batch is  -0.8216849565505981\n",
      "Val Loss for batch is  -1.0079234838485718\n",
      "Val Loss for batch is  -0.7574136853218079\n",
      "Val Loss for batch is  -1.6906312704086304\n",
      "|Iter  419  | Total Val Loss  -4.277653396129608 |\n",
      "Loss for batch is  1.5112106800079346\n",
      "Loss for batch is  1.2834255695343018\n",
      "Loss for batch is  1.8117389678955078\n",
      "Loss for batch is  0.2750142812728882\n",
      "|Iter  420  | Total Train Loss  4.881389498710632 |\n",
      "Val Loss for batch is  -0.6167771220207214\n",
      "Val Loss for batch is  -0.8490892648696899\n",
      "Val Loss for batch is  -0.722536563873291\n",
      "Val Loss for batch is  -1.7994780540466309\n",
      "|Iter  420  | Total Val Loss  -3.9878810048103333 |\n",
      "Loss for batch is  1.7247974872589111\n",
      "Loss for batch is  1.3824265003204346\n",
      "Loss for batch is  1.8101780414581299\n",
      "Loss for batch is  0.2415311336517334\n",
      "|Iter  421  | Total Train Loss  5.158933162689209 |\n",
      "Val Loss for batch is  -0.6537817716598511\n",
      "Val Loss for batch is  -0.8632034659385681\n",
      "Val Loss for batch is  -0.6049917936325073\n",
      "Val Loss for batch is  -1.759901762008667\n",
      "|Iter  421  | Total Val Loss  -3.8818787932395935 |\n",
      "Loss for batch is  1.7057702541351318\n",
      "Loss for batch is  1.422354817390442\n",
      "Loss for batch is  1.7030034065246582\n",
      "Loss for batch is  0.2660706043243408\n",
      "|Iter  422  | Total Train Loss  5.097199082374573 |\n",
      "Val Loss for batch is  -0.6018773317337036\n",
      "Val Loss for batch is  -0.9322845935821533\n",
      "Val Loss for batch is  -0.6710140705108643\n",
      "Val Loss for batch is  -1.7230523824691772\n",
      "|Iter  422  | Total Val Loss  -3.9282283782958984 |\n",
      "Loss for batch is  1.7466691732406616\n",
      "Loss for batch is  1.4037657976150513\n",
      "Loss for batch is  1.7328925132751465\n",
      "Loss for batch is  0.3001441955566406\n",
      "|Iter  423  | Total Train Loss  5.1834716796875 |\n",
      "Val Loss for batch is  -0.8643648624420166\n",
      "Val Loss for batch is  -0.9482628107070923\n",
      "Val Loss for batch is  -0.7644602656364441\n",
      "Val Loss for batch is  -1.7706103324890137\n",
      "|Iter  423  | Total Val Loss  -4.347698271274567 |\n",
      "Loss for batch is  1.6485744714736938\n",
      "Loss for batch is  1.4023313522338867\n",
      "Loss for batch is  1.6832466125488281\n",
      "Loss for batch is  0.28075408935546875\n",
      "|Iter  424  | Total Train Loss  5.014906525611877 |\n",
      "Val Loss for batch is  -0.8470928072929382\n",
      "Val Loss for batch is  -1.029359221458435\n",
      "Val Loss for batch is  -0.7457338571548462\n",
      "Val Loss for batch is  -1.702109694480896\n",
      "|Iter  424  | Total Val Loss  -4.3242955803871155 |\n",
      "Loss for batch is  1.5671223402023315\n",
      "Loss for batch is  1.3880445957183838\n",
      "Loss for batch is  1.6428660154342651\n",
      "Loss for batch is  0.2798581123352051\n",
      "|Iter  425  | Total Train Loss  4.8778910636901855 |\n",
      "Val Loss for batch is  -0.8609399199485779\n",
      "Val Loss for batch is  -1.0254751443862915\n",
      "Val Loss for batch is  -0.7491306662559509\n",
      "Val Loss for batch is  -1.7692739963531494\n",
      "|Iter  425  | Total Val Loss  -4.40481972694397 |\n",
      "Loss for batch is  1.6035324335098267\n",
      "Loss for batch is  1.3522124290466309\n",
      "Loss for batch is  1.6460859775543213\n",
      "Loss for batch is  0.2583315372467041\n",
      "|Iter  426  | Total Train Loss  4.860162377357483 |\n",
      "Val Loss for batch is  -0.8416297435760498\n",
      "Val Loss for batch is  -1.0575557947158813\n",
      "Val Loss for batch is  -0.7061932682991028\n",
      "Val Loss for batch is  -1.734082818031311\n",
      "|Iter  426  | Total Val Loss  -4.339461624622345 |\n",
      "Loss for batch is  1.5907143354415894\n",
      "Loss for batch is  1.344010829925537\n",
      "Loss for batch is  1.6005544662475586\n",
      "Loss for batch is  0.271154522895813\n",
      "|Iter  427  | Total Train Loss  4.806434154510498 |\n",
      "Val Loss for batch is  -0.9062646627426147\n",
      "Val Loss for batch is  -1.0299345254898071\n",
      "Val Loss for batch is  -0.665676474571228\n",
      "Val Loss for batch is  -1.7322763204574585\n",
      "|Iter  427  | Total Val Loss  -4.334151983261108 |\n",
      "Loss for batch is  1.557955265045166\n",
      "Loss for batch is  1.347430944442749\n",
      "Loss for batch is  1.5435010194778442\n",
      "Loss for batch is  0.23129475116729736\n",
      "|Iter  428  | Total Train Loss  4.680181980133057 |\n",
      "Val Loss for batch is  -0.8758272528648376\n",
      "Val Loss for batch is  -0.9454143643379211\n",
      "Val Loss for batch is  -0.8343285918235779\n",
      "Val Loss for batch is  -1.726334810256958\n",
      "|Iter  428  | Total Val Loss  -4.381905019283295 |\n",
      "Loss for batch is  1.5472371578216553\n",
      "Loss for batch is  1.3149745464324951\n",
      "Loss for batch is  1.5617649555206299\n",
      "Loss for batch is  0.2623640298843384\n",
      "|Iter  429  | Total Train Loss  4.686340689659119 |\n",
      "Val Loss for batch is  -0.9154084920883179\n",
      "Val Loss for batch is  -1.0247392654418945\n",
      "Val Loss for batch is  -0.7924028038978577\n",
      "Val Loss for batch is  -1.7563323974609375\n",
      "|Iter  429  | Total Val Loss  -4.488882958889008 |\n",
      "Loss for batch is  1.5270763635635376\n",
      "Loss for batch is  1.317464828491211\n",
      "Loss for batch is  1.5211341381072998\n",
      "Loss for batch is  0.2238912582397461\n",
      "|Iter  430  | Total Train Loss  4.589566588401794 |\n",
      "Val Loss for batch is  -0.8534790277481079\n",
      "Val Loss for batch is  -1.0078141689300537\n",
      "Val Loss for batch is  -0.8305324912071228\n",
      "Val Loss for batch is  -1.7598568201065063\n",
      "|Iter  430  | Total Val Loss  -4.451682507991791 |\n",
      "Loss for batch is  1.5129135847091675\n",
      "Loss for batch is  1.2958430051803589\n",
      "Loss for batch is  1.5526882410049438\n",
      "Loss for batch is  0.20604515075683594\n",
      "|Iter  431  | Total Train Loss  4.567489981651306 |\n",
      "Val Loss for batch is  -0.8665683269500732\n",
      "Val Loss for batch is  -1.02069091796875\n",
      "Val Loss for batch is  -0.8220329880714417\n",
      "Val Loss for batch is  -1.7904764413833618\n",
      "|Iter  431  | Total Val Loss  -4.499768674373627 |\n",
      "Loss for batch is  1.5016026496887207\n",
      "Loss for batch is  1.2981252670288086\n",
      "Loss for batch is  1.554781436920166\n",
      "Loss for batch is  0.21133816242218018\n",
      "|Iter  432  | Total Train Loss  4.5658475160598755 |\n",
      "Val Loss for batch is  -0.7722748517990112\n",
      "Val Loss for batch is  -0.999752402305603\n",
      "Val Loss for batch is  -0.7783358097076416\n",
      "Val Loss for batch is  -1.69033682346344\n",
      "|Iter  432  | Total Val Loss  -4.240699887275696 |\n",
      "Loss for batch is  1.5093027353286743\n",
      "Loss for batch is  1.2801508903503418\n",
      "Loss for batch is  1.5244728326797485\n",
      "Loss for batch is  0.22510671615600586\n",
      "|Iter  433  | Total Train Loss  4.5390331745147705 |\n",
      "Val Loss for batch is  -0.8688712120056152\n",
      "Val Loss for batch is  -0.8680309057235718\n",
      "Val Loss for batch is  -0.7216227650642395\n",
      "Val Loss for batch is  -1.744897723197937\n",
      "|Iter  433  | Total Val Loss  -4.2034226059913635 |\n",
      "Loss for batch is  1.5253262519836426\n",
      "Loss for batch is  1.2886784076690674\n",
      "Loss for batch is  1.5322465896606445\n",
      "Loss for batch is  0.20847737789154053\n",
      "|Iter  434  | Total Train Loss  4.554728627204895 |\n",
      "Val Loss for batch is  -0.8600749969482422\n",
      "Val Loss for batch is  -1.0187530517578125\n",
      "Val Loss for batch is  -0.8142141103744507\n",
      "Val Loss for batch is  -1.7534356117248535\n",
      "|Iter  434  | Total Val Loss  -4.446477770805359 |\n",
      "Loss for batch is  1.5141081809997559\n",
      "Loss for batch is  1.2401139736175537\n",
      "Loss for batch is  1.4731168746948242\n",
      "Loss for batch is  0.199379563331604\n",
      "|Iter  435  | Total Train Loss  4.426718592643738 |\n",
      "Val Loss for batch is  -0.9270379543304443\n",
      "Val Loss for batch is  -0.9496318697929382\n",
      "Val Loss for batch is  -0.6496887803077698\n",
      "Val Loss for batch is  -1.6878336668014526\n",
      "|Iter  435  | Total Val Loss  -4.214192271232605 |\n",
      "Loss for batch is  1.4798252582550049\n",
      "Loss for batch is  1.2488868236541748\n",
      "Loss for batch is  1.5418453216552734\n",
      "Loss for batch is  0.1843947172164917\n",
      "|Iter  436  | Total Train Loss  4.454952120780945 |\n",
      "Val Loss for batch is  -0.7758426666259766\n",
      "Val Loss for batch is  -0.85141521692276\n",
      "Val Loss for batch is  -0.7758381366729736\n",
      "Val Loss for batch is  -1.7880367040634155\n",
      "|Iter  436  | Total Val Loss  -4.191132724285126 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.514715552330017\n",
      "Loss for batch is  1.450714349746704\n",
      "Loss for batch is  1.587385892868042\n",
      "Loss for batch is  0.17971348762512207\n",
      "|Iter  437  | Total Train Loss  4.732529282569885 |\n",
      "Val Loss for batch is  -0.8334537148475647\n",
      "Val Loss for batch is  -0.9030449986457825\n",
      "Val Loss for batch is  -0.6708206534385681\n",
      "Val Loss for batch is  -1.7447460889816284\n",
      "|Iter  437  | Total Val Loss  -4.152065455913544 |\n",
      "Loss for batch is  1.554506778717041\n",
      "Loss for batch is  1.3468701839447021\n",
      "Loss for batch is  1.6220399141311646\n",
      "Loss for batch is  0.21653902530670166\n",
      "|Iter  438  | Total Train Loss  4.739955902099609 |\n",
      "Val Loss for batch is  -0.8060214519500732\n",
      "Val Loss for batch is  -0.9999546408653259\n",
      "Val Loss for batch is  -0.7045725584030151\n",
      "Val Loss for batch is  -1.820511817932129\n",
      "|Iter  438  | Total Val Loss  -4.331060469150543 |\n",
      "Loss for batch is  1.5489590167999268\n",
      "Loss for batch is  1.3448638916015625\n",
      "Loss for batch is  1.5349841117858887\n",
      "Loss for batch is  0.188973069190979\n",
      "|Iter  439  | Total Train Loss  4.617780089378357 |\n",
      "Val Loss for batch is  -0.9662885069847107\n",
      "Val Loss for batch is  -1.0908459424972534\n",
      "Val Loss for batch is  -0.7567189931869507\n",
      "Val Loss for batch is  -1.7961806058883667\n",
      "|Iter  439  | Total Val Loss  -4.6100340485572815 |\n",
      "Loss for batch is  1.4923062324523926\n",
      "Loss for batch is  1.2904623746871948\n",
      "Loss for batch is  1.618074655532837\n",
      "Loss for batch is  0.18530869483947754\n",
      "|Iter  440  | Total Train Loss  4.586151957511902 |\n",
      "Val Loss for batch is  -0.8730331659317017\n",
      "Val Loss for batch is  -1.0374555587768555\n",
      "Val Loss for batch is  -0.7749418020248413\n",
      "Val Loss for batch is  -1.7610502243041992\n",
      "|Iter  440  | Total Val Loss  -4.446480751037598 |\n",
      "Loss for batch is  1.5010576248168945\n",
      "Loss for batch is  1.3151633739471436\n",
      "Loss for batch is  1.4986212253570557\n",
      "Loss for batch is  0.22748112678527832\n",
      "|Iter  441  | Total Train Loss  4.542323350906372 |\n",
      "Val Loss for batch is  -0.8837695717811584\n",
      "Val Loss for batch is  -1.0553966760635376\n",
      "Val Loss for batch is  -0.8212950825691223\n",
      "Val Loss for batch is  -1.8287568092346191\n",
      "|Iter  441  | Total Val Loss  -4.5892181396484375 |\n",
      "Loss for batch is  1.4914801120758057\n",
      "Loss for batch is  1.269960880279541\n",
      "Loss for batch is  1.5505986213684082\n",
      "Loss for batch is  0.20734941959381104\n",
      "|Iter  442  | Total Train Loss  4.519389033317566 |\n",
      "Val Loss for batch is  -0.9307612776756287\n",
      "Val Loss for batch is  -1.0420589447021484\n",
      "Val Loss for batch is  -0.7826894521713257\n",
      "Val Loss for batch is  -1.7920886278152466\n",
      "|Iter  442  | Total Val Loss  -4.547598302364349 |\n",
      "Loss for batch is  1.4487513303756714\n",
      "Loss for batch is  1.273266077041626\n",
      "Loss for batch is  1.7567071914672852\n",
      "Loss for batch is  0.20689308643341064\n",
      "|Iter  443  | Total Train Loss  4.685617685317993 |\n",
      "Val Loss for batch is  -0.9208465814590454\n",
      "Val Loss for batch is  -0.9710049033164978\n",
      "Val Loss for batch is  -0.7396343350410461\n",
      "Val Loss for batch is  -1.8036913871765137\n",
      "|Iter  443  | Total Val Loss  -4.435177206993103 |\n",
      "Loss for batch is  1.4862055778503418\n",
      "Loss for batch is  1.2683628797531128\n",
      "Loss for batch is  1.517303705215454\n",
      "Loss for batch is  0.19903814792633057\n",
      "|Iter  444  | Total Train Loss  4.470910310745239 |\n",
      "Val Loss for batch is  -0.706940233707428\n",
      "Val Loss for batch is  -1.1248003244400024\n",
      "Val Loss for batch is  -0.7643589377403259\n",
      "Val Loss for batch is  -1.772057056427002\n",
      "|Iter  444  | Total Val Loss  -4.368156552314758 |\n",
      "Loss for batch is  1.6494109630584717\n",
      "Loss for batch is  1.2461822032928467\n",
      "Loss for batch is  1.5115571022033691\n",
      "Loss for batch is  0.17190933227539062\n",
      "|Iter  445  | Total Train Loss  4.579059600830078 |\n",
      "Val Loss for batch is  -0.7882035374641418\n",
      "Val Loss for batch is  -1.1089385747909546\n",
      "Val Loss for batch is  -0.7578741312026978\n",
      "Val Loss for batch is  -1.8008089065551758\n",
      "|Iter  445  | Total Val Loss  -4.45582515001297 |\n",
      "Loss for batch is  1.6131722927093506\n",
      "Loss for batch is  1.2143220901489258\n",
      "Loss for batch is  1.4852559566497803\n",
      "Loss for batch is  0.19840383529663086\n",
      "|Iter  446  | Total Train Loss  4.5111541748046875 |\n",
      "Val Loss for batch is  -0.8224549889564514\n",
      "Val Loss for batch is  -1.0802677869796753\n",
      "Val Loss for batch is  -0.7631796598434448\n",
      "Val Loss for batch is  -1.7688395977020264\n",
      "|Iter  446  | Total Val Loss  -4.434742033481598 |\n",
      "Loss for batch is  1.5007333755493164\n",
      "Loss for batch is  1.237029790878296\n",
      "Loss for batch is  1.5026218891143799\n",
      "Loss for batch is  0.16167998313903809\n",
      "|Iter  447  | Total Train Loss  4.40206503868103 |\n",
      "Val Loss for batch is  -0.9064539670944214\n",
      "Val Loss for batch is  -1.0022180080413818\n",
      "Val Loss for batch is  -0.7934502363204956\n",
      "Val Loss for batch is  -1.8285349607467651\n",
      "|Iter  447  | Total Val Loss  -4.530657172203064 |\n",
      "Loss for batch is  1.456888198852539\n",
      "Loss for batch is  1.2165096998214722\n",
      "Loss for batch is  1.433203935623169\n",
      "Loss for batch is  0.13225650787353516\n",
      "|Iter  448  | Total Train Loss  4.238858342170715 |\n",
      "Val Loss for batch is  -0.9762972593307495\n",
      "Val Loss for batch is  -1.0985064506530762\n",
      "Val Loss for batch is  -0.8589000701904297\n",
      "Val Loss for batch is  -1.8244885206222534\n",
      "|Iter  448  | Total Val Loss  -4.758192300796509 |\n",
      "Loss for batch is  1.4220671653747559\n",
      "Loss for batch is  1.2187564373016357\n",
      "Loss for batch is  1.4470783472061157\n",
      "Loss for batch is  0.1732015609741211\n",
      "|Iter  449  | Total Train Loss  4.261103510856628 |\n",
      "Val Loss for batch is  -0.8498173356056213\n",
      "Val Loss for batch is  -1.030598521232605\n",
      "Val Loss for batch is  -0.5273751020431519\n",
      "Val Loss for batch is  -1.8047581911087036\n",
      "|Iter  449  | Total Val Loss  -4.212549149990082 |\n",
      "Loss for batch is  1.459538221359253\n",
      "Loss for batch is  1.2290763854980469\n",
      "Loss for batch is  1.4000859260559082\n",
      "Loss for batch is  0.14103376865386963\n",
      "|Iter  450  | Total Train Loss  4.229734301567078 |\n",
      "Val Loss for batch is  -0.9185518026351929\n",
      "Val Loss for batch is  -1.044406771659851\n",
      "Val Loss for batch is  -0.8511800765991211\n",
      "Val Loss for batch is  -1.8979709148406982\n",
      "|Iter  450  | Total Val Loss  -4.712109565734863 |\n",
      "Loss for batch is  1.4314987659454346\n",
      "Loss for batch is  1.213261365890503\n",
      "Loss for batch is  1.4103578329086304\n",
      "Loss for batch is  0.15355420112609863\n",
      "|Iter  451  | Total Train Loss  4.2086721658706665 |\n",
      "Val Loss for batch is  -0.9560913443565369\n",
      "Val Loss for batch is  -1.0665395259857178\n",
      "Val Loss for batch is  -0.6614945530891418\n",
      "Val Loss for batch is  -1.8285419940948486\n",
      "|Iter  451  | Total Val Loss  -4.512667417526245 |\n",
      "Loss for batch is  1.3992712497711182\n",
      "Loss for batch is  1.228891372680664\n",
      "Loss for batch is  1.4277070760726929\n",
      "Loss for batch is  0.11465895175933838\n",
      "|Iter  452  | Total Train Loss  4.1705286502838135 |\n",
      "Val Loss for batch is  -0.8307148218154907\n",
      "Val Loss for batch is  -0.9161815047264099\n",
      "Val Loss for batch is  -0.7860274314880371\n",
      "Val Loss for batch is  -1.790300726890564\n",
      "|Iter  452  | Total Val Loss  -4.323224484920502 |\n",
      "Loss for batch is  1.373949646949768\n",
      "Loss for batch is  1.2339317798614502\n",
      "Loss for batch is  1.4361199140548706\n",
      "Loss for batch is  0.1394864320755005\n",
      "|Iter  453  | Total Train Loss  4.183487772941589 |\n",
      "Val Loss for batch is  -0.889382541179657\n",
      "Val Loss for batch is  -1.0995333194732666\n",
      "Val Loss for batch is  -0.8696222305297852\n",
      "Val Loss for batch is  -1.816516637802124\n",
      "|Iter  453  | Total Val Loss  -4.675054728984833 |\n",
      "Loss for batch is  1.3712455034255981\n",
      "Loss for batch is  1.2333564758300781\n",
      "Loss for batch is  1.4361836910247803\n",
      "Loss for batch is  0.13429927825927734\n",
      "|Iter  454  | Total Train Loss  4.175084948539734 |\n",
      "Val Loss for batch is  -0.9347913861274719\n",
      "Val Loss for batch is  -1.0766000747680664\n",
      "Val Loss for batch is  -0.7984957098960876\n",
      "Val Loss for batch is  -1.7989263534545898\n",
      "|Iter  454  | Total Val Loss  -4.608813524246216 |\n",
      "Loss for batch is  1.3478364944458008\n",
      "Loss for batch is  1.179356336593628\n",
      "Loss for batch is  1.3894517421722412\n",
      "Loss for batch is  0.1518346071243286\n",
      "|Iter  455  | Total Train Loss  4.0684791803359985 |\n",
      "Val Loss for batch is  -0.9449374675750732\n",
      "Val Loss for batch is  -1.06870436668396\n",
      "Val Loss for batch is  -0.8138954639434814\n",
      "Val Loss for batch is  -1.7715680599212646\n",
      "|Iter  455  | Total Val Loss  -4.599105358123779 |\n",
      "Loss for batch is  1.3197293281555176\n",
      "Loss for batch is  1.1861835718154907\n",
      "Loss for batch is  1.3640422821044922\n",
      "Loss for batch is  0.10311925411224365\n",
      "|Iter  456  | Total Train Loss  3.973074436187744 |\n",
      "Val Loss for batch is  -0.9703735113143921\n",
      "Val Loss for batch is  -1.1488049030303955\n",
      "Val Loss for batch is  -0.8700144290924072\n",
      "Val Loss for batch is  -1.881341814994812\n",
      "|Iter  456  | Total Val Loss  -4.870534658432007 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.3317856788635254\n",
      "Loss for batch is  1.148481845855713\n",
      "Loss for batch is  1.3453354835510254\n",
      "Loss for batch is  0.10257482528686523\n",
      "|Iter  457  | Total Train Loss  3.928177833557129 |\n",
      "Val Loss for batch is  -0.9811513423919678\n",
      "Val Loss for batch is  -1.149774432182312\n",
      "Val Loss for batch is  -0.9057507514953613\n",
      "Val Loss for batch is  -1.8763384819030762\n",
      "|Iter  457  | Total Val Loss  -4.913015007972717 |\n",
      "Loss for batch is  1.3121922016143799\n",
      "Loss for batch is  1.1176412105560303\n",
      "Loss for batch is  1.3693418502807617\n",
      "Loss for batch is  0.08696115016937256\n",
      "|Iter  458  | Total Train Loss  3.8861364126205444 |\n",
      "Val Loss for batch is  -0.8801161646842957\n",
      "Val Loss for batch is  -1.077296495437622\n",
      "Val Loss for batch is  -0.8104754686355591\n",
      "Val Loss for batch is  -1.7689710855484009\n",
      "|Iter  458  | Total Val Loss  -4.536859214305878 |\n",
      "Loss for batch is  1.330012559890747\n",
      "Loss for batch is  1.1480852365493774\n",
      "Loss for batch is  1.4004237651824951\n",
      "Loss for batch is  0.09117865562438965\n",
      "|Iter  459  | Total Train Loss  3.9697002172470093 |\n",
      "Val Loss for batch is  -0.9627545475959778\n",
      "Val Loss for batch is  -1.0069330930709839\n",
      "Val Loss for batch is  -0.8371807932853699\n",
      "Val Loss for batch is  -1.8248859643936157\n",
      "|Iter  459  | Total Val Loss  -4.631754398345947 |\n",
      "Loss for batch is  1.3143153190612793\n",
      "Loss for batch is  1.1094969511032104\n",
      "Loss for batch is  1.3562986850738525\n",
      "Loss for batch is  0.1076122522354126\n",
      "|Iter  460  | Total Train Loss  3.887723207473755 |\n",
      "Val Loss for batch is  -0.9596737623214722\n",
      "Val Loss for batch is  -1.0867301225662231\n",
      "Val Loss for batch is  -0.7358421683311462\n",
      "Val Loss for batch is  -1.8920215368270874\n",
      "|Iter  460  | Total Val Loss  -4.674267590045929 |\n",
      "Loss for batch is  1.293607473373413\n",
      "Loss for batch is  1.188795804977417\n",
      "Loss for batch is  1.332808494567871\n",
      "Loss for batch is  0.10640096664428711\n",
      "|Iter  461  | Total Train Loss  3.9216127395629883 |\n",
      "Val Loss for batch is  -0.9203019142150879\n",
      "Val Loss for batch is  -1.069135069847107\n",
      "Val Loss for batch is  -0.858883261680603\n",
      "Val Loss for batch is  -1.7829782962799072\n",
      "|Iter  461  | Total Val Loss  -4.631298542022705 |\n",
      "Loss for batch is  1.2679295539855957\n",
      "Loss for batch is  1.150768518447876\n",
      "Loss for batch is  1.3365877866744995\n",
      "Loss for batch is  0.08748483657836914\n",
      "|Iter  462  | Total Train Loss  3.8427706956863403 |\n",
      "Val Loss for batch is  -0.9710531830787659\n",
      "Val Loss for batch is  -1.1770437955856323\n",
      "Val Loss for batch is  -0.8644624948501587\n",
      "Val Loss for batch is  -1.8286703824996948\n",
      "|Iter  462  | Total Val Loss  -4.841229856014252 |\n",
      "Loss for batch is  1.3463454246520996\n",
      "Loss for batch is  1.0812792778015137\n",
      "Loss for batch is  1.329550862312317\n",
      "Loss for batch is  0.09165334701538086\n",
      "|Iter  463  | Total Train Loss  3.848828911781311 |\n",
      "Val Loss for batch is  -0.8590246438980103\n",
      "Val Loss for batch is  -1.0036391019821167\n",
      "Val Loss for batch is  -0.8394250273704529\n",
      "Val Loss for batch is  -1.8569519519805908\n",
      "|Iter  463  | Total Val Loss  -4.559040725231171 |\n",
      "Loss for batch is  1.2788162231445312\n",
      "Loss for batch is  1.1161085367202759\n",
      "Loss for batch is  1.3206853866577148\n",
      "Loss for batch is  0.07244610786437988\n",
      "|Iter  464  | Total Train Loss  3.788056254386902 |\n",
      "Val Loss for batch is  -0.9951444268226624\n",
      "Val Loss for batch is  -1.111274242401123\n",
      "Val Loss for batch is  -0.8862064480781555\n",
      "Val Loss for batch is  -1.8045574426651\n",
      "|Iter  464  | Total Val Loss  -4.797182559967041 |\n",
      "Loss for batch is  1.2831083536148071\n",
      "Loss for batch is  1.0911312103271484\n",
      "Loss for batch is  1.331859827041626\n",
      "Loss for batch is  0.0773230791091919\n",
      "|Iter  465  | Total Train Loss  3.7834224700927734 |\n",
      "Val Loss for batch is  -0.9818100929260254\n",
      "Val Loss for batch is  -1.068666696548462\n",
      "Val Loss for batch is  -0.9079865217208862\n",
      "Val Loss for batch is  -1.8800212144851685\n",
      "|Iter  465  | Total Val Loss  -4.838484525680542 |\n",
      "Loss for batch is  1.2852495908737183\n",
      "Loss for batch is  1.1138782501220703\n",
      "Loss for batch is  1.3475944995880127\n",
      "Loss for batch is  0.10543227195739746\n",
      "|Iter  466  | Total Train Loss  3.8521546125411987 |\n",
      "Val Loss for batch is  -0.9571536779403687\n",
      "Val Loss for batch is  -1.009522557258606\n",
      "Val Loss for batch is  -0.8828113079071045\n",
      "Val Loss for batch is  -1.7625079154968262\n",
      "|Iter  466  | Total Val Loss  -4.611995458602905 |\n",
      "Loss for batch is  1.2530213594436646\n",
      "Loss for batch is  1.129534125328064\n",
      "Loss for batch is  1.3004025220870972\n",
      "Loss for batch is  0.06485927104949951\n",
      "|Iter  467  | Total Train Loss  3.747817277908325 |\n",
      "Val Loss for batch is  -1.067016363143921\n",
      "Val Loss for batch is  -1.0947140455245972\n",
      "Val Loss for batch is  -0.8649106621742249\n",
      "Val Loss for batch is  -1.875657558441162\n",
      "|Iter  467  | Total Val Loss  -4.902298629283905 |\n",
      "Loss for batch is  1.1995184421539307\n",
      "Loss for batch is  1.0782206058502197\n",
      "Loss for batch is  1.258634328842163\n",
      "Loss for batch is  0.031149864196777344\n",
      "|Iter  468  | Total Train Loss  3.567523241043091 |\n",
      "Val Loss for batch is  -1.0398786067962646\n",
      "Val Loss for batch is  -1.1391701698303223\n",
      "Val Loss for batch is  -0.9199007749557495\n",
      "Val Loss for batch is  -1.8886499404907227\n",
      "|Iter  468  | Total Val Loss  -4.987599492073059 |\n",
      "Loss for batch is  1.2495472431182861\n",
      "Loss for batch is  1.0790455341339111\n",
      "Loss for batch is  1.2959411144256592\n",
      "Loss for batch is  0.07175612449645996\n",
      "|Iter  469  | Total Train Loss  3.6962900161743164 |\n",
      "Val Loss for batch is  -0.9742890000343323\n",
      "Val Loss for batch is  -0.9781498908996582\n",
      "Val Loss for batch is  -0.8862642049789429\n",
      "Val Loss for batch is  -1.8803235292434692\n",
      "|Iter  469  | Total Val Loss  -4.719026625156403 |\n",
      "Loss for batch is  1.2319598197937012\n",
      "Loss for batch is  1.0981101989746094\n",
      "Loss for batch is  1.2949435710906982\n",
      "Loss for batch is  0.056610107421875\n",
      "|Iter  470  | Total Train Loss  3.681623697280884 |\n",
      "Val Loss for batch is  -1.0159244537353516\n",
      "Val Loss for batch is  -1.0849785804748535\n",
      "Val Loss for batch is  -0.837503969669342\n",
      "Val Loss for batch is  -1.8844292163848877\n",
      "|Iter  470  | Total Val Loss  -4.822836220264435 |\n",
      "Loss for batch is  1.2771687507629395\n",
      "Loss for batch is  1.0613715648651123\n",
      "Loss for batch is  1.2767300605773926\n",
      "Loss for batch is  0.06458961963653564\n",
      "|Iter  471  | Total Train Loss  3.67985999584198 |\n",
      "Val Loss for batch is  -1.0527863502502441\n",
      "Val Loss for batch is  -1.1006451845169067\n",
      "Val Loss for batch is  -0.91548752784729\n",
      "Val Loss for batch is  -1.8647747039794922\n",
      "|Iter  471  | Total Val Loss  -4.933693766593933 |\n",
      "Loss for batch is  1.2377139329910278\n",
      "Loss for batch is  1.0736699104309082\n",
      "Loss for batch is  1.2815440893173218\n",
      "Loss for batch is  0.04127299785614014\n",
      "|Iter  472  | Total Train Loss  3.634200930595398 |\n",
      "Val Loss for batch is  -0.9859751462936401\n",
      "Val Loss for batch is  -1.0994508266448975\n",
      "Val Loss for batch is  -0.8541783094406128\n",
      "Val Loss for batch is  -1.853887677192688\n",
      "|Iter  472  | Total Val Loss  -4.793491959571838 |\n",
      "Loss for batch is  1.2083659172058105\n",
      "Loss for batch is  1.0670535564422607\n",
      "Loss for batch is  1.263042688369751\n",
      "Loss for batch is  0.0363004207611084\n",
      "|Iter  473  | Total Train Loss  3.5747625827789307 |\n",
      "Val Loss for batch is  -1.005474328994751\n",
      "Val Loss for batch is  -1.1784484386444092\n",
      "Val Loss for batch is  -0.9785678386688232\n",
      "Val Loss for batch is  -1.9152334928512573\n",
      "|Iter  473  | Total Val Loss  -5.077724099159241 |\n",
      "Loss for batch is  1.2268882989883423\n",
      "Loss for batch is  1.0120675563812256\n",
      "Loss for batch is  1.2030370235443115\n",
      "Loss for batch is  0.01617717742919922\n",
      "|Iter  474  | Total Train Loss  3.4581700563430786 |\n",
      "Val Loss for batch is  -0.9793131947517395\n",
      "Val Loss for batch is  -0.9576500654220581\n",
      "Val Loss for batch is  -0.9399248361587524\n",
      "Val Loss for batch is  -1.8434656858444214\n",
      "|Iter  474  | Total Val Loss  -4.720353782176971 |\n",
      "Loss for batch is  1.2160711288452148\n",
      "Loss for batch is  1.000413417816162\n",
      "Loss for batch is  1.2396204471588135\n",
      "Loss for batch is  0.009086012840270996\n",
      "|Iter  475  | Total Train Loss  3.4651910066604614 |\n",
      "Val Loss for batch is  -0.8633860349655151\n",
      "Val Loss for batch is  -0.9536561369895935\n",
      "Val Loss for batch is  -0.9668972492218018\n",
      "Val Loss for batch is  -1.8862597942352295\n",
      "|Iter  475  | Total Val Loss  -4.67019921541214 |\n",
      "Loss for batch is  1.203918695449829\n",
      "Loss for batch is  1.0405113697052002\n",
      "Loss for batch is  1.192175030708313\n",
      "Loss for batch is  -0.00981593132019043\n",
      "|Iter  476  | Total Train Loss  3.426789164543152 |\n",
      "Val Loss for batch is  -1.052355170249939\n",
      "Val Loss for batch is  -1.1342164278030396\n",
      "Val Loss for batch is  -0.9574779868125916\n",
      "Val Loss for batch is  -1.8623442649841309\n",
      "|Iter  476  | Total Val Loss  -5.006393849849701 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.2019219398498535\n",
      "Loss for batch is  0.9933332204818726\n",
      "Loss for batch is  1.2262715101242065\n",
      "Loss for batch is  -0.0032881498336791992\n",
      "|Iter  477  | Total Train Loss  3.4182385206222534 |\n",
      "Val Loss for batch is  -0.9867639541625977\n",
      "Val Loss for batch is  -1.024376630783081\n",
      "Val Loss for batch is  -0.9629005193710327\n",
      "Val Loss for batch is  -1.8908355236053467\n",
      "|Iter  477  | Total Val Loss  -4.864876627922058 |\n",
      "Loss for batch is  1.1927409172058105\n",
      "Loss for batch is  1.0012452602386475\n",
      "Loss for batch is  1.2850905656814575\n",
      "Loss for batch is  0.006017327308654785\n",
      "|Iter  478  | Total Train Loss  3.4850940704345703 |\n",
      "Val Loss for batch is  -0.9829999804496765\n",
      "Val Loss for batch is  -1.1096279621124268\n",
      "Val Loss for batch is  -0.8617118000984192\n",
      "Val Loss for batch is  -1.9011553525924683\n",
      "|Iter  478  | Total Val Loss  -4.855495095252991 |\n",
      "Loss for batch is  1.2037267684936523\n",
      "Loss for batch is  1.0317163467407227\n",
      "Loss for batch is  1.1940733194351196\n",
      "Loss for batch is  -0.029125452041625977\n",
      "|Iter  479  | Total Train Loss  3.4003909826278687 |\n",
      "Val Loss for batch is  -0.9753305315971375\n",
      "Val Loss for batch is  -1.137548565864563\n",
      "Val Loss for batch is  -0.9984132051467896\n",
      "Val Loss for batch is  -1.938559889793396\n",
      "|Iter  479  | Total Val Loss  -5.049852192401886 |\n",
      "Loss for batch is  1.201767921447754\n",
      "Loss for batch is  0.9923253655433655\n",
      "Loss for batch is  1.2118600606918335\n",
      "Loss for batch is  -0.01871359348297119\n",
      "|Iter  480  | Total Train Loss  3.3872397541999817 |\n",
      "Val Loss for batch is  -0.9970871210098267\n",
      "Val Loss for batch is  -1.1028692722320557\n",
      "Val Loss for batch is  -0.9652807116508484\n",
      "Val Loss for batch is  -1.9324891567230225\n",
      "|Iter  480  | Total Val Loss  -4.997726261615753 |\n",
      "Loss for batch is  1.1822726726531982\n",
      "Loss for batch is  0.9812601208686829\n",
      "Loss for batch is  1.1924355030059814\n",
      "Loss for batch is  -0.013201594352722168\n",
      "|Iter  481  | Total Train Loss  3.3427667021751404 |\n",
      "Val Loss for batch is  -1.035667061805725\n",
      "Val Loss for batch is  -1.0416738986968994\n",
      "Val Loss for batch is  -0.9638274312019348\n",
      "Val Loss for batch is  -1.8390849828720093\n",
      "|Iter  481  | Total Val Loss  -4.880253374576569 |\n",
      "Loss for batch is  1.169621229171753\n",
      "Loss for batch is  0.9882258772850037\n",
      "Loss for batch is  1.2191898822784424\n",
      "Loss for batch is  -0.042020320892333984\n",
      "|Iter  482  | Total Train Loss  3.335016667842865 |\n",
      "Val Loss for batch is  -1.1127352714538574\n",
      "Val Loss for batch is  -1.2140069007873535\n",
      "Val Loss for batch is  -1.0027512311935425\n",
      "Val Loss for batch is  -1.9331032037734985\n",
      "|Iter  482  | Total Val Loss  -5.262596607208252 |\n",
      "Loss for batch is  1.1865768432617188\n",
      "Loss for batch is  0.9976341128349304\n",
      "Loss for batch is  1.262145757675171\n",
      "Loss for batch is  -0.031193017959594727\n",
      "|Iter  483  | Total Train Loss  3.4151636958122253 |\n",
      "Val Loss for batch is  -1.0317572355270386\n",
      "Val Loss for batch is  -1.0500141382217407\n",
      "Val Loss for batch is  -0.9601151943206787\n",
      "Val Loss for batch is  -1.9278373718261719\n",
      "|Iter  483  | Total Val Loss  -4.96972393989563 |\n",
      "Loss for batch is  1.2055788040161133\n",
      "Loss for batch is  0.9748170971870422\n",
      "Loss for batch is  1.4760596752166748\n",
      "Loss for batch is  0.028699040412902832\n",
      "|Iter  484  | Total Train Loss  3.685154616832733 |\n",
      "Val Loss for batch is  -0.8652676343917847\n",
      "Val Loss for batch is  -0.8848918676376343\n",
      "Val Loss for batch is  -0.921782910823822\n",
      "Val Loss for batch is  -1.9170339107513428\n",
      "|Iter  484  | Total Val Loss  -4.588976323604584 |\n",
      "Loss for batch is  1.2736741304397583\n",
      "Loss for batch is  1.2615057229995728\n",
      "Loss for batch is  1.284590244293213\n",
      "Loss for batch is  0.02254951000213623\n",
      "|Iter  485  | Total Train Loss  3.84231960773468 |\n",
      "Val Loss for batch is  -1.0041943788528442\n",
      "Val Loss for batch is  -1.1092411279678345\n",
      "Val Loss for batch is  -0.8423998951911926\n",
      "Val Loss for batch is  -1.9763963222503662\n",
      "|Iter  485  | Total Val Loss  -4.9322317242622375 |\n",
      "Loss for batch is  1.2519617080688477\n",
      "Loss for batch is  1.1173681020736694\n",
      "Loss for batch is  1.363040566444397\n",
      "Loss for batch is  0.02711665630340576\n",
      "|Iter  486  | Total Train Loss  3.75948703289032 |\n",
      "Val Loss for batch is  -0.9802517890930176\n",
      "Val Loss for batch is  -1.1069073677062988\n",
      "Val Loss for batch is  -0.7892929315567017\n",
      "Val Loss for batch is  -1.8995684385299683\n",
      "|Iter  486  | Total Val Loss  -4.776020526885986 |\n",
      "Loss for batch is  1.2525676488876343\n",
      "Loss for batch is  1.0864064693450928\n",
      "Loss for batch is  1.319342851638794\n",
      "Loss for batch is  0.03407478332519531\n",
      "|Iter  487  | Total Train Loss  3.6923917531967163 |\n",
      "Val Loss for batch is  -1.0897020101547241\n",
      "Val Loss for batch is  -1.1486289501190186\n",
      "Val Loss for batch is  -0.8810002207756042\n",
      "Val Loss for batch is  -1.902430534362793\n",
      "|Iter  487  | Total Val Loss  -5.02176171541214 |\n",
      "Loss for batch is  1.1961541175842285\n",
      "Loss for batch is  1.0870027542114258\n",
      "Loss for batch is  1.2650965452194214\n",
      "Loss for batch is  0.04058361053466797\n",
      "|Iter  488  | Total Train Loss  3.5888370275497437 |\n",
      "Val Loss for batch is  -1.0984747409820557\n",
      "Val Loss for batch is  -1.1965067386627197\n",
      "Val Loss for batch is  -0.9966939091682434\n",
      "Val Loss for batch is  -1.9383798837661743\n",
      "|Iter  488  | Total Val Loss  -5.230055272579193 |\n",
      "Loss for batch is  1.217267632484436\n",
      "Loss for batch is  1.0484884977340698\n",
      "Loss for batch is  1.3060088157653809\n",
      "Loss for batch is  0.014254331588745117\n",
      "|Iter  489  | Total Train Loss  3.586019277572632 |\n",
      "Val Loss for batch is  -0.9410955309867859\n",
      "Val Loss for batch is  -1.0940024852752686\n",
      "Val Loss for batch is  -0.801582932472229\n",
      "Val Loss for batch is  -1.8763854503631592\n",
      "|Iter  489  | Total Val Loss  -4.713066399097443 |\n",
      "Loss for batch is  1.2159709930419922\n",
      "Loss for batch is  1.0926799774169922\n",
      "Loss for batch is  1.2404065132141113\n",
      "Loss for batch is  0.023129582405090332\n",
      "|Iter  490  | Total Train Loss  3.572187066078186 |\n",
      "Val Loss for batch is  -1.0834717750549316\n",
      "Val Loss for batch is  -1.2485904693603516\n",
      "Val Loss for batch is  -0.9780387282371521\n",
      "Val Loss for batch is  -1.9450170993804932\n",
      "|Iter  490  | Total Val Loss  -5.2551180720329285 |\n",
      "Loss for batch is  1.1438219547271729\n",
      "Loss for batch is  1.0381722450256348\n",
      "Loss for batch is  1.192352533340454\n",
      "Loss for batch is  -0.006531119346618652\n",
      "|Iter  491  | Total Train Loss  3.367815613746643 |\n",
      "Val Loss for batch is  -1.0733537673950195\n",
      "Val Loss for batch is  -1.2316335439682007\n",
      "Val Loss for batch is  -0.8936132192611694\n",
      "Val Loss for batch is  -1.9325790405273438\n",
      "|Iter  491  | Total Val Loss  -5.131179571151733 |\n",
      "Loss for batch is  1.173081636428833\n",
      "Loss for batch is  0.9875292181968689\n",
      "Loss for batch is  1.183054804801941\n",
      "Loss for batch is  0.03034651279449463\n",
      "|Iter  492  | Total Train Loss  3.3740121722221375 |\n",
      "Val Loss for batch is  -1.0470253229141235\n",
      "Val Loss for batch is  -1.1529278755187988\n",
      "Val Loss for batch is  -0.905461847782135\n",
      "Val Loss for batch is  -1.9104889631271362\n",
      "|Iter  492  | Total Val Loss  -5.015904009342194 |\n",
      "Loss for batch is  1.1386547088623047\n",
      "Loss for batch is  1.0399794578552246\n",
      "Loss for batch is  1.2329087257385254\n",
      "Loss for batch is  -0.0193483829498291\n",
      "|Iter  493  | Total Train Loss  3.3921945095062256 |\n",
      "Val Loss for batch is  -0.9953056573867798\n",
      "Val Loss for batch is  -1.106733798980713\n",
      "Val Loss for batch is  -0.8428876399993896\n",
      "Val Loss for batch is  -1.8928523063659668\n",
      "|Iter  493  | Total Val Loss  -4.837779402732849 |\n",
      "Loss for batch is  1.161170482635498\n",
      "Loss for batch is  0.9788148403167725\n",
      "Loss for batch is  1.190568208694458\n",
      "Loss for batch is  0.0006716251373291016\n",
      "|Iter  494  | Total Train Loss  3.3312251567840576 |\n",
      "Val Loss for batch is  -1.1068063974380493\n",
      "Val Loss for batch is  -1.1403048038482666\n",
      "Val Loss for batch is  -0.9716996550559998\n",
      "Val Loss for batch is  -1.9647611379623413\n",
      "|Iter  494  | Total Val Loss  -5.183571994304657 |\n",
      "Loss for batch is  1.084141731262207\n",
      "Loss for batch is  1.0316014289855957\n",
      "Loss for batch is  1.1488192081451416\n",
      "Loss for batch is  -0.018760204315185547\n",
      "|Iter  495  | Total Train Loss  3.245802164077759 |\n",
      "Val Loss for batch is  -1.1133172512054443\n",
      "Val Loss for batch is  -1.1707971096038818\n",
      "Val Loss for batch is  -0.9482842087745667\n",
      "Val Loss for batch is  -1.9749302864074707\n",
      "|Iter  495  | Total Val Loss  -5.2073288559913635 |\n",
      "Loss for batch is  1.0956761837005615\n",
      "Loss for batch is  0.9845933318138123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.1216977834701538\n",
      "Loss for batch is  -0.045439839363098145\n",
      "|Iter  496  | Total Train Loss  3.1565274596214294 |\n",
      "Val Loss for batch is  -1.039000391960144\n",
      "Val Loss for batch is  -1.1520963907241821\n",
      "Val Loss for batch is  -0.9441088438034058\n",
      "Val Loss for batch is  -1.9208550453186035\n",
      "|Iter  496  | Total Val Loss  -5.0560606718063354 |\n",
      "Loss for batch is  1.126408338546753\n",
      "Loss for batch is  0.9653153419494629\n",
      "Loss for batch is  1.18288254737854\n",
      "Loss for batch is  -0.025064826011657715\n",
      "|Iter  497  | Total Train Loss  3.249541401863098 |\n",
      "Val Loss for batch is  -1.0773859024047852\n",
      "Val Loss for batch is  -1.1603727340698242\n",
      "Val Loss for batch is  -0.919432520866394\n",
      "Val Loss for batch is  -1.8959523439407349\n",
      "|Iter  497  | Total Val Loss  -5.053143501281738 |\n",
      "Loss for batch is  1.1114647388458252\n",
      "Loss for batch is  0.984760582447052\n",
      "Loss for batch is  1.176459789276123\n",
      "Loss for batch is  -0.06623589992523193\n",
      "|Iter  498  | Total Train Loss  3.2064492106437683 |\n",
      "Val Loss for batch is  -1.0406254529953003\n",
      "Val Loss for batch is  -1.0276850461959839\n",
      "Val Loss for batch is  -0.8647941946983337\n",
      "Val Loss for batch is  -1.9416428804397583\n",
      "|Iter  498  | Total Val Loss  -4.874747574329376 |\n",
      "Loss for batch is  1.141439437866211\n",
      "Loss for batch is  0.96642005443573\n",
      "Loss for batch is  1.184615135192871\n",
      "Loss for batch is  -0.07959771156311035\n",
      "|Iter  499  | Total Train Loss  3.2128769159317017 |\n",
      "Val Loss for batch is  -1.0427149534225464\n",
      "Val Loss for batch is  -1.127163290977478\n",
      "Val Loss for batch is  -0.9016125202178955\n",
      "Val Loss for batch is  -1.859167218208313\n",
      "|Iter  499  | Total Val Loss  -4.930657982826233 |\n",
      "Loss for batch is  1.1283812522888184\n",
      "Loss for batch is  1.0210504531860352\n",
      "Loss for batch is  1.1095353364944458\n",
      "Loss for batch is  -0.04120588302612305\n",
      "|Iter  500  | Total Train Loss  3.2177611589431763 |\n",
      "Val Loss for batch is  -1.0751266479492188\n",
      "Val Loss for batch is  -1.2351446151733398\n",
      "Val Loss for batch is  -1.0308527946472168\n",
      "Val Loss for batch is  -1.9279266595840454\n",
      "|Iter  500  | Total Val Loss  -5.269050717353821 |\n",
      "Loss for batch is  1.090261697769165\n",
      "Loss for batch is  0.962788999080658\n",
      "Loss for batch is  1.135716438293457\n",
      "Loss for batch is  -0.06336557865142822\n",
      "|Iter  501  | Total Train Loss  3.125401556491852 |\n",
      "Val Loss for batch is  -1.1142605543136597\n",
      "Val Loss for batch is  -1.1278653144836426\n",
      "Val Loss for batch is  -0.9555559158325195\n",
      "Val Loss for batch is  -1.9584600925445557\n",
      "|Iter  501  | Total Val Loss  -5.156141877174377 |\n",
      "Loss for batch is  1.074925184249878\n",
      "Loss for batch is  0.9710744023323059\n",
      "Loss for batch is  1.0867621898651123\n",
      "Loss for batch is  -0.05285799503326416\n",
      "|Iter  502  | Total Train Loss  3.079903781414032 |\n",
      "Val Loss for batch is  -1.1776798963546753\n",
      "Val Loss for batch is  -1.254162311553955\n",
      "Val Loss for batch is  -0.9401861429214478\n",
      "Val Loss for batch is  -2.0207443237304688\n",
      "|Iter  502  | Total Val Loss  -5.392772674560547 |\n",
      "Loss for batch is  1.0651814937591553\n",
      "Loss for batch is  0.9032576680183411\n",
      "Loss for batch is  1.1271271705627441\n",
      "Loss for batch is  -0.09771978855133057\n",
      "|Iter  503  | Total Train Loss  2.99784654378891 |\n",
      "Val Loss for batch is  -1.1080331802368164\n",
      "Val Loss for batch is  -1.1497993469238281\n",
      "Val Loss for batch is  -0.9382221102714539\n",
      "Val Loss for batch is  -1.82741379737854\n",
      "|Iter  503  | Total Val Loss  -5.023468434810638 |\n",
      "Loss for batch is  1.0613532066345215\n",
      "Loss for batch is  0.9031296968460083\n",
      "Loss for batch is  1.0940757989883423\n",
      "Loss for batch is  -0.06365334987640381\n",
      "|Iter  504  | Total Train Loss  2.9949053525924683 |\n",
      "Val Loss for batch is  -1.0261857509613037\n",
      "Val Loss for batch is  -1.2340478897094727\n",
      "Val Loss for batch is  -0.913317084312439\n",
      "Val Loss for batch is  -1.9564998149871826\n",
      "|Iter  504  | Total Val Loss  -5.130050539970398 |\n",
      "Loss for batch is  1.0290712118148804\n",
      "Loss for batch is  0.9246208667755127\n",
      "Loss for batch is  1.059952735900879\n",
      "Loss for batch is  -0.05591309070587158\n",
      "|Iter  505  | Total Train Loss  2.9577317237854004 |\n",
      "Val Loss for batch is  -1.1237709522247314\n",
      "Val Loss for batch is  -1.2772879600524902\n",
      "Val Loss for batch is  -1.051269769668579\n",
      "Val Loss for batch is  -2.0457050800323486\n",
      "|Iter  505  | Total Val Loss  -5.498033761978149 |\n",
      "Loss for batch is  1.0101841688156128\n",
      "Loss for batch is  0.9037570357322693\n",
      "Loss for batch is  1.0677568912506104\n",
      "Loss for batch is  -0.06987488269805908\n",
      "|Iter  506  | Total Train Loss  2.9118232131004333 |\n",
      "Val Loss for batch is  -1.0452210903167725\n",
      "Val Loss for batch is  -1.089001178741455\n",
      "Val Loss for batch is  -1.002173662185669\n",
      "Val Loss for batch is  -1.9758599996566772\n",
      "|Iter  506  | Total Val Loss  -5.112255930900574 |\n",
      "Loss for batch is  1.067237377166748\n",
      "Loss for batch is  0.8999848961830139\n",
      "Loss for batch is  1.0860449075698853\n",
      "Loss for batch is  -0.10887432098388672\n",
      "|Iter  507  | Total Train Loss  2.9443928599357605 |\n",
      "Val Loss for batch is  -1.0569485425949097\n",
      "Val Loss for batch is  -1.1956146955490112\n",
      "Val Loss for batch is  -1.0114799737930298\n",
      "Val Loss for batch is  -1.9664677381515503\n",
      "|Iter  507  | Total Val Loss  -5.230510950088501 |\n",
      "Loss for batch is  1.0122897624969482\n",
      "Loss for batch is  0.9344448447227478\n",
      "Loss for batch is  1.0598278045654297\n",
      "Loss for batch is  -0.12025856971740723\n",
      "|Iter  508  | Total Train Loss  2.8863038420677185 |\n",
      "Val Loss for batch is  -1.1507148742675781\n",
      "Val Loss for batch is  -1.2075446844100952\n",
      "Val Loss for batch is  -1.0430785417556763\n",
      "Val Loss for batch is  -2.0112524032592773\n",
      "|Iter  508  | Total Val Loss  -5.412590503692627 |\n",
      "Loss for batch is  0.9997488260269165\n",
      "Loss for batch is  0.8832335472106934\n",
      "Loss for batch is  1.0325543880462646\n",
      "Loss for batch is  -0.1221846342086792\n",
      "|Iter  509  | Total Train Loss  2.7933521270751953 |\n",
      "Val Loss for batch is  -1.064466118812561\n",
      "Val Loss for batch is  -1.1625216007232666\n",
      "Val Loss for batch is  -1.0026187896728516\n",
      "Val Loss for batch is  -1.9461281299591064\n",
      "|Iter  509  | Total Val Loss  -5.175734639167786 |\n",
      "Loss for batch is  1.003124713897705\n",
      "Loss for batch is  0.8713355660438538\n",
      "Loss for batch is  1.0151515007019043\n",
      "Loss for batch is  -0.11499440670013428\n",
      "|Iter  510  | Total Train Loss  2.774617373943329 |\n",
      "Val Loss for batch is  -1.0730812549591064\n",
      "Val Loss for batch is  -1.2283354997634888\n",
      "Val Loss for batch is  -1.0169360637664795\n",
      "Val Loss for batch is  -1.953928828239441\n",
      "|Iter  510  | Total Val Loss  -5.272281646728516 |\n",
      "Loss for batch is  1.0109014511108398\n",
      "Loss for batch is  0.8845317363739014\n",
      "Loss for batch is  1.0176869630813599\n",
      "Loss for batch is  -0.1350334882736206\n",
      "|Iter  511  | Total Train Loss  2.7780866622924805 |\n",
      "Val Loss for batch is  -1.1181176900863647\n",
      "Val Loss for batch is  -1.262731909751892\n",
      "Val Loss for batch is  -1.0466976165771484\n",
      "Val Loss for batch is  -1.9985030889511108\n",
      "|Iter  511  | Total Val Loss  -5.426050305366516 |\n",
      "Loss for batch is  1.0258188247680664\n",
      "Loss for batch is  0.8328832387924194\n",
      "Loss for batch is  1.0079655647277832\n",
      "Loss for batch is  -0.12168741226196289\n",
      "|Iter  512  | Total Train Loss  2.744980216026306 |\n",
      "Val Loss for batch is  -1.1142468452453613\n",
      "Val Loss for batch is  -1.2342066764831543\n",
      "Val Loss for batch is  -0.945243239402771\n",
      "Val Loss for batch is  -1.960889458656311\n",
      "|Iter  512  | Total Val Loss  -5.254586219787598 |\n",
      "Loss for batch is  0.9733074903488159\n",
      "Loss for batch is  0.8502529859542847\n",
      "Loss for batch is  0.9852315187454224\n",
      "Loss for batch is  -0.1397160291671753\n",
      "|Iter  513  | Total Train Loss  2.6690759658813477 |\n",
      "Val Loss for batch is  -1.1077743768692017\n",
      "Val Loss for batch is  -1.248801350593567\n",
      "Val Loss for batch is  -1.0682536363601685\n",
      "Val Loss for batch is  -2.04553484916687\n",
      "|Iter  513  | Total Val Loss  -5.470364212989807 |\n",
      "Loss for batch is  0.9685699343681335\n",
      "Loss for batch is  0.8729224801063538\n",
      "Loss for batch is  0.9499719142913818\n",
      "Loss for batch is  -0.1585698127746582\n",
      "|Iter  514  | Total Train Loss  2.632894515991211 |\n",
      "Val Loss for batch is  -1.2082407474517822\n",
      "Val Loss for batch is  -1.3056256771087646\n",
      "Val Loss for batch is  -1.0294616222381592\n",
      "Val Loss for batch is  -2.0488243103027344\n",
      "|Iter  514  | Total Val Loss  -5.59215235710144 |\n",
      "Loss for batch is  0.9468470811843872\n",
      "Loss for batch is  0.8559941649436951\n",
      "Loss for batch is  1.091808795928955\n",
      "Loss for batch is  -0.14179396629333496\n",
      "|Iter  515  | Total Train Loss  2.7528560757637024 |\n",
      "Val Loss for batch is  -0.9783434271812439\n",
      "Val Loss for batch is  -1.1014738082885742\n",
      "Val Loss for batch is  -1.063072681427002\n",
      "Val Loss for batch is  -2.0352559089660645\n",
      "|Iter  515  | Total Val Loss  -5.1781458258628845 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  1.0301265716552734\n",
      "Loss for batch is  0.9184656739234924\n",
      "Loss for batch is  1.0911763906478882\n",
      "Loss for batch is  -0.11264526844024658\n",
      "|Iter  516  | Total Train Loss  2.9271233677864075 |\n",
      "Val Loss for batch is  -1.1912254095077515\n",
      "Val Loss for batch is  -1.241848111152649\n",
      "Val Loss for batch is  -1.0338914394378662\n",
      "Val Loss for batch is  -2.0118393898010254\n",
      "|Iter  516  | Total Val Loss  -5.478804349899292 |\n",
      "Loss for batch is  0.9757627844810486\n",
      "Loss for batch is  0.8634462952613831\n",
      "Loss for batch is  1.0119731426239014\n",
      "Loss for batch is  -0.11809289455413818\n",
      "|Iter  517  | Total Train Loss  2.733089327812195 |\n",
      "Val Loss for batch is  -1.0678603649139404\n",
      "Val Loss for batch is  -1.2973860502243042\n",
      "Val Loss for batch is  -1.0296368598937988\n",
      "Val Loss for batch is  -1.9510109424591064\n",
      "|Iter  517  | Total Val Loss  -5.34589421749115 |\n",
      "Loss for batch is  0.9820611476898193\n",
      "Loss for batch is  0.7993518710136414\n",
      "Loss for batch is  1.0263434648513794\n",
      "Loss for batch is  -0.16486132144927979\n",
      "|Iter  518  | Total Train Loss  2.6428951621055603 |\n",
      "Val Loss for batch is  -1.1862131357192993\n",
      "Val Loss for batch is  -1.2747182846069336\n",
      "Val Loss for batch is  -1.0444570779800415\n",
      "Val Loss for batch is  -2.0108625888824463\n",
      "|Iter  518  | Total Val Loss  -5.516251087188721 |\n",
      "Loss for batch is  0.9442915916442871\n",
      "Loss for batch is  0.8150754570960999\n",
      "Loss for batch is  0.9780572056770325\n",
      "Loss for batch is  -0.1731114387512207\n",
      "|Iter  519  | Total Train Loss  2.5643128156661987 |\n",
      "Val Loss for batch is  -1.232582449913025\n",
      "Val Loss for batch is  -1.2344775199890137\n",
      "Val Loss for batch is  -1.0389702320098877\n",
      "Val Loss for batch is  -1.9651553630828857\n",
      "|Iter  519  | Total Val Loss  -5.471185564994812 |\n",
      "Loss for batch is  0.9021968841552734\n",
      "Loss for batch is  0.7923503518104553\n",
      "Loss for batch is  0.962206244468689\n",
      "Loss for batch is  -0.17922353744506836\n",
      "|Iter  520  | Total Train Loss  2.4775299429893494 |\n",
      "Val Loss for batch is  -1.1853690147399902\n",
      "Val Loss for batch is  -1.282322883605957\n",
      "Val Loss for batch is  -1.0478919744491577\n",
      "Val Loss for batch is  -2.0019328594207764\n",
      "|Iter  520  | Total Val Loss  -5.517516732215881 |\n",
      "Loss for batch is  0.9087148904800415\n",
      "Loss for batch is  0.766869843006134\n",
      "Loss for batch is  0.9823862910270691\n",
      "Loss for batch is  -0.21228623390197754\n",
      "|Iter  521  | Total Train Loss  2.445684790611267 |\n",
      "Val Loss for batch is  -1.1294219493865967\n",
      "Val Loss for batch is  -1.2079707384109497\n",
      "Val Loss for batch is  -1.0090941190719604\n",
      "Val Loss for batch is  -2.0069847106933594\n",
      "|Iter  521  | Total Val Loss  -5.353471517562866 |\n",
      "Loss for batch is  0.923560380935669\n",
      "Loss for batch is  0.7517169117927551\n",
      "Loss for batch is  0.9240211844444275\n",
      "Loss for batch is  -0.20111870765686035\n",
      "|Iter  522  | Total Train Loss  2.398179769515991 |\n",
      "Val Loss for batch is  -1.1253739595413208\n",
      "Val Loss for batch is  -1.2321468591690063\n",
      "Val Loss for batch is  -1.0728132724761963\n",
      "Val Loss for batch is  -1.9275025129318237\n",
      "|Iter  522  | Total Val Loss  -5.357836604118347 |\n",
      "Loss for batch is  0.9044662117958069\n",
      "Loss for batch is  0.7789013981819153\n",
      "Loss for batch is  0.9372717142105103\n",
      "Loss for batch is  -0.19781696796417236\n",
      "|Iter  523  | Total Train Loss  2.42282235622406 |\n",
      "Val Loss for batch is  -1.1846606731414795\n",
      "Val Loss for batch is  -1.2423495054244995\n",
      "Val Loss for batch is  -1.069379210472107\n",
      "Val Loss for batch is  -2.1355392932891846\n",
      "|Iter  523  | Total Val Loss  -5.6319286823272705 |\n",
      "Loss for batch is  0.8690171837806702\n",
      "Loss for batch is  0.7821438312530518\n",
      "Loss for batch is  0.8844501376152039\n",
      "Loss for batch is  -0.1705847978591919\n",
      "|Iter  524  | Total Train Loss  2.365026354789734 |\n",
      "Val Loss for batch is  -1.181532859802246\n",
      "Val Loss for batch is  -1.2781418561935425\n",
      "Val Loss for batch is  -1.1265093088150024\n",
      "Val Loss for batch is  -2.064486265182495\n",
      "|Iter  524  | Total Val Loss  -5.650670289993286 |\n",
      "Loss for batch is  0.8674880266189575\n",
      "Loss for batch is  0.8040338158607483\n",
      "Loss for batch is  0.8885859847068787\n",
      "Loss for batch is  -0.19450139999389648\n",
      "|Iter  525  | Total Train Loss  2.365606427192688 |\n",
      "Val Loss for batch is  -1.140486478805542\n",
      "Val Loss for batch is  -1.163957118988037\n",
      "Val Loss for batch is  -1.1012341976165771\n",
      "Val Loss for batch is  -2.081555128097534\n",
      "|Iter  525  | Total Val Loss  -5.48723292350769 |\n",
      "Loss for batch is  0.9122058153152466\n",
      "Loss for batch is  0.7656838297843933\n",
      "Loss for batch is  0.9150097966194153\n",
      "Loss for batch is  -0.18430101871490479\n",
      "|Iter  526  | Total Train Loss  2.4085984230041504 |\n",
      "Val Loss for batch is  -1.0929632186889648\n",
      "Val Loss for batch is  -1.2226444482803345\n",
      "Val Loss for batch is  -1.016666293144226\n",
      "Val Loss for batch is  -1.9852112531661987\n",
      "|Iter  526  | Total Val Loss  -5.317485213279724 |\n",
      "Loss for batch is  0.9590377807617188\n",
      "Loss for batch is  0.742658793926239\n",
      "Loss for batch is  1.0016218423843384\n",
      "Loss for batch is  -0.14568936824798584\n",
      "|Iter  527  | Total Train Loss  2.5576290488243103 |\n",
      "Val Loss for batch is  -1.0199130773544312\n",
      "Val Loss for batch is  -1.115901231765747\n",
      "Val Loss for batch is  -1.0212751626968384\n",
      "Val Loss for batch is  -1.9992541074752808\n",
      "|Iter  527  | Total Val Loss  -5.156343579292297 |\n",
      "Loss for batch is  1.003448247909546\n",
      "Loss for batch is  0.868606448173523\n",
      "Loss for batch is  0.974795937538147\n",
      "Loss for batch is  -0.06217753887176514\n",
      "|Iter  528  | Total Train Loss  2.7846730947494507 |\n",
      "Val Loss for batch is  -1.103527307510376\n",
      "Val Loss for batch is  -1.2093805074691772\n",
      "Val Loss for batch is  -0.9948203563690186\n",
      "Val Loss for batch is  -1.994960904121399\n",
      "|Iter  528  | Total Val Loss  -5.302689075469971 |\n",
      "Loss for batch is  0.9786980748176575\n",
      "Loss for batch is  0.8713639974594116\n",
      "Loss for batch is  1.1559650897979736\n",
      "Loss for batch is  -0.17330396175384521\n",
      "|Iter  529  | Total Train Loss  2.8327232003211975 |\n",
      "Val Loss for batch is  -1.0130412578582764\n",
      "Val Loss for batch is  -1.0178313255310059\n",
      "Val Loss for batch is  -0.8438541889190674\n",
      "Val Loss for batch is  -1.9505610466003418\n",
      "|Iter  529  | Total Val Loss  -4.825287818908691 |\n",
      "Loss for batch is  1.0516562461853027\n",
      "Loss for batch is  0.8479437232017517\n",
      "Loss for batch is  1.1367127895355225\n",
      "Loss for batch is  -0.06432914733886719\n",
      "|Iter  530  | Total Train Loss  2.9719836115837097 |\n",
      "Val Loss for batch is  -1.0029479265213013\n",
      "Val Loss for batch is  -1.2127896547317505\n",
      "Val Loss for batch is  -0.9710330963134766\n",
      "Val Loss for batch is  -2.00868821144104\n",
      "|Iter  530  | Total Val Loss  -5.195458889007568 |\n",
      "Loss for batch is  0.9518256783485413\n",
      "Loss for batch is  0.9846027493476868\n",
      "Loss for batch is  0.962544858455658\n",
      "Loss for batch is  -0.14439904689788818\n",
      "|Iter  531  | Total Train Loss  2.754574239253998 |\n",
      "Val Loss for batch is  -1.165190577507019\n",
      "Val Loss for batch is  -1.2240347862243652\n",
      "Val Loss for batch is  -1.0695645809173584\n",
      "Val Loss for batch is  -1.9845629930496216\n",
      "|Iter  531  | Total Val Loss  -5.443352937698364 |\n",
      "Loss for batch is  1.0049227476119995\n",
      "Loss for batch is  0.785315752029419\n",
      "Loss for batch is  0.9611014127731323\n",
      "Loss for batch is  -0.10909700393676758\n",
      "|Iter  532  | Total Train Loss  2.642242908477783 |\n",
      "Val Loss for batch is  -1.1716512441635132\n",
      "Val Loss for batch is  -1.2730845212936401\n",
      "Val Loss for batch is  -1.1162101030349731\n",
      "Val Loss for batch is  -1.9954700469970703\n",
      "|Iter  532  | Total Val Loss  -5.556415915489197 |\n",
      "Loss for batch is  0.9184848666191101\n",
      "Loss for batch is  0.7835148572921753\n",
      "Loss for batch is  1.0026001930236816\n",
      "Loss for batch is  -0.1553640365600586\n",
      "|Iter  533  | Total Train Loss  2.5492358803749084 |\n",
      "Val Loss for batch is  -1.1149650812149048\n",
      "Val Loss for batch is  -1.2374776601791382\n",
      "Val Loss for batch is  -1.05967378616333\n",
      "Val Loss for batch is  -1.999226689338684\n",
      "|Iter  533  | Total Val Loss  -5.411343216896057 |\n",
      "Loss for batch is  0.9276243448257446\n",
      "Loss for batch is  0.7563610076904297\n",
      "Loss for batch is  0.9552190899848938\n",
      "Loss for batch is  -0.18179690837860107\n",
      "|Iter  534  | Total Train Loss  2.457407534122467 |\n",
      "Val Loss for batch is  -1.3032941818237305\n",
      "Val Loss for batch is  -1.4274672269821167\n",
      "Val Loss for batch is  -1.163600206375122\n",
      "Val Loss for batch is  -2.1046977043151855\n",
      "|Iter  534  | Total Val Loss  -5.999059319496155 |\n",
      "Loss for batch is  0.8744027018547058\n",
      "Loss for batch is  0.7408401966094971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.8956055641174316\n",
      "Loss for batch is  -0.1965235471725464\n",
      "|Iter  535  | Total Train Loss  2.314324915409088 |\n",
      "Val Loss for batch is  -1.2382875680923462\n",
      "Val Loss for batch is  -1.2533468008041382\n",
      "Val Loss for batch is  -1.1791200637817383\n",
      "Val Loss for batch is  -2.0765066146850586\n",
      "|Iter  535  | Total Val Loss  -5.747261047363281 |\n",
      "Loss for batch is  0.8348027467727661\n",
      "Loss for batch is  0.7228330969810486\n",
      "Loss for batch is  0.8460404872894287\n",
      "Loss for batch is  -0.20761334896087646\n",
      "|Iter  536  | Total Train Loss  2.196062982082367 |\n",
      "Val Loss for batch is  -1.1745970249176025\n",
      "Val Loss for batch is  -1.3420807123184204\n",
      "Val Loss for batch is  -1.138203501701355\n",
      "Val Loss for batch is  -2.0349881649017334\n",
      "|Iter  536  | Total Val Loss  -5.689869403839111 |\n",
      "Loss for batch is  0.8408617377281189\n",
      "Loss for batch is  0.6992936730384827\n",
      "Loss for batch is  0.9156249761581421\n",
      "Loss for batch is  -0.2474963665008545\n",
      "|Iter  537  | Total Train Loss  2.208284020423889 |\n",
      "Val Loss for batch is  -1.1596136093139648\n",
      "Val Loss for batch is  -1.2646117210388184\n",
      "Val Loss for batch is  -1.0673613548278809\n",
      "Val Loss for batch is  -2.0635664463043213\n",
      "|Iter  537  | Total Val Loss  -5.555153131484985 |\n",
      "Loss for batch is  0.9050372242927551\n",
      "Loss for batch is  0.689419686794281\n",
      "Loss for batch is  0.9092273116111755\n",
      "Loss for batch is  -0.2574946880340576\n",
      "|Iter  538  | Total Train Loss  2.246189534664154 |\n",
      "Val Loss for batch is  -1.2564959526062012\n",
      "Val Loss for batch is  -1.2792024612426758\n",
      "Val Loss for batch is  -1.1350995302200317\n",
      "Val Loss for batch is  -2.1134581565856934\n",
      "|Iter  538  | Total Val Loss  -5.784256100654602 |\n",
      "Loss for batch is  0.8301867246627808\n",
      "Loss for batch is  0.7421604990959167\n",
      "Loss for batch is  0.8323169946670532\n",
      "Loss for batch is  -0.22091805934906006\n",
      "|Iter  539  | Total Train Loss  2.1837461590766907 |\n",
      "Val Loss for batch is  -1.1766982078552246\n",
      "Val Loss for batch is  -1.2535724639892578\n",
      "Val Loss for batch is  -1.1103236675262451\n",
      "Val Loss for batch is  -2.058866500854492\n",
      "|Iter  539  | Total Val Loss  -5.59946084022522 |\n",
      "Loss for batch is  0.804197371006012\n",
      "Loss for batch is  0.7056936621665955\n",
      "Loss for batch is  0.7792549133300781\n",
      "Loss for batch is  -0.23204326629638672\n",
      "|Iter  540  | Total Train Loss  2.057102680206299 |\n",
      "Val Loss for batch is  -1.2045515775680542\n",
      "Val Loss for batch is  -1.3117094039916992\n",
      "Val Loss for batch is  -1.1461660861968994\n",
      "Val Loss for batch is  -2.1016342639923096\n",
      "|Iter  540  | Total Val Loss  -5.764061331748962 |\n",
      "Loss for batch is  0.8688164353370667\n",
      "Loss for batch is  0.6755259037017822\n",
      "Loss for batch is  0.8573022484779358\n",
      "Loss for batch is  -0.2452714443206787\n",
      "|Iter  541  | Total Train Loss  2.156373143196106 |\n",
      "Val Loss for batch is  -1.2074007987976074\n",
      "Val Loss for batch is  -1.29664945602417\n",
      "Val Loss for batch is  -1.0781394243240356\n",
      "Val Loss for batch is  -2.075815200805664\n",
      "|Iter  541  | Total Val Loss  -5.658004879951477 |\n",
      "Loss for batch is  0.8225905299186707\n",
      "Loss for batch is  0.7045696377754211\n",
      "Loss for batch is  0.8621253967285156\n",
      "Loss for batch is  -0.2010786533355713\n",
      "|Iter  542  | Total Train Loss  2.188206911087036 |\n",
      "Val Loss for batch is  -1.1471537351608276\n",
      "Val Loss for batch is  -1.3141225576400757\n",
      "Val Loss for batch is  -1.1354259252548218\n",
      "Val Loss for batch is  -2.0908091068267822\n",
      "|Iter  542  | Total Val Loss  -5.687511324882507 |\n",
      "Loss for batch is  0.8492009043693542\n",
      "Loss for batch is  0.7449795603752136\n",
      "Loss for batch is  0.9145201444625854\n",
      "Loss for batch is  -0.23383498191833496\n",
      "|Iter  543  | Total Train Loss  2.2748656272888184 |\n",
      "Val Loss for batch is  -1.1114680767059326\n",
      "Val Loss for batch is  -1.1707353591918945\n",
      "Val Loss for batch is  -1.0918362140655518\n",
      "Val Loss for batch is  -2.0320911407470703\n",
      "|Iter  543  | Total Val Loss  -5.406130790710449 |\n",
      "Loss for batch is  0.9230018854141235\n",
      "Loss for batch is  0.6742187142372131\n",
      "Loss for batch is  0.9859463572502136\n",
      "Loss for batch is  -0.1950920820236206\n",
      "|Iter  544  | Total Train Loss  2.3880748748779297 |\n",
      "Val Loss for batch is  -1.0438367128372192\n",
      "Val Loss for batch is  -1.2614296674728394\n",
      "Val Loss for batch is  -1.132533073425293\n",
      "Val Loss for batch is  -2.054891347885132\n",
      "|Iter  544  | Total Val Loss  -5.492690801620483 |\n",
      "Loss for batch is  0.9097842574119568\n",
      "Loss for batch is  0.7279309034347534\n",
      "Loss for batch is  0.9241098761558533\n",
      "Loss for batch is  -0.1965634822845459\n",
      "|Iter  545  | Total Train Loss  2.3652615547180176 |\n",
      "Val Loss for batch is  -1.0531270503997803\n",
      "Val Loss for batch is  -1.1927119493484497\n",
      "Val Loss for batch is  -1.0915414094924927\n",
      "Val Loss for batch is  -2.040874719619751\n",
      "|Iter  545  | Total Val Loss  -5.378255128860474 |\n",
      "Loss for batch is  0.909661591053009\n",
      "Loss for batch is  0.7088726758956909\n",
      "Loss for batch is  0.8910366296768188\n",
      "Loss for batch is  -0.20895493030548096\n",
      "|Iter  546  | Total Train Loss  2.300615966320038 |\n",
      "Val Loss for batch is  -1.1829719543457031\n",
      "Val Loss for batch is  -1.3579857349395752\n",
      "Val Loss for batch is  -1.1688921451568604\n",
      "Val Loss for batch is  -2.1032187938690186\n",
      "|Iter  546  | Total Val Loss  -5.813068628311157 |\n",
      "Loss for batch is  0.884083092212677\n",
      "Loss for batch is  0.7042107582092285\n",
      "Loss for batch is  0.9505675435066223\n",
      "Loss for batch is  -0.23120379447937012\n",
      "|Iter  547  | Total Train Loss  2.3076575994491577 |\n",
      "Val Loss for batch is  -1.1065479516983032\n",
      "Val Loss for batch is  -1.2799152135849\n",
      "Val Loss for batch is  -1.167025089263916\n",
      "Val Loss for batch is  -2.0118978023529053\n",
      "|Iter  547  | Total Val Loss  -5.565386056900024 |\n",
      "Loss for batch is  0.9324600696563721\n",
      "Loss for batch is  0.6577009558677673\n",
      "Loss for batch is  0.9401276111602783\n",
      "Loss for batch is  -0.24495172500610352\n",
      "|Iter  548  | Total Train Loss  2.285336911678314 |\n",
      "Val Loss for batch is  -1.1960781812667847\n",
      "Val Loss for batch is  -1.315086841583252\n",
      "Val Loss for batch is  -1.1032060384750366\n",
      "Val Loss for batch is  -2.0823137760162354\n",
      "|Iter  548  | Total Val Loss  -5.696684837341309 |\n",
      "Loss for batch is  0.8416104912757874\n",
      "Loss for batch is  0.726526141166687\n",
      "Loss for batch is  0.8473959565162659\n",
      "Loss for batch is  -0.2038952112197876\n",
      "|Iter  549  | Total Train Loss  2.2116373777389526 |\n",
      "Val Loss for batch is  -1.1980408430099487\n",
      "Val Loss for batch is  -1.313883662223816\n",
      "Val Loss for batch is  -1.2407008409500122\n",
      "Val Loss for batch is  -2.151026487350464\n",
      "|Iter  549  | Total Val Loss  -5.903651833534241 |\n",
      "Loss for batch is  0.8375745415687561\n",
      "Loss for batch is  0.7265597581863403\n",
      "Loss for batch is  0.7704861760139465\n",
      "Loss for batch is  -0.2711036205291748\n",
      "|Iter  550  | Total Train Loss  2.063516855239868 |\n",
      "Val Loss for batch is  -1.2247328758239746\n",
      "Val Loss for batch is  -1.3560121059417725\n",
      "Val Loss for batch is  -1.1850029230117798\n",
      "Val Loss for batch is  -2.054652690887451\n",
      "|Iter  550  | Total Val Loss  -5.820400595664978 |\n",
      "Loss for batch is  0.8223575949668884\n",
      "Loss for batch is  0.6557118892669678\n",
      "Loss for batch is  0.7864151000976562\n",
      "Loss for batch is  -0.2625465393066406\n",
      "|Iter  551  | Total Train Loss  2.001938045024872 |\n",
      "Val Loss for batch is  -1.1960982084274292\n",
      "Val Loss for batch is  -1.3297532796859741\n",
      "Val Loss for batch is  -1.124284267425537\n",
      "Val Loss for batch is  -2.116353750228882\n",
      "|Iter  551  | Total Val Loss  -5.766489505767822 |\n",
      "Loss for batch is  0.797540545463562\n",
      "Loss for batch is  0.6275386810302734\n",
      "Loss for batch is  0.8199142813682556\n",
      "Loss for batch is  -0.27257275581359863\n",
      "|Iter  552  | Total Train Loss  1.9724207520484924 |\n",
      "Val Loss for batch is  -1.1757010221481323\n",
      "Val Loss for batch is  -1.2212978601455688\n",
      "Val Loss for batch is  -1.1323708295822144\n",
      "Val Loss for batch is  -2.125326633453369\n",
      "|Iter  552  | Total Val Loss  -5.654696345329285 |\n",
      "Loss for batch is  0.8257797360420227\n",
      "Loss for batch is  0.6049597263336182\n",
      "Loss for batch is  0.8285440802574158\n",
      "Loss for batch is  -0.28524065017700195\n",
      "|Iter  553  | Total Train Loss  1.9740428924560547 |\n",
      "Val Loss for batch is  -1.2703208923339844\n",
      "Val Loss for batch is  -1.346312165260315\n",
      "Val Loss for batch is  -1.147821068763733\n",
      "Val Loss for batch is  -2.165208339691162\n",
      "|Iter  553  | Total Val Loss  -5.929662466049194 |\n",
      "Loss for batch is  0.7634614109992981\n",
      "Loss for batch is  0.6864800453186035\n",
      "Loss for batch is  0.7675104141235352\n",
      "Loss for batch is  -0.2879127264022827\n",
      "|Iter  554  | Total Train Loss  1.929539144039154 |\n",
      "Val Loss for batch is  -1.2759900093078613\n",
      "Val Loss for batch is  -1.3444525003433228\n",
      "Val Loss for batch is  -1.2539191246032715\n",
      "Val Loss for batch is  -2.1432127952575684\n",
      "|Iter  554  | Total Val Loss  -6.017574429512024 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.7587372660636902\n",
      "Loss for batch is  0.6287922263145447\n",
      "Loss for batch is  0.7382170557975769\n",
      "Loss for batch is  -0.33701062202453613\n",
      "|Iter  555  | Total Train Loss  1.7887359261512756 |\n",
      "Val Loss for batch is  -1.1965758800506592\n",
      "Val Loss for batch is  -1.2873823642730713\n",
      "Val Loss for batch is  -1.1324843168258667\n",
      "Val Loss for batch is  -2.129216194152832\n",
      "|Iter  555  | Total Val Loss  -5.745658755302429 |\n",
      "Loss for batch is  0.7765246629714966\n",
      "Loss for batch is  0.6051007509231567\n",
      "Loss for batch is  0.7333346009254456\n",
      "Loss for batch is  -0.3058406114578247\n",
      "|Iter  556  | Total Train Loss  1.8091194033622742 |\n",
      "Val Loss for batch is  -1.2328178882598877\n",
      "Val Loss for batch is  -1.345258355140686\n",
      "Val Loss for batch is  -1.1692901849746704\n",
      "Val Loss for batch is  -2.1372992992401123\n",
      "|Iter  556  | Total Val Loss  -5.8846657276153564 |\n",
      "Loss for batch is  0.7419793605804443\n",
      "Loss for batch is  0.606074869632721\n",
      "Loss for batch is  0.7246133685112\n",
      "Loss for batch is  -0.28303349018096924\n",
      "|Iter  557  | Total Train Loss  1.789634108543396 |\n",
      "Val Loss for batch is  -1.2464909553527832\n",
      "Val Loss for batch is  -1.3677586317062378\n",
      "Val Loss for batch is  -1.224669098854065\n",
      "Val Loss for batch is  -2.0980911254882812\n",
      "|Iter  557  | Total Val Loss  -5.937009811401367 |\n",
      "Loss for batch is  0.809138298034668\n",
      "Loss for batch is  0.5715036392211914\n",
      "Loss for batch is  0.830588161945343\n",
      "Loss for batch is  -0.3008686304092407\n",
      "|Iter  558  | Total Train Loss  1.9103614687919617 |\n",
      "Val Loss for batch is  -1.1691890954971313\n",
      "Val Loss for batch is  -1.2913522720336914\n",
      "Val Loss for batch is  -1.1159220933914185\n",
      "Val Loss for batch is  -2.09474778175354\n",
      "|Iter  558  | Total Val Loss  -5.671211242675781 |\n",
      "Loss for batch is  0.7855480909347534\n",
      "Loss for batch is  0.5590953826904297\n",
      "Loss for batch is  0.7970009446144104\n",
      "Loss for batch is  -0.31172776222229004\n",
      "|Iter  559  | Total Train Loss  1.8299166560173035 |\n",
      "Val Loss for batch is  -1.1503629684448242\n",
      "Val Loss for batch is  -1.1510945558547974\n",
      "Val Loss for batch is  -1.1771180629730225\n",
      "Val Loss for batch is  -2.128540277481079\n",
      "|Iter  559  | Total Val Loss  -5.607115864753723 |\n",
      "Loss for batch is  0.7638059258460999\n",
      "Loss for batch is  0.661966860294342\n",
      "Loss for batch is  0.7473048567771912\n",
      "Loss for batch is  -0.272433876991272\n",
      "|Iter  560  | Total Train Loss  1.900643765926361 |\n",
      "Val Loss for batch is  -1.3185125589370728\n",
      "Val Loss for batch is  -1.434932827949524\n",
      "Val Loss for batch is  -1.261125922203064\n",
      "Val Loss for batch is  -2.2077267169952393\n",
      "|Iter  560  | Total Val Loss  -6.2222980260849 |\n",
      "Loss for batch is  0.7501472234725952\n",
      "Loss for batch is  0.6365426182746887\n",
      "Loss for batch is  0.70351243019104\n",
      "Loss for batch is  -0.3221663236618042\n",
      "|Iter  561  | Total Train Loss  1.7680359482765198 |\n",
      "Val Loss for batch is  -1.2388731241226196\n",
      "Val Loss for batch is  -1.4072000980377197\n",
      "Val Loss for batch is  -1.2299816608428955\n",
      "Val Loss for batch is  -2.1060216426849365\n",
      "|Iter  561  | Total Val Loss  -5.982076525688171 |\n",
      "Loss for batch is  0.7560427188873291\n",
      "Loss for batch is  0.5556447505950928\n",
      "Loss for batch is  0.7362140417098999\n",
      "Loss for batch is  -0.33272862434387207\n",
      "|Iter  562  | Total Train Loss  1.7151728868484497 |\n",
      "Val Loss for batch is  -1.2734979391098022\n",
      "Val Loss for batch is  -1.365480899810791\n",
      "Val Loss for batch is  -1.162739872932434\n",
      "Val Loss for batch is  -2.0445632934570312\n",
      "|Iter  562  | Total Val Loss  -5.846282005310059 |\n",
      "Loss for batch is  0.6986898183822632\n",
      "Loss for batch is  0.5672808885574341\n",
      "Loss for batch is  0.6995921730995178\n",
      "Loss for batch is  -0.33923304080963135\n",
      "|Iter  563  | Total Train Loss  1.6263298392295837 |\n",
      "Val Loss for batch is  -1.333507776260376\n",
      "Val Loss for batch is  -1.3892371654510498\n",
      "Val Loss for batch is  -1.2858266830444336\n",
      "Val Loss for batch is  -2.1503400802612305\n",
      "|Iter  563  | Total Val Loss  -6.15891170501709 |\n",
      "Loss for batch is  0.6835070848464966\n",
      "Loss for batch is  0.5412248373031616\n",
      "Loss for batch is  0.7305753827095032\n",
      "Loss for batch is  -0.35310494899749756\n",
      "|Iter  564  | Total Train Loss  1.6022023558616638 |\n",
      "Val Loss for batch is  -1.1848714351654053\n",
      "Val Loss for batch is  -1.2556933164596558\n",
      "Val Loss for batch is  -1.1800963878631592\n",
      "Val Loss for batch is  -2.136258840560913\n",
      "|Iter  564  | Total Val Loss  -5.756919980049133 |\n",
      "Loss for batch is  0.7796161770820618\n",
      "Loss for batch is  0.527414083480835\n",
      "Loss for batch is  0.8227120041847229\n",
      "Loss for batch is  -0.31425559520721436\n",
      "|Iter  565  | Total Train Loss  1.8154866695404053 |\n",
      "Val Loss for batch is  -1.186325192451477\n",
      "Val Loss for batch is  -1.255200743675232\n",
      "Val Loss for batch is  -1.1078424453735352\n",
      "Val Loss for batch is  -2.106107234954834\n",
      "|Iter  565  | Total Val Loss  -5.655475616455078 |\n",
      "Loss for batch is  0.7392807006835938\n",
      "Loss for batch is  0.6215850710868835\n",
      "Loss for batch is  0.7114991545677185\n",
      "Loss for batch is  -0.2723350524902344\n",
      "|Iter  566  | Total Train Loss  1.8000298738479614 |\n",
      "Val Loss for batch is  -1.2089446783065796\n",
      "Val Loss for batch is  -1.4193904399871826\n",
      "Val Loss for batch is  -1.217868447303772\n",
      "Val Loss for batch is  -2.1874122619628906\n",
      "|Iter  566  | Total Val Loss  -6.033615827560425 |\n",
      "Loss for batch is  0.7338702082633972\n",
      "Loss for batch is  0.5986061096191406\n",
      "Loss for batch is  0.7413508296012878\n",
      "Loss for batch is  -0.3940873146057129\n",
      "|Iter  567  | Total Train Loss  1.6797398328781128 |\n",
      "Val Loss for batch is  -1.1634738445281982\n",
      "Val Loss for batch is  -1.375549077987671\n",
      "Val Loss for batch is  -1.1717958450317383\n",
      "Val Loss for batch is  -2.0494585037231445\n",
      "|Iter  567  | Total Val Loss  -5.760277271270752 |\n",
      "Loss for batch is  0.7788735032081604\n",
      "Loss for batch is  0.5428791046142578\n",
      "Loss for batch is  0.6653832793235779\n",
      "Loss for batch is  -0.2641979455947876\n",
      "|Iter  568  | Total Train Loss  1.7229379415512085 |\n",
      "Val Loss for batch is  -1.1891038417816162\n",
      "Val Loss for batch is  -1.3508797883987427\n",
      "Val Loss for batch is  -1.167659044265747\n",
      "Val Loss for batch is  -2.196378707885742\n",
      "|Iter  568  | Total Val Loss  -5.904021382331848 |\n",
      "Loss for batch is  0.7489198446273804\n",
      "Loss for batch is  0.5667259693145752\n",
      "Loss for batch is  0.7768152356147766\n",
      "Loss for batch is  -0.3787243366241455\n",
      "|Iter  569  | Total Train Loss  1.7137367129325867 |\n",
      "Val Loss for batch is  -0.9986230731010437\n",
      "Val Loss for batch is  -1.2416030168533325\n",
      "Val Loss for batch is  -1.0901415348052979\n",
      "Val Loss for batch is  -2.0934150218963623\n",
      "|Iter  569  | Total Val Loss  -5.423782646656036 |\n",
      "Loss for batch is  0.8411058783531189\n",
      "Loss for batch is  0.5554555654525757\n",
      "Loss for batch is  0.8370134830474854\n",
      "Loss for batch is  -0.2980870008468628\n",
      "|Iter  570  | Total Train Loss  1.9354879260063171 |\n",
      "Val Loss for batch is  -1.2787984609603882\n",
      "Val Loss for batch is  -1.442405343055725\n",
      "Val Loss for batch is  -1.2384072542190552\n",
      "Val Loss for batch is  -2.1733832359313965\n",
      "|Iter  570  | Total Val Loss  -6.132994294166565 |\n",
      "Loss for batch is  0.7121429443359375\n",
      "Loss for batch is  0.6456838250160217\n",
      "Loss for batch is  0.6976173520088196\n",
      "Loss for batch is  -0.25415873527526855\n",
      "|Iter  571  | Total Train Loss  1.8012853860855103 |\n",
      "Val Loss for batch is  -1.198230504989624\n",
      "Val Loss for batch is  -1.3696691989898682\n",
      "Val Loss for batch is  -1.1718149185180664\n",
      "Val Loss for batch is  -2.0800576210021973\n",
      "|Iter  571  | Total Val Loss  -5.819772243499756 |\n",
      "Loss for batch is  0.7822666168212891\n",
      "Loss for batch is  0.5695226192474365\n",
      "Loss for batch is  0.7778499126434326\n",
      "Loss for batch is  -0.31771934032440186\n",
      "|Iter  572  | Total Train Loss  1.8119198083877563 |\n",
      "Val Loss for batch is  -1.2696170806884766\n",
      "Val Loss for batch is  -1.3481922149658203\n",
      "Val Loss for batch is  -1.2503085136413574\n",
      "Val Loss for batch is  -2.1237385272979736\n",
      "|Iter  572  | Total Val Loss  -5.991856336593628 |\n",
      "Loss for batch is  0.6914471387863159\n",
      "Loss for batch is  0.585003674030304\n",
      "Loss for batch is  0.7289585471153259\n",
      "Loss for batch is  -0.2660866975784302\n",
      "|Iter  573  | Total Train Loss  1.7393226623535156 |\n",
      "Val Loss for batch is  -1.1266326904296875\n",
      "Val Loss for batch is  -1.3039785623550415\n",
      "Val Loss for batch is  -1.2064056396484375\n",
      "Val Loss for batch is  -2.071671962738037\n",
      "|Iter  573  | Total Val Loss  -5.708688855171204 |\n",
      "Loss for batch is  0.8516727089881897\n",
      "Loss for batch is  0.5560809373855591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.8575121760368347\n",
      "Loss for batch is  -0.31033289432525635\n",
      "|Iter  574  | Total Train Loss  1.9549329280853271 |\n",
      "Val Loss for batch is  -1.207336664199829\n",
      "Val Loss for batch is  -1.3154382705688477\n",
      "Val Loss for batch is  -1.1873345375061035\n",
      "Val Loss for batch is  -2.0938162803649902\n",
      "|Iter  574  | Total Val Loss  -5.8039257526397705 |\n",
      "Loss for batch is  0.7345221042633057\n",
      "Loss for batch is  0.5943248271942139\n",
      "Loss for batch is  0.7321798205375671\n",
      "Loss for batch is  -0.30308103561401367\n",
      "|Iter  575  | Total Train Loss  1.757945716381073 |\n",
      "Val Loss for batch is  -1.2596263885498047\n",
      "Val Loss for batch is  -1.3748233318328857\n",
      "Val Loss for batch is  -1.2519780397415161\n",
      "Val Loss for batch is  -2.1092934608459473\n",
      "|Iter  575  | Total Val Loss  -5.995721220970154 |\n",
      "Loss for batch is  0.709938108921051\n",
      "Loss for batch is  0.58094722032547\n",
      "Loss for batch is  0.7461805939674377\n",
      "Loss for batch is  -0.3591477870941162\n",
      "|Iter  576  | Total Train Loss  1.6779181361198425 |\n",
      "Val Loss for batch is  -1.2507303953170776\n",
      "Val Loss for batch is  -1.4132027626037598\n",
      "Val Loss for batch is  -1.250322937965393\n",
      "Val Loss for batch is  -2.133617401123047\n",
      "|Iter  576  | Total Val Loss  -6.047873497009277 |\n",
      "Loss for batch is  0.7289949059486389\n",
      "Loss for batch is  0.5629042387008667\n",
      "Loss for batch is  0.7189590334892273\n",
      "Loss for batch is  -0.37408900260925293\n",
      "|Iter  577  | Total Train Loss  1.63676917552948 |\n",
      "Val Loss for batch is  -1.286630630493164\n",
      "Val Loss for batch is  -1.4186193943023682\n",
      "Val Loss for batch is  -1.3022370338439941\n",
      "Val Loss for batch is  -2.1631078720092773\n",
      "|Iter  577  | Total Val Loss  -6.170594930648804 |\n",
      "Loss for batch is  0.7152000665664673\n",
      "Loss for batch is  0.5493944883346558\n",
      "Loss for batch is  0.7440590262413025\n",
      "Loss for batch is  -0.37033069133758545\n",
      "|Iter  578  | Total Train Loss  1.63832288980484 |\n",
      "Val Loss for batch is  -1.2519135475158691\n",
      "Val Loss for batch is  -1.3302239179611206\n",
      "Val Loss for batch is  -1.2190587520599365\n",
      "Val Loss for batch is  -2.1105098724365234\n",
      "|Iter  578  | Total Val Loss  -5.91170608997345 |\n",
      "Loss for batch is  0.6684208512306213\n",
      "Loss for batch is  0.5272475481033325\n",
      "Loss for batch is  0.6652320027351379\n",
      "Loss for batch is  -0.34899866580963135\n",
      "|Iter  579  | Total Train Loss  1.5119017362594604 |\n",
      "Val Loss for batch is  -1.2896991968154907\n",
      "Val Loss for batch is  -1.4701484441757202\n",
      "Val Loss for batch is  -1.295175552368164\n",
      "Val Loss for batch is  -2.2023205757141113\n",
      "|Iter  579  | Total Val Loss  -6.257343769073486 |\n",
      "Loss for batch is  0.6865734457969666\n",
      "Loss for batch is  0.5266662836074829\n",
      "Loss for batch is  0.6550053954124451\n",
      "Loss for batch is  -0.38239753246307373\n",
      "|Iter  580  | Total Train Loss  1.4858475923538208 |\n",
      "Val Loss for batch is  -1.2222744226455688\n",
      "Val Loss for batch is  -1.4315224885940552\n",
      "Val Loss for batch is  -1.2559436559677124\n",
      "Val Loss for batch is  -2.1481540203094482\n",
      "|Iter  580  | Total Val Loss  -6.057894587516785 |\n",
      "Loss for batch is  0.6247456073760986\n",
      "Loss for batch is  0.45388293266296387\n",
      "Loss for batch is  0.6160274744033813\n",
      "Loss for batch is  -0.4175255298614502\n",
      "|Iter  581  | Total Train Loss  1.2771304845809937 |\n",
      "Val Loss for batch is  -1.3456796407699585\n",
      "Val Loss for batch is  -1.4776307344436646\n",
      "Val Loss for batch is  -1.3284170627593994\n",
      "Val Loss for batch is  -2.1914374828338623\n",
      "|Iter  581  | Total Val Loss  -6.343164920806885 |\n",
      "Loss for batch is  0.6116634607315063\n",
      "Loss for batch is  0.43979883193969727\n",
      "Loss for batch is  0.5712201595306396\n",
      "Loss for batch is  -0.4212535619735718\n",
      "|Iter  582  | Total Train Loss  1.2014288902282715 |\n",
      "Val Loss for batch is  -1.3379193544387817\n",
      "Val Loss for batch is  -1.4662913084030151\n",
      "Val Loss for batch is  -1.3082926273345947\n",
      "Val Loss for batch is  -2.2494263648986816\n",
      "|Iter  582  | Total Val Loss  -6.361929655075073 |\n",
      "Loss for batch is  0.6097708344459534\n",
      "Loss for batch is  0.44051384925842285\n",
      "Loss for batch is  0.646983802318573\n",
      "Loss for batch is  -0.4261918067932129\n",
      "|Iter  583  | Total Train Loss  1.2710766792297363 |\n",
      "Val Loss for batch is  -1.2023614645004272\n",
      "Val Loss for batch is  -1.3342926502227783\n",
      "Val Loss for batch is  -1.313602328300476\n",
      "Val Loss for batch is  -2.1608335971832275\n",
      "|Iter  583  | Total Val Loss  -6.011090040206909 |\n",
      "Loss for batch is  0.6525750160217285\n",
      "Loss for batch is  0.44186222553253174\n",
      "Loss for batch is  0.7089368104934692\n",
      "Loss for batch is  -0.4324958324432373\n",
      "|Iter  584  | Total Train Loss  1.3708782196044922 |\n",
      "Val Loss for batch is  -1.2158567905426025\n",
      "Val Loss for batch is  -1.3772664070129395\n",
      "Val Loss for batch is  -1.1313750743865967\n",
      "Val Loss for batch is  -2.2055914402008057\n",
      "|Iter  584  | Total Val Loss  -5.930089712142944 |\n",
      "Loss for batch is  0.6868013739585876\n",
      "Loss for batch is  0.44063591957092285\n",
      "Loss for batch is  0.6690354943275452\n",
      "Loss for batch is  -0.367367148399353\n",
      "|Iter  585  | Total Train Loss  1.4291056394577026 |\n",
      "Val Loss for batch is  -1.3260498046875\n",
      "Val Loss for batch is  -1.4363762140274048\n",
      "Val Loss for batch is  -1.3357774019241333\n",
      "Val Loss for batch is  -2.2377586364746094\n",
      "|Iter  585  | Total Val Loss  -6.3359620571136475 |\n",
      "Loss for batch is  0.6031814217567444\n",
      "Loss for batch is  0.483394980430603\n",
      "Loss for batch is  0.5947818160057068\n",
      "Loss for batch is  -0.360562801361084\n",
      "|Iter  586  | Total Train Loss  1.3207954168319702 |\n",
      "Val Loss for batch is  -1.1549546718597412\n",
      "Val Loss for batch is  -1.3828035593032837\n",
      "Val Loss for batch is  -1.2276102304458618\n",
      "Val Loss for batch is  -2.133165121078491\n",
      "|Iter  586  | Total Val Loss  -5.898533582687378 |\n",
      "Loss for batch is  0.6401658654212952\n",
      "Loss for batch is  0.49418020248413086\n",
      "Loss for batch is  0.703887939453125\n",
      "Loss for batch is  -0.41267621517181396\n",
      "|Iter  587  | Total Train Loss  1.425557792186737 |\n",
      "Val Loss for batch is  -1.205876111984253\n",
      "Val Loss for batch is  -1.3355741500854492\n",
      "Val Loss for batch is  -1.1944466829299927\n",
      "Val Loss for batch is  -2.139815330505371\n",
      "|Iter  587  | Total Val Loss  -5.875712275505066 |\n",
      "Loss for batch is  0.7649559378623962\n",
      "Loss for batch is  0.46278905868530273\n",
      "Loss for batch is  0.8316817879676819\n",
      "Loss for batch is  -0.40631985664367676\n",
      "|Iter  588  | Total Train Loss  1.653106927871704 |\n",
      "Val Loss for batch is  -1.1581145524978638\n",
      "Val Loss for batch is  -1.2557709217071533\n",
      "Val Loss for batch is  -1.125340223312378\n",
      "Val Loss for batch is  -2.080620527267456\n",
      "|Iter  588  | Total Val Loss  -5.619846224784851 |\n",
      "Loss for batch is  0.7419171333312988\n",
      "Loss for batch is  0.5292297601699829\n",
      "Loss for batch is  0.7018571496009827\n",
      "Loss for batch is  -0.2847343683242798\n",
      "|Iter  589  | Total Train Loss  1.6882696747779846 |\n",
      "Val Loss for batch is  -1.2794837951660156\n",
      "Val Loss for batch is  -1.3857029676437378\n",
      "Val Loss for batch is  -1.1721043586730957\n",
      "Val Loss for batch is  -2.1645264625549316\n",
      "|Iter  589  | Total Val Loss  -6.001817584037781 |\n",
      "Loss for batch is  0.6416690945625305\n",
      "Loss for batch is  0.5484281778335571\n",
      "Loss for batch is  0.7709974050521851\n",
      "Loss for batch is  -0.3507211208343506\n",
      "|Iter  590  | Total Train Loss  1.6103735566139221 |\n",
      "Val Loss for batch is  -1.1075018644332886\n",
      "Val Loss for batch is  -1.34971022605896\n",
      "Val Loss for batch is  -1.2413057088851929\n",
      "Val Loss for batch is  -2.1373887062072754\n",
      "|Iter  590  | Total Val Loss  -5.835906505584717 |\n",
      "Loss for batch is  0.7969555854797363\n",
      "Loss for batch is  0.5170199871063232\n",
      "Loss for batch is  0.7429816722869873\n",
      "Loss for batch is  -0.3521387577056885\n",
      "|Iter  591  | Total Train Loss  1.7048184871673584 |\n",
      "Val Loss for batch is  -1.264562964439392\n",
      "Val Loss for batch is  -1.4326821565628052\n",
      "Val Loss for batch is  -1.2390763759613037\n",
      "Val Loss for batch is  -2.1735928058624268\n",
      "|Iter  591  | Total Val Loss  -6.109914302825928 |\n",
      "Loss for batch is  0.6718554496765137\n",
      "Loss for batch is  0.4963231086730957\n",
      "Loss for batch is  0.612856924533844\n",
      "Loss for batch is  -0.3710979223251343\n",
      "|Iter  592  | Total Train Loss  1.409937560558319 |\n",
      "Val Loss for batch is  -1.3821202516555786\n",
      "Val Loss for batch is  -1.4419533014297485\n",
      "Val Loss for batch is  -1.3014965057373047\n",
      "Val Loss for batch is  -2.152003765106201\n",
      "|Iter  592  | Total Val Loss  -6.277573823928833 |\n",
      "Loss for batch is  0.5772480368614197\n",
      "Loss for batch is  0.43471455574035645\n",
      "Loss for batch is  0.5709036588668823\n",
      "Loss for batch is  -0.38192200660705566\n",
      "|Iter  593  | Total Train Loss  1.2009442448616028 |\n",
      "Val Loss for batch is  -1.3369897603988647\n",
      "Val Loss for batch is  -1.444145679473877\n",
      "Val Loss for batch is  -1.3303433656692505\n",
      "Val Loss for batch is  -2.196307420730591\n",
      "|Iter  593  | Total Val Loss  -6.307786226272583 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.551650881767273\n",
      "Loss for batch is  0.4225618839263916\n",
      "Loss for batch is  0.5949544906616211\n",
      "Loss for batch is  -0.4365617036819458\n",
      "|Iter  594  | Total Train Loss  1.1326055526733398 |\n",
      "Val Loss for batch is  -1.2916040420532227\n",
      "Val Loss for batch is  -1.4923945665359497\n",
      "Val Loss for batch is  -1.3083994388580322\n",
      "Val Loss for batch is  -2.2310702800750732\n",
      "|Iter  594  | Total Val Loss  -6.323468327522278 |\n",
      "Loss for batch is  0.5967878103256226\n",
      "Loss for batch is  0.40968751907348633\n",
      "Loss for batch is  0.5533913373947144\n",
      "Loss for batch is  -0.47014105319976807\n",
      "|Iter  595  | Total Train Loss  1.0897256135940552 |\n",
      "Val Loss for batch is  -1.3108503818511963\n",
      "Val Loss for batch is  -1.5148279666900635\n",
      "Val Loss for batch is  -1.3400517702102661\n",
      "Val Loss for batch is  -2.2567896842956543\n",
      "|Iter  595  | Total Val Loss  -6.42251980304718 |\n",
      "Loss for batch is  0.5451669692993164\n",
      "Loss for batch is  0.39918482303619385\n",
      "Loss for batch is  0.563124418258667\n",
      "Loss for batch is  -0.4519920349121094\n",
      "|Iter  596  | Total Train Loss  1.0554841756820679 |\n",
      "Val Loss for batch is  -1.3774752616882324\n",
      "Val Loss for batch is  -1.5025391578674316\n",
      "Val Loss for batch is  -1.3638776540756226\n",
      "Val Loss for batch is  -2.279334306716919\n",
      "|Iter  596  | Total Val Loss  -6.523226380348206 |\n",
      "Loss for batch is  0.5743541717529297\n",
      "Loss for batch is  0.37866508960723877\n",
      "Loss for batch is  0.547045111656189\n",
      "Loss for batch is  -0.43514466285705566\n",
      "|Iter  597  | Total Train Loss  1.0649197101593018 |\n",
      "Val Loss for batch is  -1.3015204668045044\n",
      "Val Loss for batch is  -1.4162505865097046\n",
      "Val Loss for batch is  -1.321750283241272\n",
      "Val Loss for batch is  -2.2984461784362793\n",
      "|Iter  597  | Total Val Loss  -6.33796751499176 |\n",
      "Loss for batch is  0.5657858848571777\n",
      "Loss for batch is  0.38468992710113525\n",
      "Loss for batch is  0.5747950077056885\n",
      "Loss for batch is  -0.4802209138870239\n",
      "|Iter  598  | Total Train Loss  1.0450499057769775 |\n",
      "Val Loss for batch is  -1.2577754259109497\n",
      "Val Loss for batch is  -1.4956403970718384\n",
      "Val Loss for batch is  -1.3227821588516235\n",
      "Val Loss for batch is  -2.1618759632110596\n",
      "|Iter  598  | Total Val Loss  -6.238073945045471 |\n",
      "Loss for batch is  0.5997880101203918\n",
      "Loss for batch is  0.40209269523620605\n",
      "Loss for batch is  0.5602359771728516\n",
      "Loss for batch is  -0.47633397579193115\n",
      "|Iter  599  | Total Train Loss  1.0857827067375183 |\n",
      "Val Loss for batch is  -1.2691229581832886\n",
      "Val Loss for batch is  -1.4849328994750977\n",
      "Val Loss for batch is  -1.328092336654663\n",
      "Val Loss for batch is  -2.2042124271392822\n",
      "|Iter  599  | Total Val Loss  -6.2863606214523315 |\n",
      "Loss for batch is  0.624996542930603\n",
      "Loss for batch is  0.4065624475479126\n",
      "Loss for batch is  0.5867473483085632\n",
      "Loss for batch is  -0.48667168617248535\n",
      "|Iter  600  | Total Train Loss  1.1316346526145935 |\n",
      "Val Loss for batch is  -1.2445405721664429\n",
      "Val Loss for batch is  -1.4263499975204468\n",
      "Val Loss for batch is  -1.189972162246704\n",
      "Val Loss for batch is  -2.212179660797119\n",
      "|Iter  600  | Total Val Loss  -6.073042392730713 |\n",
      "Loss for batch is  0.6209036707878113\n",
      "Loss for batch is  0.3727792501449585\n",
      "Loss for batch is  0.6035508513450623\n",
      "Loss for batch is  -0.4852672815322876\n",
      "|Iter  601  | Total Train Loss  1.1119664907455444 |\n",
      "Val Loss for batch is  -1.2849016189575195\n",
      "Val Loss for batch is  -1.3832993507385254\n",
      "Val Loss for batch is  -1.1844274997711182\n",
      "Val Loss for batch is  -2.2310431003570557\n",
      "|Iter  601  | Total Val Loss  -6.083671569824219 |\n",
      "Loss for batch is  0.6155091524124146\n",
      "Loss for batch is  0.386316180229187\n",
      "Loss for batch is  0.5916975140571594\n",
      "Loss for batch is  -0.4423333406448364\n",
      "|Iter  602  | Total Train Loss  1.1511895060539246 |\n",
      "Val Loss for batch is  -1.298702359199524\n",
      "Val Loss for batch is  -1.4842182397842407\n",
      "Val Loss for batch is  -1.2864415645599365\n",
      "Val Loss for batch is  -2.221278667449951\n",
      "|Iter  602  | Total Val Loss  -6.290640830993652 |\n",
      "Loss for batch is  0.5855775475502014\n",
      "Loss for batch is  0.4541062116622925\n",
      "Loss for batch is  0.5399349927902222\n",
      "Loss for batch is  -0.34868621826171875\n",
      "|Iter  603  | Total Train Loss  1.2309325337409973 |\n",
      "Val Loss for batch is  -1.2262002229690552\n",
      "Val Loss for batch is  -1.4546459913253784\n",
      "Val Loss for batch is  -1.2991186380386353\n",
      "Val Loss for batch is  -2.2240006923675537\n",
      "|Iter  603  | Total Val Loss  -6.203965544700623 |\n",
      "Loss for batch is  0.6527117490768433\n",
      "Loss for batch is  0.5043965578079224\n",
      "Loss for batch is  0.6540222764015198\n",
      "Loss for batch is  -0.46583616733551025\n",
      "|Iter  604  | Total Train Loss  1.3452944159507751 |\n",
      "Val Loss for batch is  -1.0395488739013672\n",
      "Val Loss for batch is  -1.2563296556472778\n",
      "Val Loss for batch is  -0.971093475818634\n",
      "Val Loss for batch is  -2.048916816711426\n",
      "|Iter  604  | Total Val Loss  -5.315888822078705 |\n",
      "Loss for batch is  0.8350856304168701\n",
      "Loss for batch is  0.4329555034637451\n",
      "Loss for batch is  0.6440163254737854\n",
      "Loss for batch is  -0.18594896793365479\n",
      "|Iter  605  | Total Train Loss  1.7261084914207458 |\n",
      "Val Loss for batch is  -1.1013108491897583\n",
      "Val Loss for batch is  -1.4103158712387085\n",
      "Val Loss for batch is  -1.2629815340042114\n",
      "Val Loss for batch is  -2.195647716522217\n",
      "|Iter  605  | Total Val Loss  -5.970255970954895 |\n",
      "Loss for batch is  0.7951491475105286\n",
      "Loss for batch is  0.6111041307449341\n",
      "Loss for batch is  0.7889506220817566\n",
      "Loss for batch is  -0.3906334638595581\n",
      "|Iter  606  | Total Train Loss  1.8045704364776611 |\n",
      "Val Loss for batch is  -1.014031171798706\n",
      "Val Loss for batch is  -1.181758165359497\n",
      "Val Loss for batch is  -1.0022048950195312\n",
      "Val Loss for batch is  -1.9609235525131226\n",
      "|Iter  606  | Total Val Loss  -5.158917784690857 |\n",
      "Loss for batch is  0.8233867883682251\n",
      "Loss for batch is  0.48984503746032715\n",
      "Loss for batch is  0.7857052087783813\n",
      "Loss for batch is  -0.32224559783935547\n",
      "|Iter  607  | Total Train Loss  1.7766914367675781 |\n",
      "Val Loss for batch is  -1.2514489889144897\n",
      "Val Loss for batch is  -1.3202292919158936\n",
      "Val Loss for batch is  -1.3020797967910767\n",
      "Val Loss for batch is  -2.1171536445617676\n",
      "|Iter  607  | Total Val Loss  -5.9909117221832275 |\n",
      "Loss for batch is  0.6629806160926819\n",
      "Loss for batch is  0.6281191110610962\n",
      "Loss for batch is  0.708453893661499\n",
      "Loss for batch is  -0.37547552585601807\n",
      "|Iter  608  | Total Train Loss  1.624078094959259 |\n",
      "Val Loss for batch is  -1.3281387090682983\n",
      "Val Loss for batch is  -1.4210314750671387\n",
      "Val Loss for batch is  -1.1511995792388916\n",
      "Val Loss for batch is  -2.170056104660034\n",
      "|Iter  608  | Total Val Loss  -6.070425868034363 |\n",
      "Loss for batch is  0.6719629764556885\n",
      "Loss for batch is  0.5380711555480957\n",
      "Loss for batch is  0.7952594757080078\n",
      "Loss for batch is  -0.42391347885131836\n",
      "|Iter  609  | Total Train Loss  1.5813801288604736 |\n",
      "Val Loss for batch is  -1.1053749322891235\n",
      "Val Loss for batch is  -1.361093282699585\n",
      "Val Loss for batch is  -1.1098954677581787\n",
      "Val Loss for batch is  -2.1550509929656982\n",
      "|Iter  609  | Total Val Loss  -5.7314146757125854 |\n",
      "Loss for batch is  0.7596741914749146\n",
      "Loss for batch is  0.5130757093429565\n",
      "Loss for batch is  0.6464849710464478\n",
      "Loss for batch is  -0.39260923862457275\n",
      "|Iter  610  | Total Train Loss  1.526625633239746 |\n",
      "Val Loss for batch is  -1.391850233078003\n",
      "Val Loss for batch is  -1.5292832851409912\n",
      "Val Loss for batch is  -1.3482435941696167\n",
      "Val Loss for batch is  -2.261619806289673\n",
      "|Iter  610  | Total Val Loss  -6.530996918678284 |\n",
      "Loss for batch is  0.5958099961280823\n",
      "Loss for batch is  0.5126112699508667\n",
      "Loss for batch is  0.7304379343986511\n",
      "Loss for batch is  -0.42839717864990234\n",
      "|Iter  611  | Total Train Loss  1.4104620218276978 |\n",
      "Val Loss for batch is  -1.3807315826416016\n",
      "Val Loss for batch is  -1.475527286529541\n",
      "Val Loss for batch is  -1.291884183883667\n",
      "Val Loss for batch is  -2.2237298488616943\n",
      "|Iter  611  | Total Val Loss  -6.371872901916504 |\n",
      "Loss for batch is  0.6045663952827454\n",
      "Loss for batch is  0.47827255725860596\n",
      "Loss for batch is  0.7016530632972717\n",
      "Loss for batch is  -0.42929720878601074\n",
      "|Iter  612  | Total Train Loss  1.3551948070526123 |\n",
      "Val Loss for batch is  -1.3932108879089355\n",
      "Val Loss for batch is  -1.3822816610336304\n",
      "Val Loss for batch is  -1.2578186988830566\n",
      "Val Loss for batch is  -2.2132935523986816\n",
      "|Iter  612  | Total Val Loss  -6.246604800224304 |\n",
      "Loss for batch is  0.5631774663925171\n",
      "Loss for batch is  0.44777345657348633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.6459630727767944\n",
      "Loss for batch is  -0.47719085216522217\n",
      "|Iter  613  | Total Train Loss  1.1797231435775757 |\n",
      "Val Loss for batch is  -1.3733179569244385\n",
      "Val Loss for batch is  -1.501063346862793\n",
      "Val Loss for batch is  -1.3120367527008057\n",
      "Val Loss for batch is  -2.2616915702819824\n",
      "|Iter  613  | Total Val Loss  -6.4481096267700195 |\n",
      "Loss for batch is  0.5669848322868347\n",
      "Loss for batch is  0.4019167423248291\n",
      "Loss for batch is  0.6063363552093506\n",
      "Loss for batch is  -0.47363734245300293\n",
      "|Iter  614  | Total Train Loss  1.1016005873680115 |\n",
      "Val Loss for batch is  -1.3997153043746948\n",
      "Val Loss for batch is  -1.5186772346496582\n",
      "Val Loss for batch is  -1.3831266164779663\n",
      "Val Loss for batch is  -2.271577835083008\n",
      "|Iter  614  | Total Val Loss  -6.573096990585327 |\n",
      "Loss for batch is  0.5389838218688965\n",
      "Loss for batch is  0.38669073581695557\n",
      "Loss for batch is  0.6011976003646851\n",
      "Loss for batch is  -0.4981043338775635\n",
      "|Iter  615  | Total Train Loss  1.0287678241729736 |\n",
      "Val Loss for batch is  -1.2497577667236328\n",
      "Val Loss for batch is  -1.5026218891143799\n",
      "Val Loss for batch is  -1.2358708381652832\n",
      "Val Loss for batch is  -2.2249693870544434\n",
      "|Iter  615  | Total Val Loss  -6.213219881057739 |\n",
      "Loss for batch is  0.55763179063797\n",
      "Loss for batch is  0.37127506732940674\n",
      "Loss for batch is  0.6437850594520569\n",
      "Loss for batch is  -0.5095456838607788\n",
      "|Iter  616  | Total Train Loss  1.0631462335586548 |\n",
      "Val Loss for batch is  -1.415095567703247\n",
      "Val Loss for batch is  -1.5698413848876953\n",
      "Val Loss for batch is  -1.271342158317566\n",
      "Val Loss for batch is  -2.2799196243286133\n",
      "|Iter  616  | Total Val Loss  -6.536198735237122 |\n",
      "Loss for batch is  0.5583834648132324\n",
      "Loss for batch is  0.39505648612976074\n",
      "Loss for batch is  0.5339335203170776\n",
      "Loss for batch is  -0.45367681980133057\n",
      "|Iter  617  | Total Train Loss  1.0336966514587402 |\n",
      "Val Loss for batch is  -1.337196946144104\n",
      "Val Loss for batch is  -1.4469789266586304\n",
      "Val Loss for batch is  -1.2653578519821167\n",
      "Val Loss for batch is  -2.196599006652832\n",
      "|Iter  617  | Total Val Loss  -6.246132731437683 |\n",
      "Loss for batch is  0.5512090921401978\n",
      "Loss for batch is  0.40467190742492676\n",
      "Loss for batch is  0.5971997380256653\n",
      "Loss for batch is  -0.4822518825531006\n",
      "|Iter  618  | Total Train Loss  1.0708288550376892 |\n",
      "Val Loss for batch is  -1.2744272947311401\n",
      "Val Loss for batch is  -1.3674407005310059\n",
      "Val Loss for batch is  -1.267569661140442\n",
      "Val Loss for batch is  -2.1659271717071533\n",
      "|Iter  618  | Total Val Loss  -6.075364828109741 |\n",
      "Loss for batch is  0.5900754332542419\n",
      "Loss for batch is  0.388647198677063\n",
      "Loss for batch is  0.6616320610046387\n",
      "Loss for batch is  -0.47658705711364746\n",
      "|Iter  619  | Total Train Loss  1.1637676358222961 |\n",
      "Val Loss for batch is  -1.3099985122680664\n",
      "Val Loss for batch is  -1.4059721231460571\n",
      "Val Loss for batch is  -1.2313895225524902\n",
      "Val Loss for batch is  -2.232985258102417\n",
      "|Iter  619  | Total Val Loss  -6.180345416069031 |\n",
      "Loss for batch is  0.6220385432243347\n",
      "Loss for batch is  0.45396745204925537\n",
      "Loss for batch is  0.6322206854820251\n",
      "Loss for batch is  -0.3356318473815918\n",
      "|Iter  620  | Total Train Loss  1.3725948333740234 |\n",
      "Val Loss for batch is  -1.3230386972427368\n",
      "Val Loss for batch is  -1.5232502222061157\n",
      "Val Loss for batch is  -1.3164961338043213\n",
      "Val Loss for batch is  -2.2251293659210205\n",
      "|Iter  620  | Total Val Loss  -6.387914419174194 |\n",
      "Loss for batch is  0.551673948764801\n",
      "Loss for batch is  0.4044303894042969\n",
      "Loss for batch is  0.49494004249572754\n",
      "Loss for batch is  -0.4682539701461792\n",
      "|Iter  621  | Total Train Loss  0.9827904105186462 |\n",
      "Val Loss for batch is  -1.293980360031128\n",
      "Val Loss for batch is  -1.4969482421875\n",
      "Val Loss for batch is  -1.2889044284820557\n",
      "Val Loss for batch is  -2.2349905967712402\n",
      "|Iter  621  | Total Val Loss  -6.314823627471924 |\n",
      "Loss for batch is  0.6201196908950806\n",
      "Loss for batch is  0.3459184169769287\n",
      "Loss for batch is  0.563927412033081\n",
      "Loss for batch is  -0.5063985586166382\n",
      "|Iter  622  | Total Train Loss  1.0235669612884521 |\n",
      "Val Loss for batch is  -1.401232123374939\n",
      "Val Loss for batch is  -1.5645016431808472\n",
      "Val Loss for batch is  -1.3775826692581177\n",
      "Val Loss for batch is  -2.2745330333709717\n",
      "|Iter  622  | Total Val Loss  -6.6178494691848755 |\n",
      "Loss for batch is  0.5371315479278564\n",
      "Loss for batch is  0.38275814056396484\n",
      "Loss for batch is  0.5007201433181763\n",
      "Loss for batch is  -0.47931981086730957\n",
      "|Iter  623  | Total Train Loss  0.941290020942688 |\n",
      "Val Loss for batch is  -1.3402879238128662\n",
      "Val Loss for batch is  -1.5179847478866577\n",
      "Val Loss for batch is  -1.298417329788208\n",
      "Val Loss for batch is  -2.2430195808410645\n",
      "|Iter  623  | Total Val Loss  -6.399709582328796 |\n",
      "Loss for batch is  0.5421376824378967\n",
      "Loss for batch is  0.34233200550079346\n",
      "Loss for batch is  0.5518286228179932\n",
      "Loss for batch is  -0.528315544128418\n",
      "|Iter  624  | Total Train Loss  0.9079827666282654 |\n",
      "Val Loss for batch is  -1.362465262413025\n",
      "Val Loss for batch is  -1.5540063381195068\n",
      "Val Loss for batch is  -1.2690712213516235\n",
      "Val Loss for batch is  -2.237269878387451\n",
      "|Iter  624  | Total Val Loss  -6.4228127002716064 |\n",
      "Loss for batch is  0.5048918724060059\n",
      "Loss for batch is  0.3241860866546631\n",
      "Loss for batch is  0.469940185546875\n",
      "Loss for batch is  -0.5222576856613159\n",
      "|Iter  625  | Total Train Loss  0.776760458946228 |\n",
      "Val Loss for batch is  -1.3741133213043213\n",
      "Val Loss for batch is  -1.5345609188079834\n",
      "Val Loss for batch is  -1.4547039270401\n",
      "Val Loss for batch is  -2.3115525245666504\n",
      "|Iter  625  | Total Val Loss  -6.674930691719055 |\n",
      "Loss for batch is  0.4720417261123657\n",
      "Loss for batch is  0.3699394464492798\n",
      "Loss for batch is  0.3926764726638794\n",
      "Loss for batch is  -0.5193363428115845\n",
      "|Iter  626  | Total Train Loss  0.7153213024139404 |\n",
      "Val Loss for batch is  -1.413057565689087\n",
      "Val Loss for batch is  -1.5434843301773071\n",
      "Val Loss for batch is  -1.3133866786956787\n",
      "Val Loss for batch is  -2.301539182662964\n",
      "|Iter  626  | Total Val Loss  -6.571467757225037 |\n",
      "Loss for batch is  0.45465588569641113\n",
      "Loss for batch is  0.29780399799346924\n",
      "Loss for batch is  0.4462658166885376\n",
      "Loss for batch is  -0.5589172840118408\n",
      "|Iter  627  | Total Train Loss  0.6398084163665771 |\n",
      "Val Loss for batch is  -1.2566907405853271\n",
      "Val Loss for batch is  -1.5202476978302002\n",
      "Val Loss for batch is  -1.2611674070358276\n",
      "Val Loss for batch is  -2.25705885887146\n",
      "|Iter  627  | Total Val Loss  -6.295164704322815 |\n",
      "Loss for batch is  0.5059131383895874\n",
      "Loss for batch is  0.23353207111358643\n",
      "Loss for batch is  0.4645808935165405\n",
      "Loss for batch is  -0.5592426061630249\n",
      "|Iter  628  | Total Train Loss  0.6447834968566895 |\n",
      "Val Loss for batch is  -1.3562102317810059\n",
      "Val Loss for batch is  -1.5874272584915161\n",
      "Val Loss for batch is  -1.3926466703414917\n",
      "Val Loss for batch is  -2.2618043422698975\n",
      "|Iter  628  | Total Val Loss  -6.598088502883911 |\n",
      "Loss for batch is  0.49015796184539795\n",
      "Loss for batch is  0.26075100898742676\n",
      "Loss for batch is  0.4391312599182129\n",
      "Loss for batch is  -0.5143187046051025\n",
      "|Iter  629  | Total Train Loss  0.6757215261459351 |\n",
      "Val Loss for batch is  -1.3947104215621948\n",
      "Val Loss for batch is  -1.5808286666870117\n",
      "Val Loss for batch is  -1.3863605260849\n",
      "Val Loss for batch is  -2.322577714920044\n",
      "|Iter  629  | Total Val Loss  -6.68447732925415 |\n",
      "Loss for batch is  0.46981334686279297\n",
      "Loss for batch is  0.3540884256362915\n",
      "Loss for batch is  0.47525978088378906\n",
      "Loss for batch is  -0.5312906503677368\n",
      "|Iter  630  | Total Train Loss  0.7678709030151367 |\n",
      "Val Loss for batch is  -1.167751431465149\n",
      "Val Loss for batch is  -1.3479794263839722\n",
      "Val Loss for batch is  -1.1203393936157227\n",
      "Val Loss for batch is  -2.212055206298828\n",
      "|Iter  630  | Total Val Loss  -5.848125457763672 |\n",
      "Loss for batch is  0.6136699914932251\n",
      "Loss for batch is  0.2461249828338623\n",
      "Loss for batch is  0.6710504293441772\n",
      "Loss for batch is  -0.6039206981658936\n",
      "|Iter  631  | Total Train Loss  0.9269247055053711 |\n",
      "Val Loss for batch is  -1.1858599185943604\n",
      "Val Loss for batch is  -1.2981712818145752\n",
      "Val Loss for batch is  -1.1175132989883423\n",
      "Val Loss for batch is  -2.1425182819366455\n",
      "|Iter  631  | Total Val Loss  -5.744062781333923 |\n",
      "Loss for batch is  0.6242331862449646\n",
      "Loss for batch is  0.42222118377685547\n",
      "Loss for batch is  0.48630475997924805\n",
      "Loss for batch is  -0.30110275745391846\n",
      "|Iter  632  | Total Train Loss  1.2316563725471497 |\n",
      "Val Loss for batch is  -1.295668125152588\n",
      "Val Loss for batch is  -1.5447083711624146\n",
      "Val Loss for batch is  -1.2555311918258667\n",
      "Val Loss for batch is  -2.276118040084839\n",
      "|Iter  632  | Total Val Loss  -6.372025728225708 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.4708331823348999\n",
      "Loss for batch is  0.43050599098205566\n",
      "Loss for batch is  0.5626769661903381\n",
      "Loss for batch is  -0.4789234399795532\n",
      "|Iter  633  | Total Train Loss  0.9850926995277405 |\n",
      "Val Loss for batch is  -1.2561843395233154\n",
      "Val Loss for batch is  -1.4865047931671143\n",
      "Val Loss for batch is  -1.2488700151443481\n",
      "Val Loss for batch is  -2.156935453414917\n",
      "|Iter  633  | Total Val Loss  -6.148494601249695 |\n",
      "Loss for batch is  0.6211471557617188\n",
      "Loss for batch is  0.34868812561035156\n",
      "Loss for batch is  0.5277683734893799\n",
      "Loss for batch is  -0.4165794849395752\n",
      "|Iter  634  | Total Train Loss  1.081024169921875 |\n",
      "Val Loss for batch is  -1.1522581577301025\n",
      "Val Loss for batch is  -1.3678557872772217\n",
      "Val Loss for batch is  -1.293432354927063\n",
      "Val Loss for batch is  -2.108306646347046\n",
      "|Iter  634  | Total Val Loss  -5.921852946281433 |\n",
      "Loss for batch is  0.6924853324890137\n",
      "Loss for batch is  0.2879253625869751\n",
      "Loss for batch is  0.8003538846969604\n",
      "Loss for batch is  -0.45791196823120117\n",
      "|Iter  635  | Total Train Loss  1.322852611541748 |\n",
      "Val Loss for batch is  -1.1342718601226807\n",
      "Val Loss for batch is  -1.272372841835022\n",
      "Val Loss for batch is  -1.1383434534072876\n",
      "Val Loss for batch is  -1.9991774559020996\n",
      "|Iter  635  | Total Val Loss  -5.54416561126709 |\n",
      "Loss for batch is  0.6838697791099548\n",
      "Loss for batch is  0.4505960941314697\n",
      "Loss for batch is  0.5326859951019287\n",
      "Loss for batch is  -0.41680967807769775\n",
      "|Iter  636  | Total Train Loss  1.2503421902656555 |\n",
      "Val Loss for batch is  -1.3506451845169067\n",
      "Val Loss for batch is  -1.5530083179473877\n",
      "Val Loss for batch is  -1.362568974494934\n",
      "Val Loss for batch is  -2.2527987957000732\n",
      "|Iter  636  | Total Val Loss  -6.519021272659302 |\n",
      "Loss for batch is  0.5807810425758362\n",
      "Loss for batch is  0.3526954650878906\n",
      "Loss for batch is  0.5606170892715454\n",
      "Loss for batch is  -0.45685815811157227\n",
      "|Iter  637  | Total Train Loss  1.0372354388237 |\n",
      "Val Loss for batch is  -1.3406004905700684\n",
      "Val Loss for batch is  -1.5026905536651611\n",
      "Val Loss for batch is  -1.3244061470031738\n",
      "Val Loss for batch is  -2.18503737449646\n",
      "|Iter  637  | Total Val Loss  -6.352734565734863 |\n",
      "Loss for batch is  0.5198709964752197\n",
      "Loss for batch is  0.31892454624176025\n",
      "Loss for batch is  0.4824274778366089\n",
      "Loss for batch is  -0.5124976634979248\n",
      "|Iter  638  | Total Train Loss  0.8087253570556641 |\n",
      "Val Loss for batch is  -1.3861083984375\n",
      "Val Loss for batch is  -1.4925884008407593\n",
      "Val Loss for batch is  -1.3069446086883545\n",
      "Val Loss for batch is  -2.240283250808716\n",
      "|Iter  638  | Total Val Loss  -6.42592465877533 |\n",
      "Loss for batch is  0.436226487159729\n",
      "Loss for batch is  0.3142479658126831\n",
      "Loss for batch is  0.475278377532959\n",
      "Loss for batch is  -0.5412931442260742\n",
      "|Iter  639  | Total Train Loss  0.6844596862792969 |\n",
      "Val Loss for batch is  -1.4635672569274902\n",
      "Val Loss for batch is  -1.5945051908493042\n",
      "Val Loss for batch is  -1.476639747619629\n",
      "Val Loss for batch is  -2.3100414276123047\n",
      "|Iter  639  | Total Val Loss  -6.844753623008728 |\n",
      "Loss for batch is  0.4238849878311157\n",
      "Loss for batch is  0.24655437469482422\n",
      "Loss for batch is  0.421766996383667\n",
      "Loss for batch is  -0.5867362022399902\n",
      "|Iter  640  | Total Train Loss  0.5054701566696167 |\n",
      "Val Loss for batch is  -1.3724982738494873\n",
      "Val Loss for batch is  -1.541851282119751\n",
      "Val Loss for batch is  -1.380183458328247\n",
      "Val Loss for batch is  -2.288194417953491\n",
      "|Iter  640  | Total Val Loss  -6.582727432250977 |\n",
      "Loss for batch is  0.3948390483856201\n",
      "Loss for batch is  0.21115291118621826\n",
      "Loss for batch is  0.37335240840911865\n",
      "Loss for batch is  -0.5919454097747803\n",
      "|Iter  641  | Total Train Loss  0.38739895820617676 |\n",
      "Val Loss for batch is  -1.4478007555007935\n",
      "Val Loss for batch is  -1.6008353233337402\n",
      "Val Loss for batch is  -1.4794479608535767\n",
      "Val Loss for batch is  -2.3633885383605957\n",
      "|Iter  641  | Total Val Loss  -6.891472578048706 |\n",
      "Loss for batch is  0.3863081932067871\n",
      "Loss for batch is  0.21269357204437256\n",
      "Loss for batch is  0.3604750633239746\n",
      "Loss for batch is  -0.6173281669616699\n",
      "|Iter  642  | Total Train Loss  0.34214866161346436 |\n",
      "Val Loss for batch is  -1.4131804704666138\n",
      "Val Loss for batch is  -1.62285315990448\n",
      "Val Loss for batch is  -1.4167356491088867\n",
      "Val Loss for batch is  -2.3253657817840576\n",
      "|Iter  642  | Total Val Loss  -6.778135061264038 |\n",
      "Loss for batch is  0.3890162706375122\n",
      "Loss for batch is  0.23101305961608887\n",
      "Loss for batch is  0.33710598945617676\n",
      "Loss for batch is  -0.6178188323974609\n",
      "|Iter  643  | Total Train Loss  0.3393164873123169 |\n",
      "Val Loss for batch is  -1.3902957439422607\n",
      "Val Loss for batch is  -1.5038459300994873\n",
      "Val Loss for batch is  -1.4288065433502197\n",
      "Val Loss for batch is  -2.369091272354126\n",
      "|Iter  643  | Total Val Loss  -6.692039489746094 |\n",
      "Loss for batch is  0.3953787088394165\n",
      "Loss for batch is  0.2185955047607422\n",
      "Loss for batch is  0.3297325372695923\n",
      "Loss for batch is  -0.6183053255081177\n",
      "|Iter  644  | Total Train Loss  0.3254014253616333 |\n",
      "Val Loss for batch is  -1.4311021566390991\n",
      "Val Loss for batch is  -1.5733708143234253\n",
      "Val Loss for batch is  -1.3626084327697754\n",
      "Val Loss for batch is  -2.360619306564331\n",
      "|Iter  644  | Total Val Loss  -6.727700710296631 |\n",
      "Loss for batch is  0.3779984712600708\n",
      "Loss for batch is  0.22893071174621582\n",
      "Loss for batch is  0.4043337106704712\n",
      "Loss for batch is  -0.6415451765060425\n",
      "|Iter  645  | Total Train Loss  0.36971771717071533 |\n",
      "Val Loss for batch is  -1.219482183456421\n",
      "Val Loss for batch is  -1.4357059001922607\n",
      "Val Loss for batch is  -1.317548155784607\n",
      "Val Loss for batch is  -2.2750589847564697\n",
      "|Iter  645  | Total Val Loss  -6.247795224189758 |\n",
      "Loss for batch is  0.4946850538253784\n",
      "Loss for batch is  0.2187563180923462\n",
      "Loss for batch is  0.48470020294189453\n",
      "Loss for batch is  -0.6457020044326782\n",
      "|Iter  646  | Total Train Loss  0.5524395704269409 |\n",
      "Val Loss for batch is  -1.290751338005066\n",
      "Val Loss for batch is  -1.4344403743743896\n",
      "Val Loss for batch is  -1.1607367992401123\n",
      "Val Loss for batch is  -2.2230732440948486\n",
      "|Iter  646  | Total Val Loss  -6.1090017557144165 |\n",
      "Loss for batch is  0.5470439195632935\n",
      "Loss for batch is  0.24281525611877441\n",
      "Loss for batch is  0.47229278087615967\n",
      "Loss for batch is  -0.4869288206100464\n",
      "|Iter  647  | Total Train Loss  0.7752231359481812 |\n",
      "Val Loss for batch is  -1.2691813707351685\n",
      "Val Loss for batch is  -1.5504506826400757\n",
      "Val Loss for batch is  -1.309638500213623\n",
      "Val Loss for batch is  -2.3120267391204834\n",
      "|Iter  647  | Total Val Loss  -6.441297292709351 |\n",
      "Loss for batch is  0.4470977783203125\n",
      "Loss for batch is  0.3615976572036743\n",
      "Loss for batch is  0.5253591537475586\n",
      "Loss for batch is  -0.49468910694122314\n",
      "|Iter  648  | Total Train Loss  0.8393654823303223 |\n",
      "Val Loss for batch is  -1.113769769668579\n",
      "Val Loss for batch is  -1.3610765933990479\n",
      "Val Loss for batch is  -1.1993778944015503\n",
      "Val Loss for batch is  -2.1382226943969727\n",
      "|Iter  648  | Total Val Loss  -5.81244695186615 |\n",
      "Loss for batch is  0.6813300848007202\n",
      "Loss for batch is  0.26867854595184326\n",
      "Loss for batch is  0.6139273643493652\n",
      "Loss for batch is  -0.4980034828186035\n",
      "|Iter  649  | Total Train Loss  1.0659325122833252 |\n",
      "Val Loss for batch is  -1.300604224205017\n",
      "Val Loss for batch is  -1.4899110794067383\n",
      "Val Loss for batch is  -1.316772699356079\n",
      "Val Loss for batch is  -2.216575860977173\n",
      "|Iter  649  | Total Val Loss  -6.323863863945007 |\n",
      "Loss for batch is  0.4950312376022339\n",
      "Loss for batch is  0.40151119232177734\n",
      "Loss for batch is  0.4352447986602783\n",
      "Loss for batch is  -0.5645954608917236\n",
      "|Iter  650  | Total Train Loss  0.7671917676925659 |\n",
      "Val Loss for batch is  -1.4205641746520996\n",
      "Val Loss for batch is  -1.5768619775772095\n",
      "Val Loss for batch is  -1.3639638423919678\n",
      "Val Loss for batch is  -2.320889711380005\n",
      "|Iter  650  | Total Val Loss  -6.682279706001282 |\n",
      "Loss for batch is  0.43110227584838867\n",
      "Loss for batch is  0.3051471710205078\n",
      "Loss for batch is  0.3959383964538574\n",
      "Loss for batch is  -0.5212266445159912\n",
      "|Iter  651  | Total Train Loss  0.6109611988067627 |\n",
      "Val Loss for batch is  -1.3234446048736572\n",
      "Val Loss for batch is  -1.5182894468307495\n",
      "Val Loss for batch is  -1.4424952268600464\n",
      "Val Loss for batch is  -2.2248430252075195\n",
      "|Iter  651  | Total Val Loss  -6.509072303771973 |\n",
      "Loss for batch is  0.4631296396255493\n",
      "Loss for batch is  0.2491466999053955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.46418464183807373\n",
      "Loss for batch is  -0.5744527578353882\n",
      "|Iter  652  | Total Train Loss  0.6020082235336304 |\n",
      "Val Loss for batch is  -1.4661787748336792\n",
      "Val Loss for batch is  -1.6248525381088257\n",
      "Val Loss for batch is  -1.4303873777389526\n",
      "Val Loss for batch is  -2.307211399078369\n",
      "|Iter  652  | Total Val Loss  -6.828630089759827 |\n",
      "Loss for batch is  0.38157474994659424\n",
      "Loss for batch is  0.2867443561553955\n",
      "Loss for batch is  0.5006606578826904\n",
      "Loss for batch is  -0.604336142539978\n",
      "|Iter  653  | Total Train Loss  0.5646436214447021 |\n",
      "Val Loss for batch is  -1.3905539512634277\n",
      "Val Loss for batch is  -1.5711771249771118\n",
      "Val Loss for batch is  -1.4444204568862915\n",
      "Val Loss for batch is  -2.2449073791503906\n",
      "|Iter  653  | Total Val Loss  -6.651058912277222 |\n",
      "Loss for batch is  0.422371506690979\n",
      "Loss for batch is  0.3321288824081421\n",
      "Loss for batch is  0.3405684232711792\n",
      "Loss for batch is  -0.6226963996887207\n",
      "|Iter  654  | Total Train Loss  0.4723724126815796 |\n",
      "Val Loss for batch is  -1.5149348974227905\n",
      "Val Loss for batch is  -1.650772213935852\n",
      "Val Loss for batch is  -1.4694874286651611\n",
      "Val Loss for batch is  -2.345257043838501\n",
      "|Iter  654  | Total Val Loss  -6.980451583862305 |\n",
      "Loss for batch is  0.29088294506073\n",
      "Loss for batch is  0.19956731796264648\n",
      "Loss for batch is  0.41738975048065186\n",
      "Loss for batch is  -0.6511482000350952\n",
      "|Iter  655  | Total Train Loss  0.2566918134689331 |\n",
      "Val Loss for batch is  -1.4496560096740723\n",
      "Val Loss for batch is  -1.616401195526123\n",
      "Val Loss for batch is  -1.4568862915039062\n",
      "Val Loss for batch is  -2.366682529449463\n",
      "|Iter  655  | Total Val Loss  -6.8896260261535645 |\n",
      "Loss for batch is  0.30178749561309814\n",
      "Loss for batch is  0.17615127563476562\n",
      "Loss for batch is  0.33271729946136475\n",
      "Loss for batch is  -0.6395341157913208\n",
      "|Iter  656  | Total Train Loss  0.17112195491790771 |\n",
      "Val Loss for batch is  -1.4986727237701416\n",
      "Val Loss for batch is  -1.6513667106628418\n",
      "Val Loss for batch is  -1.4619910717010498\n",
      "Val Loss for batch is  -2.397081136703491\n",
      "|Iter  656  | Total Val Loss  -7.009111642837524 |\n",
      "Loss for batch is  0.3165595531463623\n",
      "Loss for batch is  0.14324617385864258\n",
      "Loss for batch is  0.3067106008529663\n",
      "Loss for batch is  -0.62254798412323\n",
      "|Iter  657  | Total Train Loss  0.1439683437347412 |\n",
      "Val Loss for batch is  -1.4614547491073608\n",
      "Val Loss for batch is  -1.548356056213379\n",
      "Val Loss for batch is  -1.465994954109192\n",
      "Val Loss for batch is  -2.405744791030884\n",
      "|Iter  657  | Total Val Loss  -6.881550550460815 |\n",
      "Loss for batch is  0.3453429937362671\n",
      "Loss for batch is  0.14828944206237793\n",
      "Loss for batch is  0.3097764253616333\n",
      "Loss for batch is  -0.6782921552658081\n",
      "|Iter  658  | Total Train Loss  0.12511670589447021 |\n",
      "Val Loss for batch is  -1.3038299083709717\n",
      "Val Loss for batch is  -1.5295398235321045\n",
      "Val Loss for batch is  -1.3893463611602783\n",
      "Val Loss for batch is  -2.334406614303589\n",
      "|Iter  658  | Total Val Loss  -6.557122707366943 |\n",
      "Loss for batch is  0.385431170463562\n",
      "Loss for batch is  0.18402135372161865\n",
      "Loss for batch is  0.33328187465667725\n",
      "Loss for batch is  -0.6660066843032837\n",
      "|Iter  659  | Total Train Loss  0.23672771453857422 |\n",
      "Val Loss for batch is  -1.3014353513717651\n",
      "Val Loss for batch is  -1.520246148109436\n",
      "Val Loss for batch is  -1.3396631479263306\n",
      "Val Loss for batch is  -2.3225646018981934\n",
      "|Iter  659  | Total Val Loss  -6.483909249305725 |\n",
      "Loss for batch is  0.47141051292419434\n",
      "Loss for batch is  0.18583226203918457\n",
      "Loss for batch is  0.5191899538040161\n",
      "Loss for batch is  -0.6552191972732544\n",
      "|Iter  660  | Total Train Loss  0.5212135314941406 |\n",
      "Val Loss for batch is  -1.0438750982284546\n",
      "Val Loss for batch is  -1.1531040668487549\n",
      "Val Loss for batch is  -0.8537667989730835\n",
      "Val Loss for batch is  -2.204848051071167\n",
      "|Iter  660  | Total Val Loss  -5.25559401512146 |\n",
      "Loss for batch is  0.6286439895629883\n",
      "Loss for batch is  0.17541778087615967\n",
      "Loss for batch is  0.5818353891372681\n",
      "Loss for batch is  -0.47089433670043945\n",
      "|Iter  661  | Total Train Loss  0.9150028228759766 |\n",
      "Val Loss for batch is  -1.3292099237442017\n",
      "Val Loss for batch is  -1.4487383365631104\n",
      "Val Loss for batch is  -1.3390491008758545\n",
      "Val Loss for batch is  -2.3695805072784424\n",
      "|Iter  661  | Total Val Loss  -6.486577868461609 |\n",
      "Loss for batch is  0.41284775733947754\n",
      "Loss for batch is  0.531197190284729\n",
      "Loss for batch is  0.4920225143432617\n",
      "Loss for batch is  -0.45618438720703125\n",
      "|Iter  662  | Total Train Loss  0.979883074760437 |\n",
      "Val Loss for batch is  -1.1042970418930054\n",
      "Val Loss for batch is  -1.2950425148010254\n",
      "Val Loss for batch is  -1.1843301057815552\n",
      "Val Loss for batch is  -2.1294336318969727\n",
      "|Iter  662  | Total Val Loss  -5.713103294372559 |\n",
      "Loss for batch is  0.6820976138114929\n",
      "Loss for batch is  0.353814959526062\n",
      "Loss for batch is  0.5351802110671997\n",
      "Loss for batch is  -0.48766255378723145\n",
      "|Iter  663  | Total Train Loss  1.0834302306175232 |\n",
      "Val Loss for batch is  -1.4357280731201172\n",
      "Val Loss for batch is  -1.5805160999298096\n",
      "Val Loss for batch is  -1.3713117837905884\n",
      "Val Loss for batch is  -2.2338361740112305\n",
      "|Iter  663  | Total Val Loss  -6.621392130851746 |\n",
      "Loss for batch is  0.4253147840499878\n",
      "Loss for batch is  0.232346773147583\n",
      "Loss for batch is  0.4189871549606323\n",
      "Loss for batch is  -0.5051023960113525\n",
      "|Iter  664  | Total Train Loss  0.5715463161468506 |\n",
      "Val Loss for batch is  -1.4377987384796143\n",
      "Val Loss for batch is  -1.6146906614303589\n",
      "Val Loss for batch is  -1.4168349504470825\n",
      "Val Loss for batch is  -2.2914061546325684\n",
      "|Iter  664  | Total Val Loss  -6.760730504989624 |\n",
      "Loss for batch is  0.38435661792755127\n",
      "Loss for batch is  0.2534102201461792\n",
      "Loss for batch is  0.4657043218612671\n",
      "Loss for batch is  -0.5814621448516846\n",
      "|Iter  665  | Total Train Loss  0.522009015083313 |\n",
      "Val Loss for batch is  -1.4882451295852661\n",
      "Val Loss for batch is  -1.6160999536514282\n",
      "Val Loss for batch is  -1.4446063041687012\n",
      "Val Loss for batch is  -2.301945924758911\n",
      "|Iter  665  | Total Val Loss  -6.850897312164307 |\n",
      "Loss for batch is  0.37745392322540283\n",
      "Loss for batch is  0.20560717582702637\n",
      "Loss for batch is  0.35025322437286377\n",
      "Loss for batch is  -0.5677856206893921\n",
      "|Iter  666  | Total Train Loss  0.3655287027359009 |\n",
      "Val Loss for batch is  -1.5441150665283203\n",
      "Val Loss for batch is  -1.6600042581558228\n",
      "Val Loss for batch is  -1.5561554431915283\n",
      "Val Loss for batch is  -2.3614304065704346\n",
      "|Iter  666  | Total Val Loss  -7.121705174446106 |\n",
      "Loss for batch is  0.3434385061264038\n",
      "Loss for batch is  0.17683148384094238\n",
      "Loss for batch is  0.32495272159576416\n",
      "Loss for batch is  -0.6120413541793823\n",
      "|Iter  667  | Total Train Loss  0.23318135738372803 |\n",
      "Val Loss for batch is  -1.49354088306427\n",
      "Val Loss for batch is  -1.644930362701416\n",
      "Val Loss for batch is  -1.5016449689865112\n",
      "Val Loss for batch is  -2.3496780395507812\n",
      "|Iter  667  | Total Val Loss  -6.9897942543029785 |\n",
      "Loss for batch is  0.2888000011444092\n",
      "Loss for batch is  0.14583957195281982\n",
      "Loss for batch is  0.30084228515625\n",
      "Loss for batch is  -0.659347414970398\n",
      "|Iter  668  | Total Train Loss  0.07613444328308105 |\n",
      "Val Loss for batch is  -1.4454401731491089\n",
      "Val Loss for batch is  -1.6152262687683105\n",
      "Val Loss for batch is  -1.4678637981414795\n",
      "Val Loss for batch is  -2.342339515686035\n",
      "|Iter  668  | Total Val Loss  -6.870869755744934 |\n",
      "Loss for batch is  0.31947004795074463\n",
      "Loss for batch is  0.10360968112945557\n",
      "Loss for batch is  0.3123178482055664\n",
      "Loss for batch is  -0.6627894639968872\n",
      "|Iter  669  | Total Train Loss  0.0726081132888794 |\n",
      "Val Loss for batch is  -1.4073318243026733\n",
      "Val Loss for batch is  -1.603155255317688\n",
      "Val Loss for batch is  -1.491513967514038\n",
      "Val Loss for batch is  -2.391754627227783\n",
      "|Iter  669  | Total Val Loss  -6.893755674362183 |\n",
      "Loss for batch is  0.295245885848999\n",
      "Loss for batch is  0.09579575061798096\n",
      "Loss for batch is  0.2751041650772095\n",
      "Loss for batch is  -0.6822082996368408\n",
      "|Iter  670  | Total Train Loss  -0.016062498092651367 |\n",
      "Val Loss for batch is  -1.5030323266983032\n",
      "Val Loss for batch is  -1.6218688488006592\n",
      "Val Loss for batch is  -1.4412901401519775\n",
      "Val Loss for batch is  -2.3895161151885986\n",
      "|Iter  670  | Total Val Loss  -6.955707430839539 |\n",
      "Loss for batch is  0.28298890590667725\n",
      "Loss for batch is  0.10414314270019531\n",
      "Loss for batch is  0.2542543411254883\n",
      "Loss for batch is  -0.6848450899124146\n",
      "|Iter  671  | Total Train Loss  -0.04345870018005371 |\n",
      "Val Loss for batch is  -1.3895423412322998\n",
      "Val Loss for batch is  -1.6347322463989258\n",
      "Val Loss for batch is  -1.5359243154525757\n",
      "Val Loss for batch is  -2.4436984062194824\n",
      "|Iter  671  | Total Val Loss  -7.003897309303284 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.2685619592666626\n",
      "Loss for batch is  0.1246953010559082\n",
      "Loss for batch is  0.25216925144195557\n",
      "Loss for batch is  -0.7002371549606323\n",
      "|Iter  672  | Total Train Loss  -0.05481064319610596 |\n",
      "Val Loss for batch is  -1.43361234664917\n",
      "Val Loss for batch is  -1.6065257787704468\n",
      "Val Loss for batch is  -1.4373095035552979\n",
      "Val Loss for batch is  -2.4324443340301514\n",
      "|Iter  672  | Total Val Loss  -6.909891963005066 |\n",
      "Loss for batch is  0.3010646104812622\n",
      "Loss for batch is  0.11896049976348877\n",
      "Loss for batch is  0.36727988719940186\n",
      "Loss for batch is  -0.7148810625076294\n",
      "|Iter  673  | Total Train Loss  0.07242393493652344 |\n",
      "Val Loss for batch is  -1.351838231086731\n",
      "Val Loss for batch is  -1.5187410116195679\n",
      "Val Loss for batch is  -1.3588594198226929\n",
      "Val Loss for batch is  -2.3106656074523926\n",
      "|Iter  673  | Total Val Loss  -6.540104269981384 |\n",
      "Loss for batch is  0.4106091260910034\n",
      "Loss for batch is  0.10936760902404785\n",
      "Loss for batch is  0.39319944381713867\n",
      "Loss for batch is  -0.7061364650726318\n",
      "|Iter  674  | Total Train Loss  0.2070397138595581 |\n",
      "Val Loss for batch is  -1.297498106956482\n",
      "Val Loss for batch is  -1.543252944946289\n",
      "Val Loss for batch is  -1.3339115381240845\n",
      "Val Loss for batch is  -2.3331685066223145\n",
      "|Iter  674  | Total Val Loss  -6.50783109664917 |\n",
      "Loss for batch is  0.4380561113357544\n",
      "Loss for batch is  0.09935379028320312\n",
      "Loss for batch is  0.40193939208984375\n",
      "Loss for batch is  -0.6486279964447021\n",
      "|Iter  675  | Total Train Loss  0.2907212972640991 |\n",
      "Val Loss for batch is  -1.1436865329742432\n",
      "Val Loss for batch is  -1.427600383758545\n",
      "Val Loss for batch is  -1.2661453485488892\n",
      "Val Loss for batch is  -2.3260338306427\n",
      "|Iter  675  | Total Val Loss  -6.163466095924377 |\n",
      "Loss for batch is  0.45036065578460693\n",
      "Loss for batch is  0.2014927864074707\n",
      "Loss for batch is  0.2787405252456665\n",
      "Loss for batch is  -0.5914419889450073\n",
      "|Iter  676  | Total Train Loss  0.3391519784927368 |\n",
      "Val Loss for batch is  -1.2764467000961304\n",
      "Val Loss for batch is  -1.5137537717819214\n",
      "Val Loss for batch is  -1.3693164587020874\n",
      "Val Loss for batch is  -2.2802114486694336\n",
      "|Iter  676  | Total Val Loss  -6.439728379249573 |\n",
      "Loss for batch is  0.5635016560554504\n",
      "Loss for batch is  0.1740952730178833\n",
      "Loss for batch is  0.5244767665863037\n",
      "Loss for batch is  -0.6753566265106201\n",
      "|Iter  677  | Total Train Loss  0.5867170691490173 |\n",
      "Val Loss for batch is  -1.183589220046997\n",
      "Val Loss for batch is  -1.4303336143493652\n",
      "Val Loss for batch is  -1.2912042140960693\n",
      "Val Loss for batch is  -2.2203726768493652\n",
      "|Iter  677  | Total Val Loss  -6.125499725341797 |\n",
      "Loss for batch is  0.537932276725769\n",
      "Loss for batch is  0.2101191282272339\n",
      "Loss for batch is  0.35197150707244873\n",
      "Loss for batch is  -0.559194803237915\n",
      "|Iter  678  | Total Train Loss  0.5408281087875366 |\n",
      "Val Loss for batch is  -1.4690946340560913\n",
      "Val Loss for batch is  -1.6347317695617676\n",
      "Val Loss for batch is  -1.500935435295105\n",
      "Val Loss for batch is  -2.331807851791382\n",
      "|Iter  678  | Total Val Loss  -6.936569690704346 |\n",
      "Loss for batch is  0.2844526767730713\n",
      "Loss for batch is  0.20945966243743896\n",
      "Loss for batch is  0.33117592334747314\n",
      "Loss for batch is  -0.6634867191314697\n",
      "|Iter  679  | Total Train Loss  0.16160154342651367 |\n",
      "Val Loss for batch is  -1.448075771331787\n",
      "Val Loss for batch is  -1.6216541528701782\n",
      "Val Loss for batch is  -1.4614204168319702\n",
      "Val Loss for batch is  -2.291083574295044\n",
      "|Iter  679  | Total Val Loss  -6.8222339153289795 |\n",
      "Loss for batch is  0.34143972396850586\n",
      "Loss for batch is  0.1311403512954712\n",
      "Loss for batch is  0.2825413942337036\n",
      "Loss for batch is  -0.6513298749923706\n",
      "|Iter  680  | Total Train Loss  0.10379159450531006 |\n",
      "Val Loss for batch is  -1.4987280368804932\n",
      "Val Loss for batch is  -1.6594585180282593\n",
      "Val Loss for batch is  -1.4733545780181885\n",
      "Val Loss for batch is  -2.4048852920532227\n",
      "|Iter  680  | Total Val Loss  -7.036426424980164 |\n",
      "Loss for batch is  0.27838563919067383\n",
      "Loss for batch is  0.14497339725494385\n",
      "Loss for batch is  0.25989508628845215\n",
      "Loss for batch is  -0.6665302515029907\n",
      "|Iter  681  | Total Train Loss  0.0167238712310791 |\n",
      "Val Loss for batch is  -1.548160195350647\n",
      "Val Loss for batch is  -1.6811237335205078\n",
      "Val Loss for batch is  -1.538264274597168\n",
      "Val Loss for batch is  -2.3742072582244873\n",
      "|Iter  681  | Total Val Loss  -7.14175546169281 |\n",
      "Loss for batch is  0.24095821380615234\n",
      "Loss for batch is  0.05907714366912842\n",
      "Loss for batch is  0.2850813865661621\n",
      "Loss for batch is  -0.7123911380767822\n",
      "|Iter  682  | Total Train Loss  -0.12727439403533936 |\n",
      "Val Loss for batch is  -1.5207618474960327\n",
      "Val Loss for batch is  -1.613195776939392\n",
      "Val Loss for batch is  -1.4729000329971313\n",
      "Val Loss for batch is  -2.3604373931884766\n",
      "|Iter  682  | Total Val Loss  -6.967295050621033 |\n",
      "Loss for batch is  0.2483978271484375\n",
      "Loss for batch is  0.07523560523986816\n",
      "Loss for batch is  0.19077110290527344\n",
      "Loss for batch is  -0.6818467378616333\n",
      "|Iter  683  | Total Train Loss  -0.1674422025680542 |\n",
      "Val Loss for batch is  -1.5351899862289429\n",
      "Val Loss for batch is  -1.7026660442352295\n",
      "Val Loss for batch is  -1.5662786960601807\n",
      "Val Loss for batch is  -2.3698620796203613\n",
      "|Iter  683  | Total Val Loss  -7.173996806144714 |\n",
      "Loss for batch is  0.19526100158691406\n",
      "Loss for batch is  0.0771183967590332\n",
      "Loss for batch is  0.19038867950439453\n",
      "Loss for batch is  -0.7531917095184326\n",
      "|Iter  684  | Total Train Loss  -0.2904236316680908 |\n",
      "Val Loss for batch is  -1.4494082927703857\n",
      "Val Loss for batch is  -1.6964178085327148\n",
      "Val Loss for batch is  -1.4711419343948364\n",
      "Val Loss for batch is  -2.394883871078491\n",
      "|Iter  684  | Total Val Loss  -7.011851906776428 |\n",
      "Loss for batch is  0.29116642475128174\n",
      "Loss for batch is  0.0200345516204834\n",
      "Loss for batch is  0.25177478790283203\n",
      "Loss for batch is  -0.7485342025756836\n",
      "|Iter  685  | Total Train Loss  -0.18555843830108643 |\n",
      "Val Loss for batch is  -1.43272066116333\n",
      "Val Loss for batch is  -1.5356863737106323\n",
      "Val Loss for batch is  -1.4583698511123657\n",
      "Val Loss for batch is  -2.3376967906951904\n",
      "|Iter  685  | Total Val Loss  -6.7644736766815186 |\n",
      "Loss for batch is  0.25580644607543945\n",
      "Loss for batch is  0.0688028335571289\n",
      "Loss for batch is  0.2126145362854004\n",
      "Loss for batch is  -0.6415164470672607\n",
      "|Iter  686  | Total Train Loss  -0.10429263114929199 |\n",
      "Val Loss for batch is  -1.4042396545410156\n",
      "Val Loss for batch is  -1.6146386861801147\n",
      "Val Loss for batch is  -1.4753997325897217\n",
      "Val Loss for batch is  -2.4550232887268066\n",
      "|Iter  686  | Total Val Loss  -6.949301362037659 |\n",
      "Loss for batch is  0.3122307062149048\n",
      "Loss for batch is  0.126595139503479\n",
      "Loss for batch is  0.38714468479156494\n",
      "Loss for batch is  -0.7342181205749512\n",
      "|Iter  687  | Total Train Loss  0.09175240993499756 |\n",
      "Val Loss for batch is  -1.3278337717056274\n",
      "Val Loss for batch is  -1.3965344429016113\n",
      "Val Loss for batch is  -1.392927885055542\n",
      "Val Loss for batch is  -2.3188579082489014\n",
      "|Iter  687  | Total Val Loss  -6.436154007911682 |\n",
      "Loss for batch is  0.41282618045806885\n",
      "Loss for batch is  0.21897530555725098\n",
      "Loss for batch is  0.3641928434371948\n",
      "Loss for batch is  -0.559916615486145\n",
      "|Iter  688  | Total Train Loss  0.43607771396636963 |\n",
      "Val Loss for batch is  -1.3513069152832031\n",
      "Val Loss for batch is  -1.6144516468048096\n",
      "Val Loss for batch is  -1.3423115015029907\n",
      "Val Loss for batch is  -2.3966121673583984\n",
      "|Iter  688  | Total Val Loss  -6.704682230949402 |\n",
      "Loss for batch is  0.308646559715271\n",
      "Loss for batch is  0.22235500812530518\n",
      "Loss for batch is  0.6342782378196716\n",
      "Loss for batch is  -0.7073781490325928\n",
      "|Iter  689  | Total Train Loss  0.45790165662765503 |\n",
      "Val Loss for batch is  -1.2825400829315186\n",
      "Val Loss for batch is  -1.4456504583358765\n",
      "Val Loss for batch is  -1.199927806854248\n",
      "Val Loss for batch is  -2.206341028213501\n",
      "|Iter  689  | Total Val Loss  -6.134459376335144 |\n",
      "Loss for batch is  0.47766852378845215\n",
      "Loss for batch is  0.2640371322631836\n",
      "Loss for batch is  0.30730748176574707\n",
      "Loss for batch is  -0.6237348318099976\n",
      "|Iter  690  | Total Train Loss  0.42527830600738525 |\n",
      "Val Loss for batch is  -1.4535942077636719\n",
      "Val Loss for batch is  -1.6011855602264404\n",
      "Val Loss for batch is  -1.4707521200180054\n",
      "Val Loss for batch is  -2.3531687259674072\n",
      "|Iter  690  | Total Val Loss  -6.878700613975525 |\n",
      "Loss for batch is  0.3158477544784546\n",
      "Loss for batch is  0.17576491832733154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.42466020584106445\n",
      "Loss for batch is  -0.6375789642333984\n",
      "|Iter  691  | Total Train Loss  0.27869391441345215 |\n",
      "Val Loss for batch is  -1.4867699146270752\n",
      "Val Loss for batch is  -1.610758900642395\n",
      "Val Loss for batch is  -1.4419407844543457\n",
      "Val Loss for batch is  -2.332958459854126\n",
      "|Iter  691  | Total Val Loss  -6.872428059577942 |\n",
      "Loss for batch is  0.30536437034606934\n",
      "Loss for batch is  0.23776745796203613\n",
      "Loss for batch is  0.29694223403930664\n",
      "Loss for batch is  -0.6872907876968384\n",
      "|Iter  692  | Total Train Loss  0.15278327465057373 |\n",
      "Val Loss for batch is  -1.5008611679077148\n",
      "Val Loss for batch is  -1.686007022857666\n",
      "Val Loss for batch is  -1.5237916707992554\n",
      "Val Loss for batch is  -2.3682029247283936\n",
      "|Iter  692  | Total Val Loss  -7.07886278629303 |\n",
      "Loss for batch is  0.24996840953826904\n",
      "Loss for batch is  0.10811316967010498\n",
      "Loss for batch is  0.2626831531524658\n",
      "Loss for batch is  -0.6807396411895752\n",
      "|Iter  693  | Total Train Loss  -0.05997490882873535 |\n",
      "Val Loss for batch is  -1.5562646389007568\n",
      "Val Loss for batch is  -1.7490136623382568\n",
      "Val Loss for batch is  -1.5415923595428467\n",
      "Val Loss for batch is  -2.390418767929077\n",
      "|Iter  693  | Total Val Loss  -7.2372894287109375 |\n",
      "Loss for batch is  0.2489231824874878\n",
      "Loss for batch is  0.05553388595581055\n",
      "Loss for batch is  0.27066290378570557\n",
      "Loss for batch is  -0.7285926342010498\n",
      "|Iter  694  | Total Train Loss  -0.1534726619720459 |\n",
      "Val Loss for batch is  -1.4858218431472778\n",
      "Val Loss for batch is  -1.6796071529388428\n",
      "Val Loss for batch is  -1.5057435035705566\n",
      "Val Loss for batch is  -2.361018657684326\n",
      "|Iter  694  | Total Val Loss  -7.032191157341003 |\n",
      "Loss for batch is  0.25537192821502686\n",
      "Loss for batch is  0.04593181610107422\n",
      "Loss for batch is  0.20467114448547363\n",
      "Loss for batch is  -0.7473773956298828\n",
      "|Iter  695  | Total Train Loss  -0.2414025068283081 |\n",
      "Val Loss for batch is  -1.5256361961364746\n",
      "Val Loss for batch is  -1.7387089729309082\n",
      "Val Loss for batch is  -1.5558823347091675\n",
      "Val Loss for batch is  -2.4458987712860107\n",
      "|Iter  695  | Total Val Loss  -7.266126275062561 |\n",
      "Loss for batch is  0.2263096570968628\n",
      "Loss for batch is  -0.0010020732879638672\n",
      "Loss for batch is  0.16513502597808838\n",
      "Loss for batch is  -0.7687734365463257\n",
      "|Iter  696  | Total Train Loss  -0.3783308267593384 |\n",
      "Val Loss for batch is  -1.491957187652588\n",
      "Val Loss for batch is  -1.5978549718856812\n",
      "Val Loss for batch is  -1.503867506980896\n",
      "Val Loss for batch is  -2.456390142440796\n",
      "|Iter  696  | Total Val Loss  -7.050069808959961 |\n",
      "Loss for batch is  0.23351049423217773\n",
      "Loss for batch is  0.0023392438888549805\n",
      "Loss for batch is  0.21769201755523682\n",
      "Loss for batch is  -0.7890332937240601\n",
      "|Iter  697  | Total Train Loss  -0.3354915380477905 |\n",
      "Val Loss for batch is  -1.4856247901916504\n",
      "Val Loss for batch is  -1.7097258567810059\n",
      "Val Loss for batch is  -1.4934158325195312\n",
      "Val Loss for batch is  -2.406982660293579\n",
      "|Iter  697  | Total Val Loss  -7.095749139785767 |\n",
      "Loss for batch is  0.1936650276184082\n",
      "Loss for batch is  0.011569619178771973\n",
      "Loss for batch is  0.17737901210784912\n",
      "Loss for batch is  -0.810127854347229\n",
      "|Iter  698  | Total Train Loss  -0.4275141954421997 |\n",
      "Val Loss for batch is  -1.4978587627410889\n",
      "Val Loss for batch is  -1.708569049835205\n",
      "Val Loss for batch is  -1.556068778038025\n",
      "Val Loss for batch is  -2.504513740539551\n",
      "|Iter  698  | Total Val Loss  -7.26701033115387 |\n",
      "Loss for batch is  0.20843124389648438\n",
      "Loss for batch is  -0.016006827354431152\n",
      "Loss for batch is  0.14974737167358398\n",
      "Loss for batch is  -0.8152059316635132\n",
      "|Iter  699  | Total Train Loss  -0.473034143447876 |\n",
      "Val Loss for batch is  -1.4739807844161987\n",
      "Val Loss for batch is  -1.6756865978240967\n",
      "Val Loss for batch is  -1.5491578578948975\n",
      "Val Loss for batch is  -2.455491781234741\n",
      "|Iter  699  | Total Val Loss  -7.154317021369934 |\n",
      "Loss for batch is  0.18952536582946777\n",
      "Loss for batch is  0.02627873420715332\n",
      "Loss for batch is  0.13910448551177979\n",
      "Loss for batch is  -0.772432804107666\n",
      "|Iter  700  | Total Train Loss  -0.41752421855926514 |\n",
      "Val Loss for batch is  -1.3916082382202148\n",
      "Val Loss for batch is  -1.5157853364944458\n",
      "Val Loss for batch is  -1.4222745895385742\n",
      "Val Loss for batch is  -2.406142473220825\n",
      "|Iter  700  | Total Val Loss  -6.73581063747406 |\n",
      "Loss for batch is  0.3387157917022705\n",
      "Loss for batch is  0.08871972560882568\n",
      "Loss for batch is  0.4876762628555298\n",
      "Loss for batch is  -0.6923028230667114\n",
      "|Iter  701  | Total Train Loss  0.22280895709991455 |\n",
      "Val Loss for batch is  -1.0445942878723145\n",
      "Val Loss for batch is  -0.9486275315284729\n",
      "Val Loss for batch is  -0.7318902611732483\n",
      "Val Loss for batch is  -2.109653949737549\n",
      "|Iter  701  | Total Val Loss  -4.8347660303115845 |\n",
      "Loss for batch is  0.6707521080970764\n",
      "Loss for batch is  0.004913806915283203\n",
      "Loss for batch is  0.9401983022689819\n",
      "Loss for batch is  -0.5340068340301514\n",
      "|Iter  702  | Total Train Loss  1.0818573832511902 |\n",
      "Val Loss for batch is  -0.965671956539154\n",
      "Val Loss for batch is  -1.32222580909729\n",
      "Val Loss for batch is  -1.099985122680664\n",
      "Val Loss for batch is  -2.198058605194092\n",
      "|Iter  702  | Total Val Loss  -5.5859414935112 |\n",
      "Loss for batch is  0.5777169466018677\n",
      "Loss for batch is  0.6593300104141235\n",
      "Loss for batch is  0.5071609020233154\n",
      "Loss for batch is  -0.5131603479385376\n",
      "|Iter  703  | Total Train Loss  1.231047511100769 |\n",
      "Val Loss for batch is  -1.1438753604888916\n",
      "Val Loss for batch is  -1.4489905834197998\n",
      "Val Loss for batch is  -1.2700649499893188\n",
      "Val Loss for batch is  -2.126890182495117\n",
      "|Iter  703  | Total Val Loss  -5.989821076393127 |\n",
      "Loss for batch is  0.6182320713996887\n",
      "Loss for batch is  0.22026586532592773\n",
      "Loss for batch is  0.47521746158599854\n",
      "Loss for batch is  -0.4909651279449463\n",
      "|Iter  704  | Total Train Loss  0.8227502703666687 |\n",
      "Val Loss for batch is  -1.30854070186615\n",
      "Val Loss for batch is  -1.422509789466858\n",
      "Val Loss for batch is  -1.1815166473388672\n",
      "Val Loss for batch is  -2.2351174354553223\n",
      "|Iter  704  | Total Val Loss  -6.147684574127197 |\n",
      "Loss for batch is  0.47373509407043457\n",
      "Loss for batch is  0.19504010677337646\n",
      "Loss for batch is  0.5338749289512634\n",
      "Loss for batch is  -0.5151494741439819\n",
      "|Iter  705  | Total Train Loss  0.6875006556510925 |\n",
      "Val Loss for batch is  -1.483705759048462\n",
      "Val Loss for batch is  -1.5876802206039429\n",
      "Val Loss for batch is  -1.3285274505615234\n",
      "Val Loss for batch is  -2.235304117202759\n",
      "|Iter  705  | Total Val Loss  -6.635217547416687 |\n",
      "Loss for batch is  0.37140214443206787\n",
      "Loss for batch is  0.21477079391479492\n",
      "Loss for batch is  0.3488718271255493\n",
      "Loss for batch is  -0.5807766914367676\n",
      "|Iter  706  | Total Train Loss  0.35426807403564453 |\n",
      "Val Loss for batch is  -1.4471824169158936\n",
      "Val Loss for batch is  -1.5670663118362427\n",
      "Val Loss for batch is  -1.3840903043746948\n",
      "Val Loss for batch is  -2.2769436836242676\n",
      "|Iter  706  | Total Val Loss  -6.675282716751099 |\n",
      "Loss for batch is  0.31999635696411133\n",
      "Loss for batch is  0.1599661111831665\n",
      "Loss for batch is  0.3027857542037964\n",
      "Loss for batch is  -0.5986868143081665\n",
      "|Iter  707  | Total Train Loss  0.18406140804290771 |\n",
      "Val Loss for batch is  -1.5576857328414917\n",
      "Val Loss for batch is  -1.6108078956604004\n",
      "Val Loss for batch is  -1.4803259372711182\n",
      "Val Loss for batch is  -2.30881667137146\n",
      "|Iter  707  | Total Val Loss  -6.95763623714447 |\n",
      "Loss for batch is  0.2789217233657837\n",
      "Loss for batch is  0.11813688278198242\n",
      "Loss for batch is  0.24316024780273438\n",
      "Loss for batch is  -0.6528346538543701\n",
      "|Iter  708  | Total Train Loss  -0.012615799903869629 |\n",
      "Val Loss for batch is  -1.4930267333984375\n",
      "Val Loss for batch is  -1.6386198997497559\n",
      "Val Loss for batch is  -1.5332602262496948\n",
      "Val Loss for batch is  -2.368440628051758\n",
      "|Iter  708  | Total Val Loss  -7.033347487449646 |\n",
      "Loss for batch is  0.20523178577423096\n",
      "Loss for batch is  0.07728719711303711\n",
      "Loss for batch is  0.19393599033355713\n",
      "Loss for batch is  -0.697189211845398\n",
      "|Iter  709  | Total Train Loss  -0.22073423862457275 |\n",
      "Val Loss for batch is  -1.5592780113220215\n",
      "Val Loss for batch is  -1.640894889831543\n",
      "Val Loss for batch is  -1.569385290145874\n",
      "Val Loss for batch is  -2.3995325565338135\n",
      "|Iter  709  | Total Val Loss  -7.169090747833252 |\n",
      "Loss for batch is  0.17806708812713623\n",
      "Loss for batch is  0.08769786357879639\n",
      "Loss for batch is  0.20237982273101807\n",
      "Loss for batch is  -0.7533992528915405\n",
      "|Iter  710  | Total Train Loss  -0.28525447845458984 |\n",
      "Val Loss for batch is  -1.5569840669631958\n",
      "Val Loss for batch is  -1.6327422857284546\n",
      "Val Loss for batch is  -1.5723936557769775\n",
      "Val Loss for batch is  -2.428126573562622\n",
      "|Iter  710  | Total Val Loss  -7.19024658203125 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.16722750663757324\n",
      "Loss for batch is  0.027792692184448242\n",
      "Loss for batch is  0.13795256614685059\n",
      "Loss for batch is  -0.768964409828186\n",
      "|Iter  711  | Total Train Loss  -0.43599164485931396 |\n",
      "Val Loss for batch is  -1.5412490367889404\n",
      "Val Loss for batch is  -1.6924442052841187\n",
      "Val Loss for batch is  -1.523891568183899\n",
      "Val Loss for batch is  -2.5173180103302\n",
      "|Iter  711  | Total Val Loss  -7.274902820587158 |\n",
      "Loss for batch is  0.1500554084777832\n",
      "Loss for batch is  0.026316285133361816\n",
      "Loss for batch is  0.11507952213287354\n",
      "Loss for batch is  -0.804851770401001\n",
      "|Iter  712  | Total Train Loss  -0.5134005546569824 |\n",
      "Val Loss for batch is  -1.4746081829071045\n",
      "Val Loss for batch is  -1.5180261135101318\n",
      "Val Loss for batch is  -1.5653622150421143\n",
      "Val Loss for batch is  -2.418863534927368\n",
      "|Iter  712  | Total Val Loss  -6.976860046386719 |\n",
      "Loss for batch is  0.21425879001617432\n",
      "Loss for batch is  0.029303312301635742\n",
      "Loss for batch is  0.18984389305114746\n",
      "Loss for batch is  -0.8355731964111328\n",
      "|Iter  713  | Total Train Loss  -0.4021672010421753 |\n",
      "Val Loss for batch is  -1.4897760152816772\n",
      "Val Loss for batch is  -1.6914710998535156\n",
      "Val Loss for batch is  -1.5343217849731445\n",
      "Val Loss for batch is  -2.4792368412017822\n",
      "|Iter  713  | Total Val Loss  -7.19480574131012 |\n",
      "Loss for batch is  0.1903085708618164\n",
      "Loss for batch is  -0.034287095069885254\n",
      "Loss for batch is  0.1573859453201294\n",
      "Loss for batch is  -0.8424477577209473\n",
      "|Iter  714  | Total Train Loss  -0.5290403366088867 |\n",
      "Val Loss for batch is  -1.4856140613555908\n",
      "Val Loss for batch is  -1.6803523302078247\n",
      "Val Loss for batch is  -1.5128371715545654\n",
      "Val Loss for batch is  -2.4772379398345947\n",
      "|Iter  714  | Total Val Loss  -7.156041502952576 |\n",
      "Loss for batch is  0.1674354076385498\n",
      "Loss for batch is  0.004254817962646484\n",
      "Loss for batch is  0.12456178665161133\n",
      "Loss for batch is  -0.774773120880127\n",
      "|Iter  715  | Total Train Loss  -0.47852110862731934 |\n",
      "Val Loss for batch is  -1.4773013591766357\n",
      "Val Loss for batch is  -1.722760558128357\n",
      "Val Loss for batch is  -1.6135421991348267\n",
      "Val Loss for batch is  -2.506356954574585\n",
      "|Iter  715  | Total Val Loss  -7.319961071014404 |\n",
      "Loss for batch is  0.12639546394348145\n",
      "Loss for batch is  0.05157959461212158\n",
      "Loss for batch is  0.14480304718017578\n",
      "Loss for batch is  -0.7113703489303589\n",
      "|Iter  716  | Total Train Loss  -0.3885922431945801 |\n",
      "Val Loss for batch is  -1.1895300149917603\n",
      "Val Loss for batch is  -1.3813996315002441\n",
      "Val Loss for batch is  -1.2972160577774048\n",
      "Val Loss for batch is  -2.3603477478027344\n",
      "|Iter  716  | Total Val Loss  -6.2284934520721436 |\n",
      "Loss for batch is  0.5585692524909973\n",
      "Loss for batch is  0.07741057872772217\n",
      "Loss for batch is  0.780668318271637\n",
      "Loss for batch is  -0.8092541694641113\n",
      "|Iter  717  | Total Train Loss  0.6073939800262451 |\n",
      "Val Loss for batch is  -1.0641535520553589\n",
      "Val Loss for batch is  -1.0984127521514893\n",
      "Val Loss for batch is  -1.013354778289795\n",
      "Val Loss for batch is  -2.0707826614379883\n",
      "|Iter  717  | Total Val Loss  -5.246703743934631 |\n",
      "Loss for batch is  0.6563032865524292\n",
      "Loss for batch is  0.40179121494293213\n",
      "Loss for batch is  0.3046516180038452\n",
      "Loss for batch is  -0.3176323175430298\n",
      "|Iter  718  | Total Train Loss  1.0451138019561768 |\n",
      "Val Loss for batch is  -1.1218795776367188\n",
      "Val Loss for batch is  -1.4533705711364746\n",
      "Val Loss for batch is  -1.2510930299758911\n",
      "Val Loss for batch is  -2.2127504348754883\n",
      "|Iter  718  | Total Val Loss  -6.039093613624573 |\n",
      "Loss for batch is  0.5315176248550415\n",
      "Loss for batch is  0.1773921251296997\n",
      "Loss for batch is  0.3949928283691406\n",
      "Loss for batch is  -0.510316014289856\n",
      "|Iter  719  | Total Train Loss  0.5935865640640259 |\n",
      "Val Loss for batch is  -1.1707180738449097\n",
      "Val Loss for batch is  -1.4555883407592773\n",
      "Val Loss for batch is  -1.372875690460205\n",
      "Val Loss for batch is  -2.185929298400879\n",
      "|Iter  719  | Total Val Loss  -6.185111403465271 |\n",
      "Loss for batch is  0.4995086193084717\n",
      "Loss for batch is  0.18846452236175537\n",
      "Loss for batch is  0.4657813310623169\n",
      "Loss for batch is  -0.5989304780960083\n",
      "|Iter  720  | Total Train Loss  0.5548239946365356 |\n",
      "Val Loss for batch is  -1.41599440574646\n",
      "Val Loss for batch is  -1.6038525104522705\n",
      "Val Loss for batch is  -1.4051238298416138\n",
      "Val Loss for batch is  -2.2789511680603027\n",
      "|Iter  720  | Total Val Loss  -6.703921914100647 |\n",
      "Loss for batch is  0.3601806163787842\n",
      "Loss for batch is  0.2227306365966797\n",
      "Loss for batch is  0.3522298336029053\n",
      "Loss for batch is  -0.6265304088592529\n",
      "|Iter  721  | Total Train Loss  0.3086106777191162 |\n",
      "Val Loss for batch is  -1.4795175790786743\n",
      "Val Loss for batch is  -1.6491029262542725\n",
      "Val Loss for batch is  -1.484535574913025\n",
      "Val Loss for batch is  -2.3521809577941895\n",
      "|Iter  721  | Total Val Loss  -6.965337038040161 |\n",
      "Loss for batch is  0.25924110412597656\n",
      "Loss for batch is  0.15539228916168213\n",
      "Loss for batch is  0.2661179304122925\n",
      "Loss for batch is  -0.6654466390609741\n",
      "|Iter  722  | Total Train Loss  0.01530468463897705 |\n",
      "Val Loss for batch is  -1.5388760566711426\n",
      "Val Loss for batch is  -1.6534581184387207\n",
      "Val Loss for batch is  -1.5153592824935913\n",
      "Val Loss for batch is  -2.3701701164245605\n",
      "|Iter  722  | Total Val Loss  -7.077863574028015 |\n",
      "Loss for batch is  0.23378479480743408\n",
      "Loss for batch is  0.11221301555633545\n",
      "Loss for batch is  0.2074589729309082\n",
      "Loss for batch is  -0.6787992715835571\n",
      "|Iter  723  | Total Train Loss  -0.1253424882888794 |\n",
      "Val Loss for batch is  -1.5555239915847778\n",
      "Val Loss for batch is  -1.6226139068603516\n",
      "Val Loss for batch is  -1.5223876237869263\n",
      "Val Loss for batch is  -2.401458263397217\n",
      "|Iter  723  | Total Val Loss  -7.1019837856292725 |\n",
      "Loss for batch is  0.17985796928405762\n",
      "Loss for batch is  0.030738353729248047\n",
      "Loss for batch is  0.18273842334747314\n",
      "Loss for batch is  -0.7499808073043823\n",
      "|Iter  724  | Total Train Loss  -0.3566460609436035 |\n",
      "Val Loss for batch is  -1.5737942457199097\n",
      "Val Loss for batch is  -1.6799633502960205\n",
      "Val Loss for batch is  -1.4975111484527588\n",
      "Val Loss for batch is  -2.3972439765930176\n",
      "|Iter  724  | Total Val Loss  -7.1485127210617065 |\n",
      "Loss for batch is  0.16014409065246582\n",
      "Loss for batch is  0.0018978118896484375\n",
      "Loss for batch is  0.12222778797149658\n",
      "Loss for batch is  -0.7836949825286865\n",
      "|Iter  725  | Total Train Loss  -0.4994252920150757 |\n",
      "Val Loss for batch is  -1.6077299118041992\n",
      "Val Loss for batch is  -1.7203826904296875\n",
      "Val Loss for batch is  -1.5650804042816162\n",
      "Val Loss for batch is  -2.4629995822906494\n",
      "|Iter  725  | Total Val Loss  -7.356192588806152 |\n",
      "Loss for batch is  0.10141479969024658\n",
      "Loss for batch is  -0.04726982116699219\n",
      "Loss for batch is  0.09260129928588867\n",
      "Loss for batch is  -0.8254334926605225\n",
      "|Iter  726  | Total Train Loss  -0.6786872148513794 |\n",
      "Val Loss for batch is  -1.5839223861694336\n",
      "Val Loss for batch is  -1.7457709312438965\n",
      "Val Loss for batch is  -1.6136327981948853\n",
      "Val Loss for batch is  -2.511868476867676\n",
      "|Iter  726  | Total Val Loss  -7.455194592475891 |\n",
      "Loss for batch is  0.09929120540618896\n",
      "Loss for batch is  -0.06635093688964844\n",
      "Loss for batch is  0.06883013248443604\n",
      "Loss for batch is  -0.8653641939163208\n",
      "|Iter  727  | Total Train Loss  -0.7635937929153442 |\n",
      "Val Loss for batch is  -1.5401499271392822\n",
      "Val Loss for batch is  -1.6968085765838623\n",
      "Val Loss for batch is  -1.5225698947906494\n",
      "Val Loss for batch is  -2.4904165267944336\n",
      "|Iter  727  | Total Val Loss  -7.2499449253082275 |\n",
      "Loss for batch is  0.08788406848907471\n",
      "Loss for batch is  -0.08587348461151123\n",
      "Loss for batch is  0.0825645923614502\n",
      "Loss for batch is  -0.8646636009216309\n",
      "|Iter  728  | Total Train Loss  -0.7800884246826172 |\n",
      "Val Loss for batch is  -1.5996264219284058\n",
      "Val Loss for batch is  -1.7750416994094849\n",
      "Val Loss for batch is  -1.6273733377456665\n",
      "Val Loss for batch is  -2.5211334228515625\n",
      "|Iter  728  | Total Val Loss  -7.52317488193512 |\n",
      "Loss for batch is  0.10322427749633789\n",
      "Loss for batch is  -0.062194228172302246\n",
      "Loss for batch is  0.062796950340271\n",
      "Loss for batch is  -0.8753232955932617\n",
      "|Iter  729  | Total Train Loss  -0.7714962959289551 |\n",
      "Val Loss for batch is  -1.5258013010025024\n",
      "Val Loss for batch is  -1.6899105310440063\n",
      "Val Loss for batch is  -1.6589173078536987\n",
      "Val Loss for batch is  -2.535733699798584\n",
      "|Iter  729  | Total Val Loss  -7.4103628396987915 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.09868192672729492\n",
      "Loss for batch is  -0.07056653499603271\n",
      "Loss for batch is  0.06094098091125488\n",
      "Loss for batch is  -0.8857961893081665\n",
      "|Iter  730  | Total Train Loss  -0.7967398166656494 |\n",
      "Val Loss for batch is  -1.5380910634994507\n",
      "Val Loss for batch is  -1.69962477684021\n",
      "Val Loss for batch is  -1.413730263710022\n",
      "Val Loss for batch is  -2.407464027404785\n",
      "|Iter  730  | Total Val Loss  -7.058910131454468 |\n",
      "Loss for batch is  0.17126429080963135\n",
      "Loss for batch is  -0.02652418613433838\n",
      "Loss for batch is  0.1620335578918457\n",
      "Loss for batch is  -0.878366231918335\n",
      "|Iter  731  | Total Train Loss  -0.5715925693511963 |\n",
      "Val Loss for batch is  -1.2420117855072021\n",
      "Val Loss for batch is  -1.2849905490875244\n",
      "Val Loss for batch is  -0.9905564785003662\n",
      "Val Loss for batch is  -2.3994839191436768\n",
      "|Iter  731  | Total Val Loss  -5.9170427322387695 |\n",
      "Loss for batch is  0.32632994651794434\n",
      "Loss for batch is  -0.06040644645690918\n",
      "Loss for batch is  0.34886014461517334\n",
      "Loss for batch is  -0.8365203142166138\n",
      "|Iter  732  | Total Train Loss  -0.22173666954040527 |\n",
      "Val Loss for batch is  -1.3208247423171997\n",
      "Val Loss for batch is  -1.5004664659500122\n",
      "Val Loss for batch is  -1.2600735425949097\n",
      "Val Loss for batch is  -2.329165458679199\n",
      "|Iter  732  | Total Val Loss  -6.410530209541321 |\n",
      "Loss for batch is  0.2689993381500244\n",
      "Loss for batch is  0.06580126285552979\n",
      "Loss for batch is  0.22266662120819092\n",
      "Loss for batch is  -0.6233131885528564\n",
      "|Iter  733  | Total Train Loss  -0.06584596633911133 |\n",
      "Val Loss for batch is  -1.402712345123291\n",
      "Val Loss for batch is  -1.6368787288665771\n",
      "Val Loss for batch is  -1.4822807312011719\n",
      "Val Loss for batch is  -2.423840045928955\n",
      "|Iter  733  | Total Val Loss  -6.945711851119995 |\n",
      "Loss for batch is  0.27129876613616943\n",
      "Loss for batch is  0.07536983489990234\n",
      "Loss for batch is  0.3995863199234009\n",
      "Loss for batch is  -0.7031717300415039\n",
      "|Iter  734  | Total Train Loss  0.04308319091796875 |\n",
      "Val Loss for batch is  -1.3798288106918335\n",
      "Val Loss for batch is  -1.5180236101150513\n",
      "Val Loss for batch is  -1.436364769935608\n",
      "Val Loss for batch is  -2.276095151901245\n",
      "|Iter  734  | Total Val Loss  -6.610312342643738 |\n",
      "Loss for batch is  0.2728557586669922\n",
      "Loss for batch is  0.13144564628601074\n",
      "Loss for batch is  0.1842283010482788\n",
      "Loss for batch is  -0.7238142490386963\n",
      "|Iter  735  | Total Train Loss  -0.13528454303741455 |\n",
      "Val Loss for batch is  -1.505799412727356\n",
      "Val Loss for batch is  -1.7342418432235718\n",
      "Val Loss for batch is  -1.5430569648742676\n",
      "Val Loss for batch is  -2.4242846965789795\n",
      "|Iter  735  | Total Val Loss  -7.207382917404175 |\n",
      "Loss for batch is  0.2880343198776245\n",
      "Loss for batch is  0.016974925994873047\n",
      "Loss for batch is  0.18605411052703857\n",
      "Loss for batch is  -0.7116696834564209\n",
      "|Iter  736  | Total Train Loss  -0.22060632705688477 |\n",
      "Val Loss for batch is  -1.4671510457992554\n",
      "Val Loss for batch is  -1.6831556558609009\n",
      "Val Loss for batch is  -1.52989661693573\n",
      "Val Loss for batch is  -2.3689253330230713\n",
      "|Iter  736  | Total Val Loss  -7.0491286516189575 |\n",
      "Loss for batch is  0.21239721775054932\n",
      "Loss for batch is  -0.009891152381896973\n",
      "Loss for batch is  0.20966899394989014\n",
      "Loss for batch is  -0.7998921871185303\n",
      "|Iter  737  | Total Train Loss  -0.3877171277999878 |\n",
      "Val Loss for batch is  -1.5378904342651367\n",
      "Val Loss for batch is  -1.7370260953903198\n",
      "Val Loss for batch is  -1.6065083742141724\n",
      "Val Loss for batch is  -2.429532051086426\n",
      "|Iter  737  | Total Val Loss  -7.310956954956055 |\n",
      "Loss for batch is  0.16637492179870605\n",
      "Loss for batch is  -0.016559123992919922\n",
      "Loss for batch is  0.15477216243743896\n",
      "Loss for batch is  -0.7913668155670166\n",
      "|Iter  738  | Total Train Loss  -0.4867788553237915 |\n",
      "Val Loss for batch is  -1.5956811904907227\n",
      "Val Loss for batch is  -1.7149877548217773\n",
      "Val Loss for batch is  -1.6297507286071777\n",
      "Val Loss for batch is  -2.4464111328125\n",
      "|Iter  738  | Total Val Loss  -7.386830806732178 |\n",
      "Loss for batch is  0.08997189998626709\n",
      "Loss for batch is  -0.043758273124694824\n",
      "Loss for batch is  0.08032429218292236\n",
      "Loss for batch is  -0.8453879356384277\n",
      "|Iter  739  | Total Train Loss  -0.7188500165939331 |\n",
      "Val Loss for batch is  -1.5865018367767334\n",
      "Val Loss for batch is  -1.7997543811798096\n",
      "Val Loss for batch is  -1.666942834854126\n",
      "Val Loss for batch is  -2.4731180667877197\n",
      "|Iter  739  | Total Val Loss  -7.526317119598389 |\n",
      "Loss for batch is  0.06457436084747314\n",
      "Loss for batch is  -0.07431089878082275\n",
      "Loss for batch is  0.033339738845825195\n",
      "Loss for batch is  -0.869208812713623\n",
      "|Iter  740  | Total Train Loss  -0.8456056118011475 |\n",
      "Val Loss for batch is  -1.5768142938613892\n",
      "Val Loss for batch is  -1.744091272354126\n",
      "Val Loss for batch is  -1.7145123481750488\n",
      "Val Loss for batch is  -2.5024642944335938\n",
      "|Iter  740  | Total Val Loss  -7.537882208824158 |\n",
      "Loss for batch is  0.06651341915130615\n",
      "Loss for batch is  -0.10915589332580566\n",
      "Loss for batch is  0.06618762016296387\n",
      "Loss for batch is  -0.8849669694900513\n",
      "|Iter  741  | Total Train Loss  -0.8614218235015869 |\n",
      "Val Loss for batch is  -1.6034609079360962\n",
      "Val Loss for batch is  -1.7875992059707642\n",
      "Val Loss for batch is  -1.6595628261566162\n",
      "Val Loss for batch is  -2.550992965698242\n",
      "|Iter  741  | Total Val Loss  -7.601615905761719 |\n",
      "Loss for batch is  0.04848623275756836\n",
      "Loss for batch is  -0.11651074886322021\n",
      "Loss for batch is  0.04137623310089111\n",
      "Loss for batch is  -0.9166883230209351\n",
      "|Iter  742  | Total Train Loss  -0.9433366060256958 |\n",
      "Val Loss for batch is  -1.574142336845398\n",
      "Val Loss for batch is  -1.800842046737671\n",
      "Val Loss for batch is  -1.5987904071807861\n",
      "Val Loss for batch is  -2.572997570037842\n",
      "|Iter  742  | Total Val Loss  -7.546772360801697 |\n",
      "Loss for batch is  0.056093811988830566\n",
      "Loss for batch is  -0.10663473606109619\n",
      "Loss for batch is  0.04627120494842529\n",
      "Loss for batch is  -0.9392280578613281\n",
      "|Iter  743  | Total Train Loss  -0.9434977769851685 |\n",
      "Val Loss for batch is  -1.5153967142105103\n",
      "Val Loss for batch is  -1.722967505455017\n",
      "Val Loss for batch is  -1.6021627187728882\n",
      "Val Loss for batch is  -2.596755027770996\n",
      "|Iter  743  | Total Val Loss  -7.437281966209412 |\n",
      "Loss for batch is  0.09168088436126709\n",
      "Loss for batch is  -0.09866726398468018\n",
      "Loss for batch is  0.09118282794952393\n",
      "Loss for batch is  -0.9276996850967407\n",
      "|Iter  744  | Total Train Loss  -0.8435032367706299 |\n",
      "Val Loss for batch is  -1.560754656791687\n",
      "Val Loss for batch is  -1.7616336345672607\n",
      "Val Loss for batch is  -1.5465667247772217\n",
      "Val Loss for batch is  -2.499645709991455\n",
      "|Iter  744  | Total Val Loss  -7.3686007261276245 |\n",
      "Loss for batch is  0.08919596672058105\n",
      "Loss for batch is  -0.13287270069122314\n",
      "Loss for batch is  0.10657954216003418\n",
      "Loss for batch is  -0.876178503036499\n",
      "|Iter  745  | Total Train Loss  -0.8132756948471069 |\n",
      "Val Loss for batch is  -1.6119250059127808\n",
      "Val Loss for batch is  -1.7714601755142212\n",
      "Val Loss for batch is  -1.5923248529434204\n",
      "Val Loss for batch is  -2.555741548538208\n",
      "|Iter  745  | Total Val Loss  -7.53145158290863 |\n",
      "Loss for batch is  0.04220688343048096\n",
      "Loss for batch is  -0.07896244525909424\n",
      "Loss for batch is  0.04484379291534424\n",
      "Loss for batch is  -0.8307554721832275\n",
      "|Iter  746  | Total Train Loss  -0.8226672410964966 |\n",
      "Val Loss for batch is  -1.4690698385238647\n",
      "Val Loss for batch is  -1.7125110626220703\n",
      "Val Loss for batch is  -1.552809476852417\n",
      "Val Loss for batch is  -2.4975531101226807\n",
      "|Iter  746  | Total Val Loss  -7.231943488121033 |\n",
      "Loss for batch is  0.1343756914138794\n",
      "Loss for batch is  -0.06443357467651367\n",
      "Loss for batch is  0.27621662616729736\n",
      "Loss for batch is  -0.7797250747680664\n",
      "|Iter  747  | Total Train Loss  -0.4335663318634033 |\n",
      "Val Loss for batch is  -1.1568881273269653\n",
      "Val Loss for batch is  -1.3837966918945312\n",
      "Val Loss for batch is  -1.27191162109375\n",
      "Val Loss for batch is  -2.294557809829712\n",
      "|Iter  747  | Total Val Loss  -6.1071542501449585 |\n",
      "Loss for batch is  0.4686068296432495\n",
      "Loss for batch is  -0.09392988681793213\n",
      "Loss for batch is  0.5764767527580261\n",
      "Loss for batch is  -0.7420972585678101\n",
      "|Iter  748  | Total Train Loss  0.20905643701553345 |\n",
      "Val Loss for batch is  -1.3320486545562744\n",
      "Val Loss for batch is  -1.5514583587646484\n",
      "Val Loss for batch is  -1.417759656906128\n",
      "Val Loss for batch is  -2.4346866607666016\n",
      "|Iter  748  | Total Val Loss  -6.735953330993652 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.21315574645996094\n",
      "Loss for batch is  0.25819528102874756\n",
      "Loss for batch is  0.17344820499420166\n",
      "Loss for batch is  -0.7468363046646118\n",
      "|Iter  749  | Total Train Loss  -0.10203707218170166 |\n",
      "Val Loss for batch is  -1.1962776184082031\n",
      "Val Loss for batch is  -1.5663315057754517\n",
      "Val Loss for batch is  -1.326753854751587\n",
      "Val Loss for batch is  -2.3485476970672607\n",
      "|Iter  749  | Total Val Loss  -6.437910676002502 |\n",
      "Loss for batch is  0.36253345012664795\n",
      "Loss for batch is  -0.00782632827758789\n",
      "Loss for batch is  0.14845705032348633\n",
      "Loss for batch is  -0.6982932090759277\n",
      "|Iter  750  | Total Train Loss  -0.19512903690338135 |\n",
      "Val Loss for batch is  -1.5182472467422485\n",
      "Val Loss for batch is  -1.6813422441482544\n",
      "Val Loss for batch is  -1.5770483016967773\n",
      "Val Loss for batch is  -2.3796396255493164\n",
      "|Iter  750  | Total Val Loss  -7.156277418136597 |\n",
      "Loss for batch is  0.24022996425628662\n",
      "Loss for batch is  0.024149656295776367\n",
      "Loss for batch is  0.11665666103363037\n",
      "Loss for batch is  -0.761093258857727\n",
      "|Iter  751  | Total Train Loss  -0.3800569772720337 |\n",
      "Val Loss for batch is  -1.5212184190750122\n",
      "Val Loss for batch is  -1.7071446180343628\n",
      "Val Loss for batch is  -1.5597574710845947\n",
      "Val Loss for batch is  -2.366011619567871\n",
      "|Iter  751  | Total Val Loss  -7.154132127761841 |\n",
      "Loss for batch is  0.11336100101470947\n",
      "Loss for batch is  -0.03870391845703125\n",
      "Loss for batch is  0.08217966556549072\n",
      "Loss for batch is  -0.7960455417633057\n",
      "|Iter  752  | Total Train Loss  -0.6392087936401367 |\n",
      "Val Loss for batch is  -1.6138949394226074\n",
      "Val Loss for batch is  -1.7706718444824219\n",
      "Val Loss for batch is  -1.6251906156539917\n",
      "Val Loss for batch is  -2.476802349090576\n",
      "|Iter  752  | Total Val Loss  -7.486559748649597 |\n",
      "Loss for batch is  0.08077752590179443\n",
      "Loss for batch is  -0.08057177066802979\n",
      "Loss for batch is  0.048880577087402344\n",
      "Loss for batch is  -0.8212543725967407\n",
      "|Iter  753  | Total Train Loss  -0.7721680402755737 |\n",
      "Val Loss for batch is  -1.6235746145248413\n",
      "Val Loss for batch is  -1.7419328689575195\n",
      "Val Loss for batch is  -1.638765811920166\n",
      "Val Loss for batch is  -2.499826431274414\n",
      "|Iter  753  | Total Val Loss  -7.504099726676941 |\n",
      "Loss for batch is  0.009701371192932129\n",
      "Loss for batch is  -0.08199942111968994\n",
      "Loss for batch is  0.06532883644104004\n",
      "Loss for batch is  -0.8874697685241699\n",
      "|Iter  754  | Total Train Loss  -0.8944389820098877 |\n",
      "Val Loss for batch is  -1.6941535472869873\n",
      "Val Loss for batch is  -1.8243439197540283\n",
      "Val Loss for batch is  -1.6719286441802979\n",
      "Val Loss for batch is  -2.518108367919922\n",
      "|Iter  754  | Total Val Loss  -7.708534479141235 |\n",
      "Loss for batch is  0.033596158027648926\n",
      "Loss for batch is  -0.14323723316192627\n",
      "Loss for batch is  0.030098438262939453\n",
      "Loss for batch is  -0.9022927284240723\n",
      "|Iter  755  | Total Train Loss  -0.9818353652954102 |\n",
      "Val Loss for batch is  -1.6302984952926636\n",
      "Val Loss for batch is  -1.7742654085159302\n",
      "Val Loss for batch is  -1.6107183694839478\n",
      "Val Loss for batch is  -2.5526158809661865\n",
      "|Iter  755  | Total Val Loss  -7.567898154258728 |\n",
      "Loss for batch is  0.014055848121643066\n",
      "Loss for batch is  -0.15419435501098633\n",
      "Loss for batch is  0.01158154010772705\n",
      "Loss for batch is  -0.9395617246627808\n",
      "|Iter  756  | Total Train Loss  -1.068118691444397 |\n",
      "Val Loss for batch is  -1.7061105966567993\n",
      "Val Loss for batch is  -1.8188836574554443\n",
      "Val Loss for batch is  -1.7002283334732056\n",
      "Val Loss for batch is  -2.597182035446167\n",
      "|Iter  756  | Total Val Loss  -7.822404623031616 |\n",
      "Loss for batch is  -0.0016313791275024414\n",
      "Loss for batch is  -0.16475987434387207\n",
      "Loss for batch is  0.0003197193145751953\n",
      "Loss for batch is  -0.9463138580322266\n",
      "|Iter  757  | Total Train Loss  -1.1123853921890259 |\n",
      "Val Loss for batch is  -1.6263177394866943\n",
      "Val Loss for batch is  -1.7990727424621582\n",
      "Val Loss for batch is  -1.660596489906311\n",
      "Val Loss for batch is  -2.574981689453125\n",
      "|Iter  757  | Total Val Loss  -7.660968661308289 |\n",
      "Loss for batch is  -0.009921789169311523\n",
      "Loss for batch is  -0.19395792484283447\n",
      "Loss for batch is  -0.01228487491607666\n",
      "Loss for batch is  -0.9672809839248657\n",
      "|Iter  758  | Total Train Loss  -1.1834455728530884 |\n",
      "Val Loss for batch is  -1.6202017068862915\n",
      "Val Loss for batch is  -1.7703914642333984\n",
      "Val Loss for batch is  -1.626624345779419\n",
      "Val Loss for batch is  -2.5964834690093994\n",
      "|Iter  758  | Total Val Loss  -7.613700985908508 |\n",
      "Loss for batch is  0.040973663330078125\n",
      "Loss for batch is  -0.18332815170288086\n",
      "Loss for batch is  0.04663741588592529\n",
      "Loss for batch is  -0.9767791032791138\n",
      "|Iter  759  | Total Train Loss  -1.0724961757659912 |\n",
      "Val Loss for batch is  -1.43837308883667\n",
      "Val Loss for batch is  -1.6853957176208496\n",
      "Val Loss for batch is  -1.5701960325241089\n",
      "Val Loss for batch is  -2.5366222858428955\n",
      "|Iter  759  | Total Val Loss  -7.230587124824524 |\n",
      "Loss for batch is  0.09733664989471436\n",
      "Loss for batch is  -0.1931840181350708\n",
      "Loss for batch is  0.028077244758605957\n",
      "Loss for batch is  -0.9283218383789062\n",
      "|Iter  760  | Total Train Loss  -0.9960919618606567 |\n",
      "Val Loss for batch is  -1.6164064407348633\n",
      "Val Loss for batch is  -1.8140157461166382\n",
      "Val Loss for batch is  -1.5729951858520508\n",
      "Val Loss for batch is  -2.587639331817627\n",
      "|Iter  760  | Total Val Loss  -7.591056704521179 |\n",
      "Loss for batch is  0.021143674850463867\n",
      "Loss for batch is  -0.10600626468658447\n",
      "Loss for batch is  -0.0048989057540893555\n",
      "Loss for batch is  -0.8431084156036377\n",
      "|Iter  761  | Total Train Loss  -0.9328699111938477 |\n",
      "Val Loss for batch is  -1.4320878982543945\n",
      "Val Loss for batch is  -1.6627001762390137\n",
      "Val Loss for batch is  -1.6153697967529297\n",
      "Val Loss for batch is  -2.5259881019592285\n",
      "|Iter  761  | Total Val Loss  -7.236145973205566 |\n",
      "Loss for batch is  0.18866360187530518\n",
      "Loss for batch is  -0.09217548370361328\n",
      "Loss for batch is  0.30637693405151367\n",
      "Loss for batch is  -0.9136859178543091\n",
      "|Iter  762  | Total Train Loss  -0.5108208656311035 |\n",
      "Val Loss for batch is  -1.138404130935669\n",
      "Val Loss for batch is  -1.3971697092056274\n",
      "Val Loss for batch is  -1.245647668838501\n",
      "Val Loss for batch is  -2.350006103515625\n",
      "|Iter  762  | Total Val Loss  -6.131227612495422 |\n",
      "Loss for batch is  0.4502604007720947\n",
      "Loss for batch is  -0.06258666515350342\n",
      "Loss for batch is  0.29584169387817383\n",
      "Loss for batch is  -0.7558363676071167\n",
      "|Iter  763  | Total Train Loss  -0.07232093811035156 |\n",
      "Val Loss for batch is  -1.3819973468780518\n",
      "Val Loss for batch is  -1.683703064918518\n",
      "Val Loss for batch is  -1.4965460300445557\n",
      "Val Loss for batch is  -2.491110324859619\n",
      "|Iter  763  | Total Val Loss  -7.053356766700745 |\n",
      "Loss for batch is  0.2045656442642212\n",
      "Loss for batch is  0.0336834192276001\n",
      "Loss for batch is  0.14399981498718262\n",
      "Loss for batch is  -0.7912871837615967\n",
      "|Iter  764  | Total Train Loss  -0.4090383052825928 |\n",
      "Val Loss for batch is  -1.5159403085708618\n",
      "Val Loss for batch is  -1.768851637840271\n",
      "Val Loss for batch is  -1.536934733390808\n",
      "Val Loss for batch is  -2.419408082962036\n",
      "|Iter  764  | Total Val Loss  -7.241134762763977 |\n",
      "Loss for batch is  0.1339472532272339\n",
      "Loss for batch is  -0.04800009727478027\n",
      "Loss for batch is  0.15015125274658203\n",
      "Loss for batch is  -0.8524153232574463\n",
      "|Iter  765  | Total Train Loss  -0.6163169145584106 |\n",
      "Val Loss for batch is  -1.4735223054885864\n",
      "Val Loss for batch is  -1.6244235038757324\n",
      "Val Loss for batch is  -1.553563117980957\n",
      "Val Loss for batch is  -2.4112586975097656\n",
      "|Iter  765  | Total Val Loss  -7.0627676248550415 |\n",
      "Loss for batch is  0.12865424156188965\n",
      "Loss for batch is  -0.03667628765106201\n",
      "Loss for batch is  0.046372413635253906\n",
      "Loss for batch is  -0.8461804389953613\n",
      "|Iter  766  | Total Train Loss  -0.7078300714492798 |\n",
      "Val Loss for batch is  -1.6345092058181763\n",
      "Val Loss for batch is  -1.836901068687439\n",
      "Val Loss for batch is  -1.6280238628387451\n",
      "Val Loss for batch is  -2.5261290073394775\n",
      "|Iter  766  | Total Val Loss  -7.625563144683838 |\n",
      "Loss for batch is  0.04877960681915283\n",
      "Loss for batch is  -0.12014305591583252\n",
      "Loss for batch is  -0.012993454933166504\n",
      "Loss for batch is  -0.8644435405731201\n",
      "|Iter  767  | Total Train Loss  -0.9488004446029663 |\n",
      "Val Loss for batch is  -1.585404634475708\n",
      "Val Loss for batch is  -1.7445414066314697\n",
      "Val Loss for batch is  -1.658616304397583\n",
      "Val Loss for batch is  -2.510441541671753\n",
      "|Iter  767  | Total Val Loss  -7.499003887176514 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.023991107940673828\n",
      "Loss for batch is  -0.14742827415466309\n",
      "Loss for batch is  0.02289760112762451\n",
      "Loss for batch is  -0.894273042678833\n",
      "|Iter  768  | Total Train Loss  -0.9948126077651978 |\n",
      "Val Loss for batch is  -1.610029935836792\n",
      "Val Loss for batch is  -1.777952790260315\n",
      "Val Loss for batch is  -1.6968709230422974\n",
      "Val Loss for batch is  -2.568960189819336\n",
      "|Iter  768  | Total Val Loss  -7.65381383895874 |\n",
      "Loss for batch is  -0.025976896286010742\n",
      "Loss for batch is  -0.17790210247039795\n",
      "Loss for batch is  0.01918935775756836\n",
      "Loss for batch is  -0.9460494518280029\n",
      "|Iter  769  | Total Train Loss  -1.1307390928268433 |\n",
      "Val Loss for batch is  -1.6191350221633911\n",
      "Val Loss for batch is  -1.674441933631897\n",
      "Val Loss for batch is  -1.6224746704101562\n",
      "Val Loss for batch is  -2.4684808254241943\n",
      "|Iter  769  | Total Val Loss  -7.384532451629639 |\n",
      "Loss for batch is  0.02681899070739746\n",
      "Loss for batch is  -0.20062720775604248\n",
      "Loss for batch is  0.004031777381896973\n",
      "Loss for batch is  -0.9564402103424072\n",
      "|Iter  770  | Total Train Loss  -1.1262166500091553 |\n",
      "Val Loss for batch is  -1.6348568201065063\n",
      "Val Loss for batch is  -1.7520005702972412\n",
      "Val Loss for batch is  -1.667028784751892\n",
      "Val Loss for batch is  -2.596622943878174\n",
      "|Iter  770  | Total Val Loss  -7.6505091190338135 |\n",
      "Loss for batch is  -0.031572580337524414\n",
      "Loss for batch is  -0.17795038223266602\n",
      "Loss for batch is  -0.0849219560623169\n",
      "Loss for batch is  -0.9438045024871826\n",
      "|Iter  771  | Total Train Loss  -1.23824942111969 |\n",
      "Val Loss for batch is  -1.6406970024108887\n",
      "Val Loss for batch is  -1.8507535457611084\n",
      "Val Loss for batch is  -1.763388991355896\n",
      "Val Loss for batch is  -2.6232502460479736\n",
      "|Iter  771  | Total Val Loss  -7.878089785575867 |\n",
      "Loss for batch is  -0.038714051246643066\n",
      "Loss for batch is  -0.18295729160308838\n",
      "Loss for batch is  -0.059307217597961426\n",
      "Loss for batch is  -0.9877949953079224\n",
      "|Iter  772  | Total Train Loss  -1.2687735557556152 |\n",
      "Val Loss for batch is  -1.5917075872421265\n",
      "Val Loss for batch is  -1.7096619606018066\n",
      "Val Loss for batch is  -1.5728870630264282\n",
      "Val Loss for batch is  -2.5850603580474854\n",
      "|Iter  772  | Total Val Loss  -7.459316968917847 |\n",
      "Loss for batch is  0.048853278160095215\n",
      "Loss for batch is  -0.2142481803894043\n",
      "Loss for batch is  0.12570297718048096\n",
      "Loss for batch is  -0.995206356048584\n",
      "|Iter  773  | Total Train Loss  -1.034898281097412 |\n",
      "Val Loss for batch is  -1.3977528810501099\n",
      "Val Loss for batch is  -1.6590323448181152\n",
      "Val Loss for batch is  -1.369215965270996\n",
      "Val Loss for batch is  -2.4881837368011475\n",
      "|Iter  773  | Total Val Loss  -6.914184927940369 |\n",
      "Loss for batch is  0.1654876470565796\n",
      "Loss for batch is  -0.2071460485458374\n",
      "Loss for batch is  0.11917173862457275\n",
      "Loss for batch is  -0.8826941251754761\n",
      "|Iter  774  | Total Train Loss  -0.8051807880401611 |\n",
      "Val Loss for batch is  -1.5843478441238403\n",
      "Val Loss for batch is  -1.7437570095062256\n",
      "Val Loss for batch is  -1.5470294952392578\n",
      "Val Loss for batch is  -2.5062096118927\n",
      "|Iter  774  | Total Val Loss  -7.381343960762024 |\n",
      "Loss for batch is  0.027097702026367188\n",
      "Loss for batch is  -0.005360245704650879\n",
      "Loss for batch is  0.041825056076049805\n",
      "Loss for batch is  -0.8060092926025391\n",
      "|Iter  775  | Total Train Loss  -0.742446780204773 |\n",
      "Val Loss for batch is  -1.4018023014068604\n",
      "Val Loss for batch is  -1.523532748222351\n",
      "Val Loss for batch is  -1.4418320655822754\n",
      "Val Loss for batch is  -2.4894819259643555\n",
      "|Iter  775  | Total Val Loss  -6.856649041175842 |\n",
      "Loss for batch is  0.23811745643615723\n",
      "Loss for batch is  -0.15635812282562256\n",
      "Loss for batch is  0.4118490219116211\n",
      "Loss for batch is  -0.9025629758834839\n",
      "|Iter  776  | Total Train Loss  -0.4089546203613281 |\n",
      "Val Loss for batch is  -1.3770735263824463\n",
      "Val Loss for batch is  -1.640042781829834\n",
      "Val Loss for batch is  -1.5714091062545776\n",
      "Val Loss for batch is  -2.543336868286133\n",
      "|Iter  776  | Total Val Loss  -7.131862282752991 |\n",
      "Loss for batch is  0.18975257873535156\n",
      "Loss for batch is  0.05404078960418701\n",
      "Loss for batch is  0.0008261203765869141\n",
      "Loss for batch is  -0.7797466516494751\n",
      "|Iter  777  | Total Train Loss  -0.5351271629333496 |\n",
      "Val Loss for batch is  -1.4771718978881836\n",
      "Val Loss for batch is  -1.6700055599212646\n",
      "Val Loss for batch is  -1.5323578119277954\n",
      "Val Loss for batch is  -2.3986778259277344\n",
      "|Iter  777  | Total Val Loss  -7.078213095664978 |\n",
      "Loss for batch is  0.09098553657531738\n",
      "Loss for batch is  -0.13963770866394043\n",
      "Loss for batch is  0.01789259910583496\n",
      "Loss for batch is  -0.8304978609085083\n",
      "|Iter  778  | Total Train Loss  -0.8612574338912964 |\n",
      "Val Loss for batch is  -1.6146200895309448\n",
      "Val Loss for batch is  -1.7679753303527832\n",
      "Val Loss for batch is  -1.6698096990585327\n",
      "Val Loss for batch is  -2.529146909713745\n",
      "|Iter  778  | Total Val Loss  -7.581552028656006 |\n",
      "Loss for batch is  0.03775739669799805\n",
      "Loss for batch is  -0.13075220584869385\n",
      "Loss for batch is  0.015610694885253906\n",
      "Loss for batch is  -0.8814897537231445\n",
      "|Iter  779  | Total Train Loss  -0.9588738679885864 |\n",
      "Val Loss for batch is  -1.576634407043457\n",
      "Val Loss for batch is  -1.7386242151260376\n",
      "Val Loss for batch is  -1.6800510883331299\n",
      "Val Loss for batch is  -2.531813859939575\n",
      "|Iter  779  | Total Val Loss  -7.5271235704422 |\n",
      "Loss for batch is  -0.020975470542907715\n",
      "Loss for batch is  -0.1536957025527954\n",
      "Loss for batch is  -0.03229820728302002\n",
      "Loss for batch is  -0.9207010269165039\n",
      "|Iter  780  | Total Train Loss  -1.127670407295227 |\n",
      "Val Loss for batch is  -1.7358529567718506\n",
      "Val Loss for batch is  -1.8421698808670044\n",
      "Val Loss for batch is  -1.745701789855957\n",
      "Val Loss for batch is  -2.552295207977295\n",
      "|Iter  780  | Total Val Loss  -7.876019835472107 |\n",
      "Loss for batch is  -0.06741249561309814\n",
      "Loss for batch is  -0.20586073398590088\n",
      "Loss for batch is  -0.061295509338378906\n",
      "Loss for batch is  -0.9785672426223755\n",
      "|Iter  781  | Total Train Loss  -1.3131359815597534 |\n",
      "Val Loss for batch is  -1.6539809703826904\n",
      "Val Loss for batch is  -1.8490716218948364\n",
      "Val Loss for batch is  -1.6563446521759033\n",
      "Val Loss for batch is  -2.596195936203003\n",
      "|Iter  781  | Total Val Loss  -7.755593180656433 |\n",
      "Loss for batch is  -0.03270578384399414\n",
      "Loss for batch is  -0.2053847312927246\n",
      "Loss for batch is  -0.09167969226837158\n",
      "Loss for batch is  -0.9837663173675537\n",
      "|Iter  782  | Total Train Loss  -1.313536524772644 |\n",
      "Val Loss for batch is  -1.7346640825271606\n",
      "Val Loss for batch is  -1.8447051048278809\n",
      "Val Loss for batch is  -1.716808557510376\n",
      "Val Loss for batch is  -2.6236371994018555\n",
      "|Iter  782  | Total Val Loss  -7.919814944267273 |\n",
      "Loss for batch is  -0.08535373210906982\n",
      "Loss for batch is  -0.23354053497314453\n",
      "Loss for batch is  -0.10368788242340088\n",
      "Loss for batch is  -1.0147018432617188\n",
      "|Iter  783  | Total Train Loss  -1.437283992767334 |\n",
      "Val Loss for batch is  -1.6970770359039307\n",
      "Val Loss for batch is  -1.8907184600830078\n",
      "Val Loss for batch is  -1.722922921180725\n",
      "Val Loss for batch is  -2.657358407974243\n",
      "|Iter  783  | Total Val Loss  -7.968076825141907 |\n",
      "Loss for batch is  -0.10012984275817871\n",
      "Loss for batch is  -0.23674750328063965\n",
      "Loss for batch is  -0.12309670448303223\n",
      "Loss for batch is  -0.9986903667449951\n",
      "|Iter  784  | Total Train Loss  -1.4586644172668457 |\n",
      "Val Loss for batch is  -1.624696969985962\n",
      "Val Loss for batch is  -1.6955019235610962\n",
      "Val Loss for batch is  -1.6556988954544067\n",
      "Val Loss for batch is  -2.5742123126983643\n",
      "|Iter  784  | Total Val Loss  -7.550110101699829 |\n",
      "Loss for batch is  0.006254911422729492\n",
      "Loss for batch is  -0.20381760597229004\n",
      "Loss for batch is  -0.015201091766357422\n",
      "Loss for batch is  -1.006272554397583\n",
      "|Iter  785  | Total Train Loss  -1.219036340713501 |\n",
      "Val Loss for batch is  -1.4738091230392456\n",
      "Val Loss for batch is  -1.682916522026062\n",
      "Val Loss for batch is  -1.5647910833358765\n",
      "Val Loss for batch is  -2.5555076599121094\n",
      "|Iter  785  | Total Val Loss  -7.2770243883132935 |\n",
      "Loss for batch is  0.0524669885635376\n",
      "Loss for batch is  -0.15765488147735596\n",
      "Loss for batch is  0.026917695999145508\n",
      "Loss for batch is  -0.9615588188171387\n",
      "|Iter  786  | Total Train Loss  -1.0398290157318115 |\n",
      "Val Loss for batch is  -1.3795034885406494\n",
      "Val Loss for batch is  -1.4845455884933472\n",
      "Val Loss for batch is  -1.3591440916061401\n",
      "Val Loss for batch is  -2.468400478363037\n",
      "|Iter  786  | Total Val Loss  -6.691593647003174 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.1762920618057251\n",
      "Loss for batch is  -0.15667736530303955\n",
      "Loss for batch is  0.15093600749969482\n",
      "Loss for batch is  -0.8364648818969727\n",
      "|Iter  787  | Total Train Loss  -0.6659141778945923 |\n",
      "Val Loss for batch is  -1.5407154560089111\n",
      "Val Loss for batch is  -1.769452691078186\n",
      "Val Loss for batch is  -1.61029851436615\n",
      "Val Loss for batch is  -2.5563392639160156\n",
      "|Iter  787  | Total Val Loss  -7.476805925369263 |\n",
      "Loss for batch is  0.0388033390045166\n",
      "Loss for batch is  -0.0923759937286377\n",
      "Loss for batch is  0.14903712272644043\n",
      "Loss for batch is  -0.9446125030517578\n",
      "|Iter  788  | Total Train Loss  -0.8491480350494385 |\n",
      "Val Loss for batch is  -1.6126179695129395\n",
      "Val Loss for batch is  -1.854299545288086\n",
      "Val Loss for batch is  -1.710676670074463\n",
      "Val Loss for batch is  -2.524137020111084\n",
      "|Iter  788  | Total Val Loss  -7.701731204986572 |\n",
      "Loss for batch is  0.019810795783996582\n",
      "Loss for batch is  -0.03813374042510986\n",
      "Loss for batch is  -0.035237789154052734\n",
      "Loss for batch is  -0.8900872468948364\n",
      "|Iter  789  | Total Train Loss  -0.9436479806900024 |\n",
      "Val Loss for batch is  -1.490351915359497\n",
      "Val Loss for batch is  -1.76266348361969\n",
      "Val Loss for batch is  -1.6188746690750122\n",
      "Val Loss for batch is  -2.4349682331085205\n",
      "|Iter  789  | Total Val Loss  -7.30685830116272 |\n",
      "Loss for batch is  0.08994138240814209\n",
      "Loss for batch is  -0.17411184310913086\n",
      "Loss for batch is  -0.003524303436279297\n",
      "Loss for batch is  -0.8792959451675415\n",
      "|Iter  790  | Total Train Loss  -0.9669907093048096 |\n",
      "Val Loss for batch is  -1.657340407371521\n",
      "Val Loss for batch is  -1.8263049125671387\n",
      "Val Loss for batch is  -1.7041571140289307\n",
      "Val Loss for batch is  -2.5077600479125977\n",
      "|Iter  790  | Total Val Loss  -7.695562481880188 |\n",
      "Loss for batch is  -0.002227187156677246\n",
      "Loss for batch is  -0.17012441158294678\n",
      "Loss for batch is  -0.07308673858642578\n",
      "Loss for batch is  -0.9691349267959595\n",
      "|Iter  791  | Total Train Loss  -1.2145732641220093 |\n",
      "Val Loss for batch is  -1.6638476848602295\n",
      "Val Loss for batch is  -1.829697608947754\n",
      "Val Loss for batch is  -1.698253870010376\n",
      "Val Loss for batch is  -2.535921812057495\n",
      "|Iter  791  | Total Val Loss  -7.7277209758758545 |\n",
      "Loss for batch is  -0.02113950252532959\n",
      "Loss for batch is  -0.21296465396881104\n",
      "Loss for batch is  -0.0603792667388916\n",
      "Loss for batch is  -0.9715138673782349\n",
      "|Iter  792  | Total Train Loss  -1.265997290611267 |\n",
      "Val Loss for batch is  -1.7089896202087402\n",
      "Val Loss for batch is  -1.8926080465316772\n",
      "Val Loss for batch is  -1.7439974546432495\n",
      "Val Loss for batch is  -2.594698905944824\n",
      "|Iter  792  | Total Val Loss  -7.940294027328491 |\n",
      "Loss for batch is  -0.08379864692687988\n",
      "Loss for batch is  -0.22372174263000488\n",
      "Loss for batch is  -0.10695362091064453\n",
      "Loss for batch is  -1.0057141780853271\n",
      "|Iter  793  | Total Train Loss  -1.4201881885528564 |\n",
      "Val Loss for batch is  -1.7375473976135254\n",
      "Val Loss for batch is  -1.8621522188186646\n",
      "Val Loss for batch is  -1.7554658651351929\n",
      "Val Loss for batch is  -2.6371212005615234\n",
      "|Iter  793  | Total Val Loss  -7.992286682128906 |\n",
      "Loss for batch is  -0.09903037548065186\n",
      "Loss for batch is  -0.2828500270843506\n",
      "Loss for batch is  -0.08165216445922852\n",
      "Loss for batch is  -1.0330896377563477\n",
      "|Iter  794  | Total Train Loss  -1.4966222047805786 |\n",
      "Val Loss for batch is  -1.6672134399414062\n",
      "Val Loss for batch is  -1.8271361589431763\n",
      "Val Loss for batch is  -1.6862837076187134\n",
      "Val Loss for batch is  -2.639627695083618\n",
      "|Iter  794  | Total Val Loss  -7.820261001586914 |\n",
      "Loss for batch is  -0.06912493705749512\n",
      "Loss for batch is  -0.2967100143432617\n",
      "Loss for batch is  -0.08947300910949707\n",
      "Loss for batch is  -1.0454589128494263\n",
      "|Iter  795  | Total Train Loss  -1.5007668733596802 |\n",
      "Val Loss for batch is  -1.6811600923538208\n",
      "Val Loss for batch is  -1.8389939069747925\n",
      "Val Loss for batch is  -1.7516638040542603\n",
      "Val Loss for batch is  -2.6243443489074707\n",
      "|Iter  795  | Total Val Loss  -7.896162152290344 |\n",
      "Loss for batch is  -0.06082797050476074\n",
      "Loss for batch is  -0.27872657775878906\n",
      "Loss for batch is  -0.1137915849685669\n",
      "Loss for batch is  -1.040001392364502\n",
      "|Iter  796  | Total Train Loss  -1.4933475255966187 |\n",
      "Val Loss for batch is  -1.6777901649475098\n",
      "Val Loss for batch is  -1.8402475118637085\n",
      "Val Loss for batch is  -1.719184398651123\n",
      "Val Loss for batch is  -2.673020839691162\n",
      "|Iter  796  | Total Val Loss  -7.910242915153503 |\n",
      "Loss for batch is  -0.11522245407104492\n",
      "Loss for batch is  -0.20019102096557617\n",
      "Loss for batch is  -0.15042495727539062\n",
      "Loss for batch is  -0.9867653846740723\n",
      "|Iter  797  | Total Train Loss  -1.452603816986084 |\n",
      "Val Loss for batch is  -1.5757122039794922\n",
      "Val Loss for batch is  -1.8451863527297974\n",
      "Val Loss for batch is  -1.6595613956451416\n",
      "Val Loss for batch is  -2.642676591873169\n",
      "|Iter  797  | Total Val Loss  -7.7231365442276 |\n",
      "Loss for batch is  -0.017272233963012695\n",
      "Loss for batch is  -0.08444714546203613\n",
      "Loss for batch is  0.043218016624450684\n",
      "Loss for batch is  -0.9919025897979736\n",
      "|Iter  798  | Total Train Loss  -1.0504039525985718 |\n",
      "Val Loss for batch is  -0.9088000655174255\n",
      "Val Loss for batch is  -1.0832252502441406\n",
      "Val Loss for batch is  -1.0332591533660889\n",
      "Val Loss for batch is  -2.3247218132019043\n",
      "|Iter  798  | Total Val Loss  -5.350006282329559 |\n",
      "Loss for batch is  0.6571729779243469\n",
      "Loss for batch is  -0.10426664352416992\n",
      "Loss for batch is  1.098496675491333\n",
      "Loss for batch is  -0.8015316724777222\n",
      "|Iter  799  | Total Train Loss  0.8498713374137878 |\n",
      "Val Loss for batch is  -1.1723119020462036\n",
      "Val Loss for batch is  -1.506661057472229\n",
      "Val Loss for batch is  -1.0557360649108887\n",
      "Val Loss for batch is  -2.3675594329833984\n",
      "|Iter  799  | Total Val Loss  -6.10226845741272 |\n",
      "Loss for batch is  0.3676018714904785\n",
      "Loss for batch is  0.6882898807525635\n",
      "Loss for batch is  0.4232978820800781\n",
      "Loss for batch is  -0.7435603141784668\n",
      "|Iter  800  | Total Train Loss  0.7356293201446533 |\n",
      "Val Loss for batch is  -1.006540298461914\n",
      "Val Loss for batch is  -1.319190502166748\n",
      "Val Loss for batch is  -1.1501774787902832\n",
      "Val Loss for batch is  -2.102025270462036\n",
      "|Iter  800  | Total Val Loss  -5.5779335498809814 |\n",
      "Loss for batch is  0.6260908246040344\n",
      "Loss for batch is  0.27648043632507324\n",
      "Loss for batch is  0.5688276290893555\n",
      "Loss for batch is  -0.685610294342041\n",
      "|Iter  801  | Total Train Loss  0.7857885956764221 |\n",
      "Val Loss for batch is  -1.2834389209747314\n",
      "Val Loss for batch is  -1.5021837949752808\n",
      "Val Loss for batch is  -1.1618226766586304\n",
      "Val Loss for batch is  -2.24790358543396\n",
      "|Iter  801  | Total Val Loss  -6.1953489780426025 |\n",
      "Loss for batch is  0.31326115131378174\n",
      "Loss for batch is  0.15216565132141113\n",
      "Loss for batch is  0.3828573226928711\n",
      "Loss for batch is  -0.7073721885681152\n",
      "|Iter  802  | Total Train Loss  0.14091193675994873 |\n",
      "Val Loss for batch is  -1.422905683517456\n",
      "Val Loss for batch is  -1.5848350524902344\n",
      "Val Loss for batch is  -1.4292216300964355\n",
      "Val Loss for batch is  -2.3723416328430176\n",
      "|Iter  802  | Total Val Loss  -6.8093039989471436 |\n",
      "Loss for batch is  0.2813849449157715\n",
      "Loss for batch is  0.1272878646850586\n",
      "Loss for batch is  0.21495115756988525\n",
      "Loss for batch is  -0.6736197471618652\n",
      "|Iter  803  | Total Train Loss  -0.0499957799911499 |\n",
      "Val Loss for batch is  -1.5732042789459229\n",
      "Val Loss for batch is  -1.610816478729248\n",
      "Val Loss for batch is  -1.53041410446167\n",
      "Val Loss for batch is  -2.331430196762085\n",
      "|Iter  803  | Total Val Loss  -7.045865058898926 |\n",
      "Loss for batch is  0.21718215942382812\n",
      "Loss for batch is  0.07296490669250488\n",
      "Loss for batch is  0.18192219734191895\n",
      "Loss for batch is  -0.745863676071167\n",
      "|Iter  804  | Total Train Loss  -0.27379441261291504 |\n",
      "Val Loss for batch is  -1.6234644651412964\n",
      "Val Loss for batch is  -1.6675364971160889\n",
      "Val Loss for batch is  -1.5828113555908203\n",
      "Val Loss for batch is  -2.417045831680298\n",
      "|Iter  804  | Total Val Loss  -7.290858149528503 |\n",
      "Loss for batch is  0.08459591865539551\n",
      "Loss for batch is  0.000400543212890625\n",
      "Loss for batch is  0.137931227684021\n",
      "Loss for batch is  -0.7878568172454834\n",
      "|Iter  805  | Total Train Loss  -0.5649291276931763 |\n",
      "Val Loss for batch is  -1.6248618364334106\n",
      "Val Loss for batch is  -1.7022042274475098\n",
      "Val Loss for batch is  -1.6128976345062256\n",
      "Val Loss for batch is  -2.4774301052093506\n",
      "|Iter  805  | Total Val Loss  -7.417393803596497 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.059577226638793945\n",
      "Loss for batch is  -0.029935002326965332\n",
      "Loss for batch is  0.06442487239837646\n",
      "Loss for batch is  -0.8463702201843262\n",
      "|Iter  806  | Total Train Loss  -0.7523031234741211 |\n",
      "Val Loss for batch is  -1.69901442527771\n",
      "Val Loss for batch is  -1.7536152601242065\n",
      "Val Loss for batch is  -1.7013146877288818\n",
      "Val Loss for batch is  -2.4696176052093506\n",
      "|Iter  806  | Total Val Loss  -7.623561978340149 |\n",
      "Loss for batch is  0.00636899471282959\n",
      "Loss for batch is  -0.08540058135986328\n",
      "Loss for batch is  0.01217961311340332\n",
      "Loss for batch is  -0.8830223083496094\n",
      "|Iter  807  | Total Train Loss  -0.9498742818832397 |\n",
      "Val Loss for batch is  -1.7376818656921387\n",
      "Val Loss for batch is  -1.770302414894104\n",
      "Val Loss for batch is  -1.7250983715057373\n",
      "Val Loss for batch is  -2.525360584259033\n",
      "|Iter  807  | Total Val Loss  -7.758443236351013 |\n",
      "Loss for batch is  -0.06244945526123047\n",
      "Loss for batch is  -0.1401127576828003\n",
      "Loss for batch is  0.009666085243225098\n",
      "Loss for batch is  -0.959302544593811\n",
      "|Iter  808  | Total Train Loss  -1.1521986722946167 |\n",
      "Val Loss for batch is  -1.6863044500350952\n",
      "Val Loss for batch is  -1.7939473390579224\n",
      "Val Loss for batch is  -1.6861746311187744\n",
      "Val Loss for batch is  -2.5607709884643555\n",
      "|Iter  808  | Total Val Loss  -7.7271974086761475 |\n",
      "Loss for batch is  -0.06701743602752686\n",
      "Loss for batch is  -0.17611277103424072\n",
      "Loss for batch is  -0.05867302417755127\n",
      "Loss for batch is  -0.9922434091567993\n",
      "|Iter  809  | Total Train Loss  -1.2940466403961182 |\n",
      "Val Loss for batch is  -1.6640163660049438\n",
      "Val Loss for batch is  -1.862776517868042\n",
      "Val Loss for batch is  -1.7045583724975586\n",
      "Val Loss for batch is  -2.584763288497925\n",
      "|Iter  809  | Total Val Loss  -7.816114544868469 |\n",
      "Loss for batch is  -0.08318614959716797\n",
      "Loss for batch is  -0.21261370182037354\n",
      "Loss for batch is  -0.08880901336669922\n",
      "Loss for batch is  -1.0307806730270386\n",
      "|Iter  810  | Total Train Loss  -1.4153895378112793 |\n",
      "Val Loss for batch is  -1.7043546438217163\n",
      "Val Loss for batch is  -1.8481261730194092\n",
      "Val Loss for batch is  -1.7989625930786133\n",
      "Val Loss for batch is  -2.658043384552002\n",
      "|Iter  810  | Total Val Loss  -8.00948679447174 |\n",
      "Loss for batch is  -0.09175610542297363\n",
      "Loss for batch is  -0.24074184894561768\n",
      "Loss for batch is  -0.11293172836303711\n",
      "Loss for batch is  -1.044553518295288\n",
      "|Iter  811  | Total Train Loss  -1.4899832010269165 |\n",
      "Val Loss for batch is  -1.6570159196853638\n",
      "Val Loss for batch is  -1.8160864114761353\n",
      "Val Loss for batch is  -1.6780775785446167\n",
      "Val Loss for batch is  -2.6357336044311523\n",
      "|Iter  811  | Total Val Loss  -7.786913514137268 |\n",
      "Loss for batch is  -0.08484029769897461\n",
      "Loss for batch is  -0.27958083152770996\n",
      "Loss for batch is  -0.10068047046661377\n",
      "Loss for batch is  -1.0678913593292236\n",
      "|Iter  812  | Total Train Loss  -1.532992959022522 |\n",
      "Val Loss for batch is  -1.6858398914337158\n",
      "Val Loss for batch is  -1.889550805091858\n",
      "Val Loss for batch is  -1.7629796266555786\n",
      "Val Loss for batch is  -2.6756622791290283\n",
      "|Iter  812  | Total Val Loss  -8.01403260231018 |\n",
      "Loss for batch is  -0.08701217174530029\n",
      "Loss for batch is  -0.30807507038116455\n",
      "Loss for batch is  -0.1623474359512329\n",
      "Loss for batch is  -1.0626964569091797\n",
      "|Iter  813  | Total Train Loss  -1.6201311349868774 |\n",
      "Val Loss for batch is  -1.6330019235610962\n",
      "Val Loss for batch is  -1.7199383974075317\n",
      "Val Loss for batch is  -1.564023733139038\n",
      "Val Loss for batch is  -2.5811851024627686\n",
      "|Iter  813  | Total Val Loss  -7.498149156570435 |\n",
      "Loss for batch is  -0.05428624153137207\n",
      "Loss for batch is  -0.3275737762451172\n",
      "Loss for batch is  -0.14059758186340332\n",
      "Loss for batch is  -1.0835542678833008\n",
      "|Iter  814  | Total Train Loss  -1.6060118675231934 |\n",
      "Val Loss for batch is  -1.7155539989471436\n",
      "Val Loss for batch is  -1.8919382095336914\n",
      "Val Loss for batch is  -1.7670488357543945\n",
      "Val Loss for batch is  -2.689471483230591\n",
      "|Iter  814  | Total Val Loss  -8.06401252746582 |\n",
      "Loss for batch is  -0.10878777503967285\n",
      "Loss for batch is  -0.32572805881500244\n",
      "Loss for batch is  -0.18320906162261963\n",
      "Loss for batch is  -1.0487269163131714\n",
      "|Iter  815  | Total Train Loss  -1.6664518117904663 |\n",
      "Val Loss for batch is  -1.6644030809402466\n",
      "Val Loss for batch is  -1.846236228942871\n",
      "Val Loss for batch is  -1.691489815711975\n",
      "Val Loss for batch is  -2.684577703475952\n",
      "|Iter  815  | Total Val Loss  -7.886706829071045 |\n",
      "Loss for batch is  -0.10359013080596924\n",
      "Loss for batch is  -0.3100346326828003\n",
      "Loss for batch is  -0.10644161701202393\n",
      "Loss for batch is  -1.0441656112670898\n",
      "|Iter  816  | Total Train Loss  -1.5642319917678833 |\n",
      "Val Loss for batch is  -1.5166821479797363\n",
      "Val Loss for batch is  -1.6887003183364868\n",
      "Val Loss for batch is  -1.5579363107681274\n",
      "Val Loss for batch is  -2.6106085777282715\n",
      "|Iter  816  | Total Val Loss  -7.373927354812622 |\n",
      "Loss for batch is  0.07626771926879883\n",
      "Loss for batch is  -0.24024808406829834\n",
      "Loss for batch is  0.18632221221923828\n",
      "Loss for batch is  -1.0545634031295776\n",
      "|Iter  817  | Total Train Loss  -1.0322215557098389 |\n",
      "Val Loss for batch is  -1.307381272315979\n",
      "Val Loss for batch is  -1.517426609992981\n",
      "Val Loss for batch is  -1.3505035638809204\n",
      "Val Loss for batch is  -2.4117090702056885\n",
      "|Iter  817  | Total Val Loss  -6.587020516395569 |\n",
      "Loss for batch is  0.24672484397888184\n",
      "Loss for batch is  -0.14448320865631104\n",
      "Loss for batch is  0.026876091957092285\n",
      "Loss for batch is  -0.7527743577957153\n",
      "|Iter  818  | Total Train Loss  -0.6236566305160522 |\n",
      "Val Loss for batch is  -1.6156998872756958\n",
      "Val Loss for batch is  -1.8309682607650757\n",
      "Val Loss for batch is  -1.631131649017334\n",
      "Val Loss for batch is  -2.5689780712127686\n",
      "|Iter  818  | Total Val Loss  -7.646777868270874 |\n",
      "Loss for batch is  -0.05733299255371094\n",
      "Loss for batch is  -0.14850878715515137\n",
      "Loss for batch is  0.08055806159973145\n",
      "Loss for batch is  -0.9036315679550171\n",
      "|Iter  819  | Total Train Loss  -1.028915286064148 |\n",
      "Val Loss for batch is  -1.5317410230636597\n",
      "Val Loss for batch is  -1.8262381553649902\n",
      "Val Loss for batch is  -1.642507553100586\n",
      "Val Loss for batch is  -2.5437803268432617\n",
      "|Iter  819  | Total Val Loss  -7.544267058372498 |\n",
      "Loss for batch is  0.04162406921386719\n",
      "Loss for batch is  -0.16768908500671387\n",
      "Loss for batch is  -0.04018282890319824\n",
      "Loss for batch is  -0.9632241725921631\n",
      "|Iter  820  | Total Train Loss  -1.129472017288208 |\n",
      "Val Loss for batch is  -1.6350384950637817\n",
      "Val Loss for batch is  -1.76828134059906\n",
      "Val Loss for batch is  -1.6508346796035767\n",
      "Val Loss for batch is  -2.560771942138672\n",
      "|Iter  820  | Total Val Loss  -7.61492645740509 |\n",
      "Loss for batch is  -0.0710986852645874\n",
      "Loss for batch is  -0.21901631355285645\n",
      "Loss for batch is  -0.1289353370666504\n",
      "Loss for batch is  -0.9892499446868896\n",
      "|Iter  821  | Total Train Loss  -1.4083002805709839 |\n",
      "Val Loss for batch is  -1.7324095964431763\n",
      "Val Loss for batch is  -1.8884862661361694\n",
      "Val Loss for batch is  -1.7365692853927612\n",
      "Val Loss for batch is  -2.597865343093872\n",
      "|Iter  821  | Total Val Loss  -7.955330491065979 |\n",
      "Loss for batch is  -0.08513164520263672\n",
      "Loss for batch is  -0.2926226854324341\n",
      "Loss for batch is  -0.13491332530975342\n",
      "Loss for batch is  -1.0139251947402954\n",
      "|Iter  822  | Total Train Loss  -1.5265928506851196 |\n",
      "Val Loss for batch is  -1.7322602272033691\n",
      "Val Loss for batch is  -1.8844585418701172\n",
      "Val Loss for batch is  -1.7483834028244019\n",
      "Val Loss for batch is  -2.6328604221343994\n",
      "|Iter  822  | Total Val Loss  -7.997962594032288 |\n",
      "Loss for batch is  -0.14378094673156738\n",
      "Loss for batch is  -0.29641783237457275\n",
      "Loss for batch is  -0.14610159397125244\n",
      "Loss for batch is  -1.0159804821014404\n",
      "|Iter  823  | Total Train Loss  -1.602280855178833 |\n",
      "Val Loss for batch is  -1.7886686325073242\n",
      "Val Loss for batch is  -1.9307186603546143\n",
      "Val Loss for batch is  -1.836032748222351\n",
      "Val Loss for batch is  -2.6544203758239746\n",
      "|Iter  823  | Total Val Loss  -8.209840416908264 |\n",
      "Loss for batch is  -0.18575239181518555\n",
      "Loss for batch is  -0.30181610584259033\n",
      "Loss for batch is  -0.14340901374816895\n",
      "Loss for batch is  -1.0759321451187134\n",
      "|Iter  824  | Total Train Loss  -1.7069096565246582 |\n",
      "Val Loss for batch is  -1.741295576095581\n",
      "Val Loss for batch is  -1.8580875396728516\n",
      "Val Loss for batch is  -1.7752150297164917\n",
      "Val Loss for batch is  -2.6320815086364746\n",
      "|Iter  824  | Total Val Loss  -8.006679654121399 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.1488889455795288\n",
      "Loss for batch is  -0.3452650308609009\n",
      "Loss for batch is  -0.17430901527404785\n",
      "Loss for batch is  -1.0778484344482422\n",
      "|Iter  825  | Total Train Loss  -1.7463114261627197 |\n",
      "Val Loss for batch is  -1.8042412996292114\n",
      "Val Loss for batch is  -1.9243274927139282\n",
      "Val Loss for batch is  -1.7289366722106934\n",
      "Val Loss for batch is  -2.718616485595703\n",
      "|Iter  825  | Total Val Loss  -8.176121950149536 |\n",
      "Loss for batch is  -0.22335481643676758\n",
      "Loss for batch is  -0.36442136764526367\n",
      "Loss for batch is  -0.23901081085205078\n",
      "Loss for batch is  -1.1040385961532593\n",
      "|Iter  826  | Total Train Loss  -1.9308255910873413 |\n",
      "Val Loss for batch is  -1.7226065397262573\n",
      "Val Loss for batch is  -1.895853042602539\n",
      "Val Loss for batch is  -1.8176530599594116\n",
      "Val Loss for batch is  -2.7154183387756348\n",
      "|Iter  826  | Total Val Loss  -8.151530981063843 |\n",
      "Loss for batch is  -0.20021426677703857\n",
      "Loss for batch is  -0.3369936943054199\n",
      "Loss for batch is  -0.1998971700668335\n",
      "Loss for batch is  -1.0987721681594849\n",
      "|Iter  827  | Total Train Loss  -1.8358772993087769 |\n",
      "Val Loss for batch is  -1.5682300329208374\n",
      "Val Loss for batch is  -1.8541463613510132\n",
      "Val Loss for batch is  -1.66350257396698\n",
      "Val Loss for batch is  -2.689201593399048\n",
      "|Iter  827  | Total Val Loss  -7.775080561637878 |\n",
      "Loss for batch is  0.006731390953063965\n",
      "Loss for batch is  -0.24362242221832275\n",
      "Loss for batch is  0.0815269947052002\n",
      "Loss for batch is  -1.0930603742599487\n",
      "|Iter  828  | Total Train Loss  -1.2484244108200073 |\n",
      "Val Loss for batch is  -1.1750133037567139\n",
      "Val Loss for batch is  -1.3404123783111572\n",
      "Val Loss for batch is  -1.1447829008102417\n",
      "Val Loss for batch is  -2.3935463428497314\n",
      "|Iter  828  | Total Val Loss  -6.053754925727844 |\n",
      "Loss for batch is  0.331199049949646\n",
      "Loss for batch is  -0.22073662281036377\n",
      "Loss for batch is  0.031792521476745605\n",
      "Loss for batch is  -0.6439213752746582\n",
      "|Iter  829  | Total Train Loss  -0.5016664266586304 |\n",
      "Val Loss for batch is  -1.4534525871276855\n",
      "Val Loss for batch is  -1.7903190851211548\n",
      "Val Loss for batch is  -1.619905948638916\n",
      "Val Loss for batch is  -2.5334088802337646\n",
      "|Iter  829  | Total Val Loss  -7.397086501121521 |\n",
      "Loss for batch is  0.07177722454071045\n",
      "Loss for batch is  -0.1316521167755127\n",
      "Loss for batch is  0.29362952709198\n",
      "Loss for batch is  -0.9179142713546753\n",
      "|Iter  830  | Total Train Loss  -0.6841596364974976 |\n",
      "Val Loss for batch is  -1.4542713165283203\n",
      "Val Loss for batch is  -1.6480326652526855\n",
      "Val Loss for batch is  -1.550178050994873\n",
      "Val Loss for batch is  -2.4757730960845947\n",
      "|Iter  830  | Total Val Loss  -7.128255128860474 |\n",
      "Loss for batch is  0.08253896236419678\n",
      "Loss for batch is  -0.08337080478668213\n",
      "Loss for batch is  -0.0011622905731201172\n",
      "Loss for batch is  -0.9092793464660645\n",
      "|Iter  831  | Total Train Loss  -0.9112734794616699 |\n",
      "Val Loss for batch is  -1.5553967952728271\n",
      "Val Loss for batch is  -1.839646339416504\n",
      "Val Loss for batch is  -1.5923027992248535\n",
      "Val Loss for batch is  -2.5329854488372803\n",
      "|Iter  831  | Total Val Loss  -7.520331382751465 |\n",
      "Loss for batch is  0.01368093490600586\n",
      "Loss for batch is  -0.2165447473526001\n",
      "Loss for batch is  -0.037638187408447266\n",
      "Loss for batch is  -0.9507005214691162\n",
      "|Iter  832  | Total Train Loss  -1.1912025213241577 |\n",
      "Val Loss for batch is  -1.603209376335144\n",
      "Val Loss for batch is  -1.7514939308166504\n",
      "Val Loss for batch is  -1.6222026348114014\n",
      "Val Loss for batch is  -2.5296545028686523\n",
      "|Iter  832  | Total Val Loss  -7.506560444831848 |\n",
      "Loss for batch is  0.01029360294342041\n",
      "Loss for batch is  -0.2026829719543457\n",
      "Loss for batch is  -0.12047314643859863\n",
      "Loss for batch is  -0.9707285165786743\n",
      "|Iter  833  | Total Train Loss  -1.2835910320281982 |\n",
      "Val Loss for batch is  -1.7897013425827026\n",
      "Val Loss for batch is  -1.908644437789917\n",
      "Val Loss for batch is  -1.7734144926071167\n",
      "Val Loss for batch is  -2.578646421432495\n",
      "|Iter  833  | Total Val Loss  -8.050406694412231 |\n",
      "Loss for batch is  -0.10304737091064453\n",
      "Loss for batch is  -0.2518744468688965\n",
      "Loss for batch is  -0.1687328815460205\n",
      "Loss for batch is  -1.01865816116333\n",
      "|Iter  834  | Total Train Loss  -1.5423128604888916 |\n",
      "Val Loss for batch is  -1.7895841598510742\n",
      "Val Loss for batch is  -1.9355953931808472\n",
      "Val Loss for batch is  -1.803021788597107\n",
      "Val Loss for batch is  -2.6575887203216553\n",
      "|Iter  834  | Total Val Loss  -8.185790061950684 |\n",
      "Loss for batch is  -0.16555094718933105\n",
      "Loss for batch is  -0.3151576519012451\n",
      "Loss for batch is  -0.13269197940826416\n",
      "Loss for batch is  -1.085418939590454\n",
      "|Iter  835  | Total Train Loss  -1.6988195180892944 |\n",
      "Val Loss for batch is  -1.780742883682251\n",
      "Val Loss for batch is  -1.9442381858825684\n",
      "Val Loss for batch is  -1.8362891674041748\n",
      "Val Loss for batch is  -2.6950647830963135\n",
      "|Iter  835  | Total Val Loss  -8.256335020065308 |\n",
      "Loss for batch is  -0.1574077606201172\n",
      "Loss for batch is  -0.3239666223526001\n",
      "Loss for batch is  -0.2199951410293579\n",
      "Loss for batch is  -1.1108514070510864\n",
      "|Iter  836  | Total Train Loss  -1.8122209310531616 |\n",
      "Val Loss for batch is  -1.7815666198730469\n",
      "Val Loss for batch is  -1.940911889076233\n",
      "Val Loss for batch is  -1.8071081638336182\n",
      "Val Loss for batch is  -2.7105278968811035\n",
      "|Iter  836  | Total Val Loss  -8.240114569664001 |\n",
      "Loss for batch is  -0.21563804149627686\n",
      "Loss for batch is  -0.37519371509552\n",
      "Loss for batch is  -0.22794175148010254\n",
      "Loss for batch is  -1.1350207328796387\n",
      "|Iter  837  | Total Train Loss  -1.953794240951538 |\n",
      "Val Loss for batch is  -1.7574080228805542\n",
      "Val Loss for batch is  -1.9567129611968994\n",
      "Val Loss for batch is  -1.8445693254470825\n",
      "Val Loss for batch is  -2.7284042835235596\n",
      "|Iter  837  | Total Val Loss  -8.287094593048096 |\n",
      "Loss for batch is  -0.20334947109222412\n",
      "Loss for batch is  -0.38200128078460693\n",
      "Loss for batch is  -0.18985772132873535\n",
      "Loss for batch is  -1.1565054655075073\n",
      "|Iter  838  | Total Train Loss  -1.9317139387130737 |\n",
      "Val Loss for batch is  -1.6845213174819946\n",
      "Val Loss for batch is  -1.7684822082519531\n",
      "Val Loss for batch is  -1.717637300491333\n",
      "Val Loss for batch is  -2.6675491333007812\n",
      "|Iter  838  | Total Val Loss  -7.838189959526062 |\n",
      "Loss for batch is  -0.12641918659210205\n",
      "Loss for batch is  -0.4189634323120117\n",
      "Loss for batch is  -0.11946976184844971\n",
      "Loss for batch is  -1.1707627773284912\n",
      "|Iter  839  | Total Train Loss  -1.8356151580810547 |\n",
      "Val Loss for batch is  -1.5634106397628784\n",
      "Val Loss for batch is  -1.7987110614776611\n",
      "Val Loss for batch is  -1.6927576065063477\n",
      "Val Loss for batch is  -2.6743059158325195\n",
      "|Iter  839  | Total Val Loss  -7.729185223579407 |\n",
      "Loss for batch is  -0.0983816385269165\n",
      "Loss for batch is  -0.39345574378967285\n",
      "Loss for batch is  -0.1537081003189087\n",
      "Loss for batch is  -1.0665197372436523\n",
      "|Iter  840  | Total Train Loss  -1.7120652198791504 |\n",
      "Val Loss for batch is  -1.693471908569336\n",
      "Val Loss for batch is  -1.9357138872146606\n",
      "Val Loss for batch is  -1.6676386594772339\n",
      "Val Loss for batch is  -2.7246556282043457\n",
      "|Iter  840  | Total Val Loss  -8.021480083465576 |\n",
      "Loss for batch is  -0.2339845895767212\n",
      "Loss for batch is  -0.18808519840240479\n",
      "Loss for batch is  -0.28728604316711426\n",
      "Loss for batch is  -1.0172029733657837\n",
      "|Iter  841  | Total Train Loss  -1.726558804512024 |\n",
      "Val Loss for batch is  -1.4570788145065308\n",
      "Val Loss for batch is  -1.767392873764038\n",
      "Val Loss for batch is  -1.7142257690429688\n",
      "Val Loss for batch is  -2.6707603931427\n",
      "|Iter  841  | Total Val Loss  -7.609457850456238 |\n",
      "Loss for batch is  0.029790282249450684\n",
      "Loss for batch is  -0.240250825881958\n",
      "Loss for batch is  0.2016986608505249\n",
      "Loss for batch is  -0.9998384714126587\n",
      "|Iter  842  | Total Train Loss  -1.0086003541946411 |\n",
      "Val Loss for batch is  -1.3287973403930664\n",
      "Val Loss for batch is  -1.4518661499023438\n",
      "Val Loss for batch is  -1.3387209177017212\n",
      "Val Loss for batch is  -2.431072950363159\n",
      "|Iter  842  | Total Val Loss  -6.5504573583602905 |\n",
      "Loss for batch is  0.21202123165130615\n",
      "Loss for batch is  -0.02839529514312744\n",
      "Loss for batch is  0.25737428665161133\n",
      "Loss for batch is  -0.9179122447967529\n",
      "|Iter  843  | Total Train Loss  -0.4769120216369629 |\n",
      "Val Loss for batch is  -1.3444545269012451\n",
      "Val Loss for batch is  -1.546656847000122\n",
      "Val Loss for batch is  -1.4121735095977783\n",
      "Val Loss for batch is  -2.3991570472717285\n",
      "|Iter  843  | Total Val Loss  -6.702441930770874 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.1370488405227661\n",
      "Loss for batch is  0.008652567863464355\n",
      "Loss for batch is  -0.0019254684448242188\n",
      "Loss for batch is  -0.8989660739898682\n",
      "|Iter  844  | Total Train Loss  -0.7551901340484619 |\n",
      "Val Loss for batch is  -1.568131446838379\n",
      "Val Loss for batch is  -1.7673410177230835\n",
      "Val Loss for batch is  -1.5517371892929077\n",
      "Val Loss for batch is  -2.4688878059387207\n",
      "|Iter  844  | Total Val Loss  -7.356097459793091 |\n",
      "Loss for batch is  0.08162915706634521\n",
      "Loss for batch is  -0.18527591228485107\n",
      "Loss for batch is  -0.04932546615600586\n",
      "Loss for batch is  -0.9159698486328125\n",
      "|Iter  845  | Total Train Loss  -1.0689420700073242 |\n",
      "Val Loss for batch is  -1.4991337060928345\n",
      "Val Loss for batch is  -1.6638870239257812\n",
      "Val Loss for batch is  -1.5148924589157104\n",
      "Val Loss for batch is  -2.495243787765503\n",
      "|Iter  845  | Total Val Loss  -7.173156976699829 |\n",
      "Loss for batch is  0.04181241989135742\n",
      "Loss for batch is  -0.2060394287109375\n",
      "Loss for batch is  -0.07062757015228271\n",
      "Loss for batch is  -0.9379836320877075\n",
      "|Iter  846  | Total Train Loss  -1.1728382110595703 |\n",
      "Val Loss for batch is  -1.7380152940750122\n",
      "Val Loss for batch is  -1.8839616775512695\n",
      "Val Loss for batch is  -1.729272484779358\n",
      "Val Loss for batch is  -2.592993974685669\n",
      "|Iter  846  | Total Val Loss  -7.944243431091309 |\n",
      "Loss for batch is  -0.12263381481170654\n",
      "Loss for batch is  -0.25381290912628174\n",
      "Loss for batch is  -0.12941455841064453\n",
      "Loss for batch is  -0.9745930433273315\n",
      "|Iter  847  | Total Train Loss  -1.4804543256759644 |\n",
      "Val Loss for batch is  -1.7294676303863525\n",
      "Val Loss for batch is  -1.8614729642868042\n",
      "Val Loss for batch is  -1.727980375289917\n",
      "Val Loss for batch is  -2.605903148651123\n",
      "|Iter  847  | Total Val Loss  -7.924824118614197 |\n",
      "Loss for batch is  -0.14751434326171875\n",
      "Loss for batch is  -0.3075544834136963\n",
      "Loss for batch is  -0.15336251258850098\n",
      "Loss for batch is  -1.0342391729354858\n",
      "|Iter  848  | Total Train Loss  -1.6426705121994019 |\n",
      "Val Loss for batch is  -1.7311457395553589\n",
      "Val Loss for batch is  -1.9203925132751465\n",
      "Val Loss for batch is  -1.853477954864502\n",
      "Val Loss for batch is  -2.630180597305298\n",
      "|Iter  848  | Total Val Loss  -8.135196805000305 |\n",
      "Loss for batch is  -0.19392871856689453\n",
      "Loss for batch is  -0.32460248470306396\n",
      "Loss for batch is  -0.20736241340637207\n",
      "Loss for batch is  -1.0894203186035156\n",
      "|Iter  849  | Total Train Loss  -1.8153139352798462 |\n",
      "Val Loss for batch is  -1.8052902221679688\n",
      "Val Loss for batch is  -1.9539293050765991\n",
      "Val Loss for batch is  -1.6941368579864502\n",
      "Val Loss for batch is  -2.6904473304748535\n",
      "|Iter  849  | Total Val Loss  -8.143803715705872 |\n",
      "Loss for batch is  -0.24859607219696045\n",
      "Loss for batch is  -0.36616766452789307\n",
      "Loss for batch is  -0.2225130796432495\n",
      "Loss for batch is  -1.1245923042297363\n",
      "|Iter  850  | Total Train Loss  -1.9618691205978394 |\n",
      "Val Loss for batch is  -1.7492252588272095\n",
      "Val Loss for batch is  -1.9643446207046509\n",
      "Val Loss for batch is  -1.8531416654586792\n",
      "Val Loss for batch is  -2.755861759185791\n",
      "|Iter  850  | Total Val Loss  -8.32257330417633 |\n",
      "Loss for batch is  -0.24244248867034912\n",
      "Loss for batch is  -0.4029388427734375\n",
      "Loss for batch is  -0.2748394012451172\n",
      "Loss for batch is  -1.1575511693954468\n",
      "|Iter  851  | Total Train Loss  -2.0777719020843506 |\n",
      "Val Loss for batch is  -1.7687339782714844\n",
      "Val Loss for batch is  -1.9746298789978027\n",
      "Val Loss for batch is  -1.8291376829147339\n",
      "Val Loss for batch is  -2.7744216918945312\n",
      "|Iter  851  | Total Val Loss  -8.346923232078552 |\n",
      "Loss for batch is  -0.2649354934692383\n",
      "Loss for batch is  -0.43347764015197754\n",
      "Loss for batch is  -0.2805454730987549\n",
      "Loss for batch is  -1.2095601558685303\n",
      "|Iter  852  | Total Train Loss  -2.188518762588501 |\n",
      "Val Loss for batch is  -1.7852132320404053\n",
      "Val Loss for batch is  -1.9715261459350586\n",
      "Val Loss for batch is  -1.8840250968933105\n",
      "Val Loss for batch is  -2.803542137145996\n",
      "|Iter  852  | Total Val Loss  -8.44430661201477 |\n",
      "Loss for batch is  -0.2622098922729492\n",
      "Loss for batch is  -0.44850456714630127\n",
      "Loss for batch is  -0.28777194023132324\n",
      "Loss for batch is  -1.218207836151123\n",
      "|Iter  853  | Total Train Loss  -2.2166942358016968 |\n",
      "Val Loss for batch is  -1.7442905902862549\n",
      "Val Loss for batch is  -1.9914556741714478\n",
      "Val Loss for batch is  -1.8424785137176514\n",
      "Val Loss for batch is  -2.7680349349975586\n",
      "|Iter  853  | Total Val Loss  -8.346259713172913 |\n",
      "Loss for batch is  -0.22486484050750732\n",
      "Loss for batch is  -0.38771867752075195\n",
      "Loss for batch is  -0.2703768014907837\n",
      "Loss for batch is  -1.144986629486084\n",
      "|Iter  854  | Total Train Loss  -2.027946949005127 |\n",
      "Val Loss for batch is  -1.6345341205596924\n",
      "Val Loss for batch is  -1.817888617515564\n",
      "Val Loss for batch is  -1.6781481504440308\n",
      "Val Loss for batch is  -2.7494211196899414\n",
      "|Iter  854  | Total Val Loss  -7.8799920082092285 |\n",
      "Loss for batch is  -0.13609588146209717\n",
      "Loss for batch is  -0.2668269872665405\n",
      "Loss for batch is  -0.07595372200012207\n",
      "Loss for batch is  -1.091138243675232\n",
      "|Iter  855  | Total Train Loss  -1.5700148344039917 |\n",
      "Val Loss for batch is  -1.0666921138763428\n",
      "Val Loss for batch is  -1.1128394603729248\n",
      "Val Loss for batch is  -0.9123334884643555\n",
      "Val Loss for batch is  -2.482949733734131\n",
      "|Iter  855  | Total Val Loss  -5.574814796447754 |\n",
      "Loss for batch is  0.46883541345596313\n",
      "Loss for batch is  -0.3908870220184326\n",
      "Loss for batch is  0.5540292859077454\n",
      "Loss for batch is  -0.7839630842208862\n",
      "|Iter  856  | Total Train Loss  -0.15198540687561035 |\n",
      "Val Loss for batch is  -1.3839083909988403\n",
      "Val Loss for batch is  -1.6225730180740356\n",
      "Val Loss for batch is  -1.5030337572097778\n",
      "Val Loss for batch is  -2.564427614212036\n",
      "|Iter  856  | Total Val Loss  -7.07394278049469 |\n",
      "Loss for batch is  0.053124070167541504\n",
      "Loss for batch is  0.37981557846069336\n",
      "Loss for batch is  0.042139530181884766\n",
      "Loss for batch is  -0.941929817199707\n",
      "|Iter  857  | Total Train Loss  -0.4668506383895874 |\n",
      "Val Loss for batch is  -1.2518737316131592\n",
      "Val Loss for batch is  -1.6492403745651245\n",
      "Val Loss for batch is  -1.400202751159668\n",
      "Val Loss for batch is  -2.3689873218536377\n",
      "|Iter  857  | Total Val Loss  -6.670304179191589 |\n",
      "Loss for batch is  0.25001347064971924\n",
      "Loss for batch is  -0.1724541187286377\n",
      "Loss for batch is  -0.044675588607788086\n",
      "Loss for batch is  -0.8576252460479736\n",
      "|Iter  858  | Total Train Loss  -0.8247414827346802 |\n",
      "Val Loss for batch is  -1.3543603420257568\n",
      "Val Loss for batch is  -1.7251800298690796\n",
      "Val Loss for batch is  -1.6117373704910278\n",
      "Val Loss for batch is  -2.4434335231781006\n",
      "|Iter  858  | Total Val Loss  -7.134711265563965 |\n",
      "Loss for batch is  0.18838262557983398\n",
      "Loss for batch is  -0.162148118019104\n",
      "Loss for batch is  0.010620713233947754\n",
      "Loss for batch is  -0.8804721832275391\n",
      "|Iter  859  | Total Train Loss  -0.8436169624328613 |\n",
      "Val Loss for batch is  -1.6682801246643066\n",
      "Val Loss for batch is  -1.8659522533416748\n",
      "Val Loss for batch is  -1.6980880498886108\n",
      "Val Loss for batch is  -2.5208852291107178\n",
      "|Iter  859  | Total Val Loss  -7.75320565700531 |\n",
      "Loss for batch is  -0.07974040508270264\n",
      "Loss for batch is  -0.23060357570648193\n",
      "Loss for batch is  -0.0743330717086792\n",
      "Loss for batch is  -0.8721967935562134\n",
      "|Iter  860  | Total Train Loss  -1.2568738460540771 |\n",
      "Val Loss for batch is  -1.7320187091827393\n",
      "Val Loss for batch is  -1.853646993637085\n",
      "Val Loss for batch is  -1.7506681680679321\n",
      "Val Loss for batch is  -2.536323308944702\n",
      "|Iter  860  | Total Val Loss  -7.8726571798324585 |\n",
      "Loss for batch is  -0.07406806945800781\n",
      "Loss for batch is  -0.25433969497680664\n",
      "Loss for batch is  -0.12833154201507568\n",
      "Loss for batch is  -0.973405122756958\n",
      "|Iter  861  | Total Train Loss  -1.4301444292068481 |\n",
      "Val Loss for batch is  -1.7452079057693481\n",
      "Val Loss for batch is  -1.862910270690918\n",
      "Val Loss for batch is  -1.7861251831054688\n",
      "Val Loss for batch is  -2.5777761936187744\n",
      "|Iter  861  | Total Val Loss  -7.972019553184509 |\n",
      "Loss for batch is  -0.1615447998046875\n",
      "Loss for batch is  -0.2635082006454468\n",
      "Loss for batch is  -0.151533842086792\n",
      "Loss for batch is  -1.0278308391571045\n",
      "|Iter  862  | Total Train Loss  -1.6044176816940308 |\n",
      "Val Loss for batch is  -1.8387467861175537\n",
      "Val Loss for batch is  -1.9442005157470703\n",
      "Val Loss for batch is  -1.8591781854629517\n",
      "Val Loss for batch is  -2.6532444953918457\n",
      "|Iter  862  | Total Val Loss  -8.295369982719421 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.21376240253448486\n",
      "Loss for batch is  -0.32268214225769043\n",
      "Loss for batch is  -0.19333148002624512\n",
      "Loss for batch is  -1.0571209192276\n",
      "|Iter  863  | Total Train Loss  -1.7868969440460205 |\n",
      "Val Loss for batch is  -1.813787817955017\n",
      "Val Loss for batch is  -1.9233851432800293\n",
      "Val Loss for batch is  -1.8645169734954834\n",
      "Val Loss for batch is  -2.663785457611084\n",
      "|Iter  863  | Total Val Loss  -8.265475392341614 |\n",
      "Loss for batch is  -0.2474193572998047\n",
      "Loss for batch is  -0.3368713855743408\n",
      "Loss for batch is  -0.2508782148361206\n",
      "Loss for batch is  -1.1274464130401611\n",
      "|Iter  864  | Total Train Loss  -1.9626153707504272 |\n",
      "Val Loss for batch is  -1.7919732332229614\n",
      "Val Loss for batch is  -1.9625126123428345\n",
      "Val Loss for batch is  -1.8573274612426758\n",
      "Val Loss for batch is  -2.71124005317688\n",
      "|Iter  864  | Total Val Loss  -8.323053359985352 |\n",
      "Loss for batch is  -0.27495288848876953\n",
      "Loss for batch is  -0.38729798793792725\n",
      "Loss for batch is  -0.2751697301864624\n",
      "Loss for batch is  -1.183935523033142\n",
      "|Iter  865  | Total Train Loss  -2.1213561296463013 |\n",
      "Val Loss for batch is  -1.798831820487976\n",
      "Val Loss for batch is  -1.9686318635940552\n",
      "Val Loss for batch is  -1.8418365716934204\n",
      "Val Loss for batch is  -2.753692626953125\n",
      "|Iter  865  | Total Val Loss  -8.362992882728577 |\n",
      "Loss for batch is  -0.28646743297576904\n",
      "Loss for batch is  -0.4397233724594116\n",
      "Loss for batch is  -0.3054388761520386\n",
      "Loss for batch is  -1.1961421966552734\n",
      "|Iter  866  | Total Train Loss  -2.2277718782424927 |\n",
      "Val Loss for batch is  -1.8017363548278809\n",
      "Val Loss for batch is  -1.9632127285003662\n",
      "Val Loss for batch is  -1.877903699874878\n",
      "Val Loss for batch is  -2.785140037536621\n",
      "|Iter  866  | Total Val Loss  -8.427992820739746 |\n",
      "Loss for batch is  -0.2787024974822998\n",
      "Loss for batch is  -0.447147011756897\n",
      "Loss for batch is  -0.32451677322387695\n",
      "Loss for batch is  -1.2129476070404053\n",
      "|Iter  867  | Total Train Loss  -2.263313889503479 |\n",
      "Val Loss for batch is  -1.7752934694290161\n",
      "Val Loss for batch is  -1.9033427238464355\n",
      "Val Loss for batch is  -1.7842962741851807\n",
      "Val Loss for batch is  -2.7831239700317383\n",
      "|Iter  867  | Total Val Loss  -8.24605643749237 |\n",
      "Loss for batch is  -0.2755160331726074\n",
      "Loss for batch is  -0.4395103454589844\n",
      "Loss for batch is  -0.2697333097457886\n",
      "Loss for batch is  -1.214545726776123\n",
      "|Iter  868  | Total Train Loss  -2.1993054151535034 |\n",
      "Val Loss for batch is  -1.5435199737548828\n",
      "Val Loss for batch is  -1.8099820613861084\n",
      "Val Loss for batch is  -1.5235111713409424\n",
      "Val Loss for batch is  -2.674243688583374\n",
      "|Iter  868  | Total Val Loss  -7.551256895065308 |\n",
      "Loss for batch is  -0.07896566390991211\n",
      "Loss for batch is  -0.46740829944610596\n",
      "Loss for batch is  -0.19308829307556152\n",
      "Loss for batch is  -1.1977927684783936\n",
      "|Iter  869  | Total Train Loss  -1.9372550249099731 |\n",
      "Val Loss for batch is  -1.5401568412780762\n",
      "Val Loss for batch is  -1.572780966758728\n",
      "Val Loss for batch is  -1.2249579429626465\n",
      "Val Loss for batch is  -2.6405038833618164\n",
      "|Iter  869  | Total Val Loss  -6.978399634361267 |\n",
      "Loss for batch is  -0.06336987018585205\n",
      "Loss for batch is  -0.3312418460845947\n",
      "Loss for batch is  -0.22576475143432617\n",
      "Loss for batch is  -0.8483424186706543\n",
      "|Iter  870  | Total Train Loss  -1.4687188863754272 |\n",
      "Val Loss for batch is  -1.6571085453033447\n",
      "Val Loss for batch is  -1.9163875579833984\n",
      "Val Loss for batch is  -1.7677496671676636\n",
      "Val Loss for batch is  -2.7781167030334473\n",
      "|Iter  870  | Total Val Loss  -8.119362473487854 |\n",
      "Loss for batch is  -0.16185498237609863\n",
      "Loss for batch is  -0.04196369647979736\n",
      "Loss for batch is  0.27443361282348633\n",
      "Loss for batch is  -0.9570223093032837\n",
      "|Iter  871  | Total Train Loss  -0.8864073753356934 |\n",
      "Val Loss for batch is  -1.0590288639068604\n",
      "Val Loss for batch is  -1.225376009941101\n",
      "Val Loss for batch is  -1.0124760866165161\n",
      "Val Loss for batch is  -2.237104654312134\n",
      "|Iter  871  | Total Val Loss  -5.533985614776611 |\n",
      "Loss for batch is  0.5229965448379517\n",
      "Loss for batch is  -0.08227324485778809\n",
      "Loss for batch is  0.6956787109375\n",
      "Loss for batch is  -0.5863031148910522\n",
      "|Iter  872  | Total Train Loss  0.5500988960266113 |\n",
      "Val Loss for batch is  -1.0886163711547852\n",
      "Val Loss for batch is  -1.3003169298171997\n",
      "Val Loss for batch is  -1.2064160108566284\n",
      "Val Loss for batch is  -2.2920572757720947\n",
      "|Iter  872  | Total Val Loss  -5.887406587600708 |\n",
      "Loss for batch is  0.4537578821182251\n",
      "Loss for batch is  0.307561993598938\n",
      "Loss for batch is  0.13982057571411133\n",
      "Loss for batch is  -0.7913999557495117\n",
      "|Iter  873  | Total Train Loss  0.1097404956817627 |\n",
      "Val Loss for batch is  -1.4026354551315308\n",
      "Val Loss for batch is  -1.6007603406906128\n",
      "Val Loss for batch is  -1.4670886993408203\n",
      "Val Loss for batch is  -2.4070897102355957\n",
      "|Iter  873  | Total Val Loss  -6.87757420539856 |\n",
      "Loss for batch is  0.24512112140655518\n",
      "Loss for batch is  -0.0021532773971557617\n",
      "Loss for batch is  0.29091548919677734\n",
      "Loss for batch is  -0.7727639675140381\n",
      "|Iter  874  | Total Train Loss  -0.23888063430786133 |\n",
      "Val Loss for batch is  -1.5916566848754883\n",
      "Val Loss for batch is  -1.6986441612243652\n",
      "Val Loss for batch is  -1.4824672937393188\n",
      "Val Loss for batch is  -2.439607858657837\n",
      "|Iter  874  | Total Val Loss  -7.212375998497009 |\n",
      "Loss for batch is  0.10563218593597412\n",
      "Loss for batch is  -0.017801642417907715\n",
      "Loss for batch is  0.0673590898513794\n",
      "Loss for batch is  -0.8754498958587646\n",
      "|Iter  875  | Total Train Loss  -0.7202602624893188 |\n",
      "Val Loss for batch is  -1.6979552507400513\n",
      "Val Loss for batch is  -1.7462050914764404\n",
      "Val Loss for batch is  -1.6454566717147827\n",
      "Val Loss for batch is  -2.473773241043091\n",
      "|Iter  875  | Total Val Loss  -7.563390254974365 |\n",
      "Loss for batch is  0.022365927696228027\n",
      "Loss for batch is  -0.05528759956359863\n",
      "Loss for batch is  -0.0029507875442504883\n",
      "Loss for batch is  -0.9063106775283813\n",
      "|Iter  876  | Total Train Loss  -0.9421831369400024 |\n",
      "Val Loss for batch is  -1.7664529085159302\n",
      "Val Loss for batch is  -1.8390940427780151\n",
      "Val Loss for batch is  -1.6930809020996094\n",
      "Val Loss for batch is  -2.5044381618499756\n",
      "|Iter  876  | Total Val Loss  -7.80306601524353 |\n",
      "Loss for batch is  -0.08061027526855469\n",
      "Loss for batch is  -0.16150712966918945\n",
      "Loss for batch is  -0.01034843921661377\n",
      "Loss for batch is  -0.9650630950927734\n",
      "|Iter  877  | Total Train Loss  -1.2175289392471313 |\n",
      "Val Loss for batch is  -1.8164595365524292\n",
      "Val Loss for batch is  -1.912305235862732\n",
      "Val Loss for batch is  -1.799336314201355\n",
      "Val Loss for batch is  -2.5890846252441406\n",
      "|Iter  877  | Total Val Loss  -8.117185711860657 |\n",
      "Loss for batch is  -0.12674975395202637\n",
      "Loss for batch is  -0.2470085620880127\n",
      "Loss for batch is  -0.11110055446624756\n",
      "Loss for batch is  -1.0218284130096436\n",
      "|Iter  878  | Total Train Loss  -1.5066872835159302 |\n",
      "Val Loss for batch is  -1.82244074344635\n",
      "Val Loss for batch is  -1.912160038948059\n",
      "Val Loss for batch is  -1.7870030403137207\n",
      "Val Loss for batch is  -2.6316099166870117\n",
      "|Iter  878  | Total Val Loss  -8.153213739395142 |\n",
      "Loss for batch is  -0.16332077980041504\n",
      "Loss for batch is  -0.28722941875457764\n",
      "Loss for batch is  -0.15830373764038086\n",
      "Loss for batch is  -1.080607533454895\n",
      "|Iter  879  | Total Train Loss  -1.6894614696502686 |\n",
      "Val Loss for batch is  -1.7591938972473145\n",
      "Val Loss for batch is  -1.9750648736953735\n",
      "Val Loss for batch is  -1.823549747467041\n",
      "Val Loss for batch is  -2.6749730110168457\n",
      "|Iter  879  | Total Val Loss  -8.232781529426575 |\n",
      "Loss for batch is  -0.2211369276046753\n",
      "Loss for batch is  -0.3519721031188965\n",
      "Loss for batch is  -0.24158883094787598\n",
      "Loss for batch is  -1.1477383375167847\n",
      "|Iter  880  | Total Train Loss  -1.9624361991882324 |\n",
      "Val Loss for batch is  -1.8573603630065918\n",
      "Val Loss for batch is  -1.9980216026306152\n",
      "Val Loss for batch is  -1.861202359199524\n",
      "Val Loss for batch is  -2.740340232849121\n",
      "|Iter  880  | Total Val Loss  -8.456924557685852 |\n",
      "Loss for batch is  -0.2557671070098877\n",
      "Loss for batch is  -0.3858766555786133\n",
      "Loss for batch is  -0.257199764251709\n",
      "Loss for batch is  -1.2032339572906494\n",
      "|Iter  881  | Total Train Loss  -2.1020774841308594 |\n",
      "Val Loss for batch is  -1.7937124967575073\n",
      "Val Loss for batch is  -2.0173728466033936\n",
      "Val Loss for batch is  -1.868149757385254\n",
      "Val Loss for batch is  -2.7530980110168457\n",
      "|Iter  881  | Total Val Loss  -8.432333111763 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.2523273229598999\n",
      "Loss for batch is  -0.421683669090271\n",
      "Loss for batch is  -0.2824360132217407\n",
      "Loss for batch is  -1.2229127883911133\n",
      "|Iter  882  | Total Train Loss  -2.179359793663025 |\n",
      "Val Loss for batch is  -1.8604662418365479\n",
      "Val Loss for batch is  -2.010230302810669\n",
      "Val Loss for batch is  -1.8591828346252441\n",
      "Val Loss for batch is  -2.7895781993865967\n",
      "|Iter  882  | Total Val Loss  -8.519457578659058 |\n",
      "Loss for batch is  -0.29295945167541504\n",
      "Loss for batch is  -0.4399416446685791\n",
      "Loss for batch is  -0.3186624050140381\n",
      "Loss for batch is  -1.2272385358810425\n",
      "|Iter  883  | Total Train Loss  -2.2788020372390747 |\n",
      "Val Loss for batch is  -1.7648718357086182\n",
      "Val Loss for batch is  -1.9150882959365845\n",
      "Val Loss for batch is  -1.7492092847824097\n",
      "Val Loss for batch is  -2.7886035442352295\n",
      "|Iter  883  | Total Val Loss  -8.217772960662842 |\n",
      "Loss for batch is  -0.23723602294921875\n",
      "Loss for batch is  -0.40871214866638184\n",
      "Loss for batch is  -0.2718461751937866\n",
      "Loss for batch is  -1.2004772424697876\n",
      "|Iter  884  | Total Train Loss  -2.118271589279175 |\n",
      "Val Loss for batch is  -1.3346641063690186\n",
      "Val Loss for batch is  -0.3893807530403137\n",
      "Val Loss for batch is  0.35610657930374146\n",
      "Val Loss for batch is  -2.6821846961975098\n",
      "|Iter  884  | Total Val Loss  -4.050122976303101 |\n",
      "Loss for batch is  0.07889080047607422\n",
      "Loss for batch is  -0.1750185489654541\n",
      "Loss for batch is  0.06275820732116699\n",
      "Loss for batch is  -0.9666190147399902\n",
      "|Iter  885  | Total Train Loss  -0.9999885559082031 |\n",
      "Val Loss for batch is  -1.353508710861206\n",
      "Val Loss for batch is  -1.3402296304702759\n",
      "Val Loss for batch is  -1.371637225151062\n",
      "Val Loss for batch is  -2.5294902324676514\n",
      "|Iter  885  | Total Val Loss  -6.594865798950195 |\n",
      "Loss for batch is  0.10022008419036865\n",
      "Loss for batch is  -0.21357762813568115\n",
      "Loss for batch is  -0.09507131576538086\n",
      "Loss for batch is  -0.8708629608154297\n",
      "|Iter  886  | Total Train Loss  -1.079291820526123 |\n",
      "Val Loss for batch is  -1.3200498819351196\n",
      "Val Loss for batch is  -1.805925965309143\n",
      "Val Loss for batch is  -1.4179576635360718\n",
      "Val Loss for batch is  -2.5737297534942627\n",
      "|Iter  886  | Total Val Loss  -7.117663264274597 |\n",
      "Loss for batch is  0.1485956907272339\n",
      "Loss for batch is  -0.2457897663116455\n",
      "Loss for batch is  0.061614274978637695\n",
      "Loss for batch is  -0.9835399389266968\n",
      "|Iter  887  | Total Train Loss  -1.0191197395324707 |\n",
      "Val Loss for batch is  -1.4972809553146362\n",
      "Val Loss for batch is  -1.6811528205871582\n",
      "Val Loss for batch is  -1.5142879486083984\n",
      "Val Loss for batch is  -2.5398378372192383\n",
      "|Iter  887  | Total Val Loss  -7.232559561729431 |\n",
      "Loss for batch is  0.05527496337890625\n",
      "Loss for batch is  -0.06776070594787598\n",
      "Loss for batch is  0.027281880378723145\n",
      "Loss for batch is  -0.9627732038497925\n",
      "|Iter  888  | Total Train Loss  -0.9479770660400391 |\n",
      "Val Loss for batch is  -1.61805260181427\n",
      "Val Loss for batch is  -1.7886204719543457\n",
      "Val Loss for batch is  -1.6611990928649902\n",
      "Val Loss for batch is  -2.5038275718688965\n",
      "|Iter  888  | Total Val Loss  -7.571699738502502 |\n",
      "Loss for batch is  -0.025734663009643555\n",
      "Loss for batch is  -0.24122166633605957\n",
      "Loss for batch is  -0.09022438526153564\n",
      "Loss for batch is  -0.9393757581710815\n",
      "|Iter  889  | Total Train Loss  -1.2965564727783203 |\n",
      "Val Loss for batch is  -1.776995062828064\n",
      "Val Loss for batch is  -1.9497027397155762\n",
      "Val Loss for batch is  -1.7590088844299316\n",
      "Val Loss for batch is  -2.5210235118865967\n",
      "|Iter  889  | Total Val Loss  -8.006730198860168 |\n",
      "Loss for batch is  -0.1202918291091919\n",
      "Loss for batch is  -0.30047333240509033\n",
      "Loss for batch is  -0.17410540580749512\n",
      "Loss for batch is  -0.977257490158081\n",
      "|Iter  890  | Total Train Loss  -1.5721280574798584 |\n",
      "Val Loss for batch is  -1.7622052431106567\n",
      "Val Loss for batch is  -1.9052752256393433\n",
      "Val Loss for batch is  -1.7769451141357422\n",
      "Val Loss for batch is  -2.5477890968322754\n",
      "|Iter  890  | Total Val Loss  -7.992214679718018 |\n",
      "Loss for batch is  -0.17741048336029053\n",
      "Loss for batch is  -0.3260878324508667\n",
      "Loss for batch is  -0.22221601009368896\n",
      "Loss for batch is  -1.0398160219192505\n",
      "|Iter  891  | Total Train Loss  -1.7655303478240967 |\n",
      "Val Loss for batch is  -1.8349957466125488\n",
      "Val Loss for batch is  -1.9074887037277222\n",
      "Val Loss for batch is  -1.8676705360412598\n",
      "Val Loss for batch is  -2.6445677280426025\n",
      "|Iter  891  | Total Val Loss  -8.254722714424133 |\n",
      "Loss for batch is  -0.23312699794769287\n",
      "Loss for batch is  -0.34671318531036377\n",
      "Loss for batch is  -0.26046788692474365\n",
      "Loss for batch is  -1.1071921586990356\n",
      "|Iter  892  | Total Train Loss  -1.947500228881836 |\n",
      "Val Loss for batch is  -1.8613698482513428\n",
      "Val Loss for batch is  -1.9710606336593628\n",
      "Val Loss for batch is  -1.8670281171798706\n",
      "Val Loss for batch is  -2.6860318183898926\n",
      "|Iter  892  | Total Val Loss  -8.385490417480469 |\n",
      "Loss for batch is  -0.280414342880249\n",
      "Loss for batch is  -0.3918715715408325\n",
      "Loss for batch is  -0.3053044080734253\n",
      "Loss for batch is  -1.1734628677368164\n",
      "|Iter  893  | Total Train Loss  -2.1510531902313232 |\n",
      "Val Loss for batch is  -1.854552984237671\n",
      "Val Loss for batch is  -1.9773958921432495\n",
      "Val Loss for batch is  -1.8485418558120728\n",
      "Val Loss for batch is  -2.722623825073242\n",
      "|Iter  893  | Total Val Loss  -8.403114557266235 |\n",
      "Loss for batch is  -0.2794153690338135\n",
      "Loss for batch is  -0.430639386177063\n",
      "Loss for batch is  -0.3289322853088379\n",
      "Loss for batch is  -1.2006844282150269\n",
      "|Iter  894  | Total Train Loss  -2.239671468734741 |\n",
      "Val Loss for batch is  -1.8874822854995728\n",
      "Val Loss for batch is  -2.0057945251464844\n",
      "Val Loss for batch is  -1.9043558835983276\n",
      "Val Loss for batch is  -2.786339282989502\n",
      "|Iter  894  | Total Val Loss  -8.583971977233887 |\n",
      "Loss for batch is  -0.33390045166015625\n",
      "Loss for batch is  -0.4646703004837036\n",
      "Loss for batch is  -0.3718235492706299\n",
      "Loss for batch is  -1.2391184568405151\n",
      "|Iter  895  | Total Train Loss  -2.409512758255005 |\n",
      "Val Loss for batch is  -1.8659040927886963\n",
      "Val Loss for batch is  -2.026581048965454\n",
      "Val Loss for batch is  -1.9449267387390137\n",
      "Val Loss for batch is  -2.7636799812316895\n",
      "|Iter  895  | Total Val Loss  -8.601091861724854 |\n",
      "Loss for batch is  -0.36095333099365234\n",
      "Loss for batch is  -0.48734307289123535\n",
      "Loss for batch is  -0.39647233486175537\n",
      "Loss for batch is  -1.268654227256775\n",
      "|Iter  896  | Total Train Loss  -2.513422966003418 |\n",
      "Val Loss for batch is  -1.7591872215270996\n",
      "Val Loss for batch is  -1.9806760549545288\n",
      "Val Loss for batch is  -1.8971896171569824\n",
      "Val Loss for batch is  -2.8046188354492188\n",
      "|Iter  896  | Total Val Loss  -8.44167172908783 |\n",
      "Loss for batch is  -0.3162730932235718\n",
      "Loss for batch is  -0.5008435249328613\n",
      "Loss for batch is  -0.33795487880706787\n",
      "Loss for batch is  -1.300561785697937\n",
      "|Iter  897  | Total Train Loss  -2.455633282661438 |\n",
      "Val Loss for batch is  -1.7380578517913818\n",
      "Val Loss for batch is  -1.949198842048645\n",
      "Val Loss for batch is  -1.746486783027649\n",
      "Val Loss for batch is  -2.8265891075134277\n",
      "|Iter  897  | Total Val Loss  -8.260332584381104 |\n",
      "Loss for batch is  -0.2334613800048828\n",
      "Loss for batch is  -0.5295083522796631\n",
      "Loss for batch is  -0.25996625423431396\n",
      "Loss for batch is  -1.277815818786621\n",
      "|Iter  898  | Total Train Loss  -2.300751805305481 |\n",
      "Val Loss for batch is  -1.6753456592559814\n",
      "Val Loss for batch is  -1.8409299850463867\n",
      "Val Loss for batch is  -1.4952726364135742\n",
      "Val Loss for batch is  -2.73880672454834\n",
      "|Iter  898  | Total Val Loss  -7.750355005264282 |\n",
      "Loss for batch is  -0.19413673877716064\n",
      "Loss for batch is  -0.4512289762496948\n",
      "Loss for batch is  -0.284359335899353\n",
      "Loss for batch is  -1.076897144317627\n",
      "|Iter  899  | Total Train Loss  -2.0066221952438354 |\n",
      "Val Loss for batch is  -1.6725963354110718\n",
      "Val Loss for batch is  -1.90451180934906\n",
      "Val Loss for batch is  -1.7300552129745483\n",
      "Val Loss for batch is  -2.776768445968628\n",
      "|Iter  899  | Total Val Loss  -8.083931803703308 |\n",
      "Loss for batch is  -0.21386003494262695\n",
      "Loss for batch is  -0.2385087013244629\n",
      "Loss for batch is  0.006078362464904785\n",
      "Loss for batch is  -1.0990197658538818\n",
      "|Iter  900  | Total Train Loss  -1.545310139656067 |\n",
      "Val Loss for batch is  -0.9598541259765625\n",
      "Val Loss for batch is  -1.156761646270752\n",
      "Val Loss for batch is  -0.8368942141532898\n",
      "Val Loss for batch is  -2.384280204772949\n",
      "|Iter  900  | Total Val Loss  -5.3377901911735535 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.5360274314880371\n",
      "Loss for batch is  -0.22117328643798828\n",
      "Loss for batch is  0.4124610424041748\n",
      "Loss for batch is  -0.5967274904251099\n",
      "|Iter  901  | Total Train Loss  0.13058769702911377 |\n",
      "Val Loss for batch is  -1.1456010341644287\n",
      "Val Loss for batch is  -1.383081078529358\n",
      "Val Loss for batch is  -1.3890935182571411\n",
      "Val Loss for batch is  -2.302394390106201\n",
      "|Iter  901  | Total Val Loss  -6.220170021057129 |\n",
      "Loss for batch is  0.2489769458770752\n",
      "Loss for batch is  0.14643073081970215\n",
      "Loss for batch is  0.14957094192504883\n",
      "Loss for batch is  -0.8779431581497192\n",
      "|Iter  902  | Total Train Loss  -0.33296453952789307 |\n",
      "Val Loss for batch is  -1.4160351753234863\n",
      "Val Loss for batch is  -1.7485980987548828\n",
      "Val Loss for batch is  -1.537564754486084\n",
      "Val Loss for batch is  -2.5065948963165283\n",
      "|Iter  902  | Total Val Loss  -7.2087929248809814 |\n",
      "Loss for batch is  0.1306551694869995\n",
      "Loss for batch is  -0.07180142402648926\n",
      "Loss for batch is  0.03819775581359863\n",
      "Loss for batch is  -0.8835152387619019\n",
      "|Iter  903  | Total Train Loss  -0.786463737487793 |\n",
      "Val Loss for batch is  -1.5671212673187256\n",
      "Val Loss for batch is  -1.603178858757019\n",
      "Val Loss for batch is  -1.5616952180862427\n",
      "Val Loss for batch is  -2.4666554927825928\n",
      "|Iter  903  | Total Val Loss  -7.19865083694458 |\n",
      "Loss for batch is  -0.01767134666442871\n",
      "Loss for batch is  -0.13060176372528076\n",
      "Loss for batch is  0.011339306831359863\n",
      "Loss for batch is  -0.984322190284729\n",
      "|Iter  904  | Total Train Loss  -1.1212559938430786 |\n",
      "Val Loss for batch is  -1.7236740589141846\n",
      "Val Loss for batch is  -1.8158371448516846\n",
      "Val Loss for batch is  -1.7120097875595093\n",
      "Val Loss for batch is  -2.5371813774108887\n",
      "|Iter  904  | Total Val Loss  -7.788702368736267 |\n",
      "Loss for batch is  -0.050905704498291016\n",
      "Loss for batch is  -0.23820602893829346\n",
      "Loss for batch is  -0.09438550472259521\n",
      "Loss for batch is  -0.9773962497711182\n",
      "|Iter  905  | Total Train Loss  -1.3608934879302979 |\n",
      "Val Loss for batch is  -1.7581126689910889\n",
      "Val Loss for batch is  -1.8784970045089722\n",
      "Val Loss for batch is  -1.696102499961853\n",
      "Val Loss for batch is  -2.5808403491973877\n",
      "|Iter  905  | Total Val Loss  -7.913552522659302 |\n",
      "Loss for batch is  -0.13145923614501953\n",
      "Loss for batch is  -0.2614706754684448\n",
      "Loss for batch is  -0.1476726531982422\n",
      "Loss for batch is  -1.0305540561676025\n",
      "|Iter  906  | Total Train Loss  -1.571156620979309 |\n",
      "Val Loss for batch is  -1.851778507232666\n",
      "Val Loss for batch is  -1.9346301555633545\n",
      "Val Loss for batch is  -1.825058102607727\n",
      "Val Loss for batch is  -2.606476068496704\n",
      "|Iter  906  | Total Val Loss  -8.217942833900452 |\n",
      "Loss for batch is  -0.19932818412780762\n",
      "Loss for batch is  -0.33504557609558105\n",
      "Loss for batch is  -0.1861724853515625\n",
      "Loss for batch is  -1.0798046588897705\n",
      "|Iter  907  | Total Train Loss  -1.8003509044647217 |\n",
      "Val Loss for batch is  -1.852134108543396\n",
      "Val Loss for batch is  -1.8705391883850098\n",
      "Val Loss for batch is  -1.8075124025344849\n",
      "Val Loss for batch is  -2.6628355979919434\n",
      "|Iter  907  | Total Val Loss  -8.193021297454834 |\n",
      "Loss for batch is  -0.2629885673522949\n",
      "Loss for batch is  -0.3722827434539795\n",
      "Loss for batch is  -0.2654290199279785\n",
      "Loss for batch is  -1.1564034223556519\n",
      "|Iter  908  | Total Train Loss  -2.057103753089905 |\n",
      "Val Loss for batch is  -1.8410167694091797\n",
      "Val Loss for batch is  -1.9645874500274658\n",
      "Val Loss for batch is  -1.9255528450012207\n",
      "Val Loss for batch is  -2.7470693588256836\n",
      "|Iter  908  | Total Val Loss  -8.47822642326355 |\n",
      "Loss for batch is  -0.31109559535980225\n",
      "Loss for batch is  -0.4377403259277344\n",
      "Loss for batch is  -0.3086533546447754\n",
      "Loss for batch is  -1.2169771194458008\n",
      "|Iter  909  | Total Train Loss  -2.274466395378113 |\n",
      "Val Loss for batch is  -1.8352179527282715\n",
      "Val Loss for batch is  -1.916354775428772\n",
      "Val Loss for batch is  -1.8798989057540894\n",
      "Val Loss for batch is  -2.7254271507263184\n",
      "|Iter  909  | Total Val Loss  -8.356898784637451 |\n",
      "Loss for batch is  -0.31857192516326904\n",
      "Loss for batch is  -0.4631298780441284\n",
      "Loss for batch is  -0.35833048820495605\n",
      "Loss for batch is  -1.2442882061004639\n",
      "|Iter  910  | Total Train Loss  -2.3843204975128174 |\n",
      "Val Loss for batch is  -1.836669921875\n",
      "Val Loss for batch is  -2.008725881576538\n",
      "Val Loss for batch is  -1.894343614578247\n",
      "Val Loss for batch is  -2.78867769241333\n",
      "|Iter  910  | Total Val Loss  -8.528417110443115 |\n",
      "Loss for batch is  -0.3307851552963257\n",
      "Loss for batch is  -0.49998974800109863\n",
      "Loss for batch is  -0.35138773918151855\n",
      "Loss for batch is  -1.2712485790252686\n",
      "|Iter  911  | Total Train Loss  -2.4534112215042114 |\n",
      "Val Loss for batch is  -1.7623224258422852\n",
      "Val Loss for batch is  -1.9730489253997803\n",
      "Val Loss for batch is  -1.8582929372787476\n",
      "Val Loss for batch is  -2.8206474781036377\n",
      "|Iter  911  | Total Val Loss  -8.41431176662445 |\n",
      "Loss for batch is  -0.30760955810546875\n",
      "Loss for batch is  -0.5323919057846069\n",
      "Loss for batch is  -0.35943496227264404\n",
      "Loss for batch is  -1.316933035850525\n",
      "|Iter  912  | Total Train Loss  -2.5163694620132446 |\n",
      "Val Loss for batch is  -1.752532720565796\n",
      "Val Loss for batch is  -1.9902633428573608\n",
      "Val Loss for batch is  -1.7699227333068848\n",
      "Val Loss for batch is  -2.826186418533325\n",
      "|Iter  912  | Total Val Loss  -8.338905215263367 |\n",
      "Loss for batch is  -0.30675554275512695\n",
      "Loss for batch is  -0.5560303926467896\n",
      "Loss for batch is  -0.3465232849121094\n",
      "Loss for batch is  -1.3081886768341064\n",
      "|Iter  913  | Total Train Loss  -2.5174978971481323 |\n",
      "Val Loss for batch is  -1.7725402116775513\n",
      "Val Loss for batch is  -1.9435787200927734\n",
      "Val Loss for batch is  -1.7723649740219116\n",
      "Val Loss for batch is  -2.8122005462646484\n",
      "|Iter  913  | Total Val Loss  -8.300684452056885 |\n",
      "Loss for batch is  -0.29172003269195557\n",
      "Loss for batch is  -0.537568211555481\n",
      "Loss for batch is  -0.35167598724365234\n",
      "Loss for batch is  -1.2782111167907715\n",
      "|Iter  914  | Total Train Loss  -2.4591753482818604 |\n",
      "Val Loss for batch is  -1.5330333709716797\n",
      "Val Loss for batch is  -1.3792158365249634\n",
      "Val Loss for batch is  -1.706810474395752\n",
      "Val Loss for batch is  -2.767780303955078\n",
      "|Iter  914  | Total Val Loss  -7.386839985847473 |\n",
      "Loss for batch is  -0.1154245138168335\n",
      "Loss for batch is  -0.3429304361343384\n",
      "Loss for batch is  -0.0892723798751831\n",
      "Loss for batch is  -1.1674901247024536\n",
      "|Iter  915  | Total Train Loss  -1.7151174545288086 |\n",
      "Val Loss for batch is  -1.587775468826294\n",
      "Val Loss for batch is  -1.5677131414413452\n",
      "Val Loss for batch is  -1.7198184728622437\n",
      "Val Loss for batch is  -2.7382335662841797\n",
      "|Iter  915  | Total Val Loss  -7.6135406494140625 |\n",
      "Loss for batch is  -0.1691267490386963\n",
      "Loss for batch is  -0.25305235385894775\n",
      "Loss for batch is  -0.0013855695724487305\n",
      "Loss for batch is  -1.0284398794174194\n",
      "|Iter  916  | Total Train Loss  -1.4520045518875122 |\n",
      "Val Loss for batch is  -1.6075687408447266\n",
      "Val Loss for batch is  -1.8268134593963623\n",
      "Val Loss for batch is  -1.6948280334472656\n",
      "Val Loss for batch is  -2.68473482131958\n",
      "|Iter  916  | Total Val Loss  -7.813945055007935 |\n",
      "Loss for batch is  -0.14214801788330078\n",
      "Loss for batch is  -0.29638051986694336\n",
      "Loss for batch is  -0.12905585765838623\n",
      "Loss for batch is  -1.09641432762146\n",
      "|Iter  917  | Total Train Loss  -1.6639987230300903 |\n",
      "Val Loss for batch is  -1.6099843978881836\n",
      "Val Loss for batch is  -1.7699209451675415\n",
      "Val Loss for batch is  -1.610731840133667\n",
      "Val Loss for batch is  -2.5865042209625244\n",
      "|Iter  917  | Total Val Loss  -7.5771414041519165 |\n",
      "Loss for batch is  -0.08143377304077148\n",
      "Loss for batch is  -0.3391534090042114\n",
      "Loss for batch is  -0.22909212112426758\n",
      "Loss for batch is  -1.0460654497146606\n",
      "|Iter  918  | Total Train Loss  -1.6957447528839111 |\n",
      "Val Loss for batch is  -1.7223503589630127\n",
      "Val Loss for batch is  -1.8513131141662598\n",
      "Val Loss for batch is  -1.7393746376037598\n",
      "Val Loss for batch is  -2.676893711090088\n",
      "|Iter  918  | Total Val Loss  -7.98993182182312 |\n",
      "Loss for batch is  -0.18180549144744873\n",
      "Loss for batch is  -0.38941681385040283\n",
      "Loss for batch is  -0.19941532611846924\n",
      "Loss for batch is  -1.0956380367279053\n",
      "|Iter  919  | Total Train Loss  -1.866275668144226 |\n",
      "Val Loss for batch is  -1.8460862636566162\n",
      "Val Loss for batch is  -1.9921385049819946\n",
      "Val Loss for batch is  -1.8463022708892822\n",
      "Val Loss for batch is  -2.7089602947235107\n",
      "|Iter  919  | Total Val Loss  -8.393487334251404 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.2919435501098633\n",
      "Loss for batch is  -0.39878201484680176\n",
      "Loss for batch is  -0.2685410976409912\n",
      "Loss for batch is  -1.164905309677124\n",
      "|Iter  920  | Total Train Loss  -2.1241719722747803 |\n",
      "Val Loss for batch is  -1.822126865386963\n",
      "Val Loss for batch is  -1.910986304283142\n",
      "Val Loss for batch is  -1.861392617225647\n",
      "Val Loss for batch is  -2.6777331829071045\n",
      "|Iter  920  | Total Val Loss  -8.272238969802856 |\n",
      "Loss for batch is  -0.3213768005371094\n",
      "Loss for batch is  -0.39379262924194336\n",
      "Loss for batch is  -0.308951735496521\n",
      "Loss for batch is  -1.2121587991714478\n",
      "|Iter  921  | Total Train Loss  -2.2362799644470215 |\n",
      "Val Loss for batch is  -1.8323842287063599\n",
      "Val Loss for batch is  -1.9911330938339233\n",
      "Val Loss for batch is  -1.8629212379455566\n",
      "Val Loss for batch is  -2.7593770027160645\n",
      "|Iter  921  | Total Val Loss  -8.445815563201904 |\n",
      "Loss for batch is  -0.31350553035736084\n",
      "Loss for batch is  -0.4537224769592285\n",
      "Loss for batch is  -0.3594846725463867\n",
      "Loss for batch is  -1.233847737312317\n",
      "|Iter  922  | Total Train Loss  -2.360560417175293 |\n",
      "Val Loss for batch is  -1.8399484157562256\n",
      "Val Loss for batch is  -1.9709441661834717\n",
      "Val Loss for batch is  -1.9418513774871826\n",
      "Val Loss for batch is  -2.7782084941864014\n",
      "|Iter  922  | Total Val Loss  -8.530952453613281 |\n",
      "Loss for batch is  -0.37129342555999756\n",
      "Loss for batch is  -0.5062848329544067\n",
      "Loss for batch is  -0.3840709924697876\n",
      "Loss for batch is  -1.2749027013778687\n",
      "|Iter  923  | Total Train Loss  -2.5365519523620605 |\n",
      "Val Loss for batch is  -1.8829920291900635\n",
      "Val Loss for batch is  -2.0431132316589355\n",
      "Val Loss for batch is  -1.963117241859436\n",
      "Val Loss for batch is  -2.840740203857422\n",
      "|Iter  923  | Total Val Loss  -8.729962706565857 |\n",
      "Loss for batch is  -0.4128636121749878\n",
      "Loss for batch is  -0.5305066108703613\n",
      "Loss for batch is  -0.41994571685791016\n",
      "Loss for batch is  -1.3268446922302246\n",
      "|Iter  924  | Total Train Loss  -2.690160632133484 |\n",
      "Val Loss for batch is  -1.8135617971420288\n",
      "Val Loss for batch is  -2.0055789947509766\n",
      "Val Loss for batch is  -1.9122064113616943\n",
      "Val Loss for batch is  -2.8456320762634277\n",
      "|Iter  924  | Total Val Loss  -8.576979279518127 |\n",
      "Loss for batch is  -0.3930462598800659\n",
      "Loss for batch is  -0.559799075126648\n",
      "Loss for batch is  -0.43409979343414307\n",
      "Loss for batch is  -1.3391644954681396\n",
      "|Iter  925  | Total Train Loss  -2.7261096239089966 |\n",
      "Val Loss for batch is  -1.8245362043380737\n",
      "Val Loss for batch is  -2.0427112579345703\n",
      "Val Loss for batch is  -1.969276785850525\n",
      "Val Loss for batch is  -2.8485169410705566\n",
      "|Iter  925  | Total Val Loss  -8.685041189193726 |\n",
      "Loss for batch is  -0.4008443355560303\n",
      "Loss for batch is  -0.5703693628311157\n",
      "Loss for batch is  -0.43951547145843506\n",
      "Loss for batch is  -1.3781654834747314\n",
      "|Iter  926  | Total Train Loss  -2.7888946533203125 |\n",
      "Val Loss for batch is  -1.8059028387069702\n",
      "Val Loss for batch is  -2.0452942848205566\n",
      "Val Loss for batch is  -1.8680419921875\n",
      "Val Loss for batch is  -2.9050471782684326\n",
      "|Iter  926  | Total Val Loss  -8.62428629398346 |\n",
      "Loss for batch is  -0.3958709239959717\n",
      "Loss for batch is  -0.5916333198547363\n",
      "Loss for batch is  -0.4412573575973511\n",
      "Loss for batch is  -1.355991244316101\n",
      "|Iter  927  | Total Train Loss  -2.78475284576416 |\n",
      "Val Loss for batch is  -1.7866265773773193\n",
      "Val Loss for batch is  -2.0430893898010254\n",
      "Val Loss for batch is  -1.8961085081100464\n",
      "Val Loss for batch is  -2.889451026916504\n",
      "|Iter  927  | Total Val Loss  -8.615275502204895 |\n",
      "Loss for batch is  -0.3821544647216797\n",
      "Loss for batch is  -0.5777175426483154\n",
      "Loss for batch is  -0.39218902587890625\n",
      "Loss for batch is  -1.3597540855407715\n",
      "|Iter  928  | Total Train Loss  -2.711815118789673 |\n",
      "Val Loss for batch is  -1.7364354133605957\n",
      "Val Loss for batch is  -1.8576955795288086\n",
      "Val Loss for batch is  -1.731031894683838\n",
      "Val Loss for batch is  -2.840219259262085\n",
      "|Iter  928  | Total Val Loss  -8.165382146835327 |\n",
      "Loss for batch is  -0.26754701137542725\n",
      "Loss for batch is  -0.4584624767303467\n",
      "Loss for batch is  -0.33542370796203613\n",
      "Loss for batch is  -1.250048041343689\n",
      "|Iter  929  | Total Train Loss  -2.311481237411499 |\n",
      "Val Loss for batch is  -1.5059212446212769\n",
      "Val Loss for batch is  -1.650965929031372\n",
      "Val Loss for batch is  -1.2872103452682495\n",
      "Val Loss for batch is  -2.733506202697754\n",
      "|Iter  929  | Total Val Loss  -7.177603721618652 |\n",
      "Loss for batch is  -0.01019275188446045\n",
      "Loss for batch is  -0.3695213794708252\n",
      "Loss for batch is  0.25658249855041504\n",
      "Loss for batch is  -1.1832233667373657\n",
      "|Iter  930  | Total Train Loss  -1.3063549995422363 |\n",
      "Val Loss for batch is  -0.9849281907081604\n",
      "Val Loss for batch is  -0.9365167021751404\n",
      "Val Loss for batch is  -0.8553937077522278\n",
      "Val Loss for batch is  -2.358020067214966\n",
      "|Iter  930  | Total Val Loss  -5.134858667850494 |\n",
      "Loss for batch is  0.48736584186553955\n",
      "Loss for batch is  0.02740633487701416\n",
      "Loss for batch is  0.1036146879196167\n",
      "Loss for batch is  -0.49006712436676025\n",
      "|Iter  931  | Total Train Loss  0.12831974029541016 |\n",
      "Val Loss for batch is  -1.0380804538726807\n",
      "Val Loss for batch is  -1.4330618381500244\n",
      "Val Loss for batch is  -1.2293373346328735\n",
      "Val Loss for batch is  -2.2672111988067627\n",
      "|Iter  931  | Total Val Loss  -5.967690825462341 |\n",
      "Loss for batch is  0.3531409502029419\n",
      "Loss for batch is  -0.29282236099243164\n",
      "Loss for batch is  -0.06060171127319336\n",
      "Loss for batch is  -0.7532761096954346\n",
      "|Iter  932  | Total Train Loss  -0.7535592317581177 |\n",
      "Val Loss for batch is  -1.4811270236968994\n",
      "Val Loss for batch is  -1.6570031642913818\n",
      "Val Loss for batch is  -1.5133283138275146\n",
      "Val Loss for batch is  -2.367650032043457\n",
      "|Iter  932  | Total Val Loss  -7.019108533859253 |\n",
      "Loss for batch is  0.1186147928237915\n",
      "Loss for batch is  -0.23887252807617188\n",
      "Loss for batch is  -0.1743180751800537\n",
      "Loss for batch is  -0.9384500980377197\n",
      "|Iter  933  | Total Train Loss  -1.2330259084701538 |\n",
      "Val Loss for batch is  -1.4895374774932861\n",
      "Val Loss for batch is  -1.6757289171218872\n",
      "Val Loss for batch is  -1.5373895168304443\n",
      "Val Loss for batch is  -2.3907954692840576\n",
      "|Iter  933  | Total Val Loss  -7.093451380729675 |\n",
      "Loss for batch is  0.030369281768798828\n",
      "Loss for batch is  -0.2287278175354004\n",
      "Loss for batch is  -0.12822413444519043\n",
      "Loss for batch is  -1.0034998655319214\n",
      "|Iter  934  | Total Train Loss  -1.3300825357437134 |\n",
      "Val Loss for batch is  -1.7342599630355835\n",
      "Val Loss for batch is  -1.9118568897247314\n",
      "Val Loss for batch is  -1.8221733570098877\n",
      "Val Loss for batch is  -2.560194969177246\n",
      "|Iter  934  | Total Val Loss  -8.028485178947449 |\n",
      "Loss for batch is  -0.15752315521240234\n",
      "Loss for batch is  -0.27256011962890625\n",
      "Loss for batch is  -0.1692180633544922\n",
      "Loss for batch is  -0.9907921552658081\n",
      "|Iter  935  | Total Train Loss  -1.5900934934616089 |\n",
      "Val Loss for batch is  -1.810943841934204\n",
      "Val Loss for batch is  -1.8539936542510986\n",
      "Val Loss for batch is  -1.7748528718948364\n",
      "Val Loss for batch is  -2.548645496368408\n",
      "|Iter  935  | Total Val Loss  -7.988435864448547 |\n",
      "Loss for batch is  -0.22505831718444824\n",
      "Loss for batch is  -0.3307708501815796\n",
      "Loss for batch is  -0.2464684247970581\n",
      "Loss for batch is  -1.0531717538833618\n",
      "|Iter  936  | Total Train Loss  -1.8554693460464478 |\n",
      "Val Loss for batch is  -1.8106626272201538\n",
      "Val Loss for batch is  -1.9727157354354858\n",
      "Val Loss for batch is  -1.8649998903274536\n",
      "Val Loss for batch is  -2.6668426990509033\n",
      "|Iter  936  | Total Val Loss  -8.315220952033997 |\n",
      "Loss for batch is  -0.24026310443878174\n",
      "Loss for batch is  -0.38555240631103516\n",
      "Loss for batch is  -0.28462350368499756\n",
      "Loss for batch is  -1.1345186233520508\n",
      "|Iter  937  | Total Train Loss  -2.0449576377868652 |\n",
      "Val Loss for batch is  -1.8111217021942139\n",
      "Val Loss for batch is  -1.896312952041626\n",
      "Val Loss for batch is  -1.782360553741455\n",
      "Val Loss for batch is  -2.706850290298462\n",
      "|Iter  937  | Total Val Loss  -8.196645498275757 |\n",
      "Loss for batch is  -0.29753386974334717\n",
      "Loss for batch is  -0.43798065185546875\n",
      "Loss for batch is  -0.33264172077178955\n",
      "Loss for batch is  -1.207658290863037\n",
      "|Iter  938  | Total Train Loss  -2.2758145332336426 |\n",
      "Val Loss for batch is  -1.8541419506072998\n",
      "Val Loss for batch is  -1.9823849201202393\n",
      "Val Loss for batch is  -1.9386026859283447\n",
      "Val Loss for batch is  -2.7446796894073486\n",
      "|Iter  938  | Total Val Loss  -8.519809246063232 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.33013272285461426\n",
      "Loss for batch is  -0.476277232170105\n",
      "Loss for batch is  -0.37655842304229736\n",
      "Loss for batch is  -1.2755459547042847\n",
      "|Iter  939  | Total Train Loss  -2.4585143327713013 |\n",
      "Val Loss for batch is  -1.82746422290802\n",
      "Val Loss for batch is  -1.986230492591858\n",
      "Val Loss for batch is  -1.8985918760299683\n",
      "Val Loss for batch is  -2.7942593097686768\n",
      "|Iter  939  | Total Val Loss  -8.506545901298523 |\n",
      "Loss for batch is  -0.3911399841308594\n",
      "Loss for batch is  -0.5253286361694336\n",
      "Loss for batch is  -0.40631604194641113\n",
      "Loss for batch is  -1.3116511106491089\n",
      "|Iter  940  | Total Train Loss  -2.634435772895813 |\n",
      "Val Loss for batch is  -1.87007474899292\n",
      "Val Loss for batch is  -2.0430166721343994\n",
      "Val Loss for batch is  -1.9220879077911377\n",
      "Val Loss for batch is  -2.8709611892700195\n",
      "|Iter  940  | Total Val Loss  -8.706140518188477 |\n",
      "Loss for batch is  -0.4039703607559204\n",
      "Loss for batch is  -0.5500913858413696\n",
      "Loss for batch is  -0.434781551361084\n",
      "Loss for batch is  -1.3447608947753906\n",
      "|Iter  941  | Total Train Loss  -2.7336041927337646 |\n",
      "Val Loss for batch is  -1.807036280632019\n",
      "Val Loss for batch is  -2.0185675621032715\n",
      "Val Loss for batch is  -1.97049081325531\n",
      "Val Loss for batch is  -2.7878785133361816\n",
      "|Iter  941  | Total Val Loss  -8.583973169326782 |\n",
      "Loss for batch is  -0.4183899164199829\n",
      "Loss for batch is  -0.5993462800979614\n",
      "Loss for batch is  -0.4509437084197998\n",
      "Loss for batch is  -1.3878087997436523\n",
      "|Iter  942  | Total Train Loss  -2.8564887046813965 |\n",
      "Val Loss for batch is  -1.8629693984985352\n",
      "Val Loss for batch is  -2.0701401233673096\n",
      "Val Loss for batch is  -1.9278929233551025\n",
      "Val Loss for batch is  -2.9246175289154053\n",
      "|Iter  942  | Total Val Loss  -8.785619974136353 |\n",
      "Loss for batch is  -0.4096766710281372\n",
      "Loss for batch is  -0.6030855178833008\n",
      "Loss for batch is  -0.47779381275177\n",
      "Loss for batch is  -1.3949507474899292\n",
      "|Iter  943  | Total Train Loss  -2.885506749153137 |\n",
      "Val Loss for batch is  -1.8372856378555298\n",
      "Val Loss for batch is  -2.041517972946167\n",
      "Val Loss for batch is  -1.9110021591186523\n",
      "Val Loss for batch is  -2.9172661304473877\n",
      "|Iter  943  | Total Val Loss  -8.707071900367737 |\n",
      "Loss for batch is  -0.4232501983642578\n",
      "Loss for batch is  -0.6115678548812866\n",
      "Loss for batch is  -0.44666337966918945\n",
      "Loss for batch is  -1.396378755569458\n",
      "|Iter  944  | Total Train Loss  -2.877860188484192 |\n",
      "Val Loss for batch is  -1.8307101726531982\n",
      "Val Loss for batch is  -2.0695323944091797\n",
      "Val Loss for batch is  -1.953284502029419\n",
      "Val Loss for batch is  -2.912379264831543\n",
      "|Iter  944  | Total Val Loss  -8.76590633392334 |\n",
      "Loss for batch is  -0.4075345993041992\n",
      "Loss for batch is  -0.5997709035873413\n",
      "Loss for batch is  -0.39785099029541016\n",
      "Loss for batch is  -1.4225356578826904\n",
      "|Iter  945  | Total Train Loss  -2.827692151069641 |\n",
      "Val Loss for batch is  -1.7205026149749756\n",
      "Val Loss for batch is  -1.9405896663665771\n",
      "Val Loss for batch is  -1.7757399082183838\n",
      "Val Loss for batch is  -2.8745973110198975\n",
      "|Iter  945  | Total Val Loss  -8.311429500579834 |\n",
      "Loss for batch is  -0.36782991886138916\n",
      "Loss for batch is  -0.6062695980072021\n",
      "Loss for batch is  -0.39903759956359863\n",
      "Loss for batch is  -1.3394417762756348\n",
      "|Iter  946  | Total Train Loss  -2.7125788927078247 |\n",
      "Val Loss for batch is  -1.7859363555908203\n",
      "Val Loss for batch is  -1.948945164680481\n",
      "Val Loss for batch is  -1.7053006887435913\n",
      "Val Loss for batch is  -2.9158971309661865\n",
      "|Iter  946  | Total Val Loss  -8.356079339981079 |\n",
      "Loss for batch is  -0.3717367649078369\n",
      "Loss for batch is  -0.4261782169342041\n",
      "Loss for batch is  -0.4047849178314209\n",
      "Loss for batch is  -1.1940865516662598\n",
      "|Iter  947  | Total Train Loss  -2.3967864513397217 |\n",
      "Val Loss for batch is  -1.5335450172424316\n",
      "Val Loss for batch is  -1.7851672172546387\n",
      "Val Loss for batch is  -1.6234725713729858\n",
      "Val Loss for batch is  -2.7872350215911865\n",
      "|Iter  947  | Total Val Loss  -7.729419827461243 |\n",
      "Loss for batch is  -0.13739752769470215\n",
      "Loss for batch is  -0.4287078380584717\n",
      "Loss for batch is  0.24962186813354492\n",
      "Loss for batch is  -1.2935205698013306\n",
      "|Iter  948  | Total Train Loss  -1.6100040674209595 |\n",
      "Val Loss for batch is  -1.2136508226394653\n",
      "Val Loss for batch is  -1.185547947883606\n",
      "Val Loss for batch is  -1.2866798639297485\n",
      "Val Loss for batch is  -2.5534651279449463\n",
      "|Iter  948  | Total Val Loss  -6.239343762397766 |\n",
      "Loss for batch is  0.23614978790283203\n",
      "Loss for batch is  0.001201629638671875\n",
      "Loss for batch is  -0.05099976062774658\n",
      "Loss for batch is  -0.7271848917007446\n",
      "|Iter  949  | Total Train Loss  -0.5408332347869873 |\n",
      "Val Loss for batch is  -1.2869080305099487\n",
      "Val Loss for batch is  -1.605283498764038\n",
      "Val Loss for batch is  -1.3232415914535522\n",
      "Val Loss for batch is  -2.401035785675049\n",
      "|Iter  949  | Total Val Loss  -6.616468906402588 |\n",
      "Loss for batch is  0.1401158571243286\n",
      "Loss for batch is  -0.3008610010147095\n",
      "Loss for batch is  -0.20326542854309082\n",
      "Loss for batch is  -1.0052413940429688\n",
      "|Iter  950  | Total Train Loss  -1.3692519664764404 |\n",
      "Val Loss for batch is  -1.4929304122924805\n",
      "Val Loss for batch is  -1.7143296003341675\n",
      "Val Loss for batch is  -1.688736081123352\n",
      "Val Loss for batch is  -2.5242881774902344\n",
      "|Iter  950  | Total Val Loss  -7.420284271240234 |\n",
      "Loss for batch is  0.02503073215484619\n",
      "Loss for batch is  -0.2815169095993042\n",
      "Loss for batch is  -0.19727838039398193\n",
      "Loss for batch is  -1.0462615489959717\n",
      "|Iter  951  | Total Train Loss  -1.5000261068344116 |\n",
      "Val Loss for batch is  -1.6948912143707275\n",
      "Val Loss for batch is  -1.8254451751708984\n",
      "Val Loss for batch is  -1.6498557329177856\n",
      "Val Loss for batch is  -2.5474436283111572\n",
      "|Iter  951  | Total Val Loss  -7.717635750770569 |\n",
      "Loss for batch is  -0.17586851119995117\n",
      "Loss for batch is  -0.3055143356323242\n",
      "Loss for batch is  -0.17967844009399414\n",
      "Loss for batch is  -1.0661488771438599\n",
      "|Iter  952  | Total Train Loss  -1.7272101640701294 |\n",
      "Val Loss for batch is  -1.8130414485931396\n",
      "Val Loss for batch is  -1.970860481262207\n",
      "Val Loss for batch is  -1.8603386878967285\n",
      "Val Loss for batch is  -2.661621570587158\n",
      "|Iter  952  | Total Val Loss  -8.305862188339233 |\n",
      "Loss for batch is  -0.21497511863708496\n",
      "Loss for batch is  -0.3714792728424072\n",
      "Loss for batch is  -0.2754364013671875\n",
      "Loss for batch is  -1.0839520692825317\n",
      "|Iter  953  | Total Train Loss  -1.9458428621292114 |\n",
      "Val Loss for batch is  -1.7749141454696655\n",
      "Val Loss for batch is  -1.9117646217346191\n",
      "Val Loss for batch is  -1.7977088689804077\n",
      "Val Loss for batch is  -2.6649250984191895\n",
      "|Iter  953  | Total Val Loss  -8.149312734603882 |\n",
      "Loss for batch is  -0.2889993190765381\n",
      "Loss for batch is  -0.3985583782196045\n",
      "Loss for batch is  -0.2835116386413574\n",
      "Loss for batch is  -1.163315773010254\n",
      "|Iter  954  | Total Train Loss  -2.134385108947754 |\n",
      "Val Loss for batch is  -1.9119493961334229\n",
      "Val Loss for batch is  -2.014418125152588\n",
      "Val Loss for batch is  -1.9472408294677734\n",
      "Val Loss for batch is  -2.748575448989868\n",
      "|Iter  954  | Total Val Loss  -8.622183799743652 |\n",
      "Loss for batch is  -0.34942150115966797\n",
      "Loss for batch is  -0.46227312088012695\n",
      "Loss for batch is  -0.3338332176208496\n",
      "Loss for batch is  -1.1795893907546997\n",
      "|Iter  955  | Total Train Loss  -2.3251172304153442 |\n",
      "Val Loss for batch is  -1.8111913204193115\n",
      "Val Loss for batch is  -1.9985381364822388\n",
      "Val Loss for batch is  -1.8953220844268799\n",
      "Val Loss for batch is  -2.7790181636810303\n",
      "|Iter  955  | Total Val Loss  -8.48406970500946 |\n",
      "Loss for batch is  -0.38003551959991455\n",
      "Loss for batch is  -0.5127215385437012\n",
      "Loss for batch is  -0.353010892868042\n",
      "Loss for batch is  -1.2345370054244995\n",
      "|Iter  956  | Total Train Loss  -2.4803049564361572 |\n",
      "Val Loss for batch is  -1.885128140449524\n",
      "Val Loss for batch is  -2.057325839996338\n",
      "Val Loss for batch is  -1.9533390998840332\n",
      "Val Loss for batch is  -2.8118510246276855\n",
      "|Iter  956  | Total Val Loss  -8.70764410495758 |\n",
      "Loss for batch is  -0.426985502243042\n",
      "Loss for batch is  -0.5401860475540161\n",
      "Loss for batch is  -0.44479572772979736\n",
      "Loss for batch is  -1.3143153190612793\n",
      "|Iter  957  | Total Train Loss  -2.7262825965881348 |\n",
      "Val Loss for batch is  -1.897337794303894\n",
      "Val Loss for batch is  -2.070935010910034\n",
      "Val Loss for batch is  -1.9479025602340698\n",
      "Val Loss for batch is  -2.848950147628784\n",
      "|Iter  957  | Total Val Loss  -8.765125513076782 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.4308267831802368\n",
      "Loss for batch is  -0.5752134323120117\n",
      "Loss for batch is  -0.4539508819580078\n",
      "Loss for batch is  -1.3577730655670166\n",
      "|Iter  958  | Total Train Loss  -2.817764163017273 |\n",
      "Val Loss for batch is  -1.9227321147918701\n",
      "Val Loss for batch is  -2.1074955463409424\n",
      "Val Loss for batch is  -1.9786431789398193\n",
      "Val Loss for batch is  -2.8695552349090576\n",
      "|Iter  958  | Total Val Loss  -8.87842607498169 |\n",
      "Loss for batch is  -0.47685158252716064\n",
      "Loss for batch is  -0.6045466661453247\n",
      "Loss for batch is  -0.5182664394378662\n",
      "Loss for batch is  -1.3957622051239014\n",
      "|Iter  959  | Total Train Loss  -2.995426893234253 |\n",
      "Val Loss for batch is  -1.9089025259017944\n",
      "Val Loss for batch is  -2.09773588180542\n",
      "Val Loss for batch is  -2.010021448135376\n",
      "Val Loss for batch is  -2.8884193897247314\n",
      "|Iter  959  | Total Val Loss  -8.905079245567322 |\n",
      "Loss for batch is  -0.49643611907958984\n",
      "Loss for batch is  -0.6428461074829102\n",
      "Loss for batch is  -0.5374600887298584\n",
      "Loss for batch is  -1.427281141281128\n",
      "|Iter  960  | Total Train Loss  -3.1040234565734863 |\n",
      "Val Loss for batch is  -1.9066534042358398\n",
      "Val Loss for batch is  -2.1027395725250244\n",
      "Val Loss for batch is  -1.9949015378952026\n",
      "Val Loss for batch is  -2.970438003540039\n",
      "|Iter  960  | Total Val Loss  -8.974732518196106 |\n",
      "Loss for batch is  -0.5078394412994385\n",
      "Loss for batch is  -0.6647690534591675\n",
      "Loss for batch is  -0.5260621309280396\n",
      "Loss for batch is  -1.4531017541885376\n",
      "|Iter  961  | Total Train Loss  -3.151772379875183 |\n",
      "Val Loss for batch is  -1.841783881187439\n",
      "Val Loss for batch is  -2.0428647994995117\n",
      "Val Loss for batch is  -1.872748613357544\n",
      "Val Loss for batch is  -2.961421012878418\n",
      "|Iter  961  | Total Val Loss  -8.718818306922913 |\n",
      "Loss for batch is  -0.4366692304611206\n",
      "Loss for batch is  -0.6744922399520874\n",
      "Loss for batch is  -0.4927576780319214\n",
      "Loss for batch is  -1.450341820716858\n",
      "|Iter  962  | Total Train Loss  -3.0542609691619873 |\n",
      "Val Loss for batch is  -1.7720727920532227\n",
      "Val Loss for batch is  -1.9420230388641357\n",
      "Val Loss for batch is  -1.7706907987594604\n",
      "Val Loss for batch is  -2.8645401000976562\n",
      "|Iter  962  | Total Val Loss  -8.349326729774475 |\n",
      "Loss for batch is  -0.4091472625732422\n",
      "Loss for batch is  -0.678810715675354\n",
      "Loss for batch is  -0.4436471462249756\n",
      "Loss for batch is  -1.4409021139144897\n",
      "|Iter  963  | Total Train Loss  -2.9725072383880615 |\n",
      "Val Loss for batch is  -1.745957374572754\n",
      "Val Loss for batch is  -1.7364892959594727\n",
      "Val Loss for batch is  -0.7909432053565979\n",
      "Val Loss for batch is  -2.897585391998291\n",
      "|Iter  963  | Total Val Loss  -7.1709752678871155 |\n",
      "Loss for batch is  -0.40911388397216797\n",
      "Loss for batch is  -0.517238974571228\n",
      "Loss for batch is  -0.2722911834716797\n",
      "Loss for batch is  -1.3663045167922974\n",
      "|Iter  964  | Total Train Loss  -2.564948558807373 |\n",
      "Val Loss for batch is  -1.2105218172073364\n",
      "Val Loss for batch is  -1.5880842208862305\n",
      "Val Loss for batch is  -0.43565717339515686\n",
      "Val Loss for batch is  -2.7832283973693848\n",
      "|Iter  964  | Total Val Loss  -6.0174916088581085 |\n",
      "Loss for batch is  -0.011986136436462402\n",
      "Loss for batch is  -0.5283397436141968\n",
      "Loss for batch is  -0.08434069156646729\n",
      "Loss for batch is  -0.5423442125320435\n",
      "|Iter  965  | Total Train Loss  -1.16701078414917 |\n",
      "Val Loss for batch is  -1.189542293548584\n",
      "Val Loss for batch is  -1.4725375175476074\n",
      "Val Loss for batch is  -1.2982101440429688\n",
      "Val Loss for batch is  -2.5625102519989014\n",
      "|Iter  965  | Total Val Loss  -6.5228002071380615 |\n",
      "Loss for batch is  0.15166044235229492\n",
      "Loss for batch is  0.17357313632965088\n",
      "Loss for batch is  0.5692245364189148\n",
      "Loss for batch is  -0.7872141599655151\n",
      "|Iter  966  | Total Train Loss  0.10724395513534546 |\n",
      "Val Loss for batch is  -1.1832550764083862\n",
      "Val Loss for batch is  -1.5802594423294067\n",
      "Val Loss for batch is  -1.4337419271469116\n",
      "Val Loss for batch is  -2.458709955215454\n",
      "|Iter  966  | Total Val Loss  -6.655966401100159 |\n",
      "Loss for batch is  0.19059956073760986\n",
      "Loss for batch is  -0.01011502742767334\n",
      "Loss for batch is  0.1709733009338379\n",
      "Loss for batch is  -0.8860127925872803\n",
      "|Iter  967  | Total Train Loss  -0.5345549583435059 |\n",
      "Val Loss for batch is  -1.5225902795791626\n",
      "Val Loss for batch is  -1.6177914142608643\n",
      "Val Loss for batch is  -1.4514849185943604\n",
      "Val Loss for batch is  -2.467280864715576\n",
      "|Iter  967  | Total Val Loss  -7.059147477149963 |\n",
      "Loss for batch is  0.11211800575256348\n",
      "Loss for batch is  -0.08699321746826172\n",
      "Loss for batch is  0.05569493770599365\n",
      "Loss for batch is  -0.9222593307495117\n",
      "|Iter  968  | Total Train Loss  -0.8414396047592163 |\n",
      "Val Loss for batch is  -1.7082637548446655\n",
      "Val Loss for batch is  -1.7357096672058105\n",
      "Val Loss for batch is  -1.6553046703338623\n",
      "Val Loss for batch is  -2.445744752883911\n",
      "|Iter  968  | Total Val Loss  -7.5450228452682495 |\n",
      "Loss for batch is  -0.060599565505981445\n",
      "Loss for batch is  -0.1763441562652588\n",
      "Loss for batch is  -0.1267836093902588\n",
      "Loss for batch is  -0.9353015422821045\n",
      "|Iter  969  | Total Train Loss  -1.2990288734436035 |\n",
      "Val Loss for batch is  -1.7903368473052979\n",
      "Val Loss for batch is  -1.8575516939163208\n",
      "Val Loss for batch is  -1.7333910465240479\n",
      "Val Loss for batch is  -2.5140576362609863\n",
      "|Iter  969  | Total Val Loss  -7.895337224006653 |\n",
      "Loss for batch is  -0.16911840438842773\n",
      "Loss for batch is  -0.2236539125442505\n",
      "Loss for batch is  -0.14906620979309082\n",
      "Loss for batch is  -1.0161718130111694\n",
      "|Iter  970  | Total Train Loss  -1.5580103397369385 |\n",
      "Val Loss for batch is  -1.7624821662902832\n",
      "Val Loss for batch is  -1.8916953802108765\n",
      "Val Loss for batch is  -1.780145287513733\n",
      "Val Loss for batch is  -2.542750597000122\n",
      "|Iter  970  | Total Val Loss  -7.977073431015015 |\n",
      "Loss for batch is  -0.18841445446014404\n",
      "Loss for batch is  -0.3144214153289795\n",
      "Loss for batch is  -0.2235732078552246\n",
      "Loss for batch is  -1.038513422012329\n",
      "|Iter  971  | Total Train Loss  -1.7649224996566772 |\n",
      "Val Loss for batch is  -1.8584038019180298\n",
      "Val Loss for batch is  -1.964545726776123\n",
      "Val Loss for batch is  -1.8952645063400269\n",
      "Val Loss for batch is  -2.61788272857666\n",
      "|Iter  971  | Total Val Loss  -8.33609676361084 |\n",
      "Loss for batch is  -0.22862768173217773\n",
      "Loss for batch is  -0.34812867641448975\n",
      "Loss for batch is  -0.25037240982055664\n",
      "Loss for batch is  -1.0865871906280518\n",
      "|Iter  972  | Total Train Loss  -1.9137159585952759 |\n",
      "Val Loss for batch is  -1.874180793762207\n",
      "Val Loss for batch is  -1.9126545190811157\n",
      "Val Loss for batch is  -1.8470898866653442\n",
      "Val Loss for batch is  -2.682568311691284\n",
      "|Iter  972  | Total Val Loss  -8.316493511199951 |\n",
      "Loss for batch is  -0.32421159744262695\n",
      "Loss for batch is  -0.39837419986724854\n",
      "Loss for batch is  -0.28369367122650146\n",
      "Loss for batch is  -1.1588919162750244\n",
      "|Iter  973  | Total Train Loss  -2.1651713848114014 |\n",
      "Val Loss for batch is  -1.8180991411209106\n",
      "Val Loss for batch is  -1.9677133560180664\n",
      "Val Loss for batch is  -1.9102765321731567\n",
      "Val Loss for batch is  -2.708000659942627\n",
      "|Iter  973  | Total Val Loss  -8.40408968925476 |\n",
      "Loss for batch is  -0.34180617332458496\n",
      "Loss for batch is  -0.45299577713012695\n",
      "Loss for batch is  -0.33955860137939453\n",
      "Loss for batch is  -1.23054039478302\n",
      "|Iter  974  | Total Train Loss  -2.3649009466171265 |\n",
      "Val Loss for batch is  -1.8921738862991333\n",
      "Val Loss for batch is  -1.966076374053955\n",
      "Val Loss for batch is  -1.9025564193725586\n",
      "Val Loss for batch is  -2.752274990081787\n",
      "|Iter  974  | Total Val Loss  -8.513081669807434 |\n",
      "Loss for batch is  -0.38816261291503906\n",
      "Loss for batch is  -0.488803505897522\n",
      "Loss for batch is  -0.4003326892852783\n",
      "Loss for batch is  -1.277705192565918\n",
      "|Iter  975  | Total Train Loss  -2.5550040006637573 |\n",
      "Val Loss for batch is  -1.8675700426101685\n",
      "Val Loss for batch is  -2.0112669467926025\n",
      "Val Loss for batch is  -1.9643573760986328\n",
      "Val Loss for batch is  -2.8101093769073486\n",
      "|Iter  975  | Total Val Loss  -8.653303742408752 |\n",
      "Loss for batch is  -0.4113517999649048\n",
      "Loss for batch is  -0.5314855575561523\n",
      "Loss for batch is  -0.42128491401672363\n",
      "Loss for batch is  -1.3239033222198486\n",
      "|Iter  976  | Total Train Loss  -2.6880255937576294 |\n",
      "Val Loss for batch is  -1.8561512231826782\n",
      "Val Loss for batch is  -2.018322706222534\n",
      "Val Loss for batch is  -1.9974215030670166\n",
      "Val Loss for batch is  -2.8265459537506104\n",
      "|Iter  976  | Total Val Loss  -8.69844138622284 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.4400331974029541\n",
      "Loss for batch is  -0.5649410486221313\n",
      "Loss for batch is  -0.46823132038116455\n",
      "Loss for batch is  -1.3724974393844604\n",
      "|Iter  977  | Total Train Loss  -2.8457030057907104 |\n",
      "Val Loss for batch is  -1.9204182624816895\n",
      "Val Loss for batch is  -2.061638832092285\n",
      "Val Loss for batch is  -2.0158166885375977\n",
      "Val Loss for batch is  -2.9080495834350586\n",
      "|Iter  977  | Total Val Loss  -8.90592336654663 |\n",
      "Loss for batch is  -0.4801067113876343\n",
      "Loss for batch is  -0.6048673391342163\n",
      "Loss for batch is  -0.495151162147522\n",
      "Loss for batch is  -1.4053928852081299\n",
      "|Iter  978  | Total Train Loss  -2.9855180978775024 |\n",
      "Val Loss for batch is  -1.931650161743164\n",
      "Val Loss for batch is  -2.0914385318756104\n",
      "Val Loss for batch is  -1.9975422620773315\n",
      "Val Loss for batch is  -2.908024549484253\n",
      "|Iter  978  | Total Val Loss  -8.928655505180359 |\n",
      "Loss for batch is  -0.487735390663147\n",
      "Loss for batch is  -0.633170485496521\n",
      "Loss for batch is  -0.5230659246444702\n",
      "Loss for batch is  -1.4205347299575806\n",
      "|Iter  979  | Total Train Loss  -3.0645065307617188 |\n",
      "Val Loss for batch is  -1.918752670288086\n",
      "Val Loss for batch is  -2.0987088680267334\n",
      "Val Loss for batch is  -2.0090253353118896\n",
      "Val Loss for batch is  -2.9776675701141357\n",
      "|Iter  979  | Total Val Loss  -9.004154443740845 |\n",
      "Loss for batch is  -0.49933743476867676\n",
      "Loss for batch is  -0.6588931083679199\n",
      "Loss for batch is  -0.5248286724090576\n",
      "Loss for batch is  -1.463108777999878\n",
      "|Iter  980  | Total Train Loss  -3.1461679935455322 |\n",
      "Val Loss for batch is  -1.9139020442962646\n",
      "Val Loss for batch is  -2.1140201091766357\n",
      "Val Loss for batch is  -2.0022811889648438\n",
      "Val Loss for batch is  -2.954535961151123\n",
      "|Iter  980  | Total Val Loss  -8.984739303588867 |\n",
      "Loss for batch is  -0.49987590312957764\n",
      "Loss for batch is  -0.6493854522705078\n",
      "Loss for batch is  -0.5249614715576172\n",
      "Loss for batch is  -1.4749242067337036\n",
      "|Iter  981  | Total Train Loss  -3.1491470336914062 |\n",
      "Val Loss for batch is  -1.9255298376083374\n",
      "Val Loss for batch is  -2.0859715938568115\n",
      "Val Loss for batch is  -1.9936336278915405\n",
      "Val Loss for batch is  -2.975198268890381\n",
      "|Iter  981  | Total Val Loss  -8.98033332824707 |\n",
      "Loss for batch is  -0.4803062677383423\n",
      "Loss for batch is  -0.6631911993026733\n",
      "Loss for batch is  -0.4756227731704712\n",
      "Loss for batch is  -1.4534757137298584\n",
      "|Iter  982  | Total Train Loss  -3.072595953941345 |\n",
      "Val Loss for batch is  -1.6829066276550293\n",
      "Val Loss for batch is  -1.9165254831314087\n",
      "Val Loss for batch is  -1.7284963130950928\n",
      "Val Loss for batch is  -2.9066693782806396\n",
      "|Iter  982  | Total Val Loss  -8.23459780216217 |\n",
      "Loss for batch is  -0.27963316440582275\n",
      "Loss for batch is  -0.5526117086410522\n",
      "Loss for batch is  -0.23359060287475586\n",
      "Loss for batch is  -1.4061577320098877\n",
      "|Iter  983  | Total Train Loss  -2.4719932079315186 |\n",
      "Val Loss for batch is  -1.3417158126831055\n",
      "Val Loss for batch is  -1.3570001125335693\n",
      "Val Loss for batch is  -1.2477017641067505\n",
      "Val Loss for batch is  -2.6894562244415283\n",
      "|Iter  983  | Total Val Loss  -6.635873913764954 |\n",
      "Loss for batch is  0.06636714935302734\n",
      "Loss for batch is  -0.5921261310577393\n",
      "Loss for batch is  -0.0356367826461792\n",
      "Loss for batch is  -1.0709497928619385\n",
      "|Iter  984  | Total Train Loss  -1.6323455572128296 |\n",
      "Val Loss for batch is  -1.5299136638641357\n",
      "Val Loss for batch is  -1.9367671012878418\n",
      "Val Loss for batch is  -1.7432790994644165\n",
      "Val Loss for batch is  -2.7816145420074463\n",
      "|Iter  984  | Total Val Loss  -7.99157440662384 |\n",
      "Loss for batch is  -0.24476635456085205\n",
      "Loss for batch is  -0.36859917640686035\n",
      "Loss for batch is  -0.2590811252593994\n",
      "Loss for batch is  -1.2836337089538574\n",
      "|Iter  985  | Total Train Loss  -2.1560803651809692 |\n",
      "Val Loss for batch is  -1.7060964107513428\n",
      "Val Loss for batch is  -1.9269696474075317\n",
      "Val Loss for batch is  -1.8179055452346802\n",
      "Val Loss for batch is  -2.7818427085876465\n",
      "|Iter  985  | Total Val Loss  -8.232814311981201 |\n",
      "Loss for batch is  -0.25897347927093506\n",
      "Loss for batch is  -0.4756413698196411\n",
      "Loss for batch is  -0.35660839080810547\n",
      "Loss for batch is  -1.2368804216384888\n",
      "|Iter  986  | Total Train Loss  -2.3281036615371704 |\n",
      "Val Loss for batch is  -1.721587061882019\n",
      "Val Loss for batch is  -1.9695374965667725\n",
      "Val Loss for batch is  -1.7419229745864868\n",
      "Val Loss for batch is  -2.759556531906128\n",
      "|Iter  986  | Total Val Loss  -8.192604064941406 |\n",
      "Loss for batch is  -0.34666764736175537\n",
      "Loss for batch is  -0.5219005346298218\n",
      "Loss for batch is  -0.4004819393157959\n",
      "Loss for batch is  -1.2688040733337402\n",
      "|Iter  987  | Total Train Loss  -2.5378541946411133 |\n",
      "Val Loss for batch is  -1.8714357614517212\n",
      "Val Loss for batch is  -2.0446534156799316\n",
      "Val Loss for batch is  -1.9337726831436157\n",
      "Val Loss for batch is  -2.8079817295074463\n",
      "|Iter  987  | Total Val Loss  -8.657843589782715 |\n",
      "Loss for batch is  -0.3637232780456543\n",
      "Loss for batch is  -0.5573385953903198\n",
      "Loss for batch is  -0.4420236349105835\n",
      "Loss for batch is  -1.3267630338668823\n",
      "|Iter  988  | Total Train Loss  -2.68984854221344 |\n",
      "Val Loss for batch is  -1.9496716260910034\n",
      "Val Loss for batch is  -2.0520384311676025\n",
      "Val Loss for batch is  -1.987361192703247\n",
      "Val Loss for batch is  -2.858203887939453\n",
      "|Iter  988  | Total Val Loss  -8.847275137901306 |\n",
      "Loss for batch is  -0.4534658193588257\n",
      "Loss for batch is  -0.559478759765625\n",
      "Loss for batch is  -0.4497990608215332\n",
      "Loss for batch is  -1.3586851358413696\n",
      "|Iter  989  | Total Train Loss  -2.8214287757873535 |\n",
      "Val Loss for batch is  -1.9807833433151245\n",
      "Val Loss for batch is  -2.107152223587036\n",
      "Val Loss for batch is  -2.0383877754211426\n",
      "Val Loss for batch is  -2.8967320919036865\n",
      "|Iter  989  | Total Val Loss  -9.02305543422699 |\n",
      "Loss for batch is  -0.4972647428512573\n",
      "Loss for batch is  -0.6213972568511963\n",
      "Loss for batch is  -0.4949793815612793\n",
      "Loss for batch is  -1.394067645072937\n",
      "|Iter  990  | Total Train Loss  -3.00770902633667 |\n",
      "Val Loss for batch is  -1.932005763053894\n",
      "Val Loss for batch is  -2.108403205871582\n",
      "Val Loss for batch is  -2.0383336544036865\n",
      "Val Loss for batch is  -2.909597635269165\n",
      "|Iter  990  | Total Val Loss  -8.988340258598328 |\n",
      "Loss for batch is  -0.5081100463867188\n",
      "Loss for batch is  -0.6484084129333496\n",
      "Loss for batch is  -0.5392688512802124\n",
      "Loss for batch is  -1.413793683052063\n",
      "|Iter  991  | Total Train Loss  -3.1095809936523438 |\n",
      "Val Loss for batch is  -1.951463222503662\n",
      "Val Loss for batch is  -2.1504364013671875\n",
      "Val Loss for batch is  -2.008674144744873\n",
      "Val Loss for batch is  -2.9505834579467773\n",
      "|Iter  991  | Total Val Loss  -9.0611572265625 |\n",
      "Loss for batch is  -0.535763144493103\n",
      "Loss for batch is  -0.6805336475372314\n",
      "Loss for batch is  -0.531994104385376\n",
      "Loss for batch is  -1.4617923498153687\n",
      "|Iter  992  | Total Train Loss  -3.210083246231079 |\n",
      "Val Loss for batch is  -1.9538543224334717\n",
      "Val Loss for batch is  -2.1365318298339844\n",
      "Val Loss for batch is  -2.0065972805023193\n",
      "Val Loss for batch is  -2.9609110355377197\n",
      "|Iter  992  | Total Val Loss  -9.057894468307495 |\n",
      "Loss for batch is  -0.5342581272125244\n",
      "Loss for batch is  -0.701001763343811\n",
      "Loss for batch is  -0.5795068740844727\n",
      "Loss for batch is  -1.4525253772735596\n",
      "|Iter  993  | Total Train Loss  -3.2672921419143677 |\n",
      "Val Loss for batch is  -2.0045905113220215\n",
      "Val Loss for batch is  -2.1554830074310303\n",
      "Val Loss for batch is  -2.054049015045166\n",
      "Val Loss for batch is  -3.0154051780700684\n",
      "|Iter  993  | Total Val Loss  -9.229527711868286 |\n",
      "Loss for batch is  -0.5841033458709717\n",
      "Loss for batch is  -0.7025930881500244\n",
      "Loss for batch is  -0.5801469087600708\n",
      "Loss for batch is  -1.4970101118087769\n",
      "|Iter  994  | Total Train Loss  -3.3638534545898438 |\n",
      "Val Loss for batch is  -1.8526685237884521\n",
      "Val Loss for batch is  -2.087082862854004\n",
      "Val Loss for batch is  -1.915550947189331\n",
      "Val Loss for batch is  -2.9774961471557617\n",
      "|Iter  994  | Total Val Loss  -8.832798480987549 |\n",
      "Loss for batch is  -0.4678546190261841\n",
      "Loss for batch is  -0.6952199935913086\n",
      "Loss for batch is  -0.42869818210601807\n",
      "Loss for batch is  -1.5280845165252686\n",
      "|Iter  995  | Total Train Loss  -3.1198573112487793 |\n",
      "Val Loss for batch is  -1.6013545989990234\n",
      "Val Loss for batch is  -1.7701929807662964\n",
      "Val Loss for batch is  -1.6265714168548584\n",
      "Val Loss for batch is  -2.842895746231079\n",
      "|Iter  995  | Total Val Loss  -7.841014742851257 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.239732027053833\n",
      "Loss for batch is  -0.7064990997314453\n",
      "Loss for batch is  -0.25589847564697266\n",
      "Loss for batch is  -1.3588597774505615\n",
      "|Iter  996  | Total Train Loss  -2.5609893798828125 |\n",
      "Val Loss for batch is  -1.7515840530395508\n",
      "Val Loss for batch is  -1.9519706964492798\n",
      "Val Loss for batch is  -1.7461493015289307\n",
      "Val Loss for batch is  -2.867304801940918\n",
      "|Iter  996  | Total Val Loss  -8.31700885295868 |\n",
      "Loss for batch is  -0.38744163513183594\n",
      "Loss for batch is  -0.19634699821472168\n",
      "Loss for batch is  -0.41783320903778076\n",
      "Loss for batch is  -1.2477442026138306\n",
      "|Iter  997  | Total Train Loss  -2.249366044998169 |\n",
      "Val Loss for batch is  -1.0103840827941895\n",
      "Val Loss for batch is  -1.2335888147354126\n",
      "Val Loss for batch is  -1.4363865852355957\n",
      "Val Loss for batch is  -2.666999340057373\n",
      "|Iter  997  | Total Val Loss  -6.347358822822571 |\n",
      "Loss for batch is  0.38279712200164795\n",
      "Loss for batch is  -0.5619369745254517\n",
      "Loss for batch is  0.1761155128479004\n",
      "Loss for batch is  -0.6789294481277466\n",
      "|Iter  998  | Total Train Loss  -0.6819537878036499 |\n",
      "Val Loss for batch is  -1.415086030960083\n",
      "Val Loss for batch is  -1.9065544605255127\n",
      "Val Loss for batch is  -1.5834468603134155\n",
      "Val Loss for batch is  -2.678601026535034\n",
      "|Iter  998  | Total Val Loss  -7.583688378334045 |\n",
      "Loss for batch is  0.009263753890991211\n",
      "Loss for batch is  -0.08551812171936035\n",
      "Loss for batch is  0.35028111934661865\n",
      "Loss for batch is  -0.9219787120819092\n",
      "|Iter  999  | Total Train Loss  -0.6479519605636597 |\n",
      "Val Loss for batch is  -1.4368529319763184\n",
      "Val Loss for batch is  -1.8323113918304443\n",
      "Val Loss for batch is  -1.5487715005874634\n",
      "Val Loss for batch is  -2.6566500663757324\n",
      "|Iter  999  | Total Val Loss  -7.4745858907699585 |\n",
      "Loss for batch is  -0.08492457866668701\n",
      "Loss for batch is  -0.27443623542785645\n",
      "Loss for batch is  -0.0747230052947998\n",
      "Loss for batch is  -0.9604839086532593\n",
      "|Iter  1000  | Total Train Loss  -1.3945677280426025 |\n",
      "Val Loss for batch is  -1.5528782606124878\n",
      "Val Loss for batch is  -1.8642280101776123\n",
      "Val Loss for batch is  -1.711134433746338\n",
      "Val Loss for batch is  -2.5685606002807617\n",
      "|Iter  1000  | Total Val Loss  -7.6968013048172 |\n",
      "Loss for batch is  -0.014338493347167969\n",
      "Loss for batch is  -0.3382291793823242\n",
      "Loss for batch is  -0.17815744876861572\n",
      "Loss for batch is  -1.0763782262802124\n",
      "|Iter  1001  | Total Train Loss  -1.6071033477783203 |\n",
      "Val Loss for batch is  -1.6900668144226074\n",
      "Val Loss for batch is  -1.8378187417984009\n",
      "Val Loss for batch is  -1.6679195165634155\n",
      "Val Loss for batch is  -2.6481282711029053\n",
      "|Iter  1001  | Total Val Loss  -7.843933343887329 |\n",
      "Loss for batch is  -0.16268444061279297\n",
      "Loss for batch is  -0.35023927688598633\n",
      "Loss for batch is  -0.23746812343597412\n",
      "Loss for batch is  -1.1298224925994873\n",
      "|Iter  1002  | Total Train Loss  -1.8802143335342407 |\n",
      "Val Loss for batch is  -1.8161901235580444\n",
      "Val Loss for batch is  -1.9373732805252075\n",
      "Val Loss for batch is  -1.8088208436965942\n",
      "Val Loss for batch is  -2.7212908267974854\n",
      "|Iter  1002  | Total Val Loss  -8.283675074577332 |\n",
      "Loss for batch is  -0.24739181995391846\n",
      "Loss for batch is  -0.4389674663543701\n",
      "Loss for batch is  -0.3113076686859131\n",
      "Loss for batch is  -1.13762629032135\n",
      "|Iter  1003  | Total Train Loss  -2.1352932453155518 |\n",
      "Val Loss for batch is  -1.8318889141082764\n",
      "Val Loss for batch is  -1.943447470664978\n",
      "Val Loss for batch is  -1.801400899887085\n",
      "Val Loss for batch is  -2.6682140827178955\n",
      "|Iter  1003  | Total Val Loss  -8.244951367378235 |\n",
      "Loss for batch is  -0.32567811012268066\n",
      "Loss for batch is  -0.46801280975341797\n",
      "Loss for batch is  -0.3324321508407593\n",
      "Loss for batch is  -1.216322898864746\n",
      "|Iter  1004  | Total Train Loss  -2.342445969581604 |\n",
      "Val Loss for batch is  -1.9000592231750488\n",
      "Val Loss for batch is  -2.017655611038208\n",
      "Val Loss for batch is  -1.9655174016952515\n",
      "Val Loss for batch is  -2.783536911010742\n",
      "|Iter  1004  | Total Val Loss  -8.66676914691925 |\n",
      "Loss for batch is  -0.3740522861480713\n",
      "Loss for batch is  -0.5082772970199585\n",
      "Loss for batch is  -0.39561784267425537\n",
      "Loss for batch is  -1.2830188274383545\n",
      "|Iter  1005  | Total Train Loss  -2.5609662532806396 |\n",
      "Val Loss for batch is  -1.8862541913986206\n",
      "Val Loss for batch is  -2.039266347885132\n",
      "Val Loss for batch is  -1.9178354740142822\n",
      "Val Loss for batch is  -2.8331403732299805\n",
      "|Iter  1005  | Total Val Loss  -8.676496386528015 |\n",
      "Loss for batch is  -0.4396167993545532\n",
      "Loss for batch is  -0.5759451389312744\n",
      "Loss for batch is  -0.46996116638183594\n",
      "Loss for batch is  -1.340274453163147\n",
      "|Iter  1006  | Total Train Loss  -2.8257975578308105 |\n",
      "Val Loss for batch is  -1.8722134828567505\n",
      "Val Loss for batch is  -2.050675392150879\n",
      "Val Loss for batch is  -1.9739887714385986\n",
      "Val Loss for batch is  -2.8747169971466064\n",
      "|Iter  1006  | Total Val Loss  -8.771594643592834 |\n",
      "Loss for batch is  -0.45500290393829346\n",
      "Loss for batch is  -0.6267709732055664\n",
      "Loss for batch is  -0.4971977472305298\n",
      "Loss for batch is  -1.4111328125\n",
      "|Iter  1007  | Total Train Loss  -2.9901044368743896 |\n",
      "Val Loss for batch is  -1.8909803628921509\n",
      "Val Loss for batch is  -2.0609798431396484\n",
      "Val Loss for batch is  -2.010899066925049\n",
      "Val Loss for batch is  -2.944563627243042\n",
      "|Iter  1007  | Total Val Loss  -8.90742290019989 |\n",
      "Loss for batch is  -0.492156982421875\n",
      "Loss for batch is  -0.6446254253387451\n",
      "Loss for batch is  -0.5496401786804199\n",
      "Loss for batch is  -1.455971121788025\n",
      "|Iter  1008  | Total Train Loss  -3.142393708229065 |\n",
      "Val Loss for batch is  -1.9207004308700562\n",
      "Val Loss for batch is  -2.1382477283477783\n",
      "Val Loss for batch is  -1.9973034858703613\n",
      "Val Loss for batch is  -2.9937074184417725\n",
      "|Iter  1008  | Total Val Loss  -9.049959063529968 |\n",
      "Loss for batch is  -0.529816746711731\n",
      "Loss for batch is  -0.7061079740524292\n",
      "Loss for batch is  -0.5593181848526001\n",
      "Loss for batch is  -1.477651596069336\n",
      "|Iter  1009  | Total Train Loss  -3.272894501686096 |\n",
      "Val Loss for batch is  -1.892651915550232\n",
      "Val Loss for batch is  -2.1472630500793457\n",
      "Val Loss for batch is  -2.023428201675415\n",
      "Val Loss for batch is  -3.0055699348449707\n",
      "|Iter  1009  | Total Val Loss  -9.068913102149963 |\n",
      "Loss for batch is  -0.5560587644577026\n",
      "Loss for batch is  -0.7071384191513062\n",
      "Loss for batch is  -0.5829501152038574\n",
      "Loss for batch is  -1.4962403774261475\n",
      "|Iter  1010  | Total Train Loss  -3.3423876762390137 |\n",
      "Val Loss for batch is  -1.8943928480148315\n",
      "Val Loss for batch is  -2.1399800777435303\n",
      "Val Loss for batch is  -1.9596130847930908\n",
      "Val Loss for batch is  -3.0202114582061768\n",
      "|Iter  1010  | Total Val Loss  -9.01419746875763 |\n",
      "Loss for batch is  -0.5345295667648315\n",
      "Loss for batch is  -0.7220104932785034\n",
      "Loss for batch is  -0.5782922506332397\n",
      "Loss for batch is  -1.5244742631912231\n",
      "|Iter  1011  | Total Train Loss  -3.359306573867798 |\n",
      "Val Loss for batch is  -1.8743093013763428\n",
      "Val Loss for batch is  -2.108574151992798\n",
      "Val Loss for batch is  -2.002126932144165\n",
      "Val Loss for batch is  -3.0419018268585205\n",
      "|Iter  1011  | Total Val Loss  -9.026912212371826 |\n",
      "Loss for batch is  -0.5104107856750488\n",
      "Loss for batch is  -0.7187412977218628\n",
      "Loss for batch is  -0.5595871210098267\n",
      "Loss for batch is  -1.5255589485168457\n",
      "|Iter  1012  | Total Train Loss  -3.314298152923584 |\n",
      "Val Loss for batch is  -1.7144386768341064\n",
      "Val Loss for batch is  -1.847858190536499\n",
      "Val Loss for batch is  -1.723071575164795\n",
      "Val Loss for batch is  -2.995126962661743\n",
      "|Iter  1012  | Total Val Loss  -8.280495405197144 |\n",
      "Loss for batch is  -0.34591472148895264\n",
      "Loss for batch is  -0.6669151782989502\n",
      "Loss for batch is  -0.38151872158050537\n",
      "Loss for batch is  -1.5267102718353271\n",
      "|Iter  1013  | Total Train Loss  -2.9210588932037354 |\n",
      "Val Loss for batch is  -1.3827930688858032\n",
      "Val Loss for batch is  -1.4662641286849976\n",
      "Val Loss for batch is  -1.2235151529312134\n",
      "Val Loss for batch is  -2.8566322326660156\n",
      "|Iter  1013  | Total Val Loss  -6.92920458316803 |\n",
      "Loss for batch is  -0.03390145301818848\n",
      "Loss for batch is  -0.6503903865814209\n",
      "Loss for batch is  -0.32009148597717285\n",
      "Loss for batch is  -1.0825631618499756\n",
      "|Iter  1014  | Total Train Loss  -2.086946487426758 |\n",
      "Val Loss for batch is  -1.623336672782898\n",
      "Val Loss for batch is  -1.9026092290878296\n",
      "Val Loss for batch is  -1.7021923065185547\n",
      "Val Loss for batch is  -2.8426129817962646\n",
      "|Iter  1014  | Total Val Loss  -8.070751190185547 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.27684223651885986\n",
      "Loss for batch is  -0.44021308422088623\n",
      "Loss for batch is  -0.3074371814727783\n",
      "Loss for batch is  -1.1878364086151123\n",
      "|Iter  1015  | Total Train Loss  -2.2123289108276367 |\n",
      "Val Loss for batch is  -1.6248795986175537\n",
      "Val Loss for batch is  -1.8754613399505615\n",
      "Val Loss for batch is  -1.6527576446533203\n",
      "Val Loss for batch is  -2.744969129562378\n",
      "|Iter  1015  | Total Val Loss  -7.8980677127838135 |\n",
      "Loss for batch is  -0.23466086387634277\n",
      "Loss for batch is  -0.6715575456619263\n",
      "Loss for batch is  -0.3797769546508789\n",
      "Loss for batch is  -1.213929295539856\n",
      "|Iter  1016  | Total Train Loss  -2.499924659729004 |\n",
      "Val Loss for batch is  -1.681114912033081\n",
      "Val Loss for batch is  -1.8949594497680664\n",
      "Val Loss for batch is  -1.7436914443969727\n",
      "Val Loss for batch is  -2.7227284908294678\n",
      "|Iter  1016  | Total Val Loss  -8.042494297027588 |\n",
      "Loss for batch is  -0.3294501304626465\n",
      "Loss for batch is  -0.5613619089126587\n",
      "Loss for batch is  -0.46440374851226807\n",
      "Loss for batch is  -1.3330297470092773\n",
      "|Iter  1017  | Total Train Loss  -2.6882455348968506 |\n",
      "Val Loss for batch is  -1.8299469947814941\n",
      "Val Loss for batch is  -2.0208539962768555\n",
      "Val Loss for batch is  -1.8803889751434326\n",
      "Val Loss for batch is  -2.8362348079681396\n",
      "|Iter  1017  | Total Val Loss  -8.567424774169922 |\n",
      "Loss for batch is  -0.37575221061706543\n",
      "Loss for batch is  -0.575403094291687\n",
      "Loss for batch is  -0.43751955032348633\n",
      "Loss for batch is  -1.3353031873703003\n",
      "|Iter  1018  | Total Train Loss  -2.723978042602539 |\n",
      "Val Loss for batch is  -1.8311054706573486\n",
      "Val Loss for batch is  -2.0350430011749268\n",
      "Val Loss for batch is  -1.9052019119262695\n",
      "Val Loss for batch is  -2.827650308609009\n",
      "|Iter  1018  | Total Val Loss  -8.599000692367554 |\n",
      "Loss for batch is  -0.4626314640045166\n",
      "Loss for batch is  -0.6230202913284302\n",
      "Loss for batch is  -0.5151808261871338\n",
      "Loss for batch is  -1.359727382659912\n",
      "|Iter  1019  | Total Train Loss  -2.9605599641799927 |\n",
      "Val Loss for batch is  -1.9151555299758911\n",
      "Val Loss for batch is  -2.117280960083008\n",
      "Val Loss for batch is  -2.036463737487793\n",
      "Val Loss for batch is  -2.876791477203369\n",
      "|Iter  1019  | Total Val Loss  -8.945691704750061 |\n",
      "Loss for batch is  -0.4782637357711792\n",
      "Loss for batch is  -0.6539626121520996\n",
      "Loss for batch is  -0.5302060842514038\n",
      "Loss for batch is  -1.4130538702011108\n",
      "|Iter  1020  | Total Train Loss  -3.0754863023757935 |\n",
      "Val Loss for batch is  -2.007063150405884\n",
      "Val Loss for batch is  -2.0687859058380127\n",
      "Val Loss for batch is  -2.022144317626953\n",
      "Val Loss for batch is  -2.956418991088867\n",
      "|Iter  1020  | Total Val Loss  -9.054412364959717 |\n",
      "Loss for batch is  -0.5541247129440308\n",
      "Loss for batch is  -0.6628127098083496\n",
      "Loss for batch is  -0.5569020509719849\n",
      "Loss for batch is  -1.4446179866790771\n",
      "|Iter  1021  | Total Train Loss  -3.2184574604034424 |\n",
      "Val Loss for batch is  -2.0037121772766113\n",
      "Val Loss for batch is  -2.1936697959899902\n",
      "Val Loss for batch is  -2.085083246231079\n",
      "Val Loss for batch is  -2.9206902980804443\n",
      "|Iter  1021  | Total Val Loss  -9.203155517578125 |\n",
      "Loss for batch is  -0.5667937994003296\n",
      "Loss for batch is  -0.7210044860839844\n",
      "Loss for batch is  -0.588007926940918\n",
      "Loss for batch is  -1.4849530458450317\n",
      "|Iter  1022  | Total Train Loss  -3.3607592582702637 |\n",
      "Val Loss for batch is  -1.988871455192566\n",
      "Val Loss for batch is  -2.149794340133667\n",
      "Val Loss for batch is  -2.073586940765381\n",
      "Val Loss for batch is  -3.019258975982666\n",
      "|Iter  1022  | Total Val Loss  -9.23151171207428 |\n",
      "Loss for batch is  -0.5775574445724487\n",
      "Loss for batch is  -0.7540223598480225\n",
      "Loss for batch is  -0.6393483877182007\n",
      "Loss for batch is  -1.5299361944198608\n",
      "|Iter  1023  | Total Train Loss  -3.5008643865585327 |\n",
      "Val Loss for batch is  -2.0159690380096436\n",
      "Val Loss for batch is  -2.2134759426116943\n",
      "Val Loss for batch is  -2.0654141902923584\n",
      "Val Loss for batch is  -3.0451736450195312\n",
      "|Iter  1023  | Total Val Loss  -9.340032815933228 |\n",
      "Loss for batch is  -0.6057819128036499\n",
      "Loss for batch is  -0.7787353992462158\n",
      "Loss for batch is  -0.6731404066085815\n",
      "Loss for batch is  -1.5543843507766724\n",
      "|Iter  1024  | Total Train Loss  -3.6120420694351196 |\n",
      "Val Loss for batch is  -1.9906930923461914\n",
      "Val Loss for batch is  -2.1993072032928467\n",
      "Val Loss for batch is  -2.0779364109039307\n",
      "Val Loss for batch is  -3.0523924827575684\n",
      "|Iter  1024  | Total Val Loss  -9.320329189300537 |\n",
      "Loss for batch is  -0.6260379552841187\n",
      "Loss for batch is  -0.7732259035110474\n",
      "Loss for batch is  -0.670285701751709\n",
      "Loss for batch is  -1.5687801837921143\n",
      "|Iter  1025  | Total Train Loss  -3.6383297443389893 |\n",
      "Val Loss for batch is  -1.951851725578308\n",
      "Val Loss for batch is  -2.1244006156921387\n",
      "Val Loss for batch is  -2.042602300643921\n",
      "Val Loss for batch is  -3.0592610836029053\n",
      "|Iter  1025  | Total Val Loss  -9.178115725517273 |\n",
      "Loss for batch is  -0.5893243551254272\n",
      "Loss for batch is  -0.7622116804122925\n",
      "Loss for batch is  -0.607840895652771\n",
      "Loss for batch is  -1.5997059345245361\n",
      "|Iter  1026  | Total Train Loss  -3.559082865715027 |\n",
      "Val Loss for batch is  -1.787170171737671\n",
      "Val Loss for batch is  -2.012713670730591\n",
      "Val Loss for batch is  -1.853391170501709\n",
      "Val Loss for batch is  -2.949712038040161\n",
      "|Iter  1026  | Total Val Loss  -8.602987051010132 |\n",
      "Loss for batch is  -0.44712376594543457\n",
      "Loss for batch is  -0.7449216842651367\n",
      "Loss for batch is  -0.39997923374176025\n",
      "Loss for batch is  -1.529072880744934\n",
      "|Iter  1027  | Total Train Loss  -3.1210975646972656 |\n",
      "Val Loss for batch is  -1.6677305698394775\n",
      "Val Loss for batch is  -1.8484611511230469\n",
      "Val Loss for batch is  -1.745125412940979\n",
      "Val Loss for batch is  -2.9161646366119385\n",
      "|Iter  1027  | Total Val Loss  -8.177481770515442 |\n",
      "Loss for batch is  -0.3396404981613159\n",
      "Loss for batch is  -0.6967741250991821\n",
      "Loss for batch is  -0.3400254249572754\n",
      "Loss for batch is  -1.40110445022583\n",
      "|Iter  1028  | Total Train Loss  -2.7775444984436035 |\n",
      "Val Loss for batch is  -1.460563063621521\n",
      "Val Loss for batch is  -1.4271798133850098\n",
      "Val Loss for batch is  -1.2705529928207397\n",
      "Val Loss for batch is  -2.8415567874908447\n",
      "|Iter  1028  | Total Val Loss  -6.999852657318115 |\n",
      "Loss for batch is  -0.21603548526763916\n",
      "Loss for batch is  -0.4444918632507324\n",
      "Loss for batch is  -0.21236276626586914\n",
      "Loss for batch is  -0.7258287668228149\n",
      "|Iter  1029  | Total Train Loss  -1.5987188816070557 |\n",
      "Val Loss for batch is  -1.7214707136154175\n",
      "Val Loss for batch is  -2.0166077613830566\n",
      "Val Loss for batch is  -1.9127459526062012\n",
      "Val Loss for batch is  -2.8676300048828125\n",
      "|Iter  1029  | Total Val Loss  -8.518454432487488 |\n",
      "Loss for batch is  -0.3685218095779419\n",
      "Loss for batch is  -0.3336629867553711\n",
      "Loss for batch is  0.26054859161376953\n",
      "Loss for batch is  -1.1656466722488403\n",
      "|Iter  1030  | Total Train Loss  -1.6072828769683838 |\n",
      "Val Loss for batch is  -1.6830291748046875\n",
      "Val Loss for batch is  -1.9038832187652588\n",
      "Val Loss for batch is  -1.6870310306549072\n",
      "Val Loss for batch is  -2.7052764892578125\n",
      "|Iter  1030  | Total Val Loss  -7.979219913482666 |\n",
      "Loss for batch is  -0.25955820083618164\n",
      "Loss for batch is  -0.3325378894805908\n",
      "Loss for batch is  -0.22866487503051758\n",
      "Loss for batch is  -1.177588701248169\n",
      "|Iter  1031  | Total Train Loss  -1.998349666595459 |\n",
      "Val Loss for batch is  -1.6486930847167969\n",
      "Val Loss for batch is  -1.8325490951538086\n",
      "Val Loss for batch is  -1.6885709762573242\n",
      "Val Loss for batch is  -2.72013258934021\n",
      "|Iter  1031  | Total Val Loss  -7.88994574546814 |\n",
      "Loss for batch is  -0.2890833616256714\n",
      "Loss for batch is  -0.4737839698791504\n",
      "Loss for batch is  -0.20823562145233154\n",
      "Loss for batch is  -1.1970434188842773\n",
      "|Iter  1032  | Total Train Loss  -2.1681463718414307 |\n",
      "Val Loss for batch is  -1.7599376440048218\n",
      "Val Loss for batch is  -1.8985919952392578\n",
      "Val Loss for batch is  -1.6083403825759888\n",
      "Val Loss for batch is  -2.7184345722198486\n",
      "|Iter  1032  | Total Val Loss  -7.985304594039917 |\n",
      "Loss for batch is  -0.37311530113220215\n",
      "Loss for batch is  -0.47331929206848145\n",
      "Loss for batch is  -0.33006012439727783\n",
      "Loss for batch is  -1.232305884361267\n",
      "|Iter  1033  | Total Train Loss  -2.4088006019592285 |\n",
      "Val Loss for batch is  -1.8619749546051025\n",
      "Val Loss for batch is  -2.000168561935425\n",
      "Val Loss for batch is  -1.9005341529846191\n",
      "Val Loss for batch is  -2.756772994995117\n",
      "|Iter  1033  | Total Val Loss  -8.519450664520264 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.4044365882873535\n",
      "Loss for batch is  -0.5059000253677368\n",
      "Loss for batch is  -0.40809011459350586\n",
      "Loss for batch is  -1.2772538661956787\n",
      "|Iter  1034  | Total Train Loss  -2.595680594444275 |\n",
      "Val Loss for batch is  -1.9725615978240967\n",
      "Val Loss for batch is  -2.0464589595794678\n",
      "Val Loss for batch is  -1.9350637197494507\n",
      "Val Loss for batch is  -2.8051371574401855\n",
      "|Iter  1034  | Total Val Loss  -8.7592214345932 |\n",
      "Loss for batch is  -0.4438134431838989\n",
      "Loss for batch is  -0.5491857528686523\n",
      "Loss for batch is  -0.39064204692840576\n",
      "Loss for batch is  -1.3496798276901245\n",
      "|Iter  1035  | Total Train Loss  -2.7333210706710815 |\n",
      "Val Loss for batch is  -1.882223129272461\n",
      "Val Loss for batch is  -1.9833616018295288\n",
      "Val Loss for batch is  -1.9162850379943848\n",
      "Val Loss for batch is  -2.82049560546875\n",
      "|Iter  1035  | Total Val Loss  -8.602365374565125 |\n",
      "Loss for batch is  -0.47870779037475586\n",
      "Loss for batch is  -0.5162365436553955\n",
      "Loss for batch is  -0.45994341373443604\n",
      "Loss for batch is  -1.412194013595581\n",
      "|Iter  1036  | Total Train Loss  -2.8670817613601685 |\n",
      "Val Loss for batch is  -1.9589824676513672\n",
      "Val Loss for batch is  -2.0899112224578857\n",
      "Val Loss for batch is  -2.0094950199127197\n",
      "Val Loss for batch is  -2.9299869537353516\n",
      "|Iter  1036  | Total Val Loss  -8.988375663757324 |\n",
      "Loss for batch is  -0.49676942825317383\n",
      "Loss for batch is  -0.581929087638855\n",
      "Loss for batch is  -0.49406230449676514\n",
      "Loss for batch is  -1.4445583820343018\n",
      "|Iter  1037  | Total Train Loss  -3.0173192024230957 |\n",
      "Val Loss for batch is  -1.9460093975067139\n",
      "Val Loss for batch is  -2.0391125679016113\n",
      "Val Loss for batch is  -1.9815303087234497\n",
      "Val Loss for batch is  -2.932147979736328\n",
      "|Iter  1037  | Total Val Loss  -8.898800253868103 |\n",
      "Loss for batch is  -0.517311692237854\n",
      "Loss for batch is  -0.6532531976699829\n",
      "Loss for batch is  -0.5650676488876343\n",
      "Loss for batch is  -1.4789050817489624\n",
      "|Iter  1038  | Total Train Loss  -3.2145376205444336 |\n",
      "Val Loss for batch is  -2.000305414199829\n",
      "Val Loss for batch is  -2.143047571182251\n",
      "Val Loss for batch is  -2.055015802383423\n",
      "Val Loss for batch is  -2.9745874404907227\n",
      "|Iter  1038  | Total Val Loss  -9.172956228256226 |\n",
      "Loss for batch is  -0.5552471876144409\n",
      "Loss for batch is  -0.7030190229415894\n",
      "Loss for batch is  -0.5965007543563843\n",
      "Loss for batch is  -1.5162171125411987\n",
      "|Iter  1039  | Total Train Loss  -3.3709840774536133 |\n",
      "Val Loss for batch is  -1.9863076210021973\n",
      "Val Loss for batch is  -2.1803348064422607\n",
      "Val Loss for batch is  -2.0653676986694336\n",
      "Val Loss for batch is  -2.9992380142211914\n",
      "|Iter  1039  | Total Val Loss  -9.231248140335083 |\n",
      "Loss for batch is  -0.5868743658065796\n",
      "Loss for batch is  -0.7554337978363037\n",
      "Loss for batch is  -0.6248414516448975\n",
      "Loss for batch is  -1.5539377927780151\n",
      "|Iter  1040  | Total Train Loss  -3.521087408065796 |\n",
      "Val Loss for batch is  -2.0530190467834473\n",
      "Val Loss for batch is  -2.203453779220581\n",
      "Val Loss for batch is  -2.082836627960205\n",
      "Val Loss for batch is  -3.0403738021850586\n",
      "|Iter  1040  | Total Val Loss  -9.379683256149292 |\n",
      "Loss for batch is  -0.6198753118515015\n",
      "Loss for batch is  -0.7702229022979736\n",
      "Loss for batch is  -0.6390112638473511\n",
      "Loss for batch is  -1.569737434387207\n",
      "|Iter  1041  | Total Train Loss  -3.598846912384033 |\n",
      "Val Loss for batch is  -1.9727777242660522\n",
      "Val Loss for batch is  -2.1814088821411133\n",
      "Val Loss for batch is  -2.0289254188537598\n",
      "Val Loss for batch is  -3.0393483638763428\n",
      "|Iter  1041  | Total Val Loss  -9.222460389137268 |\n",
      "Loss for batch is  -0.5937012434005737\n",
      "Loss for batch is  -0.7898428440093994\n",
      "Loss for batch is  -0.6029857397079468\n",
      "Loss for batch is  -1.6004362106323242\n",
      "|Iter  1042  | Total Train Loss  -3.586966037750244 |\n",
      "Val Loss for batch is  -1.84287428855896\n",
      "Val Loss for batch is  -1.9240916967391968\n",
      "Val Loss for batch is  -1.69367516040802\n",
      "Val Loss for batch is  -2.991307258605957\n",
      "|Iter  1042  | Total Val Loss  -8.451948404312134 |\n",
      "Loss for batch is  -0.46525394916534424\n",
      "Loss for batch is  -0.8115144968032837\n",
      "Loss for batch is  -0.512776255607605\n",
      "Loss for batch is  -1.6030975580215454\n",
      "|Iter  1043  | Total Train Loss  -3.3926422595977783 |\n",
      "Val Loss for batch is  -1.7304755449295044\n",
      "Val Loss for batch is  -1.8745144605636597\n",
      "Val Loss for batch is  -0.845797061920166\n",
      "Val Loss for batch is  -2.9526331424713135\n",
      "|Iter  1043  | Total Val Loss  -7.4034202098846436 |\n",
      "Loss for batch is  -0.4656416177749634\n",
      "Loss for batch is  -0.6682898998260498\n",
      "Loss for batch is  -0.5089643001556396\n",
      "Loss for batch is  -0.764556884765625\n",
      "|Iter  1044  | Total Train Loss  -2.407452702522278 |\n",
      "Val Loss for batch is  -1.6904340982437134\n",
      "Val Loss for batch is  -1.939452052116394\n",
      "Val Loss for batch is  -1.7988576889038086\n",
      "Val Loss for batch is  -2.941016674041748\n",
      "|Iter  1044  | Total Val Loss  -8.369760513305664 |\n",
      "Loss for batch is  -0.34404850006103516\n",
      "Loss for batch is  0.1064455509185791\n",
      "Loss for batch is  0.1896299123764038\n",
      "Loss for batch is  -1.2712514400482178\n",
      "|Iter  1045  | Total Train Loss  -1.31922447681427 |\n",
      "Val Loss for batch is  -0.8495866656303406\n",
      "Val Loss for batch is  -0.9560784101486206\n",
      "Val Loss for batch is  -0.9090208411216736\n",
      "Val Loss for batch is  -2.4140076637268066\n",
      "|Iter  1045  | Total Val Loss  -5.128693580627441 |\n",
      "Loss for batch is  0.47058725357055664\n",
      "Loss for batch is  0.1311119794845581\n",
      "Loss for batch is  0.017853975296020508\n",
      "Loss for batch is  -0.930901050567627\n",
      "|Iter  1046  | Total Train Loss  -0.3113478422164917 |\n",
      "Val Loss for batch is  -1.4532021284103394\n",
      "Val Loss for batch is  -1.5975956916809082\n",
      "Val Loss for batch is  -1.3879594802856445\n",
      "Val Loss for batch is  -2.408994674682617\n",
      "|Iter  1046  | Total Val Loss  -6.847751975059509 |\n",
      "Loss for batch is  -0.06283724308013916\n",
      "Loss for batch is  -0.26400184631347656\n",
      "Loss for batch is  -0.18947899341583252\n",
      "Loss for batch is  -1.0494168996810913\n",
      "|Iter  1047  | Total Train Loss  -1.5657349824905396 |\n",
      "Val Loss for batch is  -1.5368071794509888\n",
      "Val Loss for batch is  -1.6496464014053345\n",
      "Val Loss for batch is  -1.6913120746612549\n",
      "Val Loss for batch is  -2.538255453109741\n",
      "|Iter  1047  | Total Val Loss  -7.416021108627319 |\n",
      "Loss for batch is  -0.12702906131744385\n",
      "Loss for batch is  -0.3144190311431885\n",
      "Loss for batch is  -0.33620691299438477\n",
      "Loss for batch is  -1.1158004999160767\n",
      "|Iter  1048  | Total Train Loss  -1.8934555053710938 |\n",
      "Val Loss for batch is  -1.7911237478256226\n",
      "Val Loss for batch is  -1.8738842010498047\n",
      "Val Loss for batch is  -1.757323145866394\n",
      "Val Loss for batch is  -2.5870468616485596\n",
      "|Iter  1048  | Total Val Loss  -8.00937795639038 |\n",
      "Loss for batch is  -0.2907531261444092\n",
      "Loss for batch is  -0.3872791528701782\n",
      "Loss for batch is  -0.2264648675918579\n",
      "Loss for batch is  -1.1353167295455933\n",
      "|Iter  1049  | Total Train Loss  -2.0398138761520386 |\n",
      "Val Loss for batch is  -1.7915682792663574\n",
      "Val Loss for batch is  -1.904131293296814\n",
      "Val Loss for batch is  -1.864173173904419\n",
      "Val Loss for batch is  -2.6960039138793945\n",
      "|Iter  1049  | Total Val Loss  -8.255876660346985 |\n",
      "Loss for batch is  -0.3303530216217041\n",
      "Loss for batch is  -0.4504498243331909\n",
      "Loss for batch is  -0.37247753143310547\n",
      "Loss for batch is  -1.1817529201507568\n",
      "|Iter  1050  | Total Train Loss  -2.3350332975387573 |\n",
      "Val Loss for batch is  -1.8418172597885132\n",
      "Val Loss for batch is  -1.889350175857544\n",
      "Val Loss for batch is  -1.8764569759368896\n",
      "Val Loss for batch is  -2.7007906436920166\n",
      "|Iter  1050  | Total Val Loss  -8.308415055274963 |\n",
      "Loss for batch is  -0.37201130390167236\n",
      "Loss for batch is  -0.48249340057373047\n",
      "Loss for batch is  -0.40844905376434326\n",
      "Loss for batch is  -1.2183105945587158\n",
      "|Iter  1051  | Total Train Loss  -2.481264352798462 |\n",
      "Val Loss for batch is  -1.910859227180481\n",
      "Val Loss for batch is  -2.0259218215942383\n",
      "Val Loss for batch is  -1.9362355470657349\n",
      "Val Loss for batch is  -2.7369189262390137\n",
      "|Iter  1051  | Total Val Loss  -8.609935522079468 |\n",
      "Loss for batch is  -0.4165452718734741\n",
      "Loss for batch is  -0.5408543348312378\n",
      "Loss for batch is  -0.44316256046295166\n",
      "Loss for batch is  -1.319156289100647\n",
      "|Iter  1052  | Total Train Loss  -2.7197184562683105 |\n",
      "Val Loss for batch is  -1.8938488960266113\n",
      "Val Loss for batch is  -2.0085084438323975\n",
      "Val Loss for batch is  -1.9522367715835571\n",
      "Val Loss for batch is  -2.8465335369110107\n",
      "|Iter  1052  | Total Val Loss  -8.701127648353577 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.47188353538513184\n",
      "Loss for batch is  -0.5653069019317627\n",
      "Loss for batch is  -0.5003499984741211\n",
      "Loss for batch is  -1.383972406387329\n",
      "|Iter  1053  | Total Train Loss  -2.9215128421783447 |\n",
      "Val Loss for batch is  -1.9134808778762817\n",
      "Val Loss for batch is  -2.0626580715179443\n",
      "Val Loss for batch is  -1.9441554546356201\n",
      "Val Loss for batch is  -2.9120287895202637\n",
      "|Iter  1053  | Total Val Loss  -8.83232319355011 |\n",
      "Loss for batch is  -0.5217946767807007\n",
      "Loss for batch is  -0.6419726610183716\n",
      "Loss for batch is  -0.5411821603775024\n",
      "Loss for batch is  -1.411919355392456\n",
      "|Iter  1054  | Total Train Loss  -3.1168688535690308 |\n",
      "Val Loss for batch is  -1.9004392623901367\n",
      "Val Loss for batch is  -2.0733847618103027\n",
      "Val Loss for batch is  -1.9751594066619873\n",
      "Val Loss for batch is  -2.934410810470581\n",
      "|Iter  1054  | Total Val Loss  -8.883394241333008 |\n",
      "Loss for batch is  -0.5638512372970581\n",
      "Loss for batch is  -0.6859575510025024\n",
      "Loss for batch is  -0.5942853689193726\n",
      "Loss for batch is  -1.492900013923645\n",
      "|Iter  1055  | Total Train Loss  -3.336994171142578 |\n",
      "Val Loss for batch is  -1.898436188697815\n",
      "Val Loss for batch is  -2.104801654815674\n",
      "Val Loss for batch is  -2.069594144821167\n",
      "Val Loss for batch is  -3.019728183746338\n",
      "|Iter  1055  | Total Val Loss  -9.092560172080994 |\n",
      "Loss for batch is  -0.5794284343719482\n",
      "Loss for batch is  -0.7136731147766113\n",
      "Loss for batch is  -0.6165910959243774\n",
      "Loss for batch is  -1.5281814336776733\n",
      "|Iter  1056  | Total Train Loss  -3.4378740787506104 |\n",
      "Val Loss for batch is  -1.9259032011032104\n",
      "Val Loss for batch is  -2.1296327114105225\n",
      "Val Loss for batch is  -2.0555455684661865\n",
      "Val Loss for batch is  -3.023989677429199\n",
      "|Iter  1056  | Total Val Loss  -9.135071158409119 |\n",
      "Loss for batch is  -0.6036020517349243\n",
      "Loss for batch is  -0.7431049346923828\n",
      "Loss for batch is  -0.6578618288040161\n",
      "Loss for batch is  -1.5624001026153564\n",
      "|Iter  1057  | Total Train Loss  -3.5669689178466797 |\n",
      "Val Loss for batch is  -1.9722156524658203\n",
      "Val Loss for batch is  -2.1819310188293457\n",
      "Val Loss for batch is  -2.0339105129241943\n",
      "Val Loss for batch is  -3.0803213119506836\n",
      "|Iter  1057  | Total Val Loss  -9.268378496170044 |\n",
      "Loss for batch is  -0.6305508613586426\n",
      "Loss for batch is  -0.7865532636642456\n",
      "Loss for batch is  -0.6877250671386719\n",
      "Loss for batch is  -1.5923945903778076\n",
      "|Iter  1058  | Total Train Loss  -3.6972237825393677 |\n",
      "Val Loss for batch is  -1.9964655637741089\n",
      "Val Loss for batch is  -2.1408116817474365\n",
      "Val Loss for batch is  -2.103381395339966\n",
      "Val Loss for batch is  -3.073751926422119\n",
      "|Iter  1058  | Total Val Loss  -9.31441056728363 |\n",
      "Loss for batch is  -0.6469333171844482\n",
      "Loss for batch is  -0.8020272254943848\n",
      "Loss for batch is  -0.6905677318572998\n",
      "Loss for batch is  -1.613911509513855\n",
      "|Iter  1059  | Total Train Loss  -3.753439784049988 |\n",
      "Val Loss for batch is  -1.968019962310791\n",
      "Val Loss for batch is  -2.187739372253418\n",
      "Val Loss for batch is  -2.135214328765869\n",
      "Val Loss for batch is  -3.0978729724884033\n",
      "|Iter  1059  | Total Val Loss  -9.388846635818481 |\n",
      "Loss for batch is  -0.6479904651641846\n",
      "Loss for batch is  -0.7926715612411499\n",
      "Loss for batch is  -0.7122881412506104\n",
      "Loss for batch is  -1.6155189275741577\n",
      "|Iter  1060  | Total Train Loss  -3.7684690952301025 |\n",
      "Val Loss for batch is  -2.0071346759796143\n",
      "Val Loss for batch is  -2.1992526054382324\n",
      "Val Loss for batch is  -2.1107959747314453\n",
      "Val Loss for batch is  -3.1428282260894775\n",
      "|Iter  1060  | Total Val Loss  -9.46001148223877 |\n",
      "Loss for batch is  -0.6647684574127197\n",
      "Loss for batch is  -0.7497124671936035\n",
      "Loss for batch is  -0.6908299922943115\n",
      "Loss for batch is  -1.6399860382080078\n",
      "|Iter  1061  | Total Train Loss  -3.7452969551086426 |\n",
      "Val Loss for batch is  -1.7871874570846558\n",
      "Val Loss for batch is  -2.047400951385498\n",
      "Val Loss for batch is  -1.931317925453186\n",
      "Val Loss for batch is  -3.068155527114868\n",
      "|Iter  1061  | Total Val Loss  -8.834061861038208 |\n",
      "Loss for batch is  -0.49810361862182617\n",
      "Loss for batch is  -0.6985900402069092\n",
      "Loss for batch is  -0.49360191822052\n",
      "Loss for batch is  -1.5788617134094238\n",
      "|Iter  1062  | Total Train Loss  -3.269157290458679 |\n",
      "Val Loss for batch is  -1.231350302696228\n",
      "Val Loss for batch is  -1.3687912225723267\n",
      "Val Loss for batch is  -0.9091835021972656\n",
      "Val Loss for batch is  -2.588179111480713\n",
      "|Iter  1062  | Total Val Loss  -6.097504138946533 |\n",
      "Loss for batch is  -0.006443619728088379\n",
      "Loss for batch is  -0.789029598236084\n",
      "Loss for batch is  0.2386026382446289\n",
      "Loss for batch is  -1.4968310594558716\n",
      "|Iter  1063  | Total Train Loss  -2.053701639175415 |\n",
      "Val Loss for batch is  -1.492723822593689\n",
      "Val Loss for batch is  -1.442023754119873\n",
      "Val Loss for batch is  -1.2845091819763184\n",
      "Val Loss for batch is  -2.7109904289245605\n",
      "|Iter  1063  | Total Val Loss  -6.930247187614441 |\n",
      "Loss for batch is  -0.25570499897003174\n",
      "Loss for batch is  0.4095425605773926\n",
      "Loss for batch is  0.267592191696167\n",
      "Loss for batch is  -0.6308163404464722\n",
      "|Iter  1064  | Total Train Loss  -0.20938658714294434 |\n",
      "Val Loss for batch is  1.1523274183273315\n",
      "Val Loss for batch is  -0.007500210776925087\n",
      "Val Loss for batch is  1.6620575189590454\n",
      "Val Loss for batch is  -1.7019816637039185\n",
      "|Iter  1064  | Total Val Loss  1.1049030628055334 |\n",
      "Loss for batch is  1.5075241327285767\n",
      "Loss for batch is  0.3049783706665039\n",
      "Loss for batch is  2.004021167755127\n",
      "Loss for batch is  -0.49878501892089844\n",
      "|Iter  1065  | Total Train Loss  3.317738652229309 |\n",
      "Val Loss for batch is  -0.4735572040081024\n",
      "Val Loss for batch is  -0.8588935732841492\n",
      "Val Loss for batch is  -0.249983549118042\n",
      "Val Loss for batch is  -2.0977702140808105\n",
      "|Iter  1065  | Total Val Loss  -3.680204540491104 |\n",
      "Loss for batch is  1.1062809228897095\n",
      "Loss for batch is  0.6104620695114136\n",
      "Loss for batch is  0.5397654175758362\n",
      "Loss for batch is  -0.797425389289856\n",
      "|Iter  1066  | Total Train Loss  1.4590830206871033 |\n",
      "Val Loss for batch is  -1.1209322214126587\n",
      "Val Loss for batch is  -1.3739111423492432\n",
      "Val Loss for batch is  -1.2595014572143555\n",
      "Val Loss for batch is  -2.3079490661621094\n",
      "|Iter  1066  | Total Val Loss  -6.062293887138367 |\n",
      "Loss for batch is  0.6217084527015686\n",
      "Loss for batch is  0.19724130630493164\n",
      "Loss for batch is  0.31642138957977295\n",
      "Loss for batch is  -0.6409426927566528\n",
      "|Iter  1067  | Total Train Loss  0.49442845582962036 |\n",
      "Val Loss for batch is  -1.5016623735427856\n",
      "Val Loss for batch is  -1.5689696073532104\n",
      "Val Loss for batch is  -1.3454638719558716\n",
      "Val Loss for batch is  -2.175018787384033\n",
      "|Iter  1067  | Total Val Loss  -6.591114640235901 |\n",
      "Loss for batch is  0.16814672946929932\n",
      "Loss for batch is  0.04441690444946289\n",
      "Loss for batch is  0.21663212776184082\n",
      "Loss for batch is  -0.6409649848937988\n",
      "|Iter  1068  | Total Train Loss  -0.2117692232131958 |\n",
      "Val Loss for batch is  -1.618858814239502\n",
      "Val Loss for batch is  -1.7071166038513184\n",
      "Val Loss for batch is  -1.5117086172103882\n",
      "Val Loss for batch is  -2.183316469192505\n",
      "|Iter  1068  | Total Val Loss  -7.021000504493713 |\n",
      "Loss for batch is  0.0874326229095459\n",
      "Loss for batch is  0.025990605354309082\n",
      "Loss for batch is  0.21763300895690918\n",
      "Loss for batch is  -0.6221911907196045\n",
      "|Iter  1069  | Total Train Loss  -0.29113495349884033 |\n",
      "Val Loss for batch is  -1.652916669845581\n",
      "Val Loss for batch is  -1.7055045366287231\n",
      "Val Loss for batch is  -1.4907559156417847\n",
      "Val Loss for batch is  -2.21620512008667\n",
      "|Iter  1069  | Total Val Loss  -7.065382242202759 |\n",
      "Loss for batch is  0.11824357509613037\n",
      "Loss for batch is  -0.015102744102478027\n",
      "Loss for batch is  0.13377559185028076\n",
      "Loss for batch is  -0.6884897947311401\n",
      "|Iter  1070  | Total Train Loss  -0.45157337188720703 |\n",
      "Val Loss for batch is  -1.6722806692123413\n",
      "Val Loss for batch is  -1.7309777736663818\n",
      "Val Loss for batch is  -1.5062540769577026\n",
      "Val Loss for batch is  -2.2376973628997803\n",
      "|Iter  1070  | Total Val Loss  -7.147209882736206 |\n",
      "Loss for batch is  0.04675889015197754\n",
      "Loss for batch is  -0.07732105255126953\n",
      "Loss for batch is  0.11135613918304443\n",
      "Loss for batch is  -0.7339135408401489\n",
      "|Iter  1071  | Total Train Loss  -0.6531195640563965 |\n",
      "Val Loss for batch is  -1.7146766185760498\n",
      "Val Loss for batch is  -1.7762221097946167\n",
      "Val Loss for batch is  -1.5138975381851196\n",
      "Val Loss for batch is  -2.2737317085266113\n",
      "|Iter  1071  | Total Val Loss  -7.2785279750823975 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.037816524505615234\n",
      "Loss for batch is  -0.14770722389221191\n",
      "Loss for batch is  0.05854439735412598\n",
      "Loss for batch is  -0.8199425935745239\n",
      "|Iter  1072  | Total Train Loss  -0.9469219446182251 |\n",
      "Val Loss for batch is  -1.7374205589294434\n",
      "Val Loss for batch is  -1.811982274055481\n",
      "Val Loss for batch is  -1.5865693092346191\n",
      "Val Loss for batch is  -2.361666679382324\n",
      "|Iter  1072  | Total Val Loss  -7.497638821601868 |\n",
      "Loss for batch is  -0.06046187877655029\n",
      "Loss for batch is  -0.1882460117340088\n",
      "Loss for batch is  -0.0073795318603515625\n",
      "Loss for batch is  -0.8867473602294922\n",
      "|Iter  1073  | Total Train Loss  -1.1428347826004028 |\n",
      "Val Loss for batch is  -1.711958646774292\n",
      "Val Loss for batch is  -1.8153256177902222\n",
      "Val Loss for batch is  -1.602336049079895\n",
      "Val Loss for batch is  -2.42976713180542\n",
      "|Iter  1073  | Total Val Loss  -7.559387445449829 |\n",
      "Loss for batch is  -0.12330853939056396\n",
      "Loss for batch is  -0.2371540069580078\n",
      "Loss for batch is  -0.06075727939605713\n",
      "Loss for batch is  -0.9564549922943115\n",
      "|Iter  1074  | Total Train Loss  -1.3776748180389404 |\n",
      "Val Loss for batch is  -1.7544984817504883\n",
      "Val Loss for batch is  -1.8393232822418213\n",
      "Val Loss for batch is  -1.6608052253723145\n",
      "Val Loss for batch is  -2.4995365142822266\n",
      "|Iter  1074  | Total Val Loss  -7.754163503646851 |\n",
      "Loss for batch is  -0.16449105739593506\n",
      "Loss for batch is  -0.28103482723236084\n",
      "Loss for batch is  -0.11277258396148682\n",
      "Loss for batch is  -1.0180505514144897\n",
      "|Iter  1075  | Total Train Loss  -1.5763490200042725 |\n",
      "Val Loss for batch is  -1.777586817741394\n",
      "Val Loss for batch is  -1.8750821352005005\n",
      "Val Loss for batch is  -1.6622134447097778\n",
      "Val Loss for batch is  -2.5540518760681152\n",
      "|Iter  1075  | Total Val Loss  -7.868934273719788 |\n",
      "Loss for batch is  -0.22764110565185547\n",
      "Loss for batch is  -0.33635592460632324\n",
      "Loss for batch is  -0.16662383079528809\n",
      "Loss for batch is  -1.0639853477478027\n",
      "|Iter  1076  | Total Train Loss  -1.7946062088012695 |\n",
      "Val Loss for batch is  -1.8036178350448608\n",
      "Val Loss for batch is  -1.8945964574813843\n",
      "Val Loss for batch is  -1.750225305557251\n",
      "Val Loss for batch is  -2.5734641551971436\n",
      "|Iter  1076  | Total Val Loss  -8.02190375328064 |\n",
      "Loss for batch is  -0.2843964099884033\n",
      "Loss for batch is  -0.37807750701904297\n",
      "Loss for batch is  -0.2304365634918213\n",
      "Loss for batch is  -1.112822413444519\n",
      "|Iter  1077  | Total Train Loss  -2.0057328939437866 |\n",
      "Val Loss for batch is  -1.8404041528701782\n",
      "Val Loss for batch is  -1.9275848865509033\n",
      "Val Loss for batch is  -1.7985587120056152\n",
      "Val Loss for batch is  -2.6457650661468506\n",
      "|Iter  1077  | Total Val Loss  -8.212312817573547 |\n",
      "Loss for batch is  -0.300182580947876\n",
      "Loss for batch is  -0.4071485996246338\n",
      "Loss for batch is  -0.2787590026855469\n",
      "Loss for batch is  -1.1641196012496948\n",
      "|Iter  1078  | Total Train Loss  -2.1502097845077515 |\n",
      "Val Loss for batch is  -1.849776268005371\n",
      "Val Loss for batch is  -1.9655588865280151\n",
      "Val Loss for batch is  -1.797932505607605\n",
      "Val Loss for batch is  -2.661834478378296\n",
      "|Iter  1078  | Total Val Loss  -8.275102138519287 |\n",
      "Loss for batch is  -0.35255515575408936\n",
      "Loss for batch is  -0.4566549062728882\n",
      "Loss for batch is  -0.32274675369262695\n",
      "Loss for batch is  -1.1974728107452393\n",
      "|Iter  1079  | Total Train Loss  -2.3294296264648438 |\n",
      "Val Loss for batch is  -1.8846125602722168\n",
      "Val Loss for batch is  -1.9674075841903687\n",
      "Val Loss for batch is  -1.827715277671814\n",
      "Val Loss for batch is  -2.7033488750457764\n",
      "|Iter  1079  | Total Val Loss  -8.383084297180176 |\n",
      "Loss for batch is  -0.4049464464187622\n",
      "Loss for batch is  -0.5065948963165283\n",
      "Loss for batch is  -0.35000574588775635\n",
      "Loss for batch is  -1.2431315183639526\n",
      "|Iter  1080  | Total Train Loss  -2.5046786069869995 |\n",
      "Val Loss for batch is  -1.8604567050933838\n",
      "Val Loss for batch is  -2.0066163539886475\n",
      "Val Loss for batch is  -1.8835605382919312\n",
      "Val Loss for batch is  -2.7446279525756836\n",
      "|Iter  1080  | Total Val Loss  -8.495261549949646 |\n",
      "Loss for batch is  -0.4139132499694824\n",
      "Loss for batch is  -0.5428874492645264\n",
      "Loss for batch is  -0.39284610748291016\n",
      "Loss for batch is  -1.290863275527954\n",
      "|Iter  1081  | Total Train Loss  -2.640510082244873 |\n",
      "Val Loss for batch is  -1.896746277809143\n",
      "Val Loss for batch is  -2.053894281387329\n",
      "Val Loss for batch is  -1.8690131902694702\n",
      "Val Loss for batch is  -2.8049509525299072\n",
      "|Iter  1081  | Total Val Loss  -8.62460470199585 |\n",
      "Loss for batch is  -0.4529843330383301\n",
      "Loss for batch is  -0.5719159841537476\n",
      "Loss for batch is  -0.4214102029800415\n",
      "Loss for batch is  -1.3207062482833862\n",
      "|Iter  1082  | Total Train Loss  -2.7670167684555054 |\n",
      "Val Loss for batch is  -1.872357726097107\n",
      "Val Loss for batch is  -2.031221866607666\n",
      "Val Loss for batch is  -1.9079387187957764\n",
      "Val Loss for batch is  -2.813845157623291\n",
      "|Iter  1082  | Total Val Loss  -8.62536346912384 |\n",
      "Loss for batch is  -0.4858328104019165\n",
      "Loss for batch is  -0.598764181137085\n",
      "Loss for batch is  -0.4420570135116577\n",
      "Loss for batch is  -1.3529387712478638\n",
      "|Iter  1083  | Total Train Loss  -2.879592776298523 |\n",
      "Val Loss for batch is  -1.9214913845062256\n",
      "Val Loss for batch is  -2.085805654525757\n",
      "Val Loss for batch is  -1.9546644687652588\n",
      "Val Loss for batch is  -2.8500568866729736\n",
      "|Iter  1083  | Total Val Loss  -8.812018394470215 |\n",
      "Loss for batch is  -0.5000627040863037\n",
      "Loss for batch is  -0.62456214427948\n",
      "Loss for batch is  -0.4734783172607422\n",
      "Loss for batch is  -1.378562569618225\n",
      "|Iter  1084  | Total Train Loss  -2.976665735244751 |\n",
      "Val Loss for batch is  -1.9073854684829712\n",
      "Val Loss for batch is  -2.0764856338500977\n",
      "Val Loss for batch is  -1.9445465803146362\n",
      "Val Loss for batch is  -2.878659963607788\n",
      "|Iter  1084  | Total Val Loss  -8.807077646255493 |\n",
      "Loss for batch is  -0.5208029747009277\n",
      "Loss for batch is  -0.6510769128799438\n",
      "Loss for batch is  -0.48853135108947754\n",
      "Loss for batch is  -1.4014400243759155\n",
      "|Iter  1085  | Total Train Loss  -3.0618512630462646 |\n",
      "Val Loss for batch is  -1.9637641906738281\n",
      "Val Loss for batch is  -2.097472906112671\n",
      "Val Loss for batch is  -1.977494239807129\n",
      "Val Loss for batch is  -2.8976151943206787\n",
      "|Iter  1085  | Total Val Loss  -8.936346530914307 |\n",
      "Loss for batch is  -0.5310860872268677\n",
      "Loss for batch is  -0.6687757968902588\n",
      "Loss for batch is  -0.509617805480957\n",
      "Loss for batch is  -1.4267135858535767\n",
      "|Iter  1086  | Total Train Loss  -3.13619327545166 |\n",
      "Val Loss for batch is  -1.9201866388320923\n",
      "Val Loss for batch is  -2.1215803623199463\n",
      "Val Loss for batch is  -1.9754269123077393\n",
      "Val Loss for batch is  -2.9075374603271484\n",
      "|Iter  1086  | Total Val Loss  -8.924731373786926 |\n",
      "Loss for batch is  -0.5462638139724731\n",
      "Loss for batch is  -0.6697062253952026\n",
      "Loss for batch is  -0.524917483329773\n",
      "Loss for batch is  -1.4416865110397339\n",
      "|Iter  1087  | Total Train Loss  -3.1825740337371826 |\n",
      "Val Loss for batch is  -1.9132757186889648\n",
      "Val Loss for batch is  -2.101243495941162\n",
      "Val Loss for batch is  -2.017700433731079\n",
      "Val Loss for batch is  -2.929253578186035\n",
      "|Iter  1087  | Total Val Loss  -8.961473226547241 |\n",
      "Loss for batch is  -0.5406758785247803\n",
      "Loss for batch is  -0.6870349645614624\n",
      "Loss for batch is  -0.49802982807159424\n",
      "Loss for batch is  -1.452121615409851\n",
      "|Iter  1088  | Total Train Loss  -3.177862286567688 |\n",
      "Val Loss for batch is  -1.911580204963684\n",
      "Val Loss for batch is  -2.090268135070801\n",
      "Val Loss for batch is  -1.9240740537643433\n",
      "Val Loss for batch is  -2.9009292125701904\n",
      "|Iter  1088  | Total Val Loss  -8.826851606369019 |\n",
      "Loss for batch is  -0.5412552356719971\n",
      "Loss for batch is  -0.6697852611541748\n",
      "Loss for batch is  -0.5399713516235352\n",
      "Loss for batch is  -1.439782738685608\n",
      "|Iter  1089  | Total Train Loss  -3.190794587135315 |\n",
      "Val Loss for batch is  -1.9690604209899902\n",
      "Val Loss for batch is  -2.1369402408599854\n",
      "Val Loss for batch is  -2.0383989810943604\n",
      "Val Loss for batch is  -2.9777305126190186\n",
      "|Iter  1089  | Total Val Loss  -9.122130155563354 |\n",
      "Loss for batch is  -0.5537153482437134\n",
      "Loss for batch is  -0.6327451467514038\n",
      "Loss for batch is  -0.5080298185348511\n",
      "Loss for batch is  -1.461966633796692\n",
      "|Iter  1090  | Total Train Loss  -3.15645694732666 |\n",
      "Val Loss for batch is  -1.633554458618164\n",
      "Val Loss for batch is  -1.8585948944091797\n",
      "Val Loss for batch is  -1.7751599550247192\n",
      "Val Loss for batch is  -2.8765289783477783\n",
      "|Iter  1090  | Total Val Loss  -8.143838286399841 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.25144827365875244\n",
      "Loss for batch is  -0.7210900783538818\n",
      "Loss for batch is  -0.19049286842346191\n",
      "Loss for batch is  -1.3705698251724243\n",
      "|Iter  1091  | Total Train Loss  -2.5336010456085205 |\n",
      "Val Loss for batch is  -1.7404026985168457\n",
      "Val Loss for batch is  -1.9019067287445068\n",
      "Val Loss for batch is  -1.8459715843200684\n",
      "Val Loss for batch is  -2.8695905208587646\n",
      "|Iter  1091  | Total Val Loss  -8.357871532440186 |\n",
      "Loss for batch is  -0.45366501808166504\n",
      "Loss for batch is  -0.4093475341796875\n",
      "Loss for batch is  -0.4951140880584717\n",
      "Loss for batch is  -1.346158504486084\n",
      "|Iter  1092  | Total Train Loss  -2.704285144805908 |\n",
      "Val Loss for batch is  -1.6178622245788574\n",
      "Val Loss for batch is  -1.9931273460388184\n",
      "Val Loss for batch is  -1.815945029258728\n",
      "Val Loss for batch is  -2.7939035892486572\n",
      "|Iter  1092  | Total Val Loss  -8.220838189125061 |\n",
      "Loss for batch is  -0.30027079582214355\n",
      "Loss for batch is  -0.6730115413665771\n",
      "Loss for batch is  -0.45768141746520996\n",
      "Loss for batch is  -1.2555292844772339\n",
      "|Iter  1093  | Total Train Loss  -2.6864930391311646 |\n",
      "Val Loss for batch is  -1.8747795820236206\n",
      "Val Loss for batch is  -2.0360453128814697\n",
      "Val Loss for batch is  -1.9366815090179443\n",
      "Val Loss for batch is  -2.8366241455078125\n",
      "|Iter  1093  | Total Val Loss  -8.684130549430847 |\n",
      "Loss for batch is  -0.4992612600326538\n",
      "Loss for batch is  -0.6639524698257446\n",
      "Loss for batch is  -0.41266024112701416\n",
      "Loss for batch is  -1.3800325393676758\n",
      "|Iter  1094  | Total Train Loss  -2.9559065103530884 |\n",
      "Val Loss for batch is  -1.9254778623580933\n",
      "Val Loss for batch is  -2.126646041870117\n",
      "Val Loss for batch is  -1.9935513734817505\n",
      "Val Loss for batch is  -2.918607234954834\n",
      "|Iter  1094  | Total Val Loss  -8.964282512664795 |\n",
      "Loss for batch is  -0.5529775619506836\n",
      "Loss for batch is  -0.6439504623413086\n",
      "Loss for batch is  -0.5300992727279663\n",
      "Loss for batch is  -1.432403326034546\n",
      "|Iter  1095  | Total Train Loss  -3.1594306230545044 |\n",
      "Val Loss for batch is  -1.9778037071228027\n",
      "Val Loss for batch is  -2.107818365097046\n",
      "Val Loss for batch is  -1.9837125539779663\n",
      "Val Loss for batch is  -2.917628288269043\n",
      "|Iter  1095  | Total Val Loss  -8.986962914466858 |\n",
      "Loss for batch is  -0.5712721347808838\n",
      "Loss for batch is  -0.6899194717407227\n",
      "Loss for batch is  -0.5426875352859497\n",
      "Loss for batch is  -1.4655206203460693\n",
      "|Iter  1096  | Total Train Loss  -3.2693997621536255 |\n",
      "Val Loss for batch is  -1.9756755828857422\n",
      "Val Loss for batch is  -2.1194026470184326\n",
      "Val Loss for batch is  -1.9724303483963013\n",
      "Val Loss for batch is  -2.9528696537017822\n",
      "|Iter  1096  | Total Val Loss  -9.020378232002258 |\n",
      "Loss for batch is  -0.6096129417419434\n",
      "Loss for batch is  -0.7270970344543457\n",
      "Loss for batch is  -0.5896499156951904\n",
      "Loss for batch is  -1.489971399307251\n",
      "|Iter  1097  | Total Train Loss  -3.4163312911987305 |\n",
      "Val Loss for batch is  -1.9794890880584717\n",
      "Val Loss for batch is  -2.17925763130188\n",
      "Val Loss for batch is  -2.0272021293640137\n",
      "Val Loss for batch is  -3.0004193782806396\n",
      "|Iter  1097  | Total Val Loss  -9.186368227005005 |\n",
      "Loss for batch is  -0.6450351476669312\n",
      "Loss for batch is  -0.7570921182632446\n",
      "Loss for batch is  -0.6211239099502563\n",
      "Loss for batch is  -1.5096200704574585\n",
      "|Iter  1098  | Total Train Loss  -3.5328712463378906 |\n",
      "Val Loss for batch is  -2.0060603618621826\n",
      "Val Loss for batch is  -2.202342987060547\n",
      "Val Loss for batch is  -2.069638967514038\n",
      "Val Loss for batch is  -3.0041208267211914\n",
      "|Iter  1098  | Total Val Loss  -9.282163143157959 |\n",
      "Loss for batch is  -0.6330776214599609\n",
      "Loss for batch is  -0.7889708280563354\n",
      "Loss for batch is  -0.6366127729415894\n",
      "Loss for batch is  -1.547518014907837\n",
      "|Iter  1099  | Total Train Loss  -3.6061792373657227 |\n",
      "Val Loss for batch is  -2.0212044715881348\n",
      "Val Loss for batch is  -2.1841347217559814\n",
      "Val Loss for batch is  -2.055471897125244\n",
      "Val Loss for batch is  -3.023340940475464\n",
      "|Iter  1099  | Total Val Loss  -9.284152030944824 |\n",
      "Loss for batch is  -0.6727728843688965\n",
      "Loss for batch is  -0.8026266098022461\n",
      "Loss for batch is  -0.6775041818618774\n",
      "Loss for batch is  -1.5780413150787354\n",
      "|Iter  1100  | Total Train Loss  -3.7309449911117554 |\n",
      "Val Loss for batch is  -2.012181043624878\n",
      "Val Loss for batch is  -2.2642714977264404\n",
      "Val Loss for batch is  -2.1126134395599365\n",
      "Val Loss for batch is  -3.068267822265625\n",
      "|Iter  1100  | Total Val Loss  -9.45733380317688 |\n",
      "Loss for batch is  -0.6915334463119507\n",
      "Loss for batch is  -0.810737133026123\n",
      "Loss for batch is  -0.6674098968505859\n",
      "Loss for batch is  -1.5981515645980835\n",
      "|Iter  1101  | Total Train Loss  -3.767832040786743 |\n",
      "Val Loss for batch is  -2.034879446029663\n",
      "Val Loss for batch is  -2.2119228839874268\n",
      "Val Loss for batch is  -2.114771604537964\n",
      "Val Loss for batch is  -3.0727007389068604\n",
      "|Iter  1101  | Total Val Loss  -9.434274673461914 |\n",
      "Loss for batch is  -0.6769230365753174\n",
      "Loss for batch is  -0.801915168762207\n",
      "Loss for batch is  -0.6767009496688843\n",
      "Loss for batch is  -1.6083229780197144\n",
      "|Iter  1102  | Total Train Loss  -3.763862133026123 |\n",
      "Val Loss for batch is  -1.901041030883789\n",
      "Val Loss for batch is  -2.06567120552063\n",
      "Val Loss for batch is  -1.9646469354629517\n",
      "Val Loss for batch is  -3.0468266010284424\n",
      "|Iter  1102  | Total Val Loss  -8.978185772895813 |\n",
      "Loss for batch is  -0.5454823970794678\n",
      "Loss for batch is  -0.7439113855361938\n",
      "Loss for batch is  -0.55808424949646\n",
      "Loss for batch is  -1.5809216499328613\n",
      "|Iter  1103  | Total Train Loss  -3.428399682044983 |\n",
      "Val Loss for batch is  -1.3649563789367676\n",
      "Val Loss for batch is  -1.5623326301574707\n",
      "Val Loss for batch is  -1.38675057888031\n",
      "Val Loss for batch is  -2.8966636657714844\n",
      "|Iter  1103  | Total Val Loss  -7.210703253746033 |\n",
      "Loss for batch is  -0.11957979202270508\n",
      "Loss for batch is  -0.7165348529815674\n",
      "Loss for batch is  -0.08962070941925049\n",
      "Loss for batch is  -1.4720765352249146\n",
      "|Iter  1104  | Total Train Loss  -2.3978118896484375 |\n",
      "Val Loss for batch is  -1.7763277292251587\n",
      "Val Loss for batch is  -1.8645941019058228\n",
      "Val Loss for batch is  -1.7505064010620117\n",
      "Val Loss for batch is  -2.939723014831543\n",
      "|Iter  1104  | Total Val Loss  -8.331151247024536 |\n",
      "Loss for batch is  -0.477264404296875\n",
      "Loss for batch is  -0.15744876861572266\n",
      "Loss for batch is  -0.5474376678466797\n",
      "Loss for batch is  -1.3851317167282104\n",
      "|Iter  1105  | Total Train Loss  -2.567282557487488 |\n",
      "Val Loss for batch is  -1.3769859075546265\n",
      "Val Loss for batch is  -1.7407523393630981\n",
      "Val Loss for batch is  -1.6196759939193726\n",
      "Val Loss for batch is  -2.6971194744110107\n",
      "|Iter  1105  | Total Val Loss  -7.434533715248108 |\n",
      "Loss for batch is  -0.1281818151473999\n",
      "Loss for batch is  -0.567975640296936\n",
      "Loss for batch is  -0.4885648488998413\n",
      "Loss for batch is  -1.3964637517929077\n",
      "|Iter  1106  | Total Train Loss  -2.581186056137085 |\n",
      "Val Loss for batch is  -1.748581886291504\n",
      "Val Loss for batch is  -1.9170687198638916\n",
      "Val Loss for batch is  -1.8465440273284912\n",
      "Val Loss for batch is  -2.8489878177642822\n",
      "|Iter  1106  | Total Val Loss  -8.361182451248169 |\n",
      "Loss for batch is  -0.3641488552093506\n",
      "Loss for batch is  -0.5603823661804199\n",
      "Loss for batch is  -0.47849857807159424\n",
      "Loss for batch is  -1.3361337184906006\n",
      "|Iter  1107  | Total Train Loss  -2.7391635179519653 |\n",
      "Val Loss for batch is  -1.7642725706100464\n",
      "Val Loss for batch is  -1.9488625526428223\n",
      "Val Loss for batch is  -1.8861353397369385\n",
      "Val Loss for batch is  -2.8240838050842285\n",
      "|Iter  1107  | Total Val Loss  -8.423354268074036 |\n",
      "Loss for batch is  -0.4199087619781494\n",
      "Loss for batch is  -0.6374984979629517\n",
      "Loss for batch is  -0.5401221513748169\n",
      "Loss for batch is  -1.4099485874176025\n",
      "|Iter  1108  | Total Train Loss  -3.0074779987335205 |\n",
      "Val Loss for batch is  -1.9424095153808594\n",
      "Val Loss for batch is  -2.028036594390869\n",
      "Val Loss for batch is  -2.0043387413024902\n",
      "Val Loss for batch is  -2.8743855953216553\n",
      "|Iter  1108  | Total Val Loss  -8.849170446395874 |\n",
      "Loss for batch is  -0.5074427127838135\n",
      "Loss for batch is  -0.6367776393890381\n",
      "Loss for batch is  -0.5560550689697266\n",
      "Loss for batch is  -1.4411271810531616\n",
      "|Iter  1109  | Total Train Loss  -3.1414026021957397 |\n",
      "Val Loss for batch is  -1.986283540725708\n",
      "Val Loss for batch is  -2.1264431476593018\n",
      "Val Loss for batch is  -1.9907433986663818\n",
      "Val Loss for batch is  -2.9297378063201904\n",
      "|Iter  1109  | Total Val Loss  -9.033207893371582 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.5934836864471436\n",
      "Loss for batch is  -0.679044246673584\n",
      "Loss for batch is  -0.5750341415405273\n",
      "Loss for batch is  -1.4561471939086914\n",
      "|Iter  1110  | Total Train Loss  -3.3037092685699463 |\n",
      "Val Loss for batch is  -2.05090594291687\n",
      "Val Loss for batch is  -2.1223835945129395\n",
      "Val Loss for batch is  -2.0625131130218506\n",
      "Val Loss for batch is  -2.9389898777008057\n",
      "|Iter  1110  | Total Val Loss  -9.174792528152466 |\n",
      "Loss for batch is  -0.6292523145675659\n",
      "Loss for batch is  -0.7195233106613159\n",
      "Loss for batch is  -0.594464898109436\n",
      "Loss for batch is  -1.5069245100021362\n",
      "|Iter  1111  | Total Train Loss  -3.450165033340454 |\n",
      "Val Loss for batch is  -2.0508511066436768\n",
      "Val Loss for batch is  -2.152985095977783\n",
      "Val Loss for batch is  -2.112981081008911\n",
      "Val Loss for batch is  -3.009509801864624\n",
      "|Iter  1111  | Total Val Loss  -9.326327085494995 |\n",
      "Loss for batch is  -0.6361473798751831\n",
      "Loss for batch is  -0.7536627054214478\n",
      "Loss for batch is  -0.6372355222702026\n",
      "Loss for batch is  -1.5446820259094238\n",
      "|Iter  1112  | Total Train Loss  -3.5717276334762573 |\n",
      "Val Loss for batch is  -2.023237943649292\n",
      "Val Loss for batch is  -2.210120916366577\n",
      "Val Loss for batch is  -2.139629364013672\n",
      "Val Loss for batch is  -2.9961259365081787\n",
      "|Iter  1112  | Total Val Loss  -9.36911416053772 |\n",
      "Loss for batch is  -0.6792559623718262\n",
      "Loss for batch is  -0.7871143817901611\n",
      "Loss for batch is  -0.6704698801040649\n",
      "Loss for batch is  -1.5622466802597046\n",
      "|Iter  1113  | Total Train Loss  -3.699086904525757 |\n",
      "Val Loss for batch is  -2.0184214115142822\n",
      "Val Loss for batch is  -2.199028968811035\n",
      "Val Loss for batch is  -2.1073172092437744\n",
      "Val Loss for batch is  -3.017652750015259\n",
      "|Iter  1113  | Total Val Loss  -9.34242033958435 |\n",
      "Loss for batch is  -0.6761965751647949\n",
      "Loss for batch is  -0.8117518424987793\n",
      "Loss for batch is  -0.6964607238769531\n",
      "Loss for batch is  -1.5986011028289795\n",
      "|Iter  1114  | Total Train Loss  -3.783010244369507 |\n",
      "Val Loss for batch is  -2.0053064823150635\n",
      "Val Loss for batch is  -2.2132463455200195\n",
      "Val Loss for batch is  -2.1064260005950928\n",
      "Val Loss for batch is  -3.0874314308166504\n",
      "|Iter  1114  | Total Val Loss  -9.412410259246826 |\n",
      "Loss for batch is  -0.6827653646469116\n",
      "Loss for batch is  -0.8430532217025757\n",
      "Loss for batch is  -0.7100892066955566\n",
      "Loss for batch is  -1.5913286209106445\n",
      "|Iter  1115  | Total Train Loss  -3.8272364139556885 |\n",
      "Val Loss for batch is  -2.0171451568603516\n",
      "Val Loss for batch is  -2.2374093532562256\n",
      "Val Loss for batch is  -2.126690149307251\n",
      "Val Loss for batch is  -3.1034348011016846\n",
      "|Iter  1115  | Total Val Loss  -9.484679460525513 |\n",
      "Loss for batch is  -0.7049005031585693\n",
      "Loss for batch is  -0.8224318027496338\n",
      "Loss for batch is  -0.7070887088775635\n",
      "Loss for batch is  -1.619723916053772\n",
      "|Iter  1116  | Total Train Loss  -3.8541449308395386 |\n",
      "Val Loss for batch is  -1.8086494207382202\n",
      "Val Loss for batch is  -2.1107358932495117\n",
      "Val Loss for batch is  -2.000018835067749\n",
      "Val Loss for batch is  -3.085833787918091\n",
      "|Iter  1116  | Total Val Loss  -9.005237936973572 |\n",
      "Loss for batch is  -0.5165724754333496\n",
      "Loss for batch is  -0.7465561628341675\n",
      "Loss for batch is  -0.47312986850738525\n",
      "Loss for batch is  -1.6048414707183838\n",
      "|Iter  1117  | Total Train Loss  -3.341099977493286 |\n",
      "Val Loss for batch is  -1.6822969913482666\n",
      "Val Loss for batch is  -1.7538435459136963\n",
      "Val Loss for batch is  -1.6484249830245972\n",
      "Val Loss for batch is  -2.9042680263519287\n",
      "|Iter  1117  | Total Val Loss  -7.988833546638489 |\n",
      "Loss for batch is  -0.3941248655319214\n",
      "Loss for batch is  -0.6238342523574829\n",
      "Loss for batch is  -0.5424326658248901\n",
      "Loss for batch is  -1.1376690864562988\n",
      "|Iter  1118  | Total Train Loss  -2.6980608701705933 |\n",
      "Val Loss for batch is  -1.740645170211792\n",
      "Val Loss for batch is  -2.0886971950531006\n",
      "Val Loss for batch is  -1.944483995437622\n",
      "Val Loss for batch is  -3.021542549133301\n",
      "|Iter  1118  | Total Val Loss  -8.795368909835815 |\n",
      "Loss for batch is  -0.4419456720352173\n",
      "Loss for batch is  -0.44886255264282227\n",
      "Loss for batch is  0.013155579566955566\n",
      "Loss for batch is  -1.4810816049575806\n",
      "|Iter  1119  | Total Train Loss  -2.3587342500686646 |\n",
      "Val Loss for batch is  -1.4056527614593506\n",
      "Val Loss for batch is  -1.616682767868042\n",
      "Val Loss for batch is  -1.5767794847488403\n",
      "Val Loss for batch is  -2.7704460620880127\n",
      "|Iter  1119  | Total Val Loss  -7.369561076164246 |\n",
      "Loss for batch is  -0.07725155353546143\n",
      "Loss for batch is  -0.28385448455810547\n",
      "Loss for batch is  -0.2997792959213257\n",
      "Loss for batch is  -1.292574405670166\n",
      "|Iter  1120  | Total Train Loss  -1.9534597396850586 |\n",
      "Val Loss for batch is  -1.6197234392166138\n",
      "Val Loss for batch is  -1.9031596183776855\n",
      "Val Loss for batch is  -1.689542293548584\n",
      "Val Loss for batch is  -2.6928296089172363\n",
      "|Iter  1120  | Total Val Loss  -7.90525496006012 |\n",
      "Loss for batch is  -0.3271557092666626\n",
      "Loss for batch is  -0.5023688077926636\n",
      "Loss for batch is  -0.321746826171875\n",
      "Loss for batch is  -1.4085630178451538\n",
      "|Iter  1121  | Total Train Loss  -2.559834361076355 |\n",
      "Val Loss for batch is  -1.846634030342102\n",
      "Val Loss for batch is  -1.9403834342956543\n",
      "Val Loss for batch is  -1.8880747556686401\n",
      "Val Loss for batch is  -2.8936169147491455\n",
      "|Iter  1121  | Total Val Loss  -8.568709135055542 |\n",
      "Loss for batch is  -0.4473600387573242\n",
      "Loss for batch is  -0.5368614196777344\n",
      "Loss for batch is  -0.4564328193664551\n",
      "Loss for batch is  -1.4034096002578735\n",
      "|Iter  1122  | Total Train Loss  -2.844063878059387 |\n",
      "Val Loss for batch is  -1.9521616697311401\n",
      "Val Loss for batch is  -2.0813026428222656\n",
      "Val Loss for batch is  -1.9580323696136475\n",
      "Val Loss for batch is  -2.8998262882232666\n",
      "|Iter  1122  | Total Val Loss  -8.89132297039032 |\n",
      "Loss for batch is  -0.5242671966552734\n",
      "Loss for batch is  -0.6183196306228638\n",
      "Loss for batch is  -0.4328117370605469\n",
      "Loss for batch is  -1.4287899732589722\n",
      "|Iter  1123  | Total Train Loss  -3.0041885375976562 |\n",
      "Val Loss for batch is  -2.009028673171997\n",
      "Val Loss for batch is  -2.0988733768463135\n",
      "Val Loss for batch is  -2.0105419158935547\n",
      "Val Loss for batch is  -2.954716920852661\n",
      "|Iter  1123  | Total Val Loss  -9.073160886764526 |\n",
      "Loss for batch is  -0.5918666124343872\n",
      "Loss for batch is  -0.65851891040802\n",
      "Loss for batch is  -0.5635216236114502\n",
      "Loss for batch is  -1.4465302228927612\n",
      "|Iter  1124  | Total Train Loss  -3.2604373693466187 |\n",
      "Val Loss for batch is  -1.9926848411560059\n",
      "Val Loss for batch is  -2.112861394882202\n",
      "Val Loss for batch is  -2.0363783836364746\n",
      "Val Loss for batch is  -2.9588801860809326\n",
      "|Iter  1124  | Total Val Loss  -9.100804805755615 |\n",
      "Loss for batch is  -0.6047896146774292\n",
      "Loss for batch is  -0.6992360353469849\n",
      "Loss for batch is  -0.5653551816940308\n",
      "Loss for batch is  -1.4790326356887817\n",
      "|Iter  1125  | Total Train Loss  -3.3484134674072266 |\n",
      "Val Loss for batch is  -2.090808868408203\n",
      "Val Loss for batch is  -2.1665878295898438\n",
      "Val Loss for batch is  -2.0703647136688232\n",
      "Val Loss for batch is  -2.993352174758911\n",
      "|Iter  1125  | Total Val Loss  -9.321113586425781 |\n",
      "Loss for batch is  -0.665584921836853\n",
      "Loss for batch is  -0.7388674020767212\n",
      "Loss for batch is  -0.594342827796936\n",
      "Loss for batch is  -1.5437532663345337\n",
      "|Iter  1126  | Total Train Loss  -3.542548418045044 |\n",
      "Val Loss for batch is  -2.088850975036621\n",
      "Val Loss for batch is  -2.1845474243164062\n",
      "Val Loss for batch is  -2.086878538131714\n",
      "Val Loss for batch is  -3.026987314224243\n",
      "|Iter  1126  | Total Val Loss  -9.387264251708984 |\n",
      "Loss for batch is  -0.6703095436096191\n",
      "Loss for batch is  -0.7624800205230713\n",
      "Loss for batch is  -0.6590629816055298\n",
      "Loss for batch is  -1.5796300172805786\n",
      "|Iter  1127  | Total Train Loss  -3.671482563018799 |\n",
      "Val Loss for batch is  -2.049621105194092\n",
      "Val Loss for batch is  -2.211416721343994\n",
      "Val Loss for batch is  -2.1003899574279785\n",
      "Val Loss for batch is  -3.0571014881134033\n",
      "|Iter  1127  | Total Val Loss  -9.418529272079468 |\n",
      "Loss for batch is  -0.7097991704940796\n",
      "Loss for batch is  -0.8039524555206299\n",
      "Loss for batch is  -0.7073377370834351\n",
      "Loss for batch is  -1.6310187578201294\n",
      "|Iter  1128  | Total Train Loss  -3.852108120918274 |\n",
      "Val Loss for batch is  -2.065078020095825\n",
      "Val Loss for batch is  -2.2153069972991943\n",
      "Val Loss for batch is  -2.1341326236724854\n",
      "Val Loss for batch is  -3.112250804901123\n",
      "|Iter  1128  | Total Val Loss  -9.526768445968628 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.727211594581604\n",
      "Loss for batch is  -0.8475221395492554\n",
      "Loss for batch is  -0.7021559476852417\n",
      "Loss for batch is  -1.6644104719161987\n",
      "|Iter  1129  | Total Train Loss  -3.9413001537323 |\n",
      "Val Loss for batch is  -2.0477843284606934\n",
      "Val Loss for batch is  -2.2343661785125732\n",
      "Val Loss for batch is  -2.132071018218994\n",
      "Val Loss for batch is  -3.1180684566497803\n",
      "|Iter  1129  | Total Val Loss  -9.532289981842041 |\n",
      "Loss for batch is  -0.729066014289856\n",
      "Loss for batch is  -0.8779119253158569\n",
      "Loss for batch is  -0.7507339715957642\n",
      "Loss for batch is  -1.670913815498352\n",
      "|Iter  1130  | Total Train Loss  -4.028625726699829 |\n",
      "Val Loss for batch is  -2.068150281906128\n",
      "Val Loss for batch is  -2.2770607471466064\n",
      "Val Loss for batch is  -2.1273670196533203\n",
      "Val Loss for batch is  -3.15078067779541\n",
      "|Iter  1130  | Total Val Loss  -9.623358726501465 |\n",
      "Loss for batch is  -0.766546368598938\n",
      "Loss for batch is  -0.8794572353363037\n",
      "Loss for batch is  -0.7680646181106567\n",
      "Loss for batch is  -1.7086611986160278\n",
      "|Iter  1131  | Total Train Loss  -4.122729420661926 |\n",
      "Val Loss for batch is  -2.1013622283935547\n",
      "Val Loss for batch is  -2.285794734954834\n",
      "Val Loss for batch is  -2.1622674465179443\n",
      "Val Loss for batch is  -3.1933581829071045\n",
      "|Iter  1131  | Total Val Loss  -9.742782592773438 |\n",
      "Loss for batch is  -0.7590725421905518\n",
      "Loss for batch is  -0.8972587585449219\n",
      "Loss for batch is  -0.7642184495925903\n",
      "Loss for batch is  -1.730286955833435\n",
      "|Iter  1132  | Total Train Loss  -4.150836706161499 |\n",
      "Val Loss for batch is  -2.0520360469818115\n",
      "Val Loss for batch is  -2.2601070404052734\n",
      "Val Loss for batch is  -2.0632846355438232\n",
      "Val Loss for batch is  -3.161102533340454\n",
      "|Iter  1132  | Total Val Loss  -9.536530256271362 |\n",
      "Loss for batch is  -0.7472943067550659\n",
      "Loss for batch is  -0.9001016616821289\n",
      "Loss for batch is  -0.7645883560180664\n",
      "Loss for batch is  -1.7333489656448364\n",
      "|Iter  1133  | Total Train Loss  -4.145333290100098 |\n",
      "Val Loss for batch is  -2.0496723651885986\n",
      "Val Loss for batch is  -2.1933109760284424\n",
      "Val Loss for batch is  -2.050568103790283\n",
      "Val Loss for batch is  -3.18837571144104\n",
      "|Iter  1133  | Total Val Loss  -9.481927156448364 |\n",
      "Loss for batch is  -0.7387758493423462\n",
      "Loss for batch is  -0.8652408123016357\n",
      "Loss for batch is  -0.7039766311645508\n",
      "Loss for batch is  -1.7396626472473145\n",
      "|Iter  1134  | Total Train Loss  -4.047655940055847 |\n",
      "Val Loss for batch is  -1.7719866037368774\n",
      "Val Loss for batch is  -1.9482876062393188\n",
      "Val Loss for batch is  -1.791730284690857\n",
      "Val Loss for batch is  -3.1234490871429443\n",
      "|Iter  1134  | Total Val Loss  -8.635453581809998 |\n",
      "Loss for batch is  -0.4844621419906616\n",
      "Loss for batch is  -0.8885780572891235\n",
      "Loss for batch is  -0.4004542827606201\n",
      "Loss for batch is  -1.683963656425476\n",
      "|Iter  1135  | Total Train Loss  -3.4574581384658813 |\n",
      "Val Loss for batch is  -1.7379037141799927\n",
      "Val Loss for batch is  -1.8699203729629517\n",
      "Val Loss for batch is  -1.7480491399765015\n",
      "Val Loss for batch is  -3.032766580581665\n",
      "|Iter  1135  | Total Val Loss  -8.38863980770111 |\n",
      "Loss for batch is  -0.4810295104980469\n",
      "Loss for batch is  -0.5214685201644897\n",
      "Loss for batch is  -0.6659756898880005\n",
      "Loss for batch is  -1.3551958799362183\n",
      "|Iter  1136  | Total Train Loss  -3.0236696004867554 |\n",
      "Val Loss for batch is  -1.3970333337783813\n",
      "Val Loss for batch is  -1.8108482360839844\n",
      "Val Loss for batch is  -1.6967718601226807\n",
      "Val Loss for batch is  -3.0394129753112793\n",
      "|Iter  1136  | Total Val Loss  -7.944066405296326 |\n",
      "Loss for batch is  -0.13302457332611084\n",
      "Loss for batch is  -0.637903094291687\n",
      "Loss for batch is  -0.143335223197937\n",
      "Loss for batch is  -1.1063015460968018\n",
      "|Iter  1137  | Total Train Loss  -2.0205644369125366 |\n",
      "Val Loss for batch is  -1.784400224685669\n",
      "Val Loss for batch is  -2.0110726356506348\n",
      "Val Loss for batch is  -1.828458309173584\n",
      "Val Loss for batch is  -2.9744436740875244\n",
      "|Iter  1137  | Total Val Loss  -8.598374843597412 |\n",
      "Loss for batch is  -0.45945560932159424\n",
      "Loss for batch is  -0.3453803062438965\n",
      "Loss for batch is  -0.2489168643951416\n",
      "Loss for batch is  -1.436428189277649\n",
      "|Iter  1138  | Total Train Loss  -2.4901809692382812 |\n",
      "Val Loss for batch is  -1.7756065130233765\n",
      "Val Loss for batch is  -1.9928103685379028\n",
      "Val Loss for batch is  -1.8142443895339966\n",
      "Val Loss for batch is  -2.871741771697998\n",
      "|Iter  1138  | Total Val Loss  -8.454403042793274 |\n",
      "Loss for batch is  -0.45871174335479736\n",
      "Loss for batch is  -0.5411360263824463\n",
      "Loss for batch is  -0.3309638500213623\n",
      "Loss for batch is  -1.361045479774475\n",
      "|Iter  1139  | Total Train Loss  -2.691857099533081 |\n",
      "Val Loss for batch is  -1.8668326139450073\n",
      "Val Loss for batch is  -2.0597171783447266\n",
      "Val Loss for batch is  -1.8874455690383911\n",
      "Val Loss for batch is  -2.898756265640259\n",
      "|Iter  1139  | Total Val Loss  -8.712751626968384 |\n",
      "Loss for batch is  -0.5474386215209961\n",
      "Loss for batch is  -0.6755458116531372\n",
      "Loss for batch is  -0.4379011392593384\n",
      "Loss for batch is  -1.3234719038009644\n",
      "|Iter  1140  | Total Train Loss  -2.984357476234436 |\n",
      "Val Loss for batch is  -1.9448320865631104\n",
      "Val Loss for batch is  -2.0547380447387695\n",
      "Val Loss for batch is  -1.959625482559204\n",
      "Val Loss for batch is  -2.834594488143921\n",
      "|Iter  1140  | Total Val Loss  -8.793790102005005 |\n",
      "Loss for batch is  -0.5662455558776855\n",
      "Loss for batch is  -0.6800869703292847\n",
      "Loss for batch is  -0.578782320022583\n",
      "Loss for batch is  -1.4399566650390625\n",
      "|Iter  1141  | Total Train Loss  -3.2650715112686157 |\n",
      "Val Loss for batch is  -2.016355514526367\n",
      "Val Loss for batch is  -2.076664686203003\n",
      "Val Loss for batch is  -1.9525706768035889\n",
      "Val Loss for batch is  -2.904879331588745\n",
      "|Iter  1141  | Total Val Loss  -8.950470209121704 |\n",
      "Loss for batch is  -0.5667489767074585\n",
      "Loss for batch is  -0.7051049470901489\n",
      "Loss for batch is  -0.6128778457641602\n",
      "Loss for batch is  -1.5109376907348633\n",
      "|Iter  1142  | Total Train Loss  -3.395669460296631 |\n",
      "Val Loss for batch is  -2.0387423038482666\n",
      "Val Loss for batch is  -2.161371946334839\n",
      "Val Loss for batch is  -2.0281827449798584\n",
      "Val Loss for batch is  -2.9880764484405518\n",
      "|Iter  1142  | Total Val Loss  -9.216373443603516 |\n",
      "Loss for batch is  -0.6451008319854736\n",
      "Loss for batch is  -0.7184751033782959\n",
      "Loss for batch is  -0.6169564723968506\n",
      "Loss for batch is  -1.5464247465133667\n",
      "|Iter  1143  | Total Train Loss  -3.526957154273987 |\n",
      "Val Loss for batch is  -2.0431954860687256\n",
      "Val Loss for batch is  -2.132688045501709\n",
      "Val Loss for batch is  -2.0733556747436523\n",
      "Val Loss for batch is  -3.052257537841797\n",
      "|Iter  1143  | Total Val Loss  -9.301496744155884 |\n",
      "Loss for batch is  -0.6909713745117188\n",
      "Loss for batch is  -0.7783983945846558\n",
      "Loss for batch is  -0.677855372428894\n",
      "Loss for batch is  -1.5727754831314087\n",
      "|Iter  1144  | Total Train Loss  -3.7200006246566772 |\n",
      "Val Loss for batch is  -2.060431480407715\n",
      "Val Loss for batch is  -2.193455457687378\n",
      "Val Loss for batch is  -2.114474058151245\n",
      "Val Loss for batch is  -3.09853196144104\n",
      "|Iter  1144  | Total Val Loss  -9.466892957687378 |\n",
      "Loss for batch is  -0.7064223289489746\n",
      "Loss for batch is  -0.8225715160369873\n",
      "Loss for batch is  -0.7014815807342529\n",
      "Loss for batch is  -1.6314384937286377\n",
      "|Iter  1145  | Total Train Loss  -3.8619139194488525 |\n",
      "Val Loss for batch is  -2.0868494510650635\n",
      "Val Loss for batch is  -2.232248544692993\n",
      "Val Loss for batch is  -2.1225974559783936\n",
      "Val Loss for batch is  -3.1162028312683105\n",
      "|Iter  1145  | Total Val Loss  -9.55789828300476 |\n",
      "Loss for batch is  -0.749835729598999\n",
      "Loss for batch is  -0.8550233840942383\n",
      "Loss for batch is  -0.7486468553543091\n",
      "Loss for batch is  -1.651253342628479\n",
      "|Iter  1146  | Total Train Loss  -4.004759311676025 |\n",
      "Val Loss for batch is  -2.0239856243133545\n",
      "Val Loss for batch is  -2.246380090713501\n",
      "Val Loss for batch is  -2.148395299911499\n",
      "Val Loss for batch is  -3.1553726196289062\n",
      "|Iter  1146  | Total Val Loss  -9.57413363456726 |\n",
      "Loss for batch is  -0.7511508464813232\n",
      "Loss for batch is  -0.8771415948867798\n",
      "Loss for batch is  -0.7609962224960327\n",
      "Loss for batch is  -1.7085374593734741\n",
      "|Iter  1147  | Total Train Loss  -4.09782612323761 |\n",
      "Val Loss for batch is  -2.0602810382843018\n",
      "Val Loss for batch is  -2.2641751766204834\n",
      "Val Loss for batch is  -2.1156318187713623\n",
      "Val Loss for batch is  -3.167325735092163\n",
      "|Iter  1147  | Total Val Loss  -9.60741376876831 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.7773975133895874\n",
      "Loss for batch is  -0.9064677953720093\n",
      "Loss for batch is  -0.794114351272583\n",
      "Loss for batch is  -1.7396713495254517\n",
      "|Iter  1148  | Total Train Loss  -4.217651009559631 |\n",
      "Val Loss for batch is  -2.054389238357544\n",
      "Val Loss for batch is  -2.25889253616333\n",
      "Val Loss for batch is  -2.1572482585906982\n",
      "Val Loss for batch is  -3.222116708755493\n",
      "|Iter  1148  | Total Val Loss  -9.692646741867065 |\n",
      "Loss for batch is  -0.7856476306915283\n",
      "Loss for batch is  -0.9190186262130737\n",
      "Loss for batch is  -0.8103114366531372\n",
      "Loss for batch is  -1.7538626194000244\n",
      "|Iter  1149  | Total Train Loss  -4.268840312957764 |\n",
      "Val Loss for batch is  -2.11739182472229\n",
      "Val Loss for batch is  -2.3156251907348633\n",
      "Val Loss for batch is  -2.1978259086608887\n",
      "Val Loss for batch is  -3.2147488594055176\n",
      "|Iter  1149  | Total Val Loss  -9.84559178352356 |\n",
      "Loss for batch is  -0.8043668270111084\n",
      "Loss for batch is  -0.9273911714553833\n",
      "Loss for batch is  -0.8174334764480591\n",
      "Loss for batch is  -1.7819337844848633\n",
      "|Iter  1150  | Total Train Loss  -4.331125259399414 |\n",
      "Val Loss for batch is  -2.0921642780303955\n",
      "Val Loss for batch is  -2.23698091506958\n",
      "Val Loss for batch is  -2.1650259494781494\n",
      "Val Loss for batch is  -3.204237222671509\n",
      "|Iter  1150  | Total Val Loss  -9.698408365249634 |\n",
      "Loss for batch is  -0.7893180847167969\n",
      "Loss for batch is  -0.9484680891036987\n",
      "Loss for batch is  -0.7978549003601074\n",
      "Loss for batch is  -1.7748929262161255\n",
      "|Iter  1151  | Total Train Loss  -4.3105340003967285 |\n",
      "Val Loss for batch is  -1.9874578714370728\n",
      "Val Loss for batch is  -2.204653263092041\n",
      "Val Loss for batch is  -2.0660409927368164\n",
      "Val Loss for batch is  -3.2280044555664062\n",
      "|Iter  1151  | Total Val Loss  -9.486156582832336 |\n",
      "Loss for batch is  -0.6913453340530396\n",
      "Loss for batch is  -0.7902117967605591\n",
      "Loss for batch is  -0.7301703691482544\n",
      "Loss for batch is  -1.7183231115341187\n",
      "|Iter  1152  | Total Train Loss  -3.9300506114959717 |\n",
      "Val Loss for batch is  -1.69467294216156\n",
      "Val Loss for batch is  -1.5410277843475342\n",
      "Val Loss for batch is  -1.7121893167495728\n",
      "Val Loss for batch is  -3.0900990962982178\n",
      "|Iter  1152  | Total Val Loss  -8.037989139556885 |\n",
      "Loss for batch is  -0.4107179641723633\n",
      "Loss for batch is  -0.7730306386947632\n",
      "Loss for batch is  -0.1596086025238037\n",
      "Loss for batch is  -1.663020133972168\n",
      "|Iter  1153  | Total Train Loss  -3.006377339363098 |\n",
      "Val Loss for batch is  -1.0349974632263184\n",
      "Val Loss for batch is  1.101782202720642\n",
      "Val Loss for batch is  -0.36739954352378845\n",
      "Val Loss for batch is  -2.754504442214966\n",
      "|Iter  1153  | Total Val Loss  -3.0551192462444305 |\n",
      "Loss for batch is  0.14432930946350098\n",
      "Loss for batch is  -0.09687304496765137\n",
      "Loss for batch is  0.08602821826934814\n",
      "Loss for batch is  0.1136636734008789\n",
      "|Iter  1154  | Total Train Loss  0.24714815616607666 |\n",
      "Val Loss for batch is  -0.9989324808120728\n",
      "Val Loss for batch is  -1.4102832078933716\n",
      "Val Loss for batch is  -1.3597716093063354\n",
      "Val Loss for batch is  -2.392272472381592\n",
      "|Iter  1154  | Total Val Loss  -6.161259770393372 |\n",
      "Loss for batch is  0.22168874740600586\n",
      "Loss for batch is  -0.1802610158920288\n",
      "Loss for batch is  0.5339742302894592\n",
      "Loss for batch is  -0.8191852569580078\n",
      "|Iter  1155  | Total Train Loss  -0.24378329515457153 |\n",
      "Val Loss for batch is  -1.3082783222198486\n",
      "Val Loss for batch is  -1.4715861082077026\n",
      "Val Loss for batch is  -1.2404168844223022\n",
      "Val Loss for batch is  -2.4068760871887207\n",
      "|Iter  1155  | Total Val Loss  -6.427157402038574 |\n",
      "Loss for batch is  -0.017363667488098145\n",
      "Loss for batch is  -0.2666546106338501\n",
      "Loss for batch is  -0.1723954677581787\n",
      "Loss for batch is  -1.0614174604415894\n",
      "|Iter  1156  | Total Train Loss  -1.5178312063217163 |\n",
      "Val Loss for batch is  -1.5506751537322998\n",
      "Val Loss for batch is  -1.670014500617981\n",
      "Val Loss for batch is  -1.605446457862854\n",
      "Val Loss for batch is  -2.487746000289917\n",
      "|Iter  1156  | Total Val Loss  -7.313882112503052 |\n",
      "Loss for batch is  -0.05318570137023926\n",
      "Loss for batch is  -0.297177791595459\n",
      "Loss for batch is  -0.25893890857696533\n",
      "Loss for batch is  -1.0212558507919312\n",
      "|Iter  1157  | Total Train Loss  -1.6305582523345947 |\n",
      "Val Loss for batch is  -1.781732201576233\n",
      "Val Loss for batch is  -1.8518396615982056\n",
      "Val Loss for batch is  -1.6689624786376953\n",
      "Val Loss for batch is  -2.494710683822632\n",
      "|Iter  1157  | Total Val Loss  -7.797245025634766 |\n",
      "Loss for batch is  -0.25065135955810547\n",
      "Loss for batch is  -0.43078672885894775\n",
      "Loss for batch is  -0.2377697229385376\n",
      "Loss for batch is  -1.0503190755844116\n",
      "|Iter  1158  | Total Train Loss  -1.9695268869400024 |\n",
      "Val Loss for batch is  -1.8780430555343628\n",
      "Val Loss for batch is  -1.9224433898925781\n",
      "Val Loss for batch is  -1.8140331506729126\n",
      "Val Loss for batch is  -2.5443458557128906\n",
      "|Iter  1158  | Total Val Loss  -8.158865451812744 |\n",
      "Loss for batch is  -0.358223557472229\n",
      "Loss for batch is  -0.47131383419036865\n",
      "Loss for batch is  -0.3428233861923218\n",
      "Loss for batch is  -1.0555084943771362\n",
      "|Iter  1159  | Total Train Loss  -2.2278692722320557 |\n",
      "Val Loss for batch is  -1.9165828227996826\n",
      "Val Loss for batch is  -1.9778016805648804\n",
      "Val Loss for batch is  -1.860273838043213\n",
      "Val Loss for batch is  -2.566805839538574\n",
      "|Iter  1159  | Total Val Loss  -8.32146418094635 |\n",
      "Loss for batch is  -0.3874542713165283\n",
      "Loss for batch is  -0.46392714977264404\n",
      "Loss for batch is  -0.3705267906188965\n",
      "Loss for batch is  -1.0920342206954956\n",
      "|Iter  1160  | Total Train Loss  -2.3139424324035645 |\n",
      "Val Loss for batch is  -1.9472943544387817\n",
      "Val Loss for batch is  -2.0012383460998535\n",
      "Val Loss for batch is  -1.9018348455429077\n",
      "Val Loss for batch is  -2.5976054668426514\n",
      "|Iter  1160  | Total Val Loss  -8.447973012924194 |\n",
      "Loss for batch is  -0.4440191984176636\n",
      "Loss for batch is  -0.5293556451797485\n",
      "Loss for batch is  -0.4270514249801636\n",
      "Loss for batch is  -1.1378915309906006\n",
      "|Iter  1161  | Total Train Loss  -2.5383177995681763 |\n",
      "Val Loss for batch is  -1.9561586380004883\n",
      "Val Loss for batch is  -2.043691873550415\n",
      "Val Loss for batch is  -1.9441248178482056\n",
      "Val Loss for batch is  -2.630178213119507\n",
      "|Iter  1161  | Total Val Loss  -8.574153542518616 |\n",
      "Loss for batch is  -0.4759957790374756\n",
      "Loss for batch is  -0.5857102870941162\n",
      "Loss for batch is  -0.4743032455444336\n",
      "Loss for batch is  -1.2041525840759277\n",
      "|Iter  1162  | Total Train Loss  -2.740161895751953 |\n",
      "Val Loss for batch is  -1.9688408374786377\n",
      "Val Loss for batch is  -2.0423953533172607\n",
      "Val Loss for batch is  -1.9680676460266113\n",
      "Val Loss for batch is  -2.740622043609619\n",
      "|Iter  1162  | Total Val Loss  -8.719925880432129 |\n",
      "Loss for batch is  -0.5067640542984009\n",
      "Loss for batch is  -0.6308022737503052\n",
      "Loss for batch is  -0.49716830253601074\n",
      "Loss for batch is  -1.2810032367706299\n",
      "|Iter  1163  | Total Train Loss  -2.9157378673553467 |\n",
      "Val Loss for batch is  -1.9551254510879517\n",
      "Val Loss for batch is  -2.0399863719940186\n",
      "Val Loss for batch is  -1.9634407758712769\n",
      "Val Loss for batch is  -2.7946226596832275\n",
      "|Iter  1163  | Total Val Loss  -8.753175258636475 |\n",
      "Loss for batch is  -0.5476166009902954\n",
      "Loss for batch is  -0.6611044406890869\n",
      "Loss for batch is  -0.5539956092834473\n",
      "Loss for batch is  -1.35677170753479\n",
      "|Iter  1164  | Total Train Loss  -3.1194883584976196 |\n",
      "Val Loss for batch is  -1.9505521059036255\n",
      "Val Loss for batch is  -2.098284959793091\n",
      "Val Loss for batch is  -1.9778826236724854\n",
      "Val Loss for batch is  -2.870600938796997\n",
      "|Iter  1164  | Total Val Loss  -8.897320628166199 |\n",
      "Loss for batch is  -0.5801534652709961\n",
      "Loss for batch is  -0.6970434188842773\n",
      "Loss for batch is  -0.5930314064025879\n",
      "Loss for batch is  -1.4220021963119507\n",
      "|Iter  1165  | Total Train Loss  -3.292230486869812 |\n",
      "Val Loss for batch is  -1.9233342409133911\n",
      "Val Loss for batch is  -2.0753631591796875\n",
      "Val Loss for batch is  -1.9501969814300537\n",
      "Val Loss for batch is  -2.9119832515716553\n",
      "|Iter  1165  | Total Val Loss  -8.860877633094788 |\n",
      "Loss for batch is  -0.6119875907897949\n",
      "Loss for batch is  -0.726333737373352\n",
      "Loss for batch is  -0.6234277486801147\n",
      "Loss for batch is  -1.4694474935531616\n",
      "|Iter  1166  | Total Train Loss  -3.4311965703964233 |\n",
      "Val Loss for batch is  -1.9402614831924438\n",
      "Val Loss for batch is  -2.134380578994751\n",
      "Val Loss for batch is  -1.997986912727356\n",
      "Val Loss for batch is  -2.966705799102783\n",
      "|Iter  1166  | Total Val Loss  -9.039334774017334 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.6354926824569702\n",
      "Loss for batch is  -0.7524764537811279\n",
      "Loss for batch is  -0.6569949388504028\n",
      "Loss for batch is  -1.5210647583007812\n",
      "|Iter  1167  | Total Train Loss  -3.5660288333892822 |\n",
      "Val Loss for batch is  -1.931344985961914\n",
      "Val Loss for batch is  -2.124467611312866\n",
      "Val Loss for batch is  -1.9882862567901611\n",
      "Val Loss for batch is  -2.995191812515259\n",
      "|Iter  1167  | Total Val Loss  -9.0392906665802 |\n",
      "Loss for batch is  -0.6529877185821533\n",
      "Loss for batch is  -0.7917847633361816\n",
      "Loss for batch is  -0.6729429960250854\n",
      "Loss for batch is  -1.5694953203201294\n",
      "|Iter  1168  | Total Train Loss  -3.68721079826355 |\n",
      "Val Loss for batch is  -1.9439843893051147\n",
      "Val Loss for batch is  -2.1276278495788574\n",
      "Val Loss for batch is  -2.0270564556121826\n",
      "Val Loss for batch is  -3.0473897457122803\n",
      "|Iter  1168  | Total Val Loss  -9.146058440208435 |\n",
      "Loss for batch is  -0.6767832040786743\n",
      "Loss for batch is  -0.7954654693603516\n",
      "Loss for batch is  -0.6899600028991699\n",
      "Loss for batch is  -1.5874619483947754\n",
      "|Iter  1169  | Total Train Loss  -3.749670624732971 |\n",
      "Val Loss for batch is  -1.9801983833312988\n",
      "Val Loss for batch is  -2.1439666748046875\n",
      "Val Loss for batch is  -2.0913522243499756\n",
      "Val Loss for batch is  -3.075972557067871\n",
      "|Iter  1169  | Total Val Loss  -9.291489839553833 |\n",
      "Loss for batch is  -0.6925697326660156\n",
      "Loss for batch is  -0.8249006271362305\n",
      "Loss for batch is  -0.7237961292266846\n",
      "Loss for batch is  -1.6134034395217896\n",
      "|Iter  1170  | Total Train Loss  -3.85466992855072 |\n",
      "Val Loss for batch is  -1.9663234949111938\n",
      "Val Loss for batch is  -2.1281449794769287\n",
      "Val Loss for batch is  -2.081648826599121\n",
      "Val Loss for batch is  -3.098250150680542\n",
      "|Iter  1170  | Total Val Loss  -9.274367451667786 |\n",
      "Loss for batch is  -0.7169721126556396\n",
      "Loss for batch is  -0.8307148218154907\n",
      "Loss for batch is  -0.746863603591919\n",
      "Loss for batch is  -1.6377081871032715\n",
      "|Iter  1171  | Total Train Loss  -3.932258725166321 |\n",
      "Val Loss for batch is  -2.013376235961914\n",
      "Val Loss for batch is  -2.1933155059814453\n",
      "Val Loss for batch is  -2.1437644958496094\n",
      "Val Loss for batch is  -3.113908529281616\n",
      "|Iter  1171  | Total Val Loss  -9.464364767074585 |\n",
      "Loss for batch is  -0.7256488800048828\n",
      "Loss for batch is  -0.8534864187240601\n",
      "Loss for batch is  -0.7523688077926636\n",
      "Loss for batch is  -1.664846658706665\n",
      "|Iter  1172  | Total Train Loss  -3.9963507652282715 |\n",
      "Val Loss for batch is  -1.982141137123108\n",
      "Val Loss for batch is  -2.2173633575439453\n",
      "Val Loss for batch is  -2.117802381515503\n",
      "Val Loss for batch is  -3.1481573581695557\n",
      "|Iter  1172  | Total Val Loss  -9.465464234352112 |\n",
      "Loss for batch is  -0.7373256683349609\n",
      "Loss for batch is  -0.8667818307876587\n",
      "Loss for batch is  -0.7640511989593506\n",
      "Loss for batch is  -1.6896247863769531\n",
      "|Iter  1173  | Total Train Loss  -4.057783484458923 |\n",
      "Val Loss for batch is  -1.9869581460952759\n",
      "Val Loss for batch is  -2.1984033584594727\n",
      "Val Loss for batch is  -2.1499269008636475\n",
      "Val Loss for batch is  -3.158858299255371\n",
      "|Iter  1173  | Total Val Loss  -9.494146704673767 |\n",
      "Loss for batch is  -0.7487807273864746\n",
      "Loss for batch is  -0.8783324956893921\n",
      "Loss for batch is  -0.7762033939361572\n",
      "Loss for batch is  -1.7072902917861938\n",
      "|Iter  1174  | Total Train Loss  -4.110606908798218 |\n",
      "Val Loss for batch is  -2.0311999320983887\n",
      "Val Loss for batch is  -2.2358198165893555\n",
      "Val Loss for batch is  -2.1277918815612793\n",
      "Val Loss for batch is  -3.183590888977051\n",
      "|Iter  1174  | Total Val Loss  -9.578402519226074 |\n",
      "Loss for batch is  -0.7458933591842651\n",
      "Loss for batch is  -0.8988854885101318\n",
      "Loss for batch is  -0.7839441299438477\n",
      "Loss for batch is  -1.7246230840682983\n",
      "|Iter  1175  | Total Train Loss  -4.153346061706543 |\n",
      "Val Loss for batch is  -1.9818341732025146\n",
      "Val Loss for batch is  -2.206505060195923\n",
      "Val Loss for batch is  -2.1535227298736572\n",
      "Val Loss for batch is  -3.1927757263183594\n",
      "|Iter  1175  | Total Val Loss  -9.534637689590454 |\n",
      "Loss for batch is  -0.763687014579773\n",
      "Loss for batch is  -0.8717405796051025\n",
      "Loss for batch is  -0.8131077289581299\n",
      "Loss for batch is  -1.703260898590088\n",
      "|Iter  1176  | Total Train Loss  -4.151796221733093 |\n",
      "Val Loss for batch is  -2.017820358276367\n",
      "Val Loss for batch is  -2.2683334350585938\n",
      "Val Loss for batch is  -2.146632194519043\n",
      "Val Loss for batch is  -3.206540822982788\n",
      "|Iter  1176  | Total Val Loss  -9.639326810836792 |\n",
      "Loss for batch is  -0.7208521366119385\n",
      "Loss for batch is  -0.7677567005157471\n",
      "Loss for batch is  -0.6164497137069702\n",
      "Loss for batch is  -1.6891580820083618\n",
      "|Iter  1177  | Total Train Loss  -3.7942166328430176 |\n",
      "Val Loss for batch is  -1.7092089653015137\n",
      "Val Loss for batch is  -1.871514916419983\n",
      "Val Loss for batch is  -1.727670669555664\n",
      "Val Loss for batch is  -3.010598659515381\n",
      "|Iter  1177  | Total Val Loss  -8.318993210792542 |\n",
      "Loss for batch is  -0.434850811958313\n",
      "Loss for batch is  -0.8661519289016724\n",
      "Loss for batch is  -0.5303361415863037\n",
      "Loss for batch is  -1.4190858602523804\n",
      "|Iter  1178  | Total Train Loss  -3.2504247426986694 |\n",
      "Val Loss for batch is  -2.0089657306671143\n",
      "Val Loss for batch is  -2.2218194007873535\n",
      "Val Loss for batch is  -2.001676321029663\n",
      "Val Loss for batch is  -3.1346356868743896\n",
      "|Iter  1178  | Total Val Loss  -9.36709713935852 |\n",
      "Loss for batch is  -0.7283769845962524\n",
      "Loss for batch is  -0.3899174928665161\n",
      "Loss for batch is  -0.5025498867034912\n",
      "Loss for batch is  -1.5972132682800293\n",
      "|Iter  1179  | Total Train Loss  -3.218057632446289 |\n",
      "Val Loss for batch is  -1.2709146738052368\n",
      "Val Loss for batch is  -1.6707088947296143\n",
      "Val Loss for batch is  -1.6578419208526611\n",
      "Val Loss for batch is  -2.871443748474121\n",
      "|Iter  1179  | Total Val Loss  -7.470909237861633 |\n",
      "Loss for batch is  -0.04432106018066406\n",
      "Loss for batch is  -0.6987764835357666\n",
      "Loss for batch is  -0.48602986335754395\n",
      "Loss for batch is  -1.1700936555862427\n",
      "|Iter  1180  | Total Train Loss  -2.3992210626602173 |\n",
      "Val Loss for batch is  -1.8857232332229614\n",
      "Val Loss for batch is  -2.114457368850708\n",
      "Val Loss for batch is  -1.9535428285598755\n",
      "Val Loss for batch is  -2.962043523788452\n",
      "|Iter  1180  | Total Val Loss  -8.915766954421997 |\n",
      "Loss for batch is  -0.6272803544998169\n",
      "Loss for batch is  -0.6713536977767944\n",
      "Loss for batch is  -0.5420464277267456\n",
      "Loss for batch is  -1.5334032773971558\n",
      "|Iter  1181  | Total Train Loss  -3.3740837574005127 |\n",
      "Val Loss for batch is  -1.9635969400405884\n",
      "Val Loss for batch is  -2.123953104019165\n",
      "Val Loss for batch is  -2.0329434871673584\n",
      "Val Loss for batch is  -3.0586681365966797\n",
      "|Iter  1181  | Total Val Loss  -9.179161667823792 |\n",
      "Loss for batch is  -0.6606792211532593\n",
      "Loss for batch is  -0.7715044021606445\n",
      "Loss for batch is  -0.5689373016357422\n",
      "Loss for batch is  -1.4965674877166748\n",
      "|Iter  1182  | Total Train Loss  -3.497688412666321 |\n",
      "Val Loss for batch is  -2.0237717628479004\n",
      "Val Loss for batch is  -2.141930341720581\n",
      "Val Loss for batch is  -2.03426194190979\n",
      "Val Loss for batch is  -3.064816474914551\n",
      "|Iter  1182  | Total Val Loss  -9.264780521392822 |\n",
      "Loss for batch is  -0.7093541622161865\n",
      "Loss for batch is  -0.7811034917831421\n",
      "Loss for batch is  -0.6598378419876099\n",
      "Loss for batch is  -1.5601664781570435\n",
      "|Iter  1183  | Total Train Loss  -3.710461974143982 |\n",
      "Val Loss for batch is  -2.0356669425964355\n",
      "Val Loss for batch is  -2.1659114360809326\n",
      "Val Loss for batch is  -2.081958532333374\n",
      "Val Loss for batch is  -3.0375723838806152\n",
      "|Iter  1183  | Total Val Loss  -9.321109294891357 |\n",
      "Loss for batch is  -0.6873605251312256\n",
      "Loss for batch is  -0.8180093765258789\n",
      "Loss for batch is  -0.6832845211029053\n",
      "Loss for batch is  -1.6081578731536865\n",
      "|Iter  1184  | Total Train Loss  -3.7968122959136963 |\n",
      "Val Loss for batch is  -2.0607750415802\n",
      "Val Loss for batch is  -2.1553871631622314\n",
      "Val Loss for batch is  -2.1059889793395996\n",
      "Val Loss for batch is  -3.1028618812561035\n",
      "|Iter  1184  | Total Val Loss  -9.425013065338135 |\n",
      "Loss for batch is  -0.753063440322876\n",
      "Loss for batch is  -0.8330891132354736\n",
      "Loss for batch is  -0.7309558391571045\n",
      "Loss for batch is  -1.6498559713363647\n",
      "|Iter  1185  | Total Train Loss  -3.966964364051819 |\n",
      "Val Loss for batch is  -2.091292381286621\n",
      "Val Loss for batch is  -2.234696388244629\n",
      "Val Loss for batch is  -2.111715316772461\n",
      "Val Loss for batch is  -3.106391429901123\n",
      "|Iter  1185  | Total Val Loss  -9.544095516204834 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.7534348964691162\n",
      "Loss for batch is  -0.8612384796142578\n",
      "Loss for batch is  -0.7702553272247314\n",
      "Loss for batch is  -1.7001608610153198\n",
      "|Iter  1186  | Total Train Loss  -4.085089564323425 |\n",
      "Val Loss for batch is  -2.074503183364868\n",
      "Val Loss for batch is  -2.221242666244507\n",
      "Val Loss for batch is  -2.1475791931152344\n",
      "Val Loss for batch is  -3.175546169281006\n",
      "|Iter  1186  | Total Val Loss  -9.618871212005615 |\n",
      "Loss for batch is  -0.7972850799560547\n",
      "Loss for batch is  -0.8819023370742798\n",
      "Loss for batch is  -0.7842390537261963\n",
      "Loss for batch is  -1.7400074005126953\n",
      "|Iter  1187  | Total Train Loss  -4.203433871269226 |\n",
      "Val Loss for batch is  -2.1093719005584717\n",
      "Val Loss for batch is  -2.285381317138672\n",
      "Val Loss for batch is  -2.1505470275878906\n",
      "Val Loss for batch is  -3.1824557781219482\n",
      "|Iter  1187  | Total Val Loss  -9.727756023406982 |\n",
      "Loss for batch is  -0.7960091829299927\n",
      "Loss for batch is  -0.9174511432647705\n",
      "Loss for batch is  -0.8192216157913208\n",
      "Loss for batch is  -1.761123776435852\n",
      "|Iter  1188  | Total Train Loss  -4.293805718421936 |\n",
      "Val Loss for batch is  -2.06030535697937\n",
      "Val Loss for batch is  -2.264321804046631\n",
      "Val Loss for batch is  -2.18265700340271\n",
      "Val Loss for batch is  -3.2249293327331543\n",
      "|Iter  1188  | Total Val Loss  -9.732213497161865 |\n",
      "Loss for batch is  -0.8258943557739258\n",
      "Loss for batch is  -0.9520469903945923\n",
      "Loss for batch is  -0.8453148603439331\n",
      "Loss for batch is  -1.7836562395095825\n",
      "|Iter  1189  | Total Train Loss  -4.406912446022034 |\n",
      "Val Loss for batch is  -2.0884804725646973\n",
      "Val Loss for batch is  -2.2975378036499023\n",
      "Val Loss for batch is  -2.185453176498413\n",
      "Val Loss for batch is  -3.2680633068084717\n",
      "|Iter  1189  | Total Val Loss  -9.839534759521484 |\n",
      "Loss for batch is  -0.8374571800231934\n",
      "Loss for batch is  -0.9524418115615845\n",
      "Loss for batch is  -0.8351569175720215\n",
      "Loss for batch is  -1.820684790611267\n",
      "|Iter  1190  | Total Train Loss  -4.445740699768066 |\n",
      "Val Loss for batch is  -1.9933955669403076\n",
      "Val Loss for batch is  -2.2513911724090576\n",
      "Val Loss for batch is  -2.0797622203826904\n",
      "Val Loss for batch is  -3.241353988647461\n",
      "|Iter  1190  | Total Val Loss  -9.565902948379517 |\n",
      "Loss for batch is  -0.7762012481689453\n",
      "Loss for batch is  -0.9758443832397461\n",
      "Loss for batch is  -0.8052090406417847\n",
      "Loss for batch is  -1.827484130859375\n",
      "|Iter  1191  | Total Train Loss  -4.384738802909851 |\n",
      "Val Loss for batch is  -2.019362449645996\n",
      "Val Loss for batch is  -2.2523984909057617\n",
      "Val Loss for batch is  -2.11426043510437\n",
      "Val Loss for batch is  -3.2527101039886475\n",
      "|Iter  1191  | Total Val Loss  -9.638731479644775 |\n",
      "Loss for batch is  -0.7635927200317383\n",
      "Loss for batch is  -0.9892019033432007\n",
      "Loss for batch is  -0.8251990079879761\n",
      "Loss for batch is  -1.8323075771331787\n",
      "|Iter  1192  | Total Train Loss  -4.410301208496094 |\n",
      "Val Loss for batch is  -2.041692018508911\n",
      "Val Loss for batch is  -2.274723529815674\n",
      "Val Loss for batch is  -2.1612462997436523\n",
      "Val Loss for batch is  -3.3119170665740967\n",
      "|Iter  1192  | Total Val Loss  -9.789578914642334 |\n",
      "Loss for batch is  -0.8432712554931641\n",
      "Loss for batch is  -0.9420194625854492\n",
      "Loss for batch is  -0.8626688718795776\n",
      "Loss for batch is  -1.8128074407577515\n",
      "|Iter  1193  | Total Train Loss  -4.460767030715942 |\n",
      "Val Loss for batch is  -1.9243673086166382\n",
      "Val Loss for batch is  -2.1834716796875\n",
      "Val Loss for batch is  -2.1021182537078857\n",
      "Val Loss for batch is  -3.257302761077881\n",
      "|Iter  1193  | Total Val Loss  -9.467260003089905 |\n",
      "Loss for batch is  -0.6474231481552124\n",
      "Loss for batch is  -0.7022764682769775\n",
      "Loss for batch is  -0.6119349002838135\n",
      "Loss for batch is  -1.7533020973205566\n",
      "|Iter  1194  | Total Train Loss  -3.71493661403656 |\n",
      "Val Loss for batch is  -1.3262794017791748\n",
      "Val Loss for batch is  -1.4783332347869873\n",
      "Val Loss for batch is  -1.501039743423462\n",
      "Val Loss for batch is  -2.935460329055786\n",
      "|Iter  1194  | Total Val Loss  -7.24111270904541 |\n",
      "Loss for batch is  -0.13238108158111572\n",
      "Loss for batch is  -0.8474786281585693\n",
      "Loss for batch is  -0.4156869649887085\n",
      "Loss for batch is  -1.3720674514770508\n",
      "|Iter  1195  | Total Train Loss  -2.7676141262054443 |\n",
      "Val Loss for batch is  -1.6729447841644287\n",
      "Val Loss for batch is  -2.0308520793914795\n",
      "Val Loss for batch is  -1.881330132484436\n",
      "Val Loss for batch is  -3.102860450744629\n",
      "|Iter  1195  | Total Val Loss  -8.687987446784973 |\n",
      "Loss for batch is  -0.4875185489654541\n",
      "Loss for batch is  -0.7125591039657593\n",
      "Loss for batch is  -0.4797171354293823\n",
      "Loss for batch is  -1.5657293796539307\n",
      "|Iter  1196  | Total Train Loss  -3.2455241680145264 |\n",
      "Val Loss for batch is  -1.7855658531188965\n",
      "Val Loss for batch is  -2.009922504425049\n",
      "Val Loss for batch is  -1.7824938297271729\n",
      "Val Loss for batch is  -2.9704830646514893\n",
      "|Iter  1196  | Total Val Loss  -8.548465251922607 |\n",
      "Loss for batch is  -0.5420067310333252\n",
      "Loss for batch is  -0.6502392292022705\n",
      "Loss for batch is  -0.4321599006652832\n",
      "Loss for batch is  -1.645985722541809\n",
      "|Iter  1197  | Total Train Loss  -3.270391583442688 |\n",
      "Val Loss for batch is  -1.7334717512130737\n",
      "Val Loss for batch is  -1.919985055923462\n",
      "Val Loss for batch is  -1.7810211181640625\n",
      "Val Loss for batch is  -2.9438815116882324\n",
      "|Iter  1197  | Total Val Loss  -8.37835943698883 |\n",
      "Loss for batch is  -0.511074423789978\n",
      "Loss for batch is  -0.5864564180374146\n",
      "Loss for batch is  -0.5153746604919434\n",
      "Loss for batch is  -1.607407808303833\n",
      "|Iter  1198  | Total Train Loss  -3.220313310623169 |\n",
      "Val Loss for batch is  -1.904184341430664\n",
      "Val Loss for batch is  -2.190857410430908\n",
      "Val Loss for batch is  -1.981547236442566\n",
      "Val Loss for batch is  -3.0647988319396973\n",
      "|Iter  1198  | Total Val Loss  -9.141387820243835 |\n",
      "Loss for batch is  -0.596259355545044\n",
      "Loss for batch is  -0.7545557022094727\n",
      "Loss for batch is  -0.5950396060943604\n",
      "Loss for batch is  -1.5607209205627441\n",
      "|Iter  1199  | Total Train Loss  -3.506575584411621 |\n",
      "Val Loss for batch is  -2.037496566772461\n",
      "Val Loss for batch is  -2.1574010848999023\n",
      "Val Loss for batch is  -2.0126171112060547\n",
      "Val Loss for batch is  -3.063925266265869\n",
      "|Iter  1199  | Total Val Loss  -9.271440029144287 |\n",
      "Loss for batch is  -0.7159472703933716\n",
      "Loss for batch is  -0.8041536808013916\n",
      "Loss for batch is  -0.674161434173584\n",
      "Loss for batch is  -1.6341420412063599\n",
      "|Iter  1200  | Total Train Loss  -3.828404426574707 |\n",
      "Val Loss for batch is  -2.02178955078125\n",
      "Val Loss for batch is  -2.1228084564208984\n",
      "Val Loss for batch is  -2.1375834941864014\n",
      "Val Loss for batch is  -3.123164176940918\n",
      "|Iter  1200  | Total Val Loss  -9.405345678329468 |\n",
      "Loss for batch is  -0.6680316925048828\n",
      "Loss for batch is  -0.7882094383239746\n",
      "Loss for batch is  -0.7324811220169067\n",
      "Loss for batch is  -1.654564380645752\n",
      "|Iter  1201  | Total Train Loss  -3.843286633491516 |\n",
      "Val Loss for batch is  -2.114872694015503\n",
      "Val Loss for batch is  -2.2213540077209473\n",
      "Val Loss for batch is  -2.1478641033172607\n",
      "Val Loss for batch is  -3.112799644470215\n",
      "|Iter  1201  | Total Val Loss  -9.596890449523926 |\n",
      "Loss for batch is  -0.7524827718734741\n",
      "Loss for batch is  -0.8623536825180054\n",
      "Loss for batch is  -0.7442787885665894\n",
      "Loss for batch is  -1.687165379524231\n",
      "|Iter  1202  | Total Train Loss  -4.0462806224823 |\n",
      "Val Loss for batch is  -2.127103805541992\n",
      "Val Loss for batch is  -2.2109341621398926\n",
      "Val Loss for batch is  -2.164816379547119\n",
      "Val Loss for batch is  -3.160189390182495\n",
      "|Iter  1202  | Total Val Loss  -9.663043737411499 |\n",
      "Loss for batch is  -0.7976893186569214\n",
      "Loss for batch is  -0.8812940120697021\n",
      "Loss for batch is  -0.7909079790115356\n",
      "Loss for batch is  -1.7106460332870483\n",
      "|Iter  1203  | Total Train Loss  -4.1805373430252075 |\n",
      "Val Loss for batch is  -2.09782338142395\n",
      "Val Loss for batch is  -2.266275644302368\n",
      "Val Loss for batch is  -2.1942195892333984\n",
      "Val Loss for batch is  -3.174832582473755\n",
      "|Iter  1203  | Total Val Loss  -9.733151197433472 |\n",
      "Loss for batch is  -0.7945191860198975\n",
      "Loss for batch is  -0.9326862096786499\n",
      "Loss for batch is  -0.8220406770706177\n",
      "Loss for batch is  -1.7632309198379517\n",
      "|Iter  1204  | Total Train Loss  -4.312476992607117 |\n",
      "Val Loss for batch is  -2.1214702129364014\n",
      "Val Loss for batch is  -2.2826685905456543\n",
      "Val Loss for batch is  -2.1930766105651855\n",
      "Val Loss for batch is  -3.193328380584717\n",
      "|Iter  1204  | Total Val Loss  -9.790543794631958 |\n",
      "Loss for batch is  -0.8246299028396606\n",
      "Loss for batch is  -0.9477956295013428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.8403326272964478\n",
      "Loss for batch is  -1.7945120334625244\n",
      "|Iter  1205  | Total Train Loss  -4.407270193099976 |\n",
      "Val Loss for batch is  -2.104219675064087\n",
      "Val Loss for batch is  -2.2899396419525146\n",
      "Val Loss for batch is  -2.2101199626922607\n",
      "Val Loss for batch is  -3.26721453666687\n",
      "|Iter  1205  | Total Val Loss  -9.871493816375732 |\n",
      "Loss for batch is  -0.8486183881759644\n",
      "Loss for batch is  -0.9705729484558105\n",
      "Loss for batch is  -0.8684467077255249\n",
      "Loss for batch is  -1.8301790952682495\n",
      "|Iter  1206  | Total Train Loss  -4.517817139625549 |\n",
      "Val Loss for batch is  -2.1306653022766113\n",
      "Val Loss for batch is  -2.2723522186279297\n",
      "Val Loss for batch is  -2.2274200916290283\n",
      "Val Loss for batch is  -3.261640787124634\n",
      "|Iter  1206  | Total Val Loss  -9.892078399658203 |\n",
      "Loss for batch is  -0.863906741142273\n",
      "Loss for batch is  -0.9998000860214233\n",
      "Loss for batch is  -0.8806802034378052\n",
      "Loss for batch is  -1.8357011079788208\n",
      "|Iter  1207  | Total Train Loss  -4.580088138580322 |\n",
      "Val Loss for batch is  -2.099700927734375\n",
      "Val Loss for batch is  -2.313253402709961\n",
      "Val Loss for batch is  -2.2061214447021484\n",
      "Val Loss for batch is  -3.272092342376709\n",
      "|Iter  1207  | Total Val Loss  -9.891168117523193 |\n",
      "Loss for batch is  -0.8856114149093628\n",
      "Loss for batch is  -1.0018043518066406\n",
      "Loss for batch is  -0.908462405204773\n",
      "Loss for batch is  -1.863049030303955\n",
      "|Iter  1208  | Total Train Loss  -4.6589272022247314 |\n",
      "Val Loss for batch is  -2.1195790767669678\n",
      "Val Loss for batch is  -2.3043389320373535\n",
      "Val Loss for batch is  -2.247100830078125\n",
      "Val Loss for batch is  -3.2961952686309814\n",
      "|Iter  1208  | Total Val Loss  -9.967214107513428 |\n",
      "Loss for batch is  -0.8602873086929321\n",
      "Loss for batch is  -1.0217081308364868\n",
      "Loss for batch is  -0.8739804029464722\n",
      "Loss for batch is  -1.8776500225067139\n",
      "|Iter  1209  | Total Train Loss  -4.633625864982605 |\n",
      "Val Loss for batch is  -1.9521783590316772\n",
      "Val Loss for batch is  -2.220069408416748\n",
      "Val Loss for batch is  -2.0602893829345703\n",
      "Val Loss for batch is  -3.145235776901245\n",
      "|Iter  1209  | Total Val Loss  -9.37777292728424 |\n",
      "Loss for batch is  -0.735437273979187\n",
      "Loss for batch is  -1.022375226020813\n",
      "Loss for batch is  -0.7752218246459961\n",
      "Loss for batch is  -1.8529170751571655\n",
      "|Iter  1210  | Total Train Loss  -4.385951399803162 |\n",
      "Val Loss for batch is  -2.0320351123809814\n",
      "Val Loss for batch is  -2.160487651824951\n",
      "Val Loss for batch is  -1.9599273204803467\n",
      "Val Loss for batch is  -3.218830108642578\n",
      "|Iter  1210  | Total Val Loss  -9.371280193328857 |\n",
      "Loss for batch is  -0.7852364778518677\n",
      "Loss for batch is  -0.8514928817749023\n",
      "Loss for batch is  -0.8263388872146606\n",
      "Loss for batch is  -1.6302047967910767\n",
      "|Iter  1211  | Total Train Loss  -4.093273043632507 |\n",
      "Val Loss for batch is  -0.9272157549858093\n",
      "Val Loss for batch is  -2.135486364364624\n",
      "Val Loss for batch is  -2.008899688720703\n",
      "Val Loss for batch is  -3.2322471141815186\n",
      "|Iter  1211  | Total Val Loss  -8.303848922252655 |\n",
      "Loss for batch is  0.09169924259185791\n",
      "Loss for batch is  0.8307823538780212\n",
      "Loss for batch is  0.5260442495346069\n",
      "Loss for batch is  -1.2108663320541382\n",
      "|Iter  1212  | Total Train Loss  0.2376595139503479 |\n",
      "Val Loss for batch is  -0.47658976912498474\n",
      "Val Loss for batch is  -0.9014409780502319\n",
      "Val Loss for batch is  -0.5092588067054749\n",
      "Val Loss for batch is  -1.972765564918518\n",
      "|Iter  1212  | Total Val Loss  -3.8600551187992096 |\n",
      "Loss for batch is  0.6953788995742798\n",
      "Loss for batch is  0.7145552039146423\n",
      "Loss for batch is  0.8042477965354919\n",
      "Loss for batch is  -1.120921015739441\n",
      "|Iter  1213  | Total Train Loss  1.0932608842849731 |\n",
      "Val Loss for batch is  -1.2173168659210205\n",
      "Val Loss for batch is  -1.6750253438949585\n",
      "Val Loss for batch is  -1.2262439727783203\n",
      "Val Loss for batch is  -2.7150137424468994\n",
      "|Iter  1213  | Total Val Loss  -6.833599925041199 |\n",
      "Loss for batch is  0.03448891639709473\n",
      "Loss for batch is  -0.17850875854492188\n",
      "Loss for batch is  0.3414452075958252\n",
      "Loss for batch is  -0.8553183078765869\n",
      "|Iter  1214  | Total Train Loss  -0.6578929424285889 |\n",
      "Val Loss for batch is  -1.2750474214553833\n",
      "Val Loss for batch is  -1.5050016641616821\n",
      "Val Loss for batch is  -1.389325499534607\n",
      "Val Loss for batch is  -2.406259298324585\n",
      "|Iter  1214  | Total Val Loss  -6.575633883476257 |\n",
      "Loss for batch is  0.1579129695892334\n",
      "Loss for batch is  -0.23895931243896484\n",
      "Loss for batch is  -0.17124557495117188\n",
      "Loss for batch is  -1.0908464193344116\n",
      "|Iter  1215  | Total Train Loss  -1.343138337135315 |\n",
      "Val Loss for batch is  -1.6768155097961426\n",
      "Val Loss for batch is  -1.6963087320327759\n",
      "Val Loss for batch is  -1.624078631401062\n",
      "Val Loss for batch is  -2.550168037414551\n",
      "|Iter  1215  | Total Val Loss  -7.547370910644531 |\n",
      "Loss for batch is  -0.20749115943908691\n",
      "Loss for batch is  -0.26910102367401123\n",
      "Loss for batch is  -0.21698129177093506\n",
      "Loss for batch is  -1.0402382612228394\n",
      "|Iter  1216  | Total Train Loss  -1.7338117361068726 |\n",
      "Val Loss for batch is  -1.7650245428085327\n",
      "Val Loss for batch is  -1.7386001348495483\n",
      "Val Loss for batch is  -1.624667763710022\n",
      "Val Loss for batch is  -2.5235729217529297\n",
      "|Iter  1216  | Total Val Loss  -7.651865363121033 |\n",
      "Loss for batch is  -0.2701014280319214\n",
      "Loss for batch is  -0.3729832172393799\n",
      "Loss for batch is  -0.31759965419769287\n",
      "Loss for batch is  -1.1071406602859497\n",
      "|Iter  1217  | Total Train Loss  -2.067824959754944 |\n",
      "Val Loss for batch is  -1.7947965860366821\n",
      "Val Loss for batch is  -1.826491117477417\n",
      "Val Loss for batch is  -1.7311651706695557\n",
      "Val Loss for batch is  -2.580442190170288\n",
      "|Iter  1217  | Total Val Loss  -7.932895064353943 |\n",
      "Loss for batch is  -0.34095561504364014\n",
      "Loss for batch is  -0.44161295890808105\n",
      "Loss for batch is  -0.369365930557251\n",
      "Loss for batch is  -1.1154338121414185\n",
      "|Iter  1218  | Total Train Loss  -2.2673683166503906 |\n",
      "Val Loss for batch is  -1.851123332977295\n",
      "Val Loss for batch is  -1.9506357908248901\n",
      "Val Loss for batch is  -1.8229142427444458\n",
      "Val Loss for batch is  -2.575606346130371\n",
      "|Iter  1218  | Total Val Loss  -8.200279712677002 |\n",
      "Loss for batch is  -0.3584015369415283\n",
      "Loss for batch is  -0.4889615774154663\n",
      "Loss for batch is  -0.38136231899261475\n",
      "Loss for batch is  -1.1494183540344238\n",
      "|Iter  1219  | Total Train Loss  -2.378143787384033 |\n",
      "Val Loss for batch is  -1.8640406131744385\n",
      "Val Loss for batch is  -1.9715001583099365\n",
      "Val Loss for batch is  -1.844386339187622\n",
      "Val Loss for batch is  -2.6538631916046143\n",
      "|Iter  1219  | Total Val Loss  -8.333790302276611 |\n",
      "Loss for batch is  -0.3999180793762207\n",
      "Loss for batch is  -0.5454163551330566\n",
      "Loss for batch is  -0.4376765489578247\n",
      "Loss for batch is  -1.224953293800354\n",
      "|Iter  1220  | Total Train Loss  -2.607964277267456 |\n",
      "Val Loss for batch is  -1.894107460975647\n",
      "Val Loss for batch is  -1.9573577642440796\n",
      "Val Loss for batch is  -1.8530840873718262\n",
      "Val Loss for batch is  -2.703860282897949\n",
      "|Iter  1220  | Total Val Loss  -8.408409595489502 |\n",
      "Loss for batch is  -0.45509958267211914\n",
      "Loss for batch is  -0.5776352882385254\n",
      "Loss for batch is  -0.48693931102752686\n",
      "Loss for batch is  -1.273826003074646\n",
      "|Iter  1221  | Total Train Loss  -2.7935001850128174 |\n",
      "Val Loss for batch is  -1.9188913106918335\n",
      "Val Loss for batch is  -2.0003974437713623\n",
      "Val Loss for batch is  -1.9175357818603516\n",
      "Val Loss for batch is  -2.7626562118530273\n",
      "|Iter  1221  | Total Val Loss  -8.599480748176575 |\n",
      "Loss for batch is  -0.5012040138244629\n",
      "Loss for batch is  -0.609392523765564\n",
      "Loss for batch is  -0.5239864587783813\n",
      "Loss for batch is  -1.3532767295837402\n",
      "|Iter  1222  | Total Train Loss  -2.9878597259521484 |\n",
      "Val Loss for batch is  -1.934510588645935\n",
      "Val Loss for batch is  -2.0174400806427\n",
      "Val Loss for batch is  -1.9055614471435547\n",
      "Val Loss for batch is  -2.8339760303497314\n",
      "|Iter  1222  | Total Val Loss  -8.691488146781921 |\n",
      "Loss for batch is  -0.550858736038208\n",
      "Loss for batch is  -0.6681771278381348\n",
      "Loss for batch is  -0.5672863721847534\n",
      "Loss for batch is  -1.4115482568740845\n",
      "|Iter  1223  | Total Train Loss  -3.1978704929351807 |\n",
      "Val Loss for batch is  -1.9186958074569702\n",
      "Val Loss for batch is  -2.0404930114746094\n",
      "Val Loss for batch is  -1.9445616006851196\n",
      "Val Loss for batch is  -2.8701319694519043\n",
      "|Iter  1223  | Total Val Loss  -8.773882389068604 |\n",
      "Loss for batch is  -0.5769087076187134\n",
      "Loss for batch is  -0.7131638526916504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.5995194911956787\n",
      "Loss for batch is  -1.4503735303878784\n",
      "|Iter  1224  | Total Train Loss  -3.339965581893921 |\n",
      "Val Loss for batch is  -1.9324764013290405\n",
      "Val Loss for batch is  -2.104597330093384\n",
      "Val Loss for batch is  -2.0065665245056152\n",
      "Val Loss for batch is  -2.922121047973633\n",
      "|Iter  1224  | Total Val Loss  -8.965761303901672 |\n",
      "Loss for batch is  -0.6242696046829224\n",
      "Loss for batch is  -0.745397686958313\n",
      "Loss for batch is  -0.6493574380874634\n",
      "Loss for batch is  -1.5123683214187622\n",
      "|Iter  1225  | Total Train Loss  -3.531393051147461 |\n",
      "Val Loss for batch is  -1.9777088165283203\n",
      "Val Loss for batch is  -2.10532283782959\n",
      "Val Loss for batch is  -2.016730785369873\n",
      "Val Loss for batch is  -2.964585065841675\n",
      "|Iter  1225  | Total Val Loss  -9.064347505569458 |\n",
      "Loss for batch is  -0.6519744396209717\n",
      "Loss for batch is  -0.7872790098190308\n",
      "Loss for batch is  -0.679624080657959\n",
      "Loss for batch is  -1.5434420108795166\n",
      "|Iter  1226  | Total Train Loss  -3.662319540977478 |\n",
      "Val Loss for batch is  -1.988694667816162\n",
      "Val Loss for batch is  -2.1434969902038574\n",
      "Val Loss for batch is  -2.0265257358551025\n",
      "Val Loss for batch is  -2.9935991764068604\n",
      "|Iter  1226  | Total Val Loss  -9.152316570281982 |\n",
      "Loss for batch is  -0.6871646642684937\n",
      "Loss for batch is  -0.8237390518188477\n",
      "Loss for batch is  -0.7074748277664185\n",
      "Loss for batch is  -1.5828416347503662\n",
      "|Iter  1227  | Total Train Loss  -3.801220178604126 |\n",
      "Val Loss for batch is  -1.9835090637207031\n",
      "Val Loss for batch is  -2.1558172702789307\n",
      "Val Loss for batch is  -2.06508469581604\n",
      "Val Loss for batch is  -3.0452394485473633\n",
      "|Iter  1227  | Total Val Loss  -9.249650478363037 |\n",
      "Loss for batch is  -0.7095563411712646\n",
      "Loss for batch is  -0.849972128868103\n",
      "Loss for batch is  -0.7451143264770508\n",
      "Loss for batch is  -1.6253341436386108\n",
      "|Iter  1228  | Total Train Loss  -3.9299769401550293 |\n",
      "Val Loss for batch is  -2.009204626083374\n",
      "Val Loss for batch is  -2.1804187297821045\n",
      "Val Loss for batch is  -2.0796196460723877\n",
      "Val Loss for batch is  -3.0634605884552\n",
      "|Iter  1228  | Total Val Loss  -9.332703590393066 |\n",
      "Loss for batch is  -0.7357223033905029\n",
      "Loss for batch is  -0.872954249382019\n",
      "Loss for batch is  -0.7659910917282104\n",
      "Loss for batch is  -1.6501730680465698\n",
      "|Iter  1229  | Total Train Loss  -4.024840712547302 |\n",
      "Val Loss for batch is  -1.983300805091858\n",
      "Val Loss for batch is  -2.215510606765747\n",
      "Val Loss for batch is  -2.0651073455810547\n",
      "Val Loss for batch is  -3.102750778198242\n",
      "|Iter  1229  | Total Val Loss  -9.366669535636902 |\n",
      "Loss for batch is  -0.7557168006896973\n",
      "Loss for batch is  -0.8985778093338013\n",
      "Loss for batch is  -0.7792748212814331\n",
      "Loss for batch is  -1.6847025156021118\n",
      "|Iter  1230  | Total Train Loss  -4.1182719469070435 |\n",
      "Val Loss for batch is  -2.007359743118286\n",
      "Val Loss for batch is  -2.2600278854370117\n",
      "Val Loss for batch is  -2.1262173652648926\n",
      "Val Loss for batch is  -3.1246747970581055\n",
      "|Iter  1230  | Total Val Loss  -9.518279790878296 |\n",
      "Loss for batch is  -0.7646329402923584\n",
      "Loss for batch is  -0.9228764772415161\n",
      "Loss for batch is  -0.8079084157943726\n",
      "Loss for batch is  -1.7034602165222168\n",
      "|Iter  1231  | Total Train Loss  -4.198878049850464 |\n",
      "Val Loss for batch is  -2.0435805320739746\n",
      "Val Loss for batch is  -2.265986919403076\n",
      "Val Loss for batch is  -2.1315557956695557\n",
      "Val Loss for batch is  -3.143744945526123\n",
      "|Iter  1231  | Total Val Loss  -9.58486819267273 |\n",
      "Loss for batch is  -0.7909302711486816\n",
      "Loss for batch is  -0.9351541996002197\n",
      "Loss for batch is  -0.8168035745620728\n",
      "Loss for batch is  -1.7273967266082764\n",
      "|Iter  1232  | Total Train Loss  -4.2702847719192505 |\n",
      "Val Loss for batch is  -2.0356252193450928\n",
      "Val Loss for batch is  -2.2789881229400635\n",
      "Val Loss for batch is  -2.1804187297821045\n",
      "Val Loss for batch is  -3.1696817874908447\n",
      "|Iter  1232  | Total Val Loss  -9.664713859558105 |\n",
      "Loss for batch is  -0.811672568321228\n",
      "Loss for batch is  -0.9600878953933716\n",
      "Loss for batch is  -0.8451828956604004\n",
      "Loss for batch is  -1.7450379133224487\n",
      "|Iter  1233  | Total Train Loss  -4.361981272697449 |\n",
      "Val Loss for batch is  -2.0562801361083984\n",
      "Val Loss for batch is  -2.2731142044067383\n",
      "Val Loss for batch is  -2.1836559772491455\n",
      "Val Loss for batch is  -3.194484233856201\n",
      "|Iter  1233  | Total Val Loss  -9.707534551620483 |\n",
      "Loss for batch is  -0.821439266204834\n",
      "Loss for batch is  -0.9744619131088257\n",
      "Loss for batch is  -0.8448358774185181\n",
      "Loss for batch is  -1.7748535871505737\n",
      "|Iter  1234  | Total Train Loss  -4.4155906438827515 |\n",
      "Val Loss for batch is  -2.085179328918457\n",
      "Val Loss for batch is  -2.3115897178649902\n",
      "Val Loss for batch is  -2.148270845413208\n",
      "Val Loss for batch is  -3.2285826206207275\n",
      "|Iter  1234  | Total Val Loss  -9.773622512817383 |\n",
      "Loss for batch is  -0.835817813873291\n",
      "Loss for batch is  -0.9843791723251343\n",
      "Loss for batch is  -0.878132700920105\n",
      "Loss for batch is  -1.7909845113754272\n",
      "|Iter  1235  | Total Train Loss  -4.4893141984939575 |\n",
      "Val Loss for batch is  -2.0874722003936768\n",
      "Val Loss for batch is  -2.2788052558898926\n",
      "Val Loss for batch is  -2.168630838394165\n",
      "Val Loss for batch is  -3.23660945892334\n",
      "|Iter  1235  | Total Val Loss  -9.771517753601074 |\n",
      "Loss for batch is  -0.8400278091430664\n",
      "Loss for batch is  -0.9957125186920166\n",
      "Loss for batch is  -0.8784371614456177\n",
      "Loss for batch is  -1.8120239973068237\n",
      "|Iter  1236  | Total Train Loss  -4.526201486587524 |\n",
      "Val Loss for batch is  -2.0631179809570312\n",
      "Val Loss for batch is  -2.3511884212493896\n",
      "Val Loss for batch is  -2.2112057209014893\n",
      "Val Loss for batch is  -3.248685836791992\n",
      "|Iter  1236  | Total Val Loss  -9.874197959899902 |\n",
      "Loss for batch is  -0.8514605760574341\n",
      "Loss for batch is  -1.0095250606536865\n",
      "Loss for batch is  -0.8735191822052002\n",
      "Loss for batch is  -1.8216038942337036\n",
      "|Iter  1237  | Total Train Loss  -4.556108713150024 |\n",
      "Val Loss for batch is  -2.0734121799468994\n",
      "Val Loss for batch is  -2.319251298904419\n",
      "Val Loss for batch is  -2.2202529907226562\n",
      "Val Loss for batch is  -3.2491884231567383\n",
      "|Iter  1237  | Total Val Loss  -9.862104892730713 |\n",
      "Loss for batch is  -0.8433573246002197\n",
      "Loss for batch is  -1.016506552696228\n",
      "Loss for batch is  -0.8840630054473877\n",
      "Loss for batch is  -1.837485671043396\n",
      "|Iter  1238  | Total Train Loss  -4.5814125537872314 |\n",
      "Val Loss for batch is  -2.0743072032928467\n",
      "Val Loss for batch is  -2.2962613105773926\n",
      "Val Loss for batch is  -2.1894729137420654\n",
      "Val Loss for batch is  -3.2377684116363525\n",
      "|Iter  1238  | Total Val Loss  -9.797809839248657 |\n",
      "Loss for batch is  -0.8208924531936646\n",
      "Loss for batch is  -1.012560248374939\n",
      "Loss for batch is  -0.8635663986206055\n",
      "Loss for batch is  -1.8491729497909546\n",
      "|Iter  1239  | Total Train Loss  -4.546192049980164 |\n",
      "Val Loss for batch is  -2.0181429386138916\n",
      "Val Loss for batch is  -2.1903581619262695\n",
      "Val Loss for batch is  -2.107991933822632\n",
      "Val Loss for batch is  -3.2721683979034424\n",
      "|Iter  1239  | Total Val Loss  -9.588661432266235 |\n",
      "Loss for batch is  -0.7954151630401611\n",
      "Loss for batch is  -1.0138945579528809\n",
      "Loss for batch is  -0.8129371404647827\n",
      "Loss for batch is  -1.8474026918411255\n",
      "|Iter  1240  | Total Train Loss  -4.46964955329895 |\n",
      "Val Loss for batch is  -1.9711583852767944\n",
      "Val Loss for batch is  -2.101254463195801\n",
      "Val Loss for batch is  -1.9387255907058716\n",
      "Val Loss for batch is  -3.2369072437286377\n",
      "|Iter  1240  | Total Val Loss  -9.248045682907104 |\n",
      "Loss for batch is  -0.7404627799987793\n",
      "Loss for batch is  -1.0048811435699463\n",
      "Loss for batch is  -0.8239352703094482\n",
      "Loss for batch is  -1.7820566892623901\n",
      "|Iter  1241  | Total Train Loss  -4.351335883140564 |\n",
      "Val Loss for batch is  -2.0422134399414062\n",
      "Val Loss for batch is  -2.2533516883850098\n",
      "Val Loss for batch is  -2.1021759510040283\n",
      "Val Loss for batch is  -3.2717673778533936\n",
      "|Iter  1241  | Total Val Loss  -9.669508457183838 |\n",
      "Loss for batch is  -0.8078678846359253\n",
      "Loss for batch is  -0.8831336498260498\n",
      "Loss for batch is  -0.7416377067565918\n",
      "Loss for batch is  -1.6928858757019043\n",
      "|Iter  1242  | Total Train Loss  -4.125525116920471 |\n",
      "Val Loss for batch is  -1.3674349784851074\n",
      "Val Loss for batch is  -1.6018692255020142\n",
      "Val Loss for batch is  -1.7709063291549683\n",
      "Val Loss for batch is  -3.078826904296875\n",
      "|Iter  1242  | Total Val Loss  -7.819037437438965 |\n",
      "Loss for batch is  -0.1276923418045044\n",
      "Loss for batch is  -0.9239914417266846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.07028794288635254\n",
      "Loss for batch is  -1.393083095550537\n",
      "|Iter  1243  | Total Train Loss  -2.3744789361953735 |\n",
      "Val Loss for batch is  -1.8392863273620605\n",
      "Val Loss for batch is  -2.0443856716156006\n",
      "Val Loss for batch is  -1.9456018209457397\n",
      "Val Loss for batch is  -3.140477418899536\n",
      "|Iter  1243  | Total Val Loss  -8.969751238822937 |\n",
      "Loss for batch is  -0.6453319787979126\n",
      "Loss for batch is  -0.3150101900100708\n",
      "Loss for batch is  -0.5823495388031006\n",
      "Loss for batch is  -1.6938396692276\n",
      "|Iter  1244  | Total Train Loss  -3.236531376838684 |\n",
      "Val Loss for batch is  -1.901296854019165\n",
      "Val Loss for batch is  -2.218196153640747\n",
      "Val Loss for batch is  -1.9462659358978271\n",
      "Val Loss for batch is  -3.1164162158966064\n",
      "|Iter  1244  | Total Val Loss  -9.182175159454346 |\n",
      "Loss for batch is  -0.6880464553833008\n",
      "Loss for batch is  -0.8306652307510376\n",
      "Loss for batch is  -0.6288461685180664\n",
      "Loss for batch is  -1.5941890478134155\n",
      "|Iter  1245  | Total Train Loss  -3.7417469024658203 |\n",
      "Val Loss for batch is  -2.0030553340911865\n",
      "Val Loss for batch is  -2.179891347885132\n",
      "Val Loss for batch is  -1.9873247146606445\n",
      "Val Loss for batch is  -3.085203170776367\n",
      "|Iter  1245  | Total Val Loss  -9.25547456741333 |\n",
      "Loss for batch is  -0.7149949073791504\n",
      "Loss for batch is  -0.9109518527984619\n",
      "Loss for batch is  -0.7360007762908936\n",
      "Loss for batch is  -1.53914213180542\n",
      "|Iter  1246  | Total Train Loss  -3.901089668273926 |\n",
      "Val Loss for batch is  -1.9733408689498901\n",
      "Val Loss for batch is  -2.151602268218994\n",
      "Val Loss for batch is  -2.012206554412842\n",
      "Val Loss for batch is  -3.073004722595215\n",
      "|Iter  1246  | Total Val Loss  -9.210154414176941 |\n",
      "Loss for batch is  -0.7128034830093384\n",
      "Loss for batch is  -0.87983238697052\n",
      "Loss for batch is  -0.778282642364502\n",
      "Loss for batch is  -1.6728956699371338\n",
      "|Iter  1247  | Total Train Loss  -4.043814182281494 |\n",
      "Val Loss for batch is  -2.0308022499084473\n",
      "Val Loss for batch is  -2.237062931060791\n",
      "Val Loss for batch is  -2.080167531967163\n",
      "Val Loss for batch is  -3.1349406242370605\n",
      "|Iter  1247  | Total Val Loss  -9.482973337173462 |\n",
      "Loss for batch is  -0.7469611167907715\n",
      "Loss for batch is  -0.9088932275772095\n",
      "Loss for batch is  -0.7835628986358643\n",
      "Loss for batch is  -1.6836718320846558\n",
      "|Iter  1248  | Total Train Loss  -4.123089075088501 |\n",
      "Val Loss for batch is  -2.0873565673828125\n",
      "Val Loss for batch is  -2.237244129180908\n",
      "Val Loss for batch is  -2.0957911014556885\n",
      "Val Loss for batch is  -3.164653778076172\n",
      "|Iter  1248  | Total Val Loss  -9.585045576095581 |\n",
      "Loss for batch is  -0.7994611263275146\n",
      "Loss for batch is  -0.9242737293243408\n",
      "Loss for batch is  -0.8295257091522217\n",
      "Loss for batch is  -1.7569512128829956\n",
      "|Iter  1249  | Total Train Loss  -4.310211777687073 |\n",
      "Val Loss for batch is  -2.0826919078826904\n",
      "Val Loss for batch is  -2.249176502227783\n",
      "Val Loss for batch is  -2.1304311752319336\n",
      "Val Loss for batch is  -3.19608211517334\n",
      "|Iter  1249  | Total Val Loss  -9.658381700515747 |\n",
      "Loss for batch is  -0.8045735359191895\n",
      "Loss for batch is  -0.9403265714645386\n",
      "Loss for batch is  -0.843576192855835\n",
      "Loss for batch is  -1.7974520921707153\n",
      "|Iter  1250  | Total Train Loss  -4.385928392410278 |\n",
      "Val Loss for batch is  -2.0864973068237305\n",
      "Val Loss for batch is  -2.2636795043945312\n",
      "Val Loss for batch is  -2.1607987880706787\n",
      "Val Loss for batch is  -3.2544312477111816\n",
      "|Iter  1250  | Total Val Loss  -9.765406847000122 |\n",
      "Loss for batch is  -0.8502160310745239\n",
      "Loss for batch is  -0.9583706855773926\n",
      "Loss for batch is  -0.878520131111145\n",
      "Loss for batch is  -1.8337528705596924\n",
      "|Iter  1251  | Total Train Loss  -4.520859718322754 |\n",
      "Val Loss for batch is  -2.097240686416626\n",
      "Val Loss for batch is  -2.2907354831695557\n",
      "Val Loss for batch is  -2.1843740940093994\n",
      "Val Loss for batch is  -3.26460862159729\n",
      "|Iter  1251  | Total Val Loss  -9.836958885192871 |\n",
      "Loss for batch is  -0.8427053689956665\n",
      "Loss for batch is  -0.9989887475967407\n",
      "Loss for batch is  -0.892325758934021\n",
      "Loss for batch is  -1.855438232421875\n",
      "|Iter  1252  | Total Train Loss  -4.589458107948303 |\n",
      "Val Loss for batch is  -2.08156681060791\n",
      "Val Loss for batch is  -2.3005287647247314\n",
      "Val Loss for batch is  -2.1918013095855713\n",
      "Val Loss for batch is  -3.305083751678467\n",
      "|Iter  1252  | Total Val Loss  -9.87898063659668 |\n",
      "Loss for batch is  -0.8820638656616211\n",
      "Loss for batch is  -1.0244358777999878\n",
      "Loss for batch is  -0.9208980798721313\n",
      "Loss for batch is  -1.8677515983581543\n",
      "|Iter  1253  | Total Train Loss  -4.6951494216918945 |\n",
      "Val Loss for batch is  -2.1287834644317627\n",
      "Val Loss for batch is  -2.327662944793701\n",
      "Val Loss for batch is  -2.2476789951324463\n",
      "Val Loss for batch is  -3.326840877532959\n",
      "|Iter  1253  | Total Val Loss  -10.03096628189087 |\n",
      "Loss for batch is  -0.872733473777771\n",
      "Loss for batch is  -1.0311614274978638\n",
      "Loss for batch is  -0.9213615655899048\n",
      "Loss for batch is  -1.902666449546814\n",
      "|Iter  1254  | Total Train Loss  -4.7279229164123535 |\n",
      "Val Loss for batch is  -2.0884907245635986\n",
      "Val Loss for batch is  -2.3040337562561035\n",
      "Val Loss for batch is  -2.217435598373413\n",
      "Val Loss for batch is  -3.3546299934387207\n",
      "|Iter  1254  | Total Val Loss  -9.964590072631836 |\n",
      "Loss for batch is  -0.8745259046554565\n",
      "Loss for batch is  -1.0403587818145752\n",
      "Loss for batch is  -0.9339393377304077\n",
      "Loss for batch is  -1.9186289310455322\n",
      "|Iter  1255  | Total Train Loss  -4.767452955245972 |\n",
      "Val Loss for batch is  -2.1276891231536865\n",
      "Val Loss for batch is  -2.366189479827881\n",
      "Val Loss for batch is  -2.2695236206054688\n",
      "Val Loss for batch is  -3.3543903827667236\n",
      "|Iter  1255  | Total Val Loss  -10.11779260635376 |\n",
      "Loss for batch is  -0.9167289733886719\n",
      "Loss for batch is  -1.0482423305511475\n",
      "Loss for batch is  -0.9538941383361816\n",
      "Loss for batch is  -1.9237265586853027\n",
      "|Iter  1256  | Total Train Loss  -4.842592000961304 |\n",
      "Val Loss for batch is  -2.1885530948638916\n",
      "Val Loss for batch is  -2.3828792572021484\n",
      "Val Loss for batch is  -2.241961717605591\n",
      "Val Loss for batch is  -3.3625526428222656\n",
      "|Iter  1256  | Total Val Loss  -10.175946712493896 |\n",
      "Loss for batch is  -0.9305264949798584\n",
      "Loss for batch is  -1.057594895362854\n",
      "Loss for batch is  -0.947250247001648\n",
      "Loss for batch is  -1.9406201839447021\n",
      "|Iter  1257  | Total Train Loss  -4.8759918212890625 |\n",
      "Val Loss for batch is  -2.006986379623413\n",
      "Val Loss for batch is  -2.3179476261138916\n",
      "Val Loss for batch is  -2.1881775856018066\n",
      "Val Loss for batch is  -3.369053840637207\n",
      "|Iter  1257  | Total Val Loss  -9.882165431976318 |\n",
      "Loss for batch is  -0.8494130373001099\n",
      "Loss for batch is  -1.042924404144287\n",
      "Loss for batch is  -0.8424137830734253\n",
      "Loss for batch is  -1.9229100942611694\n",
      "|Iter  1258  | Total Train Loss  -4.657661318778992 |\n",
      "Val Loss for batch is  -1.8080633878707886\n",
      "Val Loss for batch is  -2.0253567695617676\n",
      "Val Loss for batch is  -1.8807576894760132\n",
      "Val Loss for batch is  -3.2578747272491455\n",
      "|Iter  1258  | Total Val Loss  -8.972052574157715 |\n",
      "Loss for batch is  -0.614997386932373\n",
      "Loss for batch is  -1.031602382659912\n",
      "Loss for batch is  -0.8420594930648804\n",
      "Loss for batch is  -1.7119520902633667\n",
      "|Iter  1259  | Total Train Loss  -4.200611352920532 |\n",
      "Val Loss for batch is  -2.0510525703430176\n",
      "Val Loss for batch is  -2.3048434257507324\n",
      "Val Loss for batch is  -2.0667495727539062\n",
      "Val Loss for batch is  -3.329376459121704\n",
      "|Iter  1259  | Total Val Loss  -9.75202202796936 |\n",
      "Loss for batch is  -0.8597700595855713\n",
      "Loss for batch is  -0.8131253719329834\n",
      "Loss for batch is  -0.7619308233261108\n",
      "Loss for batch is  -1.8779520988464355\n",
      "|Iter  1260  | Total Train Loss  -4.312778353691101 |\n",
      "Val Loss for batch is  -0.6957800984382629\n",
      "Val Loss for batch is  -0.4548081159591675\n",
      "Val Loss for batch is  -1.254276990890503\n",
      "Val Loss for batch is  -3.123716115951538\n",
      "|Iter  1260  | Total Val Loss  -5.528581321239471 |\n",
      "Loss for batch is  0.9097241759300232\n",
      "Loss for batch is  -0.6196595430374146\n",
      "Loss for batch is  0.683699905872345\n",
      "Loss for batch is  -0.6547431945800781\n",
      "|Iter  1261  | Total Train Loss  0.3190213441848755 |\n",
      "Val Loss for batch is  -0.617661714553833\n",
      "Val Loss for batch is  -1.497165322303772\n",
      "Val Loss for batch is  -1.1127312183380127\n",
      "Val Loss for batch is  -2.689671277999878\n",
      "|Iter  1261  | Total Val Loss  -5.917229533195496 |\n",
      "Loss for batch is  0.7530761361122131\n",
      "Loss for batch is  -0.06379413604736328\n",
      "Loss for batch is  0.6364777088165283\n",
      "Loss for batch is  0.3365546464920044\n",
      "|Iter  1262  | Total Train Loss  1.6623143553733826 |\n",
      "Val Loss for batch is  -0.5271363854408264\n",
      "Val Loss for batch is  -1.2881712913513184\n",
      "Val Loss for batch is  -0.7344834804534912\n",
      "Val Loss for batch is  -2.15868878364563\n",
      "|Iter  1262  | Total Val Loss  -4.708479940891266 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  0.4664697051048279\n",
      "Loss for batch is  -0.21654164791107178\n",
      "Loss for batch is  0.025242924690246582\n",
      "Loss for batch is  -1.0298744440078735\n",
      "|Iter  1263  | Total Train Loss  -0.7547034621238708 |\n",
      "Val Loss for batch is  -1.0255120992660522\n",
      "Val Loss for batch is  -1.4822821617126465\n",
      "Val Loss for batch is  -1.3172105550765991\n",
      "Val Loss for batch is  -2.329500675201416\n",
      "|Iter  1263  | Total Val Loss  -6.154505491256714 |\n",
      "Loss for batch is  0.29136407375335693\n",
      "Loss for batch is  -0.14694762229919434\n",
      "Loss for batch is  -0.14492666721343994\n",
      "Loss for batch is  -1.0096485614776611\n",
      "|Iter  1264  | Total Train Loss  -1.0101587772369385 |\n",
      "Val Loss for batch is  -1.541205644607544\n",
      "Val Loss for batch is  -1.8543198108673096\n",
      "Val Loss for batch is  -1.499055027961731\n",
      "Val Loss for batch is  -2.4999427795410156\n",
      "|Iter  1264  | Total Val Loss  -7.3945232629776 |\n",
      "Loss for batch is  -0.028830528259277344\n",
      "Loss for batch is  -0.35809409618377686\n",
      "Loss for batch is  -0.14470994472503662\n",
      "Loss for batch is  -1.0364059209823608\n",
      "|Iter  1265  | Total Train Loss  -1.5680404901504517 |\n",
      "Val Loss for batch is  -1.6270979642868042\n",
      "Val Loss for batch is  -1.7288769483566284\n",
      "Val Loss for batch is  -1.5211049318313599\n",
      "Val Loss for batch is  -2.4737160205841064\n",
      "|Iter  1265  | Total Val Loss  -7.350795865058899 |\n",
      "Loss for batch is  -0.24725282192230225\n",
      "Loss for batch is  -0.4488823413848877\n",
      "Loss for batch is  -0.2536146640777588\n",
      "Loss for batch is  -1.083824872970581\n",
      "|Iter  1266  | Total Train Loss  -2.03357470035553 |\n",
      "Val Loss for batch is  -1.7766971588134766\n",
      "Val Loss for batch is  -1.9247965812683105\n",
      "Val Loss for batch is  -1.7413686513900757\n",
      "Val Loss for batch is  -2.5541110038757324\n",
      "|Iter  1266  | Total Val Loss  -7.996973395347595 |\n",
      "Loss for batch is  -0.2941775321960449\n",
      "Loss for batch is  -0.5210961103439331\n",
      "Loss for batch is  -0.3752725124359131\n",
      "Loss for batch is  -1.1056928634643555\n",
      "|Iter  1267  | Total Train Loss  -2.2962390184402466 |\n",
      "Val Loss for batch is  -1.7929961681365967\n",
      "Val Loss for batch is  -1.9473576545715332\n",
      "Val Loss for batch is  -1.816603422164917\n",
      "Val Loss for batch is  -2.557730197906494\n",
      "|Iter  1267  | Total Val Loss  -8.114687442779541 |\n",
      "Loss for batch is  -0.34128451347351074\n",
      "Loss for batch is  -0.5411425828933716\n",
      "Loss for batch is  -0.4179723262786865\n",
      "Loss for batch is  -1.1229732036590576\n",
      "|Iter  1268  | Total Train Loss  -2.4233726263046265 |\n",
      "Val Loss for batch is  -1.8159486055374146\n",
      "Val Loss for batch is  -1.925220012664795\n",
      "Val Loss for batch is  -1.8302010297775269\n",
      "Val Loss for batch is  -2.580533742904663\n",
      "|Iter  1268  | Total Val Loss  -8.1519033908844 |\n",
      "Loss for batch is  -0.3876609802246094\n",
      "Loss for batch is  -0.5879566669464111\n",
      "Loss for batch is  -0.4694420099258423\n",
      "Loss for batch is  -1.1743361949920654\n",
      "|Iter  1269  | Total Train Loss  -2.6193958520889282 |\n",
      "Val Loss for batch is  -1.8614932298660278\n",
      "Val Loss for batch is  -2.0176217555999756\n",
      "Val Loss for batch is  -1.8759430646896362\n",
      "Val Loss for batch is  -2.6417553424835205\n",
      "|Iter  1269  | Total Val Loss  -8.39681339263916 |\n",
      "Loss for batch is  -0.46682560443878174\n",
      "Loss for batch is  -0.6338860988616943\n",
      "Loss for batch is  -0.4958716630935669\n",
      "Loss for batch is  -1.2244027853012085\n",
      "|Iter  1270  | Total Train Loss  -2.8209861516952515 |\n",
      "Val Loss for batch is  -1.8790098428726196\n",
      "Val Loss for batch is  -2.018735885620117\n",
      "Val Loss for batch is  -1.8857007026672363\n",
      "Val Loss for batch is  -2.6671111583709717\n",
      "|Iter  1270  | Total Val Loss  -8.450557589530945 |\n",
      "Loss for batch is  -0.5026276111602783\n",
      "Loss for batch is  -0.682640552520752\n",
      "Loss for batch is  -0.5437644720077515\n",
      "Loss for batch is  -1.2622674703598022\n",
      "|Iter  1271  | Total Train Loss  -2.991300106048584 |\n",
      "Val Loss for batch is  -1.9000012874603271\n",
      "Val Loss for batch is  -2.0368003845214844\n",
      "Val Loss for batch is  -1.9239059686660767\n",
      "Val Loss for batch is  -2.718914031982422\n",
      "|Iter  1271  | Total Val Loss  -8.57962167263031 |\n",
      "Loss for batch is  -0.5355021953582764\n",
      "Loss for batch is  -0.7090815305709839\n",
      "Loss for batch is  -0.5805082321166992\n",
      "Loss for batch is  -1.3123027086257935\n",
      "|Iter  1272  | Total Train Loss  -3.137394666671753 |\n",
      "Val Loss for batch is  -1.8852174282073975\n",
      "Val Loss for batch is  -2.075348138809204\n",
      "Val Loss for batch is  -1.9299027919769287\n",
      "Val Loss for batch is  -2.767876386642456\n",
      "|Iter  1272  | Total Val Loss  -8.658344745635986 |\n",
      "Loss for batch is  -0.5671893358230591\n",
      "Loss for batch is  -0.7528517246246338\n",
      "Loss for batch is  -0.6175518035888672\n",
      "Loss for batch is  -1.3462368249893188\n",
      "|Iter  1273  | Total Train Loss  -3.283829689025879 |\n",
      "Val Loss for batch is  -1.919884443283081\n",
      "Val Loss for batch is  -2.111612319946289\n",
      "Val Loss for batch is  -1.9559547901153564\n",
      "Val Loss for batch is  -2.8253090381622314\n",
      "|Iter  1273  | Total Val Loss  -8.812760591506958 |\n",
      "Loss for batch is  -0.6056246757507324\n",
      "Loss for batch is  -0.7863813638687134\n",
      "Loss for batch is  -0.6274871826171875\n",
      "Loss for batch is  -1.3849225044250488\n",
      "|Iter  1274  | Total Train Loss  -3.404415726661682 |\n",
      "Val Loss for batch is  -1.9129972457885742\n",
      "Val Loss for batch is  -2.101811647415161\n",
      "Val Loss for batch is  -1.9587655067443848\n",
      "Val Loss for batch is  -2.856506824493408\n",
      "|Iter  1274  | Total Val Loss  -8.830081224441528 |\n",
      "Loss for batch is  -0.6318306922912598\n",
      "Loss for batch is  -0.8211226463317871\n",
      "Loss for batch is  -0.6674776077270508\n",
      "Loss for batch is  -1.434705376625061\n",
      "|Iter  1275  | Total Train Loss  -3.5551363229751587 |\n",
      "Val Loss for batch is  -1.9257442951202393\n",
      "Val Loss for batch is  -2.1286001205444336\n",
      "Val Loss for batch is  -1.9396086931228638\n",
      "Val Loss for batch is  -2.8882579803466797\n",
      "|Iter  1275  | Total Val Loss  -8.882211089134216 |\n",
      "Loss for batch is  -0.6418191194534302\n",
      "Loss for batch is  -0.8462328910827637\n",
      "Loss for batch is  -0.6984906196594238\n",
      "Loss for batch is  -1.4740207195281982\n",
      "|Iter  1276  | Total Train Loss  -3.660563349723816 |\n",
      "Val Loss for batch is  -1.9272372722625732\n",
      "Val Loss for batch is  -2.16279935836792\n",
      "Val Loss for batch is  -1.9899410009384155\n",
      "Val Loss for batch is  -2.9057679176330566\n",
      "|Iter  1276  | Total Val Loss  -8.985745549201965 |\n",
      "Loss for batch is  -0.6770830154418945\n",
      "Loss for batch is  -0.8739408254623413\n",
      "Loss for batch is  -0.7077640295028687\n",
      "Loss for batch is  -1.5004509687423706\n",
      "|Iter  1277  | Total Train Loss  -3.759238839149475 |\n",
      "Val Loss for batch is  -1.9121123552322388\n",
      "Val Loss for batch is  -2.1333138942718506\n",
      "Val Loss for batch is  -1.9895542860031128\n",
      "Val Loss for batch is  -2.94832181930542\n",
      "|Iter  1277  | Total Val Loss  -8.983302354812622 |\n",
      "Loss for batch is  -0.6998285055160522\n",
      "Loss for batch is  -0.8887609243392944\n",
      "Loss for batch is  -0.7424031496047974\n",
      "Loss for batch is  -1.5359628200531006\n",
      "|Iter  1278  | Total Train Loss  -3.8669553995132446 |\n",
      "Val Loss for batch is  -1.9256739616394043\n",
      "Val Loss for batch is  -2.1752142906188965\n",
      "Val Loss for batch is  -1.9914665222167969\n",
      "Val Loss for batch is  -2.9901952743530273\n",
      "|Iter  1278  | Total Val Loss  -9.082550048828125 |\n",
      "Loss for batch is  -0.7189019918441772\n",
      "Loss for batch is  -0.9116473197937012\n",
      "Loss for batch is  -0.7531415224075317\n",
      "Loss for batch is  -1.5656896829605103\n",
      "|Iter  1279  | Total Train Loss  -3.9493805170059204 |\n",
      "Val Loss for batch is  -1.9375193119049072\n",
      "Val Loss for batch is  -2.2247371673583984\n",
      "Val Loss for batch is  -2.04612398147583\n",
      "Val Loss for batch is  -3.0211668014526367\n",
      "|Iter  1279  | Total Val Loss  -9.229547262191772 |\n",
      "Loss for batch is  -0.7368913888931274\n",
      "Loss for batch is  -0.9278895854949951\n",
      "Loss for batch is  -0.7764272689819336\n",
      "Loss for batch is  -1.59079110622406\n",
      "|Iter  1280  | Total Train Loss  -4.031999349594116 |\n",
      "Val Loss for batch is  -1.9619166851043701\n",
      "Val Loss for batch is  -2.225648880004883\n",
      "Val Loss for batch is  -2.0431153774261475\n",
      "Val Loss for batch is  -3.045895576477051\n",
      "|Iter  1280  | Total Val Loss  -9.276576519012451 |\n",
      "Loss for batch is  -0.7414842844009399\n",
      "Loss for batch is  -0.9483737945556641\n",
      "Loss for batch is  -0.791890025138855\n",
      "Loss for batch is  -1.6211003065109253\n",
      "|Iter  1281  | Total Train Loss  -4.102848410606384 |\n",
      "Val Loss for batch is  -1.9900097846984863\n",
      "Val Loss for batch is  -2.2383954524993896\n",
      "Val Loss for batch is  -2.0709450244903564\n",
      "Val Loss for batch is  -3.0797696113586426\n",
      "|Iter  1281  | Total Val Loss  -9.379119873046875 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.7741001844406128\n",
      "Loss for batch is  -0.9707823991775513\n",
      "Loss for batch is  -0.812631368637085\n",
      "Loss for batch is  -1.6480529308319092\n",
      "|Iter  1282  | Total Train Loss  -4.205566883087158 |\n",
      "Val Loss for batch is  -1.9933241605758667\n",
      "Val Loss for batch is  -2.2610974311828613\n",
      "Val Loss for batch is  -2.062636137008667\n",
      "Val Loss for batch is  -3.0407817363739014\n",
      "|Iter  1282  | Total Val Loss  -9.357839465141296 |\n",
      "Loss for batch is  -0.792438268661499\n",
      "Loss for batch is  -0.9744493961334229\n",
      "Loss for batch is  -0.8443021774291992\n",
      "Loss for batch is  -1.6703966856002808\n",
      "|Iter  1283  | Total Train Loss  -4.281586527824402 |\n",
      "Val Loss for batch is  -2.0576798915863037\n",
      "Val Loss for batch is  -2.2802977561950684\n",
      "Val Loss for batch is  -2.122530460357666\n",
      "Val Loss for batch is  -3.103231191635132\n",
      "|Iter  1283  | Total Val Loss  -9.56373929977417 |\n",
      "Loss for batch is  -0.8165117502212524\n",
      "Loss for batch is  -0.9941964149475098\n",
      "Loss for batch is  -0.8567174673080444\n",
      "Loss for batch is  -1.6913560628890991\n",
      "|Iter  1284  | Total Train Loss  -4.358781695365906 |\n",
      "Val Loss for batch is  -2.0472681522369385\n",
      "Val Loss for batch is  -2.258634567260742\n",
      "Val Loss for batch is  -2.1498658657073975\n",
      "Val Loss for batch is  -3.1509361267089844\n",
      "|Iter  1284  | Total Val Loss  -9.606704711914062 |\n",
      "Loss for batch is  -0.8270819187164307\n",
      "Loss for batch is  -1.0162732601165771\n",
      "Loss for batch is  -0.874667763710022\n",
      "Loss for batch is  -1.7142629623413086\n",
      "|Iter  1285  | Total Train Loss  -4.432285904884338 |\n",
      "Val Loss for batch is  -2.0831844806671143\n",
      "Val Loss for batch is  -2.2676665782928467\n",
      "Val Loss for batch is  -2.125837564468384\n",
      "Val Loss for batch is  -3.1540870666503906\n",
      "|Iter  1285  | Total Val Loss  -9.630775690078735 |\n",
      "Loss for batch is  -0.8500349521636963\n",
      "Loss for batch is  -1.02311110496521\n",
      "Loss for batch is  -0.8909794092178345\n",
      "Loss for batch is  -1.7380887269973755\n",
      "|Iter  1286  | Total Train Loss  -4.502214193344116 |\n",
      "Val Loss for batch is  -2.0206565856933594\n",
      "Val Loss for batch is  -2.3051183223724365\n",
      "Val Loss for batch is  -2.1198017597198486\n",
      "Val Loss for batch is  -3.1518001556396484\n",
      "|Iter  1286  | Total Val Loss  -9.597376823425293 |\n",
      "Loss for batch is  -0.86902916431427\n",
      "Loss for batch is  -1.0404462814331055\n",
      "Loss for batch is  -0.8865104913711548\n",
      "Loss for batch is  -1.7572827339172363\n",
      "|Iter  1287  | Total Train Loss  -4.553268671035767 |\n",
      "Val Loss for batch is  -2.091071367263794\n",
      "Val Loss for batch is  -2.2630062103271484\n",
      "Val Loss for batch is  -2.1875791549682617\n",
      "Val Loss for batch is  -3.213109016418457\n",
      "|Iter  1287  | Total Val Loss  -9.754765748977661 |\n",
      "Loss for batch is  -0.8708357810974121\n",
      "Loss for batch is  -1.0478225946426392\n",
      "Loss for batch is  -0.9133889675140381\n",
      "Loss for batch is  -1.7802152633666992\n",
      "|Iter  1288  | Total Train Loss  -4.612262606620789 |\n",
      "Val Loss for batch is  -2.040637731552124\n",
      "Val Loss for batch is  -2.3127493858337402\n",
      "Val Loss for batch is  -2.1455798149108887\n",
      "Val Loss for batch is  -3.199230909347534\n",
      "|Iter  1288  | Total Val Loss  -9.698197841644287 |\n",
      "Loss for batch is  -0.8901333808898926\n",
      "Loss for batch is  -1.0473287105560303\n",
      "Loss for batch is  -0.9209616184234619\n",
      "Loss for batch is  -1.7898062467575073\n",
      "|Iter  1289  | Total Train Loss  -4.648229956626892 |\n",
      "Val Loss for batch is  -2.110337495803833\n",
      "Val Loss for batch is  -2.2989916801452637\n",
      "Val Loss for batch is  -2.124943733215332\n",
      "Val Loss for batch is  -3.227634906768799\n",
      "|Iter  1289  | Total Val Loss  -9.761907815933228 |\n",
      "Loss for batch is  -0.8683360815048218\n",
      "Loss for batch is  -1.0465377569198608\n",
      "Loss for batch is  -0.9093087911605835\n",
      "Loss for batch is  -1.8014676570892334\n",
      "|Iter  1290  | Total Train Loss  -4.6256502866744995 |\n",
      "Val Loss for batch is  -2.0311830043792725\n",
      "Val Loss for batch is  -2.2993643283843994\n",
      "Val Loss for batch is  -2.120532274246216\n",
      "Val Loss for batch is  -3.2007250785827637\n",
      "|Iter  1290  | Total Val Loss  -9.651804685592651 |\n",
      "Loss for batch is  -0.850867748260498\n",
      "Loss for batch is  -1.052272915840149\n",
      "Loss for batch is  -0.8686715364456177\n",
      "Loss for batch is  -1.8262760639190674\n",
      "|Iter  1291  | Total Train Loss  -4.598088264465332 |\n",
      "Val Loss for batch is  -2.057537794113159\n",
      "Val Loss for batch is  -2.2383224964141846\n",
      "Val Loss for batch is  -2.1220767498016357\n",
      "Val Loss for batch is  -3.2389090061187744\n",
      "|Iter  1291  | Total Val Loss  -9.656846046447754 |\n",
      "Loss for batch is  -0.8562227487564087\n",
      "Loss for batch is  -1.050290584564209\n",
      "Loss for batch is  -0.8984178304672241\n",
      "Loss for batch is  -1.806146264076233\n",
      "|Iter  1292  | Total Train Loss  -4.611077427864075 |\n",
      "Val Loss for batch is  -2.062455654144287\n",
      "Val Loss for batch is  -2.3279740810394287\n",
      "Val Loss for batch is  -2.0601069927215576\n",
      "Val Loss for batch is  -3.2608642578125\n",
      "|Iter  1292  | Total Val Loss  -9.711400985717773 |\n",
      "Loss for batch is  -0.8872877359390259\n",
      "Loss for batch is  -1.0010024309158325\n",
      "Loss for batch is  -0.6105462312698364\n",
      "Loss for batch is  -1.549619197845459\n",
      "|Iter  1293  | Total Train Loss  -4.048455595970154 |\n",
      "Val Loss for batch is  -0.6407289505004883\n",
      "Val Loss for batch is  -1.1089096069335938\n",
      "Val Loss for batch is  -0.6567686200141907\n",
      "Val Loss for batch is  -2.73569655418396\n",
      "|Iter  1293  | Total Val Loss  -5.142103731632233 |\n",
      "Loss for batch is  0.41663676500320435\n",
      "Loss for batch is  -0.7726820707321167\n",
      "Loss for batch is  0.7764958143234253\n",
      "Loss for batch is  -1.2009210586547852\n",
      "|Iter  1294  | Total Train Loss  -0.7804705500602722 |\n",
      "Val Loss for batch is  -1.2374351024627686\n",
      "Val Loss for batch is  -1.8041539192199707\n",
      "Val Loss for batch is  -1.5259790420532227\n",
      "Val Loss for batch is  -2.8353805541992188\n",
      "|Iter  1294  | Total Val Loss  -7.402948617935181 |\n",
      "Loss for batch is  -0.368147611618042\n",
      "Loss for batch is  -0.30694663524627686\n",
      "Loss for batch is  -0.27103543281555176\n",
      "Loss for batch is  -1.447000503540039\n",
      "|Iter  1295  | Total Train Loss  -2.3931301832199097 |\n",
      "Val Loss for batch is  -1.513853907585144\n",
      "Val Loss for batch is  -1.8841136693954468\n",
      "Val Loss for batch is  -1.61298406124115\n",
      "Val Loss for batch is  -2.867490768432617\n",
      "|Iter  1295  | Total Val Loss  -7.878442406654358 |\n",
      "Loss for batch is  -0.3448244333267212\n",
      "Loss for batch is  -0.6176420450210571\n",
      "Loss for batch is  -0.38736093044281006\n",
      "Loss for batch is  -1.4756481647491455\n",
      "|Iter  1296  | Total Train Loss  -2.825475573539734 |\n",
      "Val Loss for batch is  -1.6844618320465088\n",
      "Val Loss for batch is  -2.0583884716033936\n",
      "Val Loss for batch is  -1.733410120010376\n",
      "Val Loss for batch is  -2.7877941131591797\n",
      "|Iter  1296  | Total Val Loss  -8.264054536819458 |\n",
      "Loss for batch is  -0.521308183670044\n",
      "Loss for batch is  -0.7218170166015625\n",
      "Loss for batch is  -0.4693807363510132\n",
      "Loss for batch is  -1.480345606803894\n",
      "|Iter  1297  | Total Train Loss  -3.1928515434265137 |\n",
      "Val Loss for batch is  -1.9020005464553833\n",
      "Val Loss for batch is  -2.0593605041503906\n",
      "Val Loss for batch is  -1.8459588289260864\n",
      "Val Loss for batch is  -2.8808579444885254\n",
      "|Iter  1297  | Total Val Loss  -8.688177824020386 |\n",
      "Loss for batch is  -0.6098765134811401\n",
      "Loss for batch is  -0.7639515399932861\n",
      "Loss for batch is  -0.590261697769165\n",
      "Loss for batch is  -1.5122181177139282\n",
      "|Iter  1298  | Total Train Loss  -3.4763078689575195 |\n",
      "Val Loss for batch is  -1.9394164085388184\n",
      "Val Loss for batch is  -2.1645925045013428\n",
      "Val Loss for batch is  -1.9750564098358154\n",
      "Val Loss for batch is  -2.9269988536834717\n",
      "|Iter  1298  | Total Val Loss  -9.006064176559448 |\n",
      "Loss for batch is  -0.6569615602493286\n",
      "Loss for batch is  -0.8039319515228271\n",
      "Loss for batch is  -0.634498119354248\n",
      "Loss for batch is  -1.5006006956100464\n",
      "|Iter  1299  | Total Train Loss  -3.59599232673645 |\n",
      "Val Loss for batch is  -2.0484015941619873\n",
      "Val Loss for batch is  -2.1758289337158203\n",
      "Val Loss for batch is  -2.035968065261841\n",
      "Val Loss for batch is  -2.9822165966033936\n",
      "|Iter  1299  | Total Val Loss  -9.242415189743042 |\n",
      "Loss for batch is  -0.7037109136581421\n",
      "Loss for batch is  -0.8253506422042847\n",
      "Loss for batch is  -0.7025375366210938\n",
      "Loss for batch is  -1.5630391836166382\n",
      "|Iter  1300  | Total Train Loss  -3.7946382761001587 |\n",
      "Val Loss for batch is  -2.0470447540283203\n",
      "Val Loss for batch is  -2.1967203617095947\n",
      "Val Loss for batch is  -2.071132183074951\n",
      "Val Loss for batch is  -3.000345468521118\n",
      "|Iter  1300  | Total Val Loss  -9.315242767333984 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.7342941761016846\n",
      "Loss for batch is  -0.8508236408233643\n",
      "Loss for batch is  -0.7379214763641357\n",
      "Loss for batch is  -1.5977896451950073\n",
      "|Iter  1301  | Total Train Loss  -3.920828938484192 |\n",
      "Val Loss for batch is  -2.06540846824646\n",
      "Val Loss for batch is  -2.231022834777832\n",
      "Val Loss for batch is  -2.110823631286621\n",
      "Val Loss for batch is  -3.0406761169433594\n",
      "|Iter  1301  | Total Val Loss  -9.447931051254272 |\n",
      "Loss for batch is  -0.7818429470062256\n",
      "Loss for batch is  -0.8891028165817261\n",
      "Loss for batch is  -0.7538131475448608\n",
      "Loss for batch is  -1.629347801208496\n",
      "|Iter  1302  | Total Train Loss  -4.054106712341309 |\n",
      "Val Loss for batch is  -2.074280023574829\n",
      "Val Loss for batch is  -2.26155161857605\n",
      "Val Loss for batch is  -2.1102828979492188\n",
      "Val Loss for batch is  -3.0720937252044678\n",
      "|Iter  1302  | Total Val Loss  -9.518208265304565 |\n",
      "Loss for batch is  -0.7999223470687866\n",
      "Loss for batch is  -0.9184410572052002\n",
      "Loss for batch is  -0.7994347810745239\n",
      "Loss for batch is  -1.6874064207077026\n",
      "|Iter  1303  | Total Train Loss  -4.205204606056213 |\n",
      "Val Loss for batch is  -2.07631254196167\n",
      "Val Loss for batch is  -2.2626047134399414\n",
      "Val Loss for batch is  -2.136032819747925\n",
      "Val Loss for batch is  -3.1323344707489014\n",
      "|Iter  1303  | Total Val Loss  -9.607284545898438 |\n",
      "Loss for batch is  -0.8158504962921143\n",
      "Loss for batch is  -0.946794867515564\n",
      "Loss for batch is  -0.8254600763320923\n",
      "Loss for batch is  -1.7273592948913574\n",
      "|Iter  1304  | Total Train Loss  -4.315464735031128 |\n",
      "Val Loss for batch is  -2.089694023132324\n",
      "Val Loss for batch is  -2.3040356636047363\n",
      "Val Loss for batch is  -2.1614022254943848\n",
      "Val Loss for batch is  -3.171814441680908\n",
      "|Iter  1304  | Total Val Loss  -9.726946353912354 |\n",
      "Loss for batch is  -0.8447695970535278\n",
      "Loss for batch is  -0.9805433750152588\n",
      "Loss for batch is  -0.8595049381256104\n",
      "Loss for batch is  -1.7689999341964722\n",
      "|Iter  1305  | Total Train Loss  -4.453817844390869 |\n",
      "Val Loss for batch is  -2.0925040245056152\n",
      "Val Loss for batch is  -2.3187143802642822\n",
      "Val Loss for batch is  -2.1776387691497803\n",
      "Val Loss for batch is  -3.207773447036743\n",
      "|Iter  1305  | Total Val Loss  -9.796630620956421 |\n",
      "Loss for batch is  -0.8684418201446533\n",
      "Loss for batch is  -1.0112743377685547\n",
      "Loss for batch is  -0.8640668392181396\n",
      "Loss for batch is  -1.80055570602417\n",
      "|Iter  1306  | Total Train Loss  -4.544338703155518 |\n",
      "Val Loss for batch is  -2.054025888442993\n",
      "Val Loss for batch is  -2.306121349334717\n",
      "Val Loss for batch is  -2.1754932403564453\n",
      "Val Loss for batch is  -3.2307193279266357\n",
      "|Iter  1306  | Total Val Loss  -9.766359806060791 |\n",
      "Loss for batch is  -0.8855503797531128\n",
      "Loss for batch is  -1.0291650295257568\n",
      "Loss for batch is  -0.8898357152938843\n",
      "Loss for batch is  -1.83782160282135\n",
      "|Iter  1307  | Total Train Loss  -4.642372727394104 |\n",
      "Val Loss for batch is  -2.062403440475464\n",
      "Val Loss for batch is  -2.334362268447876\n",
      "Val Loss for batch is  -2.1943492889404297\n",
      "Val Loss for batch is  -3.2601988315582275\n",
      "|Iter  1307  | Total Val Loss  -9.851313829421997 |\n",
      "Loss for batch is  -0.8974554538726807\n",
      "Loss for batch is  -1.0421969890594482\n",
      "Loss for batch is  -0.9048326015472412\n",
      "Loss for batch is  -1.8567532300949097\n",
      "|Iter  1308  | Total Train Loss  -4.70123827457428 |\n",
      "Val Loss for batch is  -2.093282461166382\n",
      "Val Loss for batch is  -2.357128381729126\n",
      "Val Loss for batch is  -2.1882288455963135\n",
      "Val Loss for batch is  -3.2853078842163086\n",
      "|Iter  1308  | Total Val Loss  -9.92394757270813 |\n",
      "Loss for batch is  -0.9022676944732666\n",
      "Loss for batch is  -1.057019829750061\n",
      "Loss for batch is  -0.9211992025375366\n",
      "Loss for batch is  -1.8866603374481201\n",
      "|Iter  1309  | Total Train Loss  -4.767147064208984 |\n",
      "Val Loss for batch is  -2.1148104667663574\n",
      "Val Loss for batch is  -2.378340005874634\n",
      "Val Loss for batch is  -2.199793577194214\n",
      "Val Loss for batch is  -3.312192440032959\n",
      "|Iter  1309  | Total Val Loss  -10.005136489868164 |\n",
      "Loss for batch is  -0.9323447942733765\n",
      "Loss for batch is  -1.0842102766036987\n",
      "Loss for batch is  -0.9409910440444946\n",
      "Loss for batch is  -1.8969175815582275\n",
      "|Iter  1310  | Total Train Loss  -4.854463696479797 |\n",
      "Val Loss for batch is  -2.08876895904541\n",
      "Val Loss for batch is  -2.3847262859344482\n",
      "Val Loss for batch is  -2.25455904006958\n",
      "Val Loss for batch is  -3.3397297859191895\n",
      "|Iter  1310  | Total Val Loss  -10.067784070968628 |\n",
      "Loss for batch is  -0.9348492622375488\n",
      "Loss for batch is  -1.0943312644958496\n",
      "Loss for batch is  -0.9612928628921509\n",
      "Loss for batch is  -1.9152742624282837\n",
      "|Iter  1311  | Total Train Loss  -4.905747652053833 |\n",
      "Val Loss for batch is  -2.108489990234375\n",
      "Val Loss for batch is  -2.3831088542938232\n",
      "Val Loss for batch is  -2.246657609939575\n",
      "Val Loss for batch is  -3.348144292831421\n",
      "|Iter  1311  | Total Val Loss  -10.086400747299194 |\n",
      "Loss for batch is  -0.9506136178970337\n",
      "Loss for batch is  -1.110575556755066\n",
      "Loss for batch is  -0.9727787971496582\n",
      "Loss for batch is  -1.932196855545044\n",
      "|Iter  1312  | Total Train Loss  -4.966164827346802 |\n",
      "Val Loss for batch is  -2.137310266494751\n",
      "Val Loss for batch is  -2.4254655838012695\n",
      "Val Loss for batch is  -2.29046368598938\n",
      "Val Loss for batch is  -3.3458621501922607\n",
      "|Iter  1312  | Total Val Loss  -10.199101686477661 |\n",
      "Loss for batch is  -0.969723105430603\n",
      "Loss for batch is  -1.118283748626709\n",
      "Loss for batch is  -0.9856448173522949\n",
      "Loss for batch is  -1.9505242109298706\n",
      "|Iter  1313  | Total Train Loss  -5.0241758823394775 |\n",
      "Val Loss for batch is  -2.1310157775878906\n",
      "Val Loss for batch is  -2.41853666305542\n",
      "Val Loss for batch is  -2.251032590866089\n",
      "Val Loss for batch is  -3.3717241287231445\n",
      "|Iter  1313  | Total Val Loss  -10.172309160232544 |\n",
      "Loss for batch is  -0.9707366228103638\n",
      "Loss for batch is  -1.127227783203125\n",
      "Loss for batch is  -1.0050663948059082\n",
      "Loss for batch is  -1.9623583555221558\n",
      "|Iter  1314  | Total Train Loss  -5.065389156341553 |\n",
      "Val Loss for batch is  -2.1200037002563477\n",
      "Val Loss for batch is  -2.413783073425293\n",
      "Val Loss for batch is  -2.243481397628784\n",
      "Val Loss for batch is  -3.3832695484161377\n",
      "|Iter  1314  | Total Val Loss  -10.160537719726562 |\n",
      "Loss for batch is  -0.9878989458084106\n",
      "Loss for batch is  -1.1388334035873413\n",
      "Loss for batch is  -1.0111949443817139\n",
      "Loss for batch is  -1.980411171913147\n",
      "|Iter  1315  | Total Train Loss  -5.118338465690613 |\n",
      "Val Loss for batch is  -2.1701815128326416\n",
      "Val Loss for batch is  -2.423125743865967\n",
      "Val Loss for batch is  -2.286163568496704\n",
      "Val Loss for batch is  -3.3992085456848145\n",
      "|Iter  1315  | Total Val Loss  -10.278679370880127 |\n",
      "Loss for batch is  -1.0058156251907349\n",
      "Loss for batch is  -1.1298364400863647\n",
      "Loss for batch is  -1.0196402072906494\n",
      "Loss for batch is  -2.0025906562805176\n",
      "|Iter  1316  | Total Train Loss  -5.157882928848267 |\n",
      "Val Loss for batch is  -2.1381564140319824\n",
      "Val Loss for batch is  -2.4412193298339844\n",
      "Val Loss for batch is  -2.2705461978912354\n",
      "Val Loss for batch is  -3.4098823070526123\n",
      "|Iter  1316  | Total Val Loss  -10.259804248809814 |\n",
      "Loss for batch is  -1.0098810195922852\n",
      "Loss for batch is  -1.149190902709961\n",
      "Loss for batch is  -1.0170491933822632\n",
      "Loss for batch is  -2.010195732116699\n",
      "|Iter  1317  | Total Train Loss  -5.1863168478012085 |\n",
      "Val Loss for batch is  -2.132014513015747\n",
      "Val Loss for batch is  -2.385204553604126\n",
      "Val Loss for batch is  -2.2387843132019043\n",
      "Val Loss for batch is  -3.424224376678467\n",
      "|Iter  1317  | Total Val Loss  -10.180227756500244 |\n",
      "Loss for batch is  -0.9888266324996948\n",
      "Loss for batch is  -1.1546894311904907\n",
      "Loss for batch is  -1.0246713161468506\n",
      "Loss for batch is  -2.0140702724456787\n",
      "|Iter  1318  | Total Train Loss  -5.182257652282715 |\n",
      "Val Loss for batch is  -2.159080743789673\n",
      "Val Loss for batch is  -2.415623188018799\n",
      "Val Loss for batch is  -2.2496228218078613\n",
      "Val Loss for batch is  -3.422565460205078\n",
      "|Iter  1318  | Total Val Loss  -10.246892213821411 |\n",
      "Loss for batch is  -0.9969565868377686\n",
      "Loss for batch is  -1.1424897909164429\n",
      "Loss for batch is  -1.0341761112213135\n",
      "Loss for batch is  -2.009654998779297\n",
      "|Iter  1319  | Total Train Loss  -5.183277487754822 |\n",
      "Val Loss for batch is  -2.1098499298095703\n",
      "Val Loss for batch is  -2.3979127407073975\n",
      "Val Loss for batch is  -2.2769408226013184\n",
      "Val Loss for batch is  -3.446551561355591\n",
      "|Iter  1319  | Total Val Loss  -10.231255054473877 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.943061113357544\n",
      "Loss for batch is  -1.0545111894607544\n",
      "Loss for batch is  -0.9422482252120972\n",
      "Loss for batch is  -1.9828503131866455\n",
      "|Iter  1320  | Total Train Loss  -4.922670841217041 |\n",
      "Val Loss for batch is  -1.670198678970337\n",
      "Val Loss for batch is  -1.842887043952942\n",
      "Val Loss for batch is  -1.8138972520828247\n",
      "Val Loss for batch is  -3.274394989013672\n",
      "|Iter  1320  | Total Val Loss  -8.601377964019775 |\n",
      "Loss for batch is  -0.4522639513015747\n",
      "Loss for batch is  -1.0631756782531738\n",
      "Loss for batch is  -0.3170487880706787\n",
      "Loss for batch is  -1.924797773361206\n",
      "|Iter  1321  | Total Train Loss  -3.7572861909866333 |\n",
      "Val Loss for batch is  -1.30257248878479\n",
      "Val Loss for batch is  -0.29762330651283264\n",
      "Val Loss for batch is  3.332092046737671\n",
      "Val Loss for batch is  -3.0292365550994873\n",
      "|Iter  1321  | Total Val Loss  -1.297340303659439 |\n",
      "Loss for batch is  -0.24154865741729736\n",
      "Loss for batch is  -0.7175823450088501\n",
      "Loss for batch is  -0.7929985523223877\n",
      "Loss for batch is  -1.2733689546585083\n",
      "|Iter  1322  | Total Train Loss  -3.0254985094070435 |\n",
      "Val Loss for batch is  -1.4858405590057373\n",
      "Val Loss for batch is  -1.9347630739212036\n",
      "Val Loss for batch is  -1.8092433214187622\n",
      "Val Loss for batch is  -3.1083219051361084\n",
      "|Iter  1322  | Total Val Loss  -8.338168859481812 |\n",
      "Loss for batch is  -0.41773056983947754\n",
      "Loss for batch is  -0.7238757610321045\n",
      "Loss for batch is  -0.517225980758667\n",
      "Loss for batch is  -1.4264919757843018\n",
      "|Iter  1323  | Total Train Loss  -3.085324287414551 |\n",
      "Val Loss for batch is  -1.6704070568084717\n",
      "Val Loss for batch is  -2.0488195419311523\n",
      "Val Loss for batch is  -1.822333812713623\n",
      "Val Loss for batch is  -2.9067933559417725\n",
      "|Iter  1323  | Total Val Loss  -8.44835376739502 |\n",
      "Loss for batch is  -0.6111104488372803\n",
      "Loss for batch is  -0.874397873878479\n",
      "Loss for batch is  -0.7699487209320068\n",
      "Loss for batch is  -1.6113057136535645\n",
      "|Iter  1324  | Total Train Loss  -3.8667627573013306 |\n",
      "Val Loss for batch is  -1.8183801174163818\n",
      "Val Loss for batch is  -2.099390983581543\n",
      "Val Loss for batch is  -1.9495806694030762\n",
      "Val Loss for batch is  -3.065171241760254\n",
      "|Iter  1324  | Total Val Loss  -8.932523012161255 |\n",
      "Loss for batch is  -0.5940440893173218\n",
      "Loss for batch is  -0.9370592832565308\n",
      "Loss for batch is  -0.7964112758636475\n",
      "Loss for batch is  -1.71526300907135\n",
      "|Iter  1325  | Total Train Loss  -4.04277765750885 |\n",
      "Val Loss for batch is  -2.046482801437378\n",
      "Val Loss for batch is  -2.207737922668457\n",
      "Val Loss for batch is  -2.0005838871002197\n",
      "Val Loss for batch is  -3.1412017345428467\n",
      "|Iter  1325  | Total Val Loss  -9.396006345748901 |\n",
      "Loss for batch is  -0.7919031381607056\n",
      "Loss for batch is  -0.934817910194397\n",
      "Loss for batch is  -0.8069473505020142\n",
      "Loss for batch is  -1.7203534841537476\n",
      "|Iter  1326  | Total Train Loss  -4.254021883010864 |\n",
      "Val Loss for batch is  -2.0816268920898438\n",
      "Val Loss for batch is  -2.281917095184326\n",
      "Val Loss for batch is  -2.10597825050354\n",
      "Val Loss for batch is  -3.1511716842651367\n",
      "|Iter  1326  | Total Val Loss  -9.620693922042847 |\n",
      "Loss for batch is  -0.8105450868606567\n",
      "Loss for batch is  -0.982637882232666\n",
      "Loss for batch is  -0.8571686744689941\n",
      "Loss for batch is  -1.7595502138137817\n",
      "|Iter  1327  | Total Train Loss  -4.409901857376099 |\n",
      "Val Loss for batch is  -2.126927137374878\n",
      "Val Loss for batch is  -2.279498338699341\n",
      "Val Loss for batch is  -2.1625757217407227\n",
      "Val Loss for batch is  -3.206061363220215\n",
      "|Iter  1327  | Total Val Loss  -9.775062561035156 |\n",
      "Loss for batch is  -0.8629786968231201\n",
      "Loss for batch is  -1.0019911527633667\n",
      "Loss for batch is  -0.874726414680481\n",
      "Loss for batch is  -1.790208339691162\n",
      "|Iter  1328  | Total Train Loss  -4.52990460395813 |\n",
      "Val Loss for batch is  -2.1687653064727783\n",
      "Val Loss for batch is  -2.3324356079101562\n",
      "Val Loss for batch is  -2.188699245452881\n",
      "Val Loss for batch is  -3.247574806213379\n",
      "|Iter  1328  | Total Val Loss  -9.937474966049194 |\n",
      "Loss for batch is  -0.9187685251235962\n",
      "Loss for batch is  -1.0311554670333862\n",
      "Loss for batch is  -0.9047698974609375\n",
      "Loss for batch is  -1.8345216512680054\n",
      "|Iter  1329  | Total Train Loss  -4.689215540885925 |\n",
      "Val Loss for batch is  -2.1805260181427\n",
      "Val Loss for batch is  -2.3611929416656494\n",
      "Val Loss for batch is  -2.237473487854004\n",
      "Val Loss for batch is  -3.2829577922821045\n",
      "|Iter  1329  | Total Val Loss  -10.062150239944458 |\n",
      "Loss for batch is  -0.9382153749465942\n",
      "Loss for batch is  -1.0556715726852417\n",
      "Loss for batch is  -0.9426059722900391\n",
      "Loss for batch is  -1.8802040815353394\n",
      "|Iter  1330  | Total Train Loss  -4.816697001457214 |\n",
      "Val Loss for batch is  -2.136300563812256\n",
      "Val Loss for batch is  -2.3538477420806885\n",
      "Val Loss for batch is  -2.2246689796447754\n",
      "Val Loss for batch is  -3.3185224533081055\n",
      "|Iter  1330  | Total Val Loss  -10.033339738845825 |\n",
      "Loss for batch is  -0.9512543678283691\n",
      "Loss for batch is  -1.0820648670196533\n",
      "Loss for batch is  -0.970948338508606\n",
      "Loss for batch is  -1.8974509239196777\n",
      "|Iter  1331  | Total Train Loss  -4.901718497276306 |\n",
      "Val Loss for batch is  -2.1958394050598145\n",
      "Val Loss for batch is  -2.360898971557617\n",
      "Val Loss for batch is  -2.2560970783233643\n",
      "Val Loss for batch is  -3.3187689781188965\n",
      "|Iter  1331  | Total Val Loss  -10.131604433059692 |\n",
      "Loss for batch is  -0.975867509841919\n",
      "Loss for batch is  -1.083451509475708\n",
      "Loss for batch is  -0.979926347732544\n",
      "Loss for batch is  -1.9472459554672241\n",
      "|Iter  1332  | Total Train Loss  -4.986491322517395 |\n",
      "Val Loss for batch is  -2.1851255893707275\n",
      "Val Loss for batch is  -2.3943634033203125\n",
      "Val Loss for batch is  -2.2552530765533447\n",
      "Val Loss for batch is  -3.359257221221924\n",
      "|Iter  1332  | Total Val Loss  -10.193999290466309 |\n",
      "Loss for batch is  -0.9770121574401855\n",
      "Loss for batch is  -1.1129854917526245\n",
      "Loss for batch is  -1.0004688501358032\n",
      "Loss for batch is  -1.9606684446334839\n",
      "|Iter  1333  | Total Train Loss  -5.051134943962097 |\n",
      "Val Loss for batch is  -2.16165828704834\n",
      "Val Loss for batch is  -2.3566133975982666\n",
      "Val Loss for batch is  -2.279602289199829\n",
      "Val Loss for batch is  -3.386924982070923\n",
      "|Iter  1333  | Total Val Loss  -10.184798955917358 |\n",
      "Loss for batch is  -0.994255781173706\n",
      "Loss for batch is  -1.13084876537323\n",
      "Loss for batch is  -1.0242422819137573\n",
      "Loss for batch is  -1.9843440055847168\n",
      "|Iter  1334  | Total Train Loss  -5.13369083404541 |\n",
      "Val Loss for batch is  -2.1377389430999756\n",
      "Val Loss for batch is  -2.404881238937378\n",
      "Val Loss for batch is  -2.2973239421844482\n",
      "Val Loss for batch is  -3.3902857303619385\n",
      "|Iter  1334  | Total Val Loss  -10.23022985458374 |\n",
      "Loss for batch is  -1.004276990890503\n",
      "Loss for batch is  -1.1532782316207886\n",
      "Loss for batch is  -1.0335956811904907\n",
      "Loss for batch is  -2.008681297302246\n",
      "|Iter  1335  | Total Train Loss  -5.199832201004028 |\n",
      "Val Loss for batch is  -2.18813419342041\n",
      "Val Loss for batch is  -2.4133222103118896\n",
      "Val Loss for batch is  -2.2803056240081787\n",
      "Val Loss for batch is  -3.4274706840515137\n",
      "|Iter  1335  | Total Val Loss  -10.309232711791992 |\n",
      "Loss for batch is  -1.0215353965759277\n",
      "Loss for batch is  -1.1612907648086548\n",
      "Loss for batch is  -1.0433132648468018\n",
      "Loss for batch is  -2.028444290161133\n",
      "|Iter  1336  | Total Train Loss  -5.254583716392517 |\n",
      "Val Loss for batch is  -2.200895071029663\n",
      "Val Loss for batch is  -2.4211912155151367\n",
      "Val Loss for batch is  -2.3129756450653076\n",
      "Val Loss for batch is  -3.4366793632507324\n",
      "|Iter  1336  | Total Val Loss  -10.37174129486084 |\n",
      "Loss for batch is  -1.032169222831726\n",
      "Loss for batch is  -1.1718776226043701\n",
      "Loss for batch is  -1.0547767877578735\n",
      "Loss for batch is  -2.0471913814544678\n",
      "|Iter  1337  | Total Train Loss  -5.3060150146484375 |\n",
      "Val Loss for batch is  -2.1909286975860596\n",
      "Val Loss for batch is  -2.4260354042053223\n",
      "Val Loss for batch is  -2.3159472942352295\n",
      "Val Loss for batch is  -3.4649758338928223\n",
      "|Iter  1337  | Total Val Loss  -10.397887229919434 |\n",
      "Loss for batch is  -1.0334246158599854\n",
      "Loss for batch is  -1.1807798147201538\n",
      "Loss for batch is  -1.0642380714416504\n",
      "Loss for batch is  -2.0433707237243652\n",
      "|Iter  1338  | Total Train Loss  -5.321813225746155 |\n",
      "Val Loss for batch is  -2.2107925415039062\n",
      "Val Loss for batch is  -2.439027786254883\n",
      "Val Loss for batch is  -2.3317313194274902\n",
      "Val Loss for batch is  -3.468566656112671\n",
      "|Iter  1338  | Total Val Loss  -10.45011830329895 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.0443661212921143\n",
      "Loss for batch is  -1.1773239374160767\n",
      "Loss for batch is  -1.0691744089126587\n",
      "Loss for batch is  -2.0614380836486816\n",
      "|Iter  1339  | Total Train Loss  -5.352302551269531 |\n",
      "Val Loss for batch is  -2.167266845703125\n",
      "Val Loss for batch is  -2.44962477684021\n",
      "Val Loss for batch is  -2.3069956302642822\n",
      "Val Loss for batch is  -3.4660351276397705\n",
      "|Iter  1339  | Total Val Loss  -10.389922380447388 |\n",
      "Loss for batch is  -1.020918369293213\n",
      "Loss for batch is  -1.1859577894210815\n",
      "Loss for batch is  -1.030967116355896\n",
      "Loss for batch is  -2.0640501976013184\n",
      "|Iter  1340  | Total Train Loss  -5.301893472671509 |\n",
      "Val Loss for batch is  -2.1406123638153076\n",
      "Val Loss for batch is  -2.35611891746521\n",
      "Val Loss for batch is  -2.2554068565368652\n",
      "Val Loss for batch is  -3.4553182125091553\n",
      "|Iter  1340  | Total Val Loss  -10.207456350326538 |\n",
      "Loss for batch is  -0.974951982498169\n",
      "Loss for batch is  -1.1694982051849365\n",
      "Loss for batch is  -1.0108463764190674\n",
      "Loss for batch is  -2.0212666988372803\n",
      "|Iter  1341  | Total Train Loss  -5.176563262939453 |\n",
      "Val Loss for batch is  -2.087928295135498\n",
      "Val Loss for batch is  -2.3720386028289795\n",
      "Val Loss for batch is  -2.226153612136841\n",
      "Val Loss for batch is  -3.433073043823242\n",
      "|Iter  1341  | Total Val Loss  -10.11919355392456 |\n",
      "Loss for batch is  -0.9749346971511841\n",
      "Loss for batch is  -1.0879251956939697\n",
      "Loss for batch is  -0.9993319511413574\n",
      "Loss for batch is  -2.0273618698120117\n",
      "|Iter  1342  | Total Train Loss  -5.089553713798523 |\n",
      "Val Loss for batch is  -1.7900086641311646\n",
      "Val Loss for batch is  -2.0600426197052\n",
      "Val Loss for batch is  -2.0271081924438477\n",
      "Val Loss for batch is  -3.4233224391937256\n",
      "|Iter  1342  | Total Val Loss  -9.300481915473938 |\n",
      "Loss for batch is  -0.6996767520904541\n",
      "Loss for batch is  -1.1193276643753052\n",
      "Loss for batch is  -0.655785083770752\n",
      "Loss for batch is  -1.964328646659851\n",
      "|Iter  1343  | Total Train Loss  -4.439118146896362 |\n",
      "Val Loss for batch is  -1.6772114038467407\n",
      "Val Loss for batch is  -1.8715808391571045\n",
      "Val Loss for batch is  -1.7799465656280518\n",
      "Val Loss for batch is  -3.2950918674468994\n",
      "|Iter  1343  | Total Val Loss  -8.623830676078796 |\n",
      "Loss for batch is  -0.5203526020050049\n",
      "Loss for batch is  -0.8732858896255493\n",
      "Loss for batch is  -0.7417995929718018\n",
      "Loss for batch is  -1.506187081336975\n",
      "|Iter  1344  | Total Train Loss  -3.641625165939331 |\n",
      "Val Loss for batch is  -1.8329006433486938\n",
      "Val Loss for batch is  -2.0722599029541016\n",
      "Val Loss for batch is  -2.0071475505828857\n",
      "Val Loss for batch is  -3.2838072776794434\n",
      "|Iter  1344  | Total Val Loss  -9.196115374565125 |\n",
      "Loss for batch is  -0.7320060729980469\n",
      "Loss for batch is  -0.7699607610702515\n",
      "Loss for batch is  -0.6250003576278687\n",
      "Loss for batch is  -1.5733259916305542\n",
      "|Iter  1345  | Total Train Loss  -3.700293183326721 |\n",
      "Val Loss for batch is  -1.6924688816070557\n",
      "Val Loss for batch is  -1.9922947883605957\n",
      "Val Loss for batch is  -1.8104407787322998\n",
      "Val Loss for batch is  -3.2345073223114014\n",
      "|Iter  1345  | Total Val Loss  -8.729711771011353 |\n",
      "Loss for batch is  -0.6134002208709717\n",
      "Loss for batch is  -0.5451589822769165\n",
      "Loss for batch is  -0.7685432434082031\n",
      "Loss for batch is  -1.8084843158721924\n",
      "|Iter  1346  | Total Train Loss  -3.7355867624282837 |\n",
      "Val Loss for batch is  -1.9763422012329102\n",
      "Val Loss for batch is  -2.1428134441375732\n",
      "Val Loss for batch is  -2.047287940979004\n",
      "Val Loss for batch is  -3.0717337131500244\n",
      "|Iter  1346  | Total Val Loss  -9.238177299499512 |\n",
      "Loss for batch is  -0.8009024858474731\n",
      "Loss for batch is  -0.9211846590042114\n",
      "Loss for batch is  -0.7910677194595337\n",
      "Loss for batch is  -1.8183778524398804\n",
      "|Iter  1347  | Total Train Loss  -4.331532716751099 |\n",
      "Val Loss for batch is  -1.9981738328933716\n",
      "Val Loss for batch is  -2.2117135524749756\n",
      "Val Loss for batch is  -2.090857744216919\n",
      "Val Loss for batch is  -3.244110584259033\n",
      "|Iter  1347  | Total Val Loss  -9.5448557138443 |\n",
      "Loss for batch is  -0.8293707370758057\n",
      "Loss for batch is  -0.9236124753952026\n",
      "Loss for batch is  -0.8295968770980835\n",
      "Loss for batch is  -1.8268368244171143\n",
      "|Iter  1348  | Total Train Loss  -4.409416913986206 |\n",
      "Val Loss for batch is  -2.0820846557617188\n",
      "Val Loss for batch is  -2.240384578704834\n",
      "Val Loss for batch is  -2.1267330646514893\n",
      "Val Loss for batch is  -3.3087141513824463\n",
      "|Iter  1348  | Total Val Loss  -9.757916450500488 |\n",
      "Loss for batch is  -0.8815041780471802\n",
      "Loss for batch is  -0.995020866394043\n",
      "Loss for batch is  -0.8341721296310425\n",
      "Loss for batch is  -1.7705750465393066\n",
      "|Iter  1349  | Total Train Loss  -4.481272220611572 |\n",
      "Val Loss for batch is  -2.1318867206573486\n",
      "Val Loss for batch is  -2.2883760929107666\n",
      "Val Loss for batch is  -2.178039073944092\n",
      "Val Loss for batch is  -3.283611536026001\n",
      "|Iter  1349  | Total Val Loss  -9.881913423538208 |\n",
      "Loss for batch is  -0.9327270984649658\n",
      "Loss for batch is  -1.0442347526550293\n",
      "Loss for batch is  -0.9225486516952515\n",
      "Loss for batch is  -1.7711607217788696\n",
      "|Iter  1350  | Total Train Loss  -4.670671224594116 |\n",
      "Val Loss for batch is  -2.1650071144104004\n",
      "Val Loss for batch is  -2.266345262527466\n",
      "Val Loss for batch is  -2.196057081222534\n",
      "Val Loss for batch is  -3.3124449253082275\n",
      "|Iter  1350  | Total Val Loss  -9.939854383468628 |\n",
      "Loss for batch is  -0.9259766340255737\n",
      "Loss for batch is  -1.0594370365142822\n",
      "Loss for batch is  -0.9320917129516602\n",
      "Loss for batch is  -1.8598724603652954\n",
      "|Iter  1351  | Total Train Loss  -4.7773778438568115 |\n",
      "Val Loss for batch is  -2.1737687587738037\n",
      "Val Loss for batch is  -2.3598549365997314\n",
      "Val Loss for batch is  -2.2305612564086914\n",
      "Val Loss for batch is  -3.3444602489471436\n",
      "|Iter  1351  | Total Val Loss  -10.10864520072937 |\n",
      "Loss for batch is  -0.9623798131942749\n",
      "Loss for batch is  -1.0946593284606934\n",
      "Loss for batch is  -0.9845231771469116\n",
      "Loss for batch is  -1.8565915822982788\n",
      "|Iter  1352  | Total Train Loss  -4.898153901100159 |\n",
      "Val Loss for batch is  -2.192223310470581\n",
      "Val Loss for batch is  -2.3705198764801025\n",
      "Val Loss for batch is  -2.2608835697174072\n",
      "Val Loss for batch is  -3.329547643661499\n",
      "|Iter  1352  | Total Val Loss  -10.15317440032959 |\n",
      "Loss for batch is  -0.9800993204116821\n",
      "Loss for batch is  -1.111749529838562\n",
      "Loss for batch is  -0.9879450798034668\n",
      "Loss for batch is  -1.9303057193756104\n",
      "|Iter  1353  | Total Train Loss  -5.010099649429321 |\n",
      "Val Loss for batch is  -2.220705509185791\n",
      "Val Loss for batch is  -2.363259792327881\n",
      "Val Loss for batch is  -2.270799160003662\n",
      "Val Loss for batch is  -3.391935348510742\n",
      "|Iter  1353  | Total Val Loss  -10.246699810028076 |\n",
      "Loss for batch is  -1.0014334917068481\n",
      "Loss for batch is  -1.134074091911316\n",
      "Loss for batch is  -1.0263627767562866\n",
      "Loss for batch is  -1.9648377895355225\n",
      "|Iter  1354  | Total Train Loss  -5.126708149909973 |\n",
      "Val Loss for batch is  -2.1898193359375\n",
      "Val Loss for batch is  -2.3891618251800537\n",
      "Val Loss for batch is  -2.2815160751342773\n",
      "Val Loss for batch is  -3.3938844203948975\n",
      "|Iter  1354  | Total Val Loss  -10.254381656646729 |\n",
      "Loss for batch is  -1.0211936235427856\n",
      "Loss for batch is  -1.148349404335022\n",
      "Loss for batch is  -1.0175299644470215\n",
      "Loss for batch is  -1.9903182983398438\n",
      "|Iter  1355  | Total Train Loss  -5.177391290664673 |\n",
      "Val Loss for batch is  -2.200493574142456\n",
      "Val Loss for batch is  -2.4169867038726807\n",
      "Val Loss for batch is  -2.281245708465576\n",
      "Val Loss for batch is  -3.4313008785247803\n",
      "|Iter  1355  | Total Val Loss  -10.330026865005493 |\n",
      "Loss for batch is  -1.041263222694397\n",
      "Loss for batch is  -1.151224136352539\n",
      "Loss for batch is  -1.0482007265090942\n",
      "Loss for batch is  -2.039313554763794\n",
      "|Iter  1356  | Total Train Loss  -5.280001640319824 |\n",
      "Val Loss for batch is  -2.205615520477295\n",
      "Val Loss for batch is  -2.454070568084717\n",
      "Val Loss for batch is  -2.289811134338379\n",
      "Val Loss for batch is  -3.4561612606048584\n",
      "|Iter  1356  | Total Val Loss  -10.405658483505249 |\n",
      "Loss for batch is  -1.047768235206604\n",
      "Loss for batch is  -1.1647132635116577\n",
      "Loss for batch is  -1.0641337633132935\n",
      "Loss for batch is  -2.05737566947937\n",
      "|Iter  1357  | Total Train Loss  -5.333990931510925 |\n",
      "Val Loss for batch is  -2.1749677658081055\n",
      "Val Loss for batch is  -2.4371445178985596\n",
      "Val Loss for batch is  -2.3055803775787354\n",
      "Val Loss for batch is  -3.4691755771636963\n",
      "|Iter  1357  | Total Val Loss  -10.386868238449097 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.0490608215332031\n",
      "Loss for batch is  -1.1930817365646362\n",
      "Loss for batch is  -1.0941566228866577\n",
      "Loss for batch is  -2.0667243003845215\n",
      "|Iter  1358  | Total Train Loss  -5.4030234813690186 |\n",
      "Val Loss for batch is  -2.1930227279663086\n",
      "Val Loss for batch is  -2.4116649627685547\n",
      "Val Loss for batch is  -2.3282527923583984\n",
      "Val Loss for batch is  -3.4705662727355957\n",
      "|Iter  1358  | Total Val Loss  -10.403506755828857 |\n",
      "Loss for batch is  -1.0554574728012085\n",
      "Loss for batch is  -1.2016361951828003\n",
      "Loss for batch is  -1.0829601287841797\n",
      "Loss for batch is  -2.08534574508667\n",
      "|Iter  1359  | Total Train Loss  -5.425399541854858 |\n",
      "Val Loss for batch is  -2.1890316009521484\n",
      "Val Loss for batch is  -2.410910129547119\n",
      "Val Loss for batch is  -2.3523333072662354\n",
      "Val Loss for batch is  -3.4727909564971924\n",
      "|Iter  1359  | Total Val Loss  -10.425065994262695 |\n",
      "Loss for batch is  -1.0617552995681763\n",
      "Loss for batch is  -1.2114746570587158\n",
      "Loss for batch is  -1.0915110111236572\n",
      "Loss for batch is  -2.1013121604919434\n",
      "|Iter  1360  | Total Train Loss  -5.466053128242493 |\n",
      "Val Loss for batch is  -2.2014732360839844\n",
      "Val Loss for batch is  -2.41314435005188\n",
      "Val Loss for batch is  -2.3060007095336914\n",
      "Val Loss for batch is  -3.500326633453369\n",
      "|Iter  1360  | Total Val Loss  -10.420944929122925 |\n",
      "Loss for batch is  -1.0673733949661255\n",
      "Loss for batch is  -1.210220456123352\n",
      "Loss for batch is  -1.1085377931594849\n",
      "Loss for batch is  -2.089400291442871\n",
      "|Iter  1361  | Total Train Loss  -5.4755319356918335 |\n",
      "Val Loss for batch is  -2.1941022872924805\n",
      "Val Loss for batch is  -2.457676887512207\n",
      "Val Loss for batch is  -2.309842824935913\n",
      "Val Loss for batch is  -3.5407943725585938\n",
      "|Iter  1361  | Total Val Loss  -10.502416372299194 |\n",
      "Loss for batch is  -1.093813180923462\n",
      "Loss for batch is  -1.2199991941452026\n",
      "Loss for batch is  -1.0952481031417847\n",
      "Loss for batch is  -2.123887538909912\n",
      "|Iter  1362  | Total Train Loss  -5.532948017120361 |\n",
      "Val Loss for batch is  -2.159219264984131\n",
      "Val Loss for batch is  -2.4018118381500244\n",
      "Val Loss for batch is  -2.2672250270843506\n",
      "Val Loss for batch is  -3.5308685302734375\n",
      "|Iter  1362  | Total Val Loss  -10.359124660491943 |\n",
      "Loss for batch is  -1.0165436267852783\n",
      "Loss for batch is  -1.2069278955459595\n",
      "Loss for batch is  -1.0160988569259644\n",
      "Loss for batch is  -2.0956287384033203\n",
      "|Iter  1363  | Total Train Loss  -5.3351991176605225 |\n",
      "Val Loss for batch is  -2.1283881664276123\n",
      "Val Loss for batch is  -2.3603694438934326\n",
      "Val Loss for batch is  -2.1831626892089844\n",
      "Val Loss for batch is  -3.5150959491729736\n",
      "|Iter  1363  | Total Val Loss  -10.187016248703003 |\n",
      "Loss for batch is  -1.0065399408340454\n",
      "Loss for batch is  -1.148585557937622\n",
      "Loss for batch is  -1.0532886981964111\n",
      "Loss for batch is  -2.033046245574951\n",
      "|Iter  1364  | Total Train Loss  -5.24146044254303 |\n",
      "Val Loss for batch is  -2.088979959487915\n",
      "Val Loss for batch is  -2.331702709197998\n",
      "Val Loss for batch is  -2.2127010822296143\n",
      "Val Loss for batch is  -3.4848132133483887\n",
      "|Iter  1364  | Total Val Loss  -10.118196964263916 |\n",
      "Loss for batch is  -0.9293340444564819\n",
      "Loss for batch is  -1.0557365417480469\n",
      "Loss for batch is  -0.8571480512619019\n",
      "Loss for batch is  -1.8969104290008545\n",
      "|Iter  1365  | Total Train Loss  -4.739129066467285 |\n",
      "Val Loss for batch is  -1.4996243715286255\n",
      "Val Loss for batch is  -1.9090780019760132\n",
      "Val Loss for batch is  -1.7213215827941895\n",
      "Val Loss for batch is  -3.3157551288604736\n",
      "|Iter  1365  | Total Val Loss  -8.445779085159302 |\n",
      "Loss for batch is  -0.5000705718994141\n",
      "Loss for batch is  -1.04018235206604\n",
      "Loss for batch is  -0.6014838218688965\n",
      "Loss for batch is  -1.5773558616638184\n",
      "|Iter  1366  | Total Train Loss  -3.719092607498169 |\n",
      "Val Loss for batch is  -1.8281344175338745\n",
      "Val Loss for batch is  -2.1293375492095947\n",
      "Val Loss for batch is  -1.8180712461471558\n",
      "Val Loss for batch is  -3.2635374069213867\n",
      "|Iter  1366  | Total Val Loss  -9.039080619812012 |\n",
      "Loss for batch is  -0.7468808889389038\n",
      "Loss for batch is  -0.593845009803772\n",
      "Loss for batch is  -0.7926554679870605\n",
      "Loss for batch is  -1.8290308713912964\n",
      "|Iter  1367  | Total Train Loss  -3.9624122381210327 |\n",
      "Val Loss for batch is  -1.720902681350708\n",
      "Val Loss for batch is  -2.0573904514312744\n",
      "Val Loss for batch is  -1.8649051189422607\n",
      "Val Loss for batch is  -3.231076955795288\n",
      "|Iter  1367  | Total Val Loss  -8.874275207519531 |\n",
      "Loss for batch is  -0.6568955183029175\n",
      "Loss for batch is  -0.8884639739990234\n",
      "Loss for batch is  -0.9257297515869141\n",
      "Loss for batch is  -1.6997994184494019\n",
      "|Iter  1368  | Total Train Loss  -4.170888662338257 |\n",
      "Val Loss for batch is  -2.0577585697174072\n",
      "Val Loss for batch is  -2.240964889526367\n",
      "Val Loss for batch is  -2.1099886894226074\n",
      "Val Loss for batch is  -3.213113784790039\n",
      "|Iter  1368  | Total Val Loss  -9.621825933456421 |\n",
      "Loss for batch is  -0.8672215938568115\n",
      "Loss for batch is  -1.0256562232971191\n",
      "Loss for batch is  -0.9191529750823975\n",
      "Loss for batch is  -1.909669280052185\n",
      "|Iter  1369  | Total Train Loss  -4.721700072288513 |\n",
      "Val Loss for batch is  -2.033269166946411\n",
      "Val Loss for batch is  -2.1838879585266113\n",
      "Val Loss for batch is  -2.1070704460144043\n",
      "Val Loss for batch is  -3.33573055267334\n",
      "|Iter  1369  | Total Val Loss  -9.659958124160767 |\n",
      "Loss for batch is  -0.8697774410247803\n",
      "Loss for batch is  -1.004397988319397\n",
      "Loss for batch is  -0.9196003675460815\n",
      "Loss for batch is  -1.9226123094558716\n",
      "|Iter  1370  | Total Train Loss  -4.71638810634613 |\n",
      "Val Loss for batch is  -2.1814446449279785\n",
      "Val Loss for batch is  -2.320136070251465\n",
      "Val Loss for batch is  -2.216057062149048\n",
      "Val Loss for batch is  -3.327254295349121\n",
      "|Iter  1370  | Total Val Loss  -10.044892072677612 |\n",
      "Loss for batch is  -0.9760227203369141\n",
      "Loss for batch is  -1.0669403076171875\n",
      "Loss for batch is  -0.9264070987701416\n",
      "Loss for batch is  -1.912710428237915\n",
      "|Iter  1371  | Total Train Loss  -4.882080554962158 |\n",
      "Val Loss for batch is  -2.197563409805298\n",
      "Val Loss for batch is  -2.331725835800171\n",
      "Val Loss for batch is  -2.2905266284942627\n",
      "Val Loss for batch is  -3.4094605445861816\n",
      "|Iter  1371  | Total Val Loss  -10.229276418685913 |\n",
      "Loss for batch is  -1.0057812929153442\n",
      "Loss for batch is  -1.0737926959991455\n",
      "Loss for batch is  -0.9547516107559204\n",
      "Loss for batch is  -1.956594467163086\n",
      "|Iter  1372  | Total Train Loss  -4.990920066833496 |\n",
      "Val Loss for batch is  -2.1996147632598877\n",
      "Val Loss for batch is  -2.3851799964904785\n",
      "Val Loss for batch is  -2.26472544670105\n",
      "Val Loss for batch is  -3.4263124465942383\n",
      "|Iter  1372  | Total Val Loss  -10.275832653045654 |\n",
      "Loss for batch is  -1.0250346660614014\n",
      "Loss for batch is  -1.1209523677825928\n",
      "Loss for batch is  -0.9785546064376831\n",
      "Loss for batch is  -1.9309684038162231\n",
      "|Iter  1373  | Total Train Loss  -5.0555100440979 |\n",
      "Val Loss for batch is  -2.2180984020233154\n",
      "Val Loss for batch is  -2.4108283519744873\n",
      "Val Loss for batch is  -2.3152289390563965\n",
      "Val Loss for batch is  -3.4387567043304443\n",
      "|Iter  1373  | Total Val Loss  -10.382912397384644 |\n",
      "Loss for batch is  -1.0436375141143799\n",
      "Loss for batch is  -1.1346430778503418\n",
      "Loss for batch is  -0.98394775390625\n",
      "Loss for batch is  -1.978696584701538\n",
      "|Iter  1374  | Total Train Loss  -5.14092493057251 |\n",
      "Val Loss for batch is  -2.2368626594543457\n",
      "Val Loss for batch is  -2.3881330490112305\n",
      "Val Loss for batch is  -2.3142032623291016\n",
      "Val Loss for batch is  -3.455711603164673\n",
      "|Iter  1374  | Total Val Loss  -10.39491057395935 |\n",
      "Loss for batch is  -1.0538692474365234\n",
      "Loss for batch is  -1.1432769298553467\n",
      "Loss for batch is  -0.9893736839294434\n",
      "Loss for batch is  -1.9950165748596191\n",
      "|Iter  1375  | Total Train Loss  -5.181536436080933 |\n",
      "Val Loss for batch is  -2.239583969116211\n",
      "Val Loss for batch is  -2.4166882038116455\n",
      "Val Loss for batch is  -2.318666696548462\n",
      "Val Loss for batch is  -3.455939292907715\n",
      "|Iter  1375  | Total Val Loss  -10.430878162384033 |\n",
      "Loss for batch is  -1.0681992769241333\n",
      "Loss for batch is  -1.1281297206878662\n",
      "Loss for batch is  -1.0100525617599487\n",
      "Loss for batch is  -2.016162157058716\n",
      "|Iter  1376  | Total Train Loss  -5.222543716430664 |\n",
      "Val Loss for batch is  -2.188730478286743\n",
      "Val Loss for batch is  -2.4202470779418945\n",
      "Val Loss for batch is  -2.3008790016174316\n",
      "Val Loss for batch is  -3.477292776107788\n",
      "|Iter  1376  | Total Val Loss  -10.387149333953857 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.0527596473693848\n",
      "Loss for batch is  -1.1390756368637085\n",
      "Loss for batch is  -1.0167711973190308\n",
      "Loss for batch is  -2.057189464569092\n",
      "|Iter  1377  | Total Train Loss  -5.265795946121216 |\n",
      "Val Loss for batch is  -2.1715428829193115\n",
      "Val Loss for batch is  -2.4214372634887695\n",
      "Val Loss for batch is  -2.32214093208313\n",
      "Val Loss for batch is  -3.4922828674316406\n",
      "|Iter  1377  | Total Val Loss  -10.407403945922852 |\n",
      "Loss for batch is  -1.0668410062789917\n",
      "Loss for batch is  -1.158626914024353\n",
      "Loss for batch is  -1.0647066831588745\n",
      "Loss for batch is  -2.072111129760742\n",
      "|Iter  1378  | Total Train Loss  -5.362285733222961 |\n",
      "Val Loss for batch is  -2.1770401000976562\n",
      "Val Loss for batch is  -2.4356558322906494\n",
      "Val Loss for batch is  -2.330226421356201\n",
      "Val Loss for batch is  -3.4890060424804688\n",
      "|Iter  1378  | Total Val Loss  -10.431928396224976 |\n",
      "Loss for batch is  -1.066756248474121\n",
      "Loss for batch is  -1.1871578693389893\n",
      "Loss for batch is  -1.0653799772262573\n",
      "Loss for batch is  -2.088038206100464\n",
      "|Iter  1379  | Total Train Loss  -5.4073323011398315 |\n",
      "Val Loss for batch is  -2.188563108444214\n",
      "Val Loss for batch is  -2.419362783432007\n",
      "Val Loss for batch is  -2.3129444122314453\n",
      "Val Loss for batch is  -3.4858357906341553\n",
      "|Iter  1379  | Total Val Loss  -10.406706094741821 |\n",
      "Loss for batch is  -1.0763494968414307\n",
      "Loss for batch is  -1.1853817701339722\n",
      "Loss for batch is  -1.100350022315979\n",
      "Loss for batch is  -2.1039416790008545\n",
      "|Iter  1380  | Total Train Loss  -5.466022968292236 |\n",
      "Val Loss for batch is  -2.2266037464141846\n",
      "Val Loss for batch is  -2.4333252906799316\n",
      "Val Loss for batch is  -2.3076603412628174\n",
      "Val Loss for batch is  -3.4917616844177246\n",
      "|Iter  1380  | Total Val Loss  -10.459351062774658 |\n",
      "Loss for batch is  -1.0849452018737793\n",
      "Loss for batch is  -1.2180509567260742\n",
      "Loss for batch is  -1.1025810241699219\n",
      "Loss for batch is  -2.117675304412842\n",
      "|Iter  1381  | Total Train Loss  -5.523252487182617 |\n",
      "Val Loss for batch is  -2.2079267501831055\n",
      "Val Loss for batch is  -2.4589717388153076\n",
      "Val Loss for batch is  -2.3274075984954834\n",
      "Val Loss for batch is  -3.525749683380127\n",
      "|Iter  1381  | Total Val Loss  -10.520055770874023 |\n",
      "Loss for batch is  -1.1014498472213745\n",
      "Loss for batch is  -1.2090516090393066\n",
      "Loss for batch is  -1.118773102760315\n",
      "Loss for batch is  -2.1522579193115234\n",
      "|Iter  1382  | Total Train Loss  -5.5815324783325195 |\n",
      "Val Loss for batch is  -2.256568431854248\n",
      "Val Loss for batch is  -2.451016664505005\n",
      "Val Loss for batch is  -2.3338685035705566\n",
      "Val Loss for batch is  -3.5564513206481934\n",
      "|Iter  1382  | Total Val Loss  -10.597904920578003 |\n",
      "Loss for batch is  -1.1060242652893066\n",
      "Loss for batch is  -1.2355107069015503\n",
      "Loss for batch is  -1.1484107971191406\n",
      "Loss for batch is  -2.1457042694091797\n",
      "|Iter  1383  | Total Train Loss  -5.635650038719177 |\n",
      "Val Loss for batch is  -2.2192301750183105\n",
      "Val Loss for batch is  -2.4492413997650146\n",
      "Val Loss for batch is  -2.332104444503784\n",
      "Val Loss for batch is  -3.559492588043213\n",
      "|Iter  1383  | Total Val Loss  -10.560068607330322 |\n",
      "Loss for batch is  -1.1038918495178223\n",
      "Loss for batch is  -1.2487573623657227\n",
      "Loss for batch is  -1.147737979888916\n",
      "Loss for batch is  -2.1501951217651367\n",
      "|Iter  1384  | Total Train Loss  -5.650582313537598 |\n",
      "Val Loss for batch is  -2.243605852127075\n",
      "Val Loss for batch is  -2.476773500442505\n",
      "Val Loss for batch is  -2.3606157302856445\n",
      "Val Loss for batch is  -3.572659492492676\n",
      "|Iter  1384  | Total Val Loss  -10.6536545753479 |\n",
      "Loss for batch is  -1.1293480396270752\n",
      "Loss for batch is  -1.2466614246368408\n",
      "Loss for batch is  -1.1455084085464478\n",
      "Loss for batch is  -2.17358660697937\n",
      "|Iter  1385  | Total Train Loss  -5.695104479789734 |\n",
      "Val Loss for batch is  -2.198561191558838\n",
      "Val Loss for batch is  -2.466151475906372\n",
      "Val Loss for batch is  -2.1657330989837646\n",
      "Val Loss for batch is  -3.5546655654907227\n",
      "|Iter  1385  | Total Val Loss  -10.385111331939697 |\n",
      "Loss for batch is  -1.1032357215881348\n",
      "Loss for batch is  -1.241288185119629\n",
      "Loss for batch is  -1.1477149724960327\n",
      "Loss for batch is  -2.152972459793091\n",
      "|Iter  1386  | Total Train Loss  -5.645211338996887 |\n",
      "Val Loss for batch is  -2.2247493267059326\n",
      "Val Loss for batch is  -2.4777348041534424\n",
      "Val Loss for batch is  -2.3392040729522705\n",
      "Val Loss for batch is  -3.5772650241851807\n",
      "|Iter  1386  | Total Val Loss  -10.618953227996826 |\n",
      "Loss for batch is  -1.1197974681854248\n",
      "Loss for batch is  -1.2217633724212646\n",
      "Loss for batch is  -1.1301244497299194\n",
      "Loss for batch is  -2.1886672973632812\n",
      "|Iter  1387  | Total Train Loss  -5.66035258769989 |\n",
      "Val Loss for batch is  -2.1110708713531494\n",
      "Val Loss for batch is  -2.3366997241973877\n",
      "Val Loss for batch is  -2.2589163780212402\n",
      "Val Loss for batch is  -3.5809502601623535\n",
      "|Iter  1387  | Total Val Loss  -10.28763723373413 |\n",
      "Loss for batch is  -1.0168428421020508\n",
      "Loss for batch is  -1.237378478050232\n",
      "Loss for batch is  -1.034005045890808\n",
      "Loss for batch is  -2.089553117752075\n",
      "|Iter  1388  | Total Train Loss  -5.377779483795166 |\n",
      "Val Loss for batch is  -1.9626585245132446\n",
      "Val Loss for batch is  -2.13655424118042\n",
      "Val Loss for batch is  -2.0617172718048096\n",
      "Val Loss for batch is  -3.5195670127868652\n",
      "|Iter  1388  | Total Val Loss  -9.68049705028534 |\n",
      "Loss for batch is  -0.8398573398590088\n",
      "Loss for batch is  -1.1184279918670654\n",
      "Loss for batch is  -0.8641350269317627\n",
      "Loss for batch is  -2.0535850524902344\n",
      "|Iter  1389  | Total Train Loss  -4.876005411148071 |\n",
      "Val Loss for batch is  -1.7358951568603516\n",
      "Val Loss for batch is  -2.053227424621582\n",
      "Val Loss for batch is  -1.9526968002319336\n",
      "Val Loss for batch is  -3.4788432121276855\n",
      "|Iter  1389  | Total Val Loss  -9.220662593841553 |\n",
      "Loss for batch is  -0.7789100408554077\n",
      "Loss for batch is  -1.0610681772232056\n",
      "Loss for batch is  -0.9244750738143921\n",
      "Loss for batch is  -1.726343035697937\n",
      "|Iter  1390  | Total Train Loss  -4.490796327590942 |\n",
      "Val Loss for batch is  -1.7436426877975464\n",
      "Val Loss for batch is  -2.1315720081329346\n",
      "Val Loss for batch is  -1.9897818565368652\n",
      "Val Loss for batch is  -3.398768424987793\n",
      "|Iter  1390  | Total Val Loss  -9.26376497745514 |\n",
      "Loss for batch is  -0.7241225242614746\n",
      "Loss for batch is  -0.8591223955154419\n",
      "Loss for batch is  -0.7731021642684937\n",
      "Loss for batch is  -1.876558780670166\n",
      "|Iter  1391  | Total Train Loss  -4.232905864715576 |\n",
      "Val Loss for batch is  -1.8019895553588867\n",
      "Val Loss for batch is  -2.098877191543579\n",
      "Val Loss for batch is  -1.9080774784088135\n",
      "Val Loss for batch is  -3.3408470153808594\n",
      "|Iter  1391  | Total Val Loss  -9.149791240692139 |\n",
      "Loss for batch is  -0.7691402435302734\n",
      "Loss for batch is  -0.9896275997161865\n",
      "Loss for batch is  -0.9699457883834839\n",
      "Loss for batch is  -1.817847728729248\n",
      "|Iter  1392  | Total Train Loss  -4.546561360359192 |\n",
      "Val Loss for batch is  -2.065711736679077\n",
      "Val Loss for batch is  -2.246596574783325\n",
      "Val Loss for batch is  -2.078108787536621\n",
      "Val Loss for batch is  -3.2905869483947754\n",
      "|Iter  1392  | Total Val Loss  -9.681004047393799 |\n",
      "Loss for batch is  -0.9008103609085083\n",
      "Loss for batch is  -1.0914016962051392\n",
      "Loss for batch is  -0.978832483291626\n",
      "Loss for batch is  -1.9605798721313477\n",
      "|Iter  1393  | Total Train Loss  -4.931624412536621 |\n",
      "Val Loss for batch is  -2.0397305488586426\n",
      "Val Loss for batch is  -2.2202794551849365\n",
      "Val Loss for batch is  -2.0733962059020996\n",
      "Val Loss for batch is  -3.365326404571533\n",
      "|Iter  1393  | Total Val Loss  -9.698732614517212 |\n",
      "Loss for batch is  -0.8755086660385132\n",
      "Loss for batch is  -1.0840513706207275\n",
      "Loss for batch is  -1.0194400548934937\n",
      "Loss for batch is  -1.9609993696212769\n",
      "|Iter  1394  | Total Train Loss  -4.939999461174011 |\n",
      "Val Loss for batch is  -2.0963828563690186\n",
      "Val Loss for batch is  -2.3396530151367188\n",
      "Val Loss for batch is  -2.1588690280914307\n",
      "Val Loss for batch is  -3.3746604919433594\n",
      "|Iter  1394  | Total Val Loss  -9.969565391540527 |\n",
      "Loss for batch is  -0.9616585969924927\n",
      "Loss for batch is  -1.1182596683502197\n",
      "Loss for batch is  -1.0478010177612305\n",
      "Loss for batch is  -2.0377907752990723\n",
      "|Iter  1395  | Total Train Loss  -5.165510058403015 |\n",
      "Val Loss for batch is  -2.2032546997070312\n",
      "Val Loss for batch is  -2.3402187824249268\n",
      "Val Loss for batch is  -2.235938549041748\n",
      "Val Loss for batch is  -3.4318268299102783\n",
      "|Iter  1395  | Total Val Loss  -10.211238861083984 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.9923624992370605\n",
      "Loss for batch is  -1.1212323904037476\n",
      "Loss for batch is  -1.0604090690612793\n",
      "Loss for batch is  -2.045835494995117\n",
      "|Iter  1396  | Total Train Loss  -5.219839453697205 |\n",
      "Val Loss for batch is  -2.2640769481658936\n",
      "Val Loss for batch is  -2.3926947116851807\n",
      "Val Loss for batch is  -2.2683184146881104\n",
      "Val Loss for batch is  -3.444659948348999\n",
      "|Iter  1396  | Total Val Loss  -10.369750022888184 |\n",
      "Loss for batch is  -1.055634617805481\n",
      "Loss for batch is  -1.1921783685684204\n",
      "Loss for batch is  -1.102315902709961\n",
      "Loss for batch is  -2.0748701095581055\n",
      "|Iter  1397  | Total Train Loss  -5.424998998641968 |\n",
      "Val Loss for batch is  -2.268305540084839\n",
      "Val Loss for batch is  -2.4133477210998535\n",
      "Val Loss for batch is  -2.3178935050964355\n",
      "Val Loss for batch is  -3.485034704208374\n",
      "|Iter  1397  | Total Val Loss  -10.484581470489502 |\n",
      "Loss for batch is  -1.0653983354568481\n",
      "Loss for batch is  -1.1955139636993408\n",
      "Loss for batch is  -1.1066974401474\n",
      "Loss for batch is  -2.0983428955078125\n",
      "|Iter  1398  | Total Train Loss  -5.465952634811401 |\n",
      "Val Loss for batch is  -2.305964231491089\n",
      "Val Loss for batch is  -2.4689838886260986\n",
      "Val Loss for batch is  -2.3456807136535645\n",
      "Val Loss for batch is  -3.5091230869293213\n",
      "|Iter  1398  | Total Val Loss  -10.629751920700073 |\n",
      "Loss for batch is  -1.1078273057937622\n",
      "Loss for batch is  -1.2314091920852661\n",
      "Loss for batch is  -1.1281079053878784\n",
      "Loss for batch is  -2.119419574737549\n",
      "|Iter  1399  | Total Train Loss  -5.586763978004456 |\n",
      "Val Loss for batch is  -2.292325973510742\n",
      "Val Loss for batch is  -2.4896063804626465\n",
      "Val Loss for batch is  -2.3672430515289307\n",
      "Val Loss for batch is  -3.5370500087738037\n",
      "|Iter  1399  | Total Val Loss  -10.686225414276123 |\n",
      "Loss for batch is  -1.128088355064392\n",
      "Loss for batch is  -1.2393008470535278\n",
      "Loss for batch is  -1.1231060028076172\n",
      "Loss for batch is  -2.1436767578125\n",
      "|Iter  1400  | Total Train Loss  -5.634171962738037 |\n",
      "Val Loss for batch is  -2.277146816253662\n",
      "Val Loss for batch is  -2.4733920097351074\n",
      "Val Loss for batch is  -2.3270111083984375\n",
      "Val Loss for batch is  -3.551137924194336\n",
      "|Iter  1400  | Total Val Loss  -10.628687858581543 |\n",
      "Loss for batch is  -1.1347174644470215\n",
      "Loss for batch is  -1.2567291259765625\n",
      "Loss for batch is  -1.1499872207641602\n",
      "Loss for batch is  -2.158569812774658\n",
      "|Iter  1401  | Total Train Loss  -5.700003623962402 |\n",
      "Val Loss for batch is  -2.258721351623535\n",
      "Val Loss for batch is  -2.477292537689209\n",
      "Val Loss for batch is  -2.3842668533325195\n",
      "Val Loss for batch is  -3.567021369934082\n",
      "|Iter  1401  | Total Val Loss  -10.687302112579346 |\n",
      "Loss for batch is  -1.1476017236709595\n",
      "Loss for batch is  -1.2663251161575317\n",
      "Loss for batch is  -1.1598562002182007\n",
      "Loss for batch is  -2.1692214012145996\n",
      "|Iter  1402  | Total Train Loss  -5.7430044412612915 |\n",
      "Val Loss for batch is  -2.200803279876709\n",
      "Val Loss for batch is  -2.4919545650482178\n",
      "Val Loss for batch is  -2.3548130989074707\n",
      "Val Loss for batch is  -3.6032159328460693\n",
      "|Iter  1402  | Total Val Loss  -10.650786876678467 |\n",
      "Loss for batch is  -1.1654819250106812\n",
      "Loss for batch is  -1.2837932109832764\n",
      "Loss for batch is  -1.1681044101715088\n",
      "Loss for batch is  -2.193946123123169\n",
      "|Iter  1403  | Total Train Loss  -5.811325669288635 |\n",
      "Val Loss for batch is  -2.242473840713501\n",
      "Val Loss for batch is  -2.496887683868408\n",
      "Val Loss for batch is  -2.3557968139648438\n",
      "Val Loss for batch is  -3.5983707904815674\n",
      "|Iter  1403  | Total Val Loss  -10.69352912902832 |\n",
      "Loss for batch is  -1.1500630378723145\n",
      "Loss for batch is  -1.2967731952667236\n",
      "Loss for batch is  -1.1851422786712646\n",
      "Loss for batch is  -2.182809591293335\n",
      "|Iter  1404  | Total Train Loss  -5.814788103103638 |\n",
      "Val Loss for batch is  -2.2720699310302734\n",
      "Val Loss for batch is  -2.5271213054656982\n",
      "Val Loss for batch is  -2.3903937339782715\n",
      "Val Loss for batch is  -3.607999801635742\n",
      "|Iter  1404  | Total Val Loss  -10.797584772109985 |\n",
      "Loss for batch is  -1.1736124753952026\n",
      "Loss for batch is  -1.296718955039978\n",
      "Loss for batch is  -1.1796735525131226\n",
      "Loss for batch is  -2.21822452545166\n",
      "|Iter  1405  | Total Train Loss  -5.868229508399963 |\n",
      "Val Loss for batch is  -2.22261381149292\n",
      "Val Loss for batch is  -2.4607629776000977\n",
      "Val Loss for batch is  -2.3420722484588623\n",
      "Val Loss for batch is  -3.5801663398742676\n",
      "|Iter  1405  | Total Val Loss  -10.605615377426147 |\n",
      "Loss for batch is  -1.1427984237670898\n",
      "Loss for batch is  -1.2990880012512207\n",
      "Loss for batch is  -1.1868680715560913\n",
      "Loss for batch is  -2.1767330169677734\n",
      "|Iter  1406  | Total Train Loss  -5.805487513542175 |\n",
      "Val Loss for batch is  -2.241450548171997\n",
      "Val Loss for batch is  -2.508122682571411\n",
      "Val Loss for batch is  -2.334653377532959\n",
      "Val Loss for batch is  -3.6230247020721436\n",
      "|Iter  1406  | Total Val Loss  -10.70725131034851 |\n",
      "Loss for batch is  -1.1634749174118042\n",
      "Loss for batch is  -1.2658379077911377\n",
      "Loss for batch is  -1.1491135358810425\n",
      "Loss for batch is  -2.18945050239563\n",
      "|Iter  1407  | Total Train Loss  -5.767876863479614 |\n",
      "Val Loss for batch is  -2.0958495140075684\n",
      "Val Loss for batch is  -2.3316941261291504\n",
      "Val Loss for batch is  -2.2525222301483154\n",
      "Val Loss for batch is  -3.5857300758361816\n",
      "|Iter  1407  | Total Val Loss  -10.265795946121216 |\n",
      "Loss for batch is  -1.010301947593689\n",
      "Loss for batch is  -1.257574200630188\n",
      "Loss for batch is  -1.0332324504852295\n",
      "Loss for batch is  -2.074531078338623\n",
      "|Iter  1408  | Total Train Loss  -5.3756396770477295 |\n",
      "Val Loss for batch is  -1.9145910739898682\n",
      "Val Loss for batch is  -2.0918941497802734\n",
      "Val Loss for batch is  -2.0335190296173096\n",
      "Val Loss for batch is  -3.5026044845581055\n",
      "|Iter  1408  | Total Val Loss  -9.542608737945557 |\n",
      "Loss for batch is  -0.8223178386688232\n",
      "Loss for batch is  -1.0768589973449707\n",
      "Loss for batch is  -0.8237074613571167\n",
      "Loss for batch is  -2.019028902053833\n",
      "|Iter  1409  | Total Train Loss  -4.741913199424744 |\n",
      "Val Loss for batch is  -1.6001951694488525\n",
      "Val Loss for batch is  -1.8034123182296753\n",
      "Val Loss for batch is  -1.6546332836151123\n",
      "Val Loss for batch is  -3.3891594409942627\n",
      "|Iter  1409  | Total Val Loss  -8.447400212287903 |\n",
      "Loss for batch is  -0.6000550985336304\n",
      "Loss for batch is  -0.8783036470413208\n",
      "Loss for batch is  -0.9701701402664185\n",
      "Loss for batch is  -1.2438281774520874\n",
      "|Iter  1410  | Total Train Loss  -3.692357063293457 |\n",
      "Val Loss for batch is  -1.68438720703125\n",
      "Val Loss for batch is  -2.1320700645446777\n",
      "Val Loss for batch is  -2.1114394664764404\n",
      "Val Loss for batch is  -3.3207056522369385\n",
      "|Iter  1410  | Total Val Loss  -9.248602390289307 |\n",
      "Loss for batch is  -0.7326364517211914\n",
      "Loss for batch is  -1.0370079278945923\n",
      "Loss for batch is  -0.5478776693344116\n",
      "Loss for batch is  -1.353415608406067\n",
      "|Iter  1411  | Total Train Loss  -3.670937657356262 |\n",
      "Val Loss for batch is  -2.0575666427612305\n",
      "Val Loss for batch is  -2.2813961505889893\n",
      "Val Loss for batch is  -2.0406033992767334\n",
      "Val Loss for batch is  -3.29975962638855\n",
      "|Iter  1411  | Total Val Loss  -9.679325819015503 |\n",
      "Loss for batch is  -0.9616750478744507\n",
      "Loss for batch is  -0.9270401000976562\n",
      "Loss for batch is  -0.6181018352508545\n",
      "Loss for batch is  -1.8337395191192627\n",
      "|Iter  1412  | Total Train Loss  -4.340556502342224 |\n",
      "Val Loss for batch is  -2.0088706016540527\n",
      "Val Loss for batch is  -2.2659404277801514\n",
      "Val Loss for batch is  -2.0934133529663086\n",
      "Val Loss for batch is  -3.3101553916931152\n",
      "|Iter  1412  | Total Val Loss  -9.678379774093628 |\n",
      "Loss for batch is  -0.9334417581558228\n",
      "Loss for batch is  -1.1254686117172241\n",
      "Loss for batch is  -0.8494753837585449\n",
      "Loss for batch is  -1.7840335369110107\n",
      "|Iter  1413  | Total Train Loss  -4.6924192905426025 |\n",
      "Val Loss for batch is  -2.119838237762451\n",
      "Val Loss for batch is  -2.265958070755005\n",
      "Val Loss for batch is  -2.024106740951538\n",
      "Val Loss for batch is  -3.307967185974121\n",
      "|Iter  1413  | Total Val Loss  -9.717870235443115 |\n",
      "Loss for batch is  -0.9499465227127075\n",
      "Loss for batch is  -1.1248807907104492\n",
      "Loss for batch is  -0.9416166543960571\n",
      "Loss for batch is  -1.90636146068573\n",
      "|Iter  1414  | Total Train Loss  -4.922805428504944 |\n",
      "Val Loss for batch is  -2.1116209030151367\n",
      "Val Loss for batch is  -2.2437264919281006\n",
      "Val Loss for batch is  -2.068897247314453\n",
      "Val Loss for batch is  -3.202888011932373\n",
      "|Iter  1414  | Total Val Loss  -9.627132654190063 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.9534512758255005\n",
      "Loss for batch is  -1.108593225479126\n",
      "Loss for batch is  -1.0341135263442993\n",
      "Loss for batch is  -1.9774447679519653\n",
      "|Iter  1415  | Total Train Loss  -5.073602795600891 |\n",
      "Val Loss for batch is  -2.1715219020843506\n",
      "Val Loss for batch is  -2.337519645690918\n",
      "Val Loss for batch is  -2.196467399597168\n",
      "Val Loss for batch is  -3.3682305812835693\n",
      "|Iter  1415  | Total Val Loss  -10.073739528656006 |\n",
      "Loss for batch is  -1.0140146017074585\n",
      "Loss for batch is  -1.1453192234039307\n",
      "Loss for batch is  -1.031920075416565\n",
      "Loss for batch is  -2.0032718181610107\n",
      "|Iter  1416  | Total Train Loss  -5.194525718688965 |\n",
      "Val Loss for batch is  -2.241539478302002\n",
      "Val Loss for batch is  -2.409420967102051\n",
      "Val Loss for batch is  -2.277869701385498\n",
      "Val Loss for batch is  -3.4317498207092285\n",
      "|Iter  1416  | Total Val Loss  -10.36057996749878 |\n",
      "Loss for batch is  -1.0637760162353516\n",
      "Loss for batch is  -1.173459529876709\n",
      "Loss for batch is  -1.0583553314208984\n",
      "Loss for batch is  -2.02620792388916\n",
      "|Iter  1417  | Total Train Loss  -5.321798801422119 |\n",
      "Val Loss for batch is  -2.2599642276763916\n",
      "Val Loss for batch is  -2.424649953842163\n",
      "Val Loss for batch is  -2.31392502784729\n",
      "Val Loss for batch is  -3.44598650932312\n",
      "|Iter  1417  | Total Val Loss  -10.444525718688965 |\n",
      "Loss for batch is  -1.087999701499939\n",
      "Loss for batch is  -1.193170189857483\n",
      "Loss for batch is  -1.0783579349517822\n",
      "Loss for batch is  -2.0464701652526855\n",
      "|Iter  1418  | Total Train Loss  -5.40599799156189 |\n",
      "Val Loss for batch is  -2.2374167442321777\n",
      "Val Loss for batch is  -2.4417033195495605\n",
      "Val Loss for batch is  -2.332359790802002\n",
      "Val Loss for batch is  -3.4790852069854736\n",
      "|Iter  1418  | Total Val Loss  -10.490565061569214 |\n",
      "Loss for batch is  -1.1133155822753906\n",
      "Loss for batch is  -1.2238531112670898\n",
      "Loss for batch is  -1.1059998273849487\n",
      "Loss for batch is  -2.079505681991577\n",
      "|Iter  1419  | Total Train Loss  -5.522674202919006 |\n",
      "Val Loss for batch is  -2.254791021347046\n",
      "Val Loss for batch is  -2.4345526695251465\n",
      "Val Loss for batch is  -2.343818426132202\n",
      "Val Loss for batch is  -3.5143778324127197\n",
      "|Iter  1419  | Total Val Loss  -10.547539949417114 |\n",
      "Loss for batch is  -1.1221230030059814\n",
      "Loss for batch is  -1.2419744729995728\n",
      "Loss for batch is  -1.1321498155593872\n",
      "Loss for batch is  -2.114720106124878\n",
      "|Iter  1420  | Total Train Loss  -5.610967397689819 |\n",
      "Val Loss for batch is  -2.261270046234131\n",
      "Val Loss for batch is  -2.459484338760376\n",
      "Val Loss for batch is  -2.341461420059204\n",
      "Val Loss for batch is  -3.5161492824554443\n",
      "|Iter  1420  | Total Val Loss  -10.578365087509155 |\n",
      "Loss for batch is  -1.1343270540237427\n",
      "Loss for batch is  -1.250396728515625\n",
      "Loss for batch is  -1.1490806341171265\n",
      "Loss for batch is  -2.142849922180176\n",
      "|Iter  1421  | Total Train Loss  -5.67665433883667 |\n",
      "Val Loss for batch is  -2.2601277828216553\n",
      "Val Loss for batch is  -2.4948391914367676\n",
      "Val Loss for batch is  -2.3499348163604736\n",
      "Val Loss for batch is  -3.54229474067688\n",
      "|Iter  1421  | Total Val Loss  -10.647196531295776 |\n",
      "Loss for batch is  -1.1515151262283325\n",
      "Loss for batch is  -1.2720333337783813\n",
      "Loss for batch is  -1.1615902185440063\n",
      "Loss for batch is  -2.1634583473205566\n",
      "|Iter  1422  | Total Train Loss  -5.748597025871277 |\n",
      "Val Loss for batch is  -2.2390477657318115\n",
      "Val Loss for batch is  -2.497260808944702\n",
      "Val Loss for batch is  -2.373011827468872\n",
      "Val Loss for batch is  -3.570298433303833\n",
      "|Iter  1422  | Total Val Loss  -10.679618835449219 |\n",
      "Loss for batch is  -1.1662846803665161\n",
      "Loss for batch is  -1.2841558456420898\n",
      "Loss for batch is  -1.1749440431594849\n",
      "Loss for batch is  -2.1835386753082275\n",
      "|Iter  1423  | Total Train Loss  -5.808923244476318 |\n",
      "Val Loss for batch is  -2.237149953842163\n",
      "Val Loss for batch is  -2.4979076385498047\n",
      "Val Loss for batch is  -2.36468768119812\n",
      "Val Loss for batch is  -3.5684614181518555\n",
      "|Iter  1423  | Total Val Loss  -10.668206691741943 |\n",
      "Loss for batch is  -1.1712690591812134\n",
      "Loss for batch is  -1.2929636240005493\n",
      "Loss for batch is  -1.1752910614013672\n",
      "Loss for batch is  -2.2079498767852783\n",
      "|Iter  1424  | Total Train Loss  -5.847473621368408 |\n",
      "Val Loss for batch is  -2.228168487548828\n",
      "Val Loss for batch is  -2.5136494636535645\n",
      "Val Loss for batch is  -2.369494676589966\n",
      "Val Loss for batch is  -3.60996675491333\n",
      "|Iter  1424  | Total Val Loss  -10.721279382705688 |\n",
      "Loss for batch is  -1.1661306619644165\n",
      "Loss for batch is  -1.3013590574264526\n",
      "Loss for batch is  -1.1937017440795898\n",
      "Loss for batch is  -2.2097830772399902\n",
      "|Iter  1425  | Total Train Loss  -5.870974540710449 |\n",
      "Val Loss for batch is  -2.1955246925354004\n",
      "Val Loss for batch is  -2.50897216796875\n",
      "Val Loss for batch is  -2.3701329231262207\n",
      "Val Loss for batch is  -3.5657432079315186\n",
      "|Iter  1425  | Total Val Loss  -10.64037299156189 |\n",
      "Loss for batch is  -1.1849812269210815\n",
      "Loss for batch is  -1.295211911201477\n",
      "Loss for batch is  -1.2080250978469849\n",
      "Loss for batch is  -2.228809356689453\n",
      "|Iter  1426  | Total Train Loss  -5.917027592658997 |\n",
      "Val Loss for batch is  -2.2838687896728516\n",
      "Val Loss for batch is  -2.4769933223724365\n",
      "Val Loss for batch is  -2.357344388961792\n",
      "Val Loss for batch is  -3.6064813137054443\n",
      "|Iter  1426  | Total Val Loss  -10.724687814712524 |\n",
      "Loss for batch is  -1.1757100820541382\n",
      "Loss for batch is  -1.3111494779586792\n",
      "Loss for batch is  -1.1779117584228516\n",
      "Loss for batch is  -2.2203421592712402\n",
      "|Iter  1427  | Total Train Loss  -5.885113477706909 |\n",
      "Val Loss for batch is  -2.1898417472839355\n",
      "Val Loss for batch is  -2.446828603744507\n",
      "Val Loss for batch is  -2.326594829559326\n",
      "Val Loss for batch is  -3.604635715484619\n",
      "|Iter  1427  | Total Val Loss  -10.567900896072388 |\n",
      "Loss for batch is  -1.1441383361816406\n",
      "Loss for batch is  -1.299322485923767\n",
      "Loss for batch is  -1.1708347797393799\n",
      "Loss for batch is  -2.2376317977905273\n",
      "|Iter  1428  | Total Train Loss  -5.851927399635315 |\n",
      "Val Loss for batch is  -2.2130002975463867\n",
      "Val Loss for batch is  -2.4764769077301025\n",
      "Val Loss for batch is  -2.3340578079223633\n",
      "Val Loss for batch is  -3.622129201889038\n",
      "|Iter  1428  | Total Val Loss  -10.64566421508789 |\n",
      "Loss for batch is  -1.1418453454971313\n",
      "Loss for batch is  -1.289588212966919\n",
      "Loss for batch is  -1.1643744707107544\n",
      "Loss for batch is  -2.1957485675811768\n",
      "|Iter  1429  | Total Train Loss  -5.7915565967559814 |\n",
      "Val Loss for batch is  -2.10347318649292\n",
      "Val Loss for batch is  -2.3534767627716064\n",
      "Val Loss for batch is  -2.262284755706787\n",
      "Val Loss for batch is  -3.627393960952759\n",
      "|Iter  1429  | Total Val Loss  -10.346628665924072 |\n",
      "Loss for batch is  -1.0825731754302979\n",
      "Loss for batch is  -1.2371538877487183\n",
      "Loss for batch is  -1.0316622257232666\n",
      "Loss for batch is  -2.2159266471862793\n",
      "|Iter  1430  | Total Train Loss  -5.567315936088562 |\n",
      "Val Loss for batch is  -1.847453236579895\n",
      "Val Loss for batch is  -2.0398571491241455\n",
      "Val Loss for batch is  -2.069073438644409\n",
      "Val Loss for batch is  -3.5524137020111084\n",
      "|Iter  1430  | Total Val Loss  -9.508797526359558 |\n",
      "Loss for batch is  -0.8376477956771851\n",
      "Loss for batch is  -1.2474371194839478\n",
      "Loss for batch is  -0.8149420022964478\n",
      "Loss for batch is  -2.0962724685668945\n",
      "|Iter  1431  | Total Train Loss  -4.996299386024475 |\n",
      "Val Loss for batch is  -2.026940107345581\n",
      "Val Loss for batch is  -2.2282209396362305\n",
      "Val Loss for batch is  -2.003713846206665\n",
      "Val Loss for batch is  -3.5595500469207764\n",
      "|Iter  1431  | Total Val Loss  -9.818424940109253 |\n",
      "Loss for batch is  -0.917920708656311\n",
      "Loss for batch is  -0.9985402822494507\n",
      "Loss for batch is  -1.0374410152435303\n",
      "Loss for batch is  -1.935042142868042\n",
      "|Iter  1432  | Total Train Loss  -4.888944149017334 |\n",
      "Val Loss for batch is  -1.8586207628250122\n",
      "Val Loss for batch is  -2.142080307006836\n",
      "Val Loss for batch is  -2.0887274742126465\n",
      "Val Loss for batch is  -3.404557466506958\n",
      "|Iter  1432  | Total Val Loss  -9.493986010551453 |\n",
      "Loss for batch is  -0.8826812505722046\n",
      "Loss for batch is  -1.1496411561965942\n",
      "Loss for batch is  -0.8874542713165283\n",
      "Loss for batch is  -1.9970325231552124\n",
      "|Iter  1433  | Total Train Loss  -4.9168092012405396 |\n",
      "Val Loss for batch is  -2.1097893714904785\n",
      "Val Loss for batch is  -2.4027273654937744\n",
      "Val Loss for batch is  -2.2226150035858154\n",
      "Val Loss for batch is  -3.469114303588867\n",
      "|Iter  1433  | Total Val Loss  -10.204246044158936 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.0639665126800537\n",
      "Loss for batch is  -1.1727895736694336\n",
      "Loss for batch is  -1.0045875310897827\n",
      "Loss for batch is  -2.1303234100341797\n",
      "|Iter  1434  | Total Train Loss  -5.37166702747345 |\n",
      "Val Loss for batch is  -2.112093687057495\n",
      "Val Loss for batch is  -2.368443489074707\n",
      "Val Loss for batch is  -2.221616506576538\n",
      "Val Loss for batch is  -3.573591709136963\n",
      "|Iter  1434  | Total Val Loss  -10.275745391845703 |\n",
      "Loss for batch is  -1.070346474647522\n",
      "Loss for batch is  -1.1655833721160889\n",
      "Loss for batch is  -1.0673165321350098\n",
      "Loss for batch is  -2.1807379722595215\n",
      "|Iter  1435  | Total Train Loss  -5.483984351158142 |\n",
      "Val Loss for batch is  -2.210691452026367\n",
      "Val Loss for batch is  -2.4585821628570557\n",
      "Val Loss for batch is  -2.30820631980896\n",
      "Val Loss for batch is  -3.5421972274780273\n",
      "|Iter  1435  | Total Val Loss  -10.51967716217041 |\n",
      "Loss for batch is  -1.1275476217269897\n",
      "Loss for batch is  -1.2119050025939941\n",
      "Loss for batch is  -1.112106204032898\n",
      "Loss for batch is  -2.182955741882324\n",
      "|Iter  1436  | Total Train Loss  -5.634514570236206 |\n",
      "Val Loss for batch is  -2.244417667388916\n",
      "Val Loss for batch is  -2.4293594360351562\n",
      "Val Loss for batch is  -2.3166019916534424\n",
      "Val Loss for batch is  -3.5719425678253174\n",
      "|Iter  1436  | Total Val Loss  -10.562321662902832 |\n",
      "Loss for batch is  -1.1378880739212036\n",
      "Loss for batch is  -1.252751350402832\n",
      "Loss for batch is  -1.150794506072998\n",
      "Loss for batch is  -2.1870508193969727\n",
      "|Iter  1437  | Total Train Loss  -5.728484749794006 |\n",
      "Val Loss for batch is  -2.242483615875244\n",
      "Val Loss for batch is  -2.389122724533081\n",
      "Val Loss for batch is  -2.358891010284424\n",
      "Val Loss for batch is  -3.5494604110717773\n",
      "|Iter  1437  | Total Val Loss  -10.539957761764526 |\n",
      "Loss for batch is  -1.149301528930664\n",
      "Loss for batch is  -1.285465955734253\n",
      "Loss for batch is  -1.1728649139404297\n",
      "Loss for batch is  -2.2088513374328613\n",
      "|Iter  1438  | Total Train Loss  -5.816483736038208 |\n",
      "Val Loss for batch is  -2.277466058731079\n",
      "Val Loss for batch is  -2.4564125537872314\n",
      "Val Loss for batch is  -2.3906805515289307\n",
      "Val Loss for batch is  -3.590620994567871\n",
      "|Iter  1438  | Total Val Loss  -10.715180158615112 |\n",
      "Loss for batch is  -1.1737847328186035\n",
      "Loss for batch is  -1.287726640701294\n",
      "Loss for batch is  -1.2014638185501099\n",
      "Loss for batch is  -2.2216508388519287\n",
      "|Iter  1439  | Total Train Loss  -5.884626030921936 |\n",
      "Val Loss for batch is  -2.3032052516937256\n",
      "Val Loss for batch is  -2.475212812423706\n",
      "Val Loss for batch is  -2.3284101486206055\n",
      "Val Loss for batch is  -3.6108665466308594\n",
      "|Iter  1439  | Total Val Loss  -10.717694759368896 |\n",
      "Loss for batch is  -1.1844549179077148\n",
      "Loss for batch is  -1.3111908435821533\n",
      "Loss for batch is  -1.2126604318618774\n",
      "Loss for batch is  -2.2335000038146973\n",
      "|Iter  1440  | Total Train Loss  -5.941806197166443 |\n",
      "Val Loss for batch is  -2.2905519008636475\n",
      "Val Loss for batch is  -2.4981908798217773\n",
      "Val Loss for batch is  -2.3694403171539307\n",
      "Val Loss for batch is  -3.612384796142578\n",
      "|Iter  1440  | Total Val Loss  -10.770567893981934 |\n",
      "Loss for batch is  -1.1829315423965454\n",
      "Loss for batch is  -1.3219610452651978\n",
      "Loss for batch is  -1.2224640846252441\n",
      "Loss for batch is  -2.2358474731445312\n",
      "|Iter  1441  | Total Train Loss  -5.9632041454315186 |\n",
      "Val Loss for batch is  -2.2676432132720947\n",
      "Val Loss for batch is  -2.539482831954956\n",
      "Val Loss for batch is  -2.3685240745544434\n",
      "Val Loss for batch is  -3.642270565032959\n",
      "|Iter  1441  | Total Val Loss  -10.817920684814453 |\n",
      "Loss for batch is  -1.1998498439788818\n",
      "Loss for batch is  -1.332157850265503\n",
      "Loss for batch is  -1.2028331756591797\n",
      "Loss for batch is  -2.2496163845062256\n",
      "|Iter  1442  | Total Train Loss  -5.98445725440979 |\n",
      "Val Loss for batch is  -2.286893606185913\n",
      "Val Loss for batch is  -2.5439987182617188\n",
      "Val Loss for batch is  -2.3874878883361816\n",
      "Val Loss for batch is  -3.6335902214050293\n",
      "|Iter  1442  | Total Val Loss  -10.851970434188843 |\n",
      "Loss for batch is  -1.2226293087005615\n",
      "Loss for batch is  -1.3479362726211548\n",
      "Loss for batch is  -1.2225226163864136\n",
      "Loss for batch is  -2.2599048614501953\n",
      "|Iter  1443  | Total Train Loss  -6.052993059158325 |\n",
      "Val Loss for batch is  -2.2900655269622803\n",
      "Val Loss for batch is  -2.564363479614258\n",
      "Val Loss for batch is  -2.3927905559539795\n",
      "Val Loss for batch is  -3.6489861011505127\n",
      "|Iter  1443  | Total Val Loss  -10.89620566368103 |\n",
      "Loss for batch is  -1.2243014574050903\n",
      "Loss for batch is  -1.3414112329483032\n",
      "Loss for batch is  -1.2267967462539673\n",
      "Loss for batch is  -2.273512363433838\n",
      "|Iter  1444  | Total Train Loss  -6.066021800041199 |\n",
      "Val Loss for batch is  -2.2352466583251953\n",
      "Val Loss for batch is  -2.551992893218994\n",
      "Val Loss for batch is  -2.3723654747009277\n",
      "Val Loss for batch is  -3.6717417240142822\n",
      "|Iter  1444  | Total Val Loss  -10.8313467502594 |\n",
      "Loss for batch is  -1.2000356912612915\n",
      "Loss for batch is  -1.3458143472671509\n",
      "Loss for batch is  -1.2193633317947388\n",
      "Loss for batch is  -2.2657742500305176\n",
      "|Iter  1445  | Total Train Loss  -6.030987620353699 |\n",
      "Val Loss for batch is  -2.275768518447876\n",
      "Val Loss for batch is  -2.530155897140503\n",
      "Val Loss for batch is  -2.397068977355957\n",
      "Val Loss for batch is  -3.621732473373413\n",
      "|Iter  1445  | Total Val Loss  -10.824725866317749 |\n",
      "Loss for batch is  -1.189368486404419\n",
      "Loss for batch is  -1.3338875770568848\n",
      "Loss for batch is  -1.2103554010391235\n",
      "Loss for batch is  -2.2400856018066406\n",
      "|Iter  1446  | Total Train Loss  -5.973697066307068 |\n",
      "Val Loss for batch is  -2.2413315773010254\n",
      "Val Loss for batch is  -2.485464572906494\n",
      "Val Loss for batch is  -2.3384664058685303\n",
      "Val Loss for batch is  -3.6568546295166016\n",
      "|Iter  1446  | Total Val Loss  -10.722117185592651 |\n",
      "Loss for batch is  -1.1757558584213257\n",
      "Loss for batch is  -1.2922306060791016\n",
      "Loss for batch is  -1.1895238161087036\n",
      "Loss for batch is  -2.2251715660095215\n",
      "|Iter  1447  | Total Train Loss  -5.882681846618652 |\n",
      "Val Loss for batch is  -2.128666877746582\n",
      "Val Loss for batch is  -2.3743770122528076\n",
      "Val Loss for batch is  -2.2688279151916504\n",
      "Val Loss for batch is  -3.5968189239501953\n",
      "|Iter  1447  | Total Val Loss  -10.368690729141235 |\n",
      "Loss for batch is  -1.052729845046997\n",
      "Loss for batch is  -1.2978403568267822\n",
      "Loss for batch is  -1.0440667867660522\n",
      "Loss for batch is  -2.0951852798461914\n",
      "|Iter  1448  | Total Train Loss  -5.489822268486023 |\n",
      "Val Loss for batch is  -1.8547905683517456\n",
      "Val Loss for batch is  -2.1090505123138428\n",
      "Val Loss for batch is  -1.9369196891784668\n",
      "Val Loss for batch is  -3.534965991973877\n",
      "|Iter  1448  | Total Val Loss  -9.435726761817932 |\n",
      "Loss for batch is  -0.7866007089614868\n",
      "Loss for batch is  -1.0874321460723877\n",
      "Loss for batch is  -0.967225193977356\n",
      "Loss for batch is  -1.7004936933517456\n",
      "|Iter  1449  | Total Train Loss  -4.541751742362976 |\n",
      "Val Loss for batch is  -1.9971195459365845\n",
      "Val Loss for batch is  -2.3185720443725586\n",
      "Val Loss for batch is  -2.1432602405548096\n",
      "Val Loss for batch is  -3.5871541500091553\n",
      "|Iter  1449  | Total Val Loss  -10.046105980873108 |\n",
      "Loss for batch is  -0.995164155960083\n",
      "Loss for batch is  -1.062281608581543\n",
      "Loss for batch is  -0.7359870672225952\n",
      "Loss for batch is  -1.8441399335861206\n",
      "|Iter  1450  | Total Train Loss  -4.637572765350342 |\n",
      "Val Loss for batch is  -1.8256311416625977\n",
      "Val Loss for batch is  -2.129706621170044\n",
      "Val Loss for batch is  -2.1071465015411377\n",
      "Val Loss for batch is  -3.494218349456787\n",
      "|Iter  1450  | Total Val Loss  -9.556702613830566 |\n",
      "Loss for batch is  -0.9049673080444336\n",
      "Loss for batch is  -0.6966714859008789\n",
      "Loss for batch is  -0.9517852067947388\n",
      "Loss for batch is  -2.114605188369751\n",
      "|Iter  1451  | Total Train Loss  -4.668029189109802 |\n",
      "Val Loss for batch is  -1.8441519737243652\n",
      "Val Loss for batch is  -2.170538902282715\n",
      "Val Loss for batch is  -2.0375428199768066\n",
      "Val Loss for batch is  -3.347672939300537\n",
      "|Iter  1451  | Total Val Loss  -9.399906635284424 |\n",
      "Loss for batch is  -0.9283276796340942\n",
      "Loss for batch is  -1.0468432903289795\n",
      "Loss for batch is  -1.0226999521255493\n",
      "Loss for batch is  -2.1372570991516113\n",
      "|Iter  1452  | Total Train Loss  -5.135128021240234 |\n",
      "Val Loss for batch is  -2.065603494644165\n",
      "Val Loss for batch is  -2.2767410278320312\n",
      "Val Loss for batch is  -2.169370174407959\n",
      "Val Loss for batch is  -3.456681489944458\n",
      "|Iter  1452  | Total Val Loss  -9.968396186828613 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.0317332744598389\n",
      "Loss for batch is  -1.1143463850021362\n",
      "Loss for batch is  -1.0314338207244873\n",
      "Loss for batch is  -2.1235690116882324\n",
      "|Iter  1453  | Total Train Loss  -5.301082491874695 |\n",
      "Val Loss for batch is  -2.188830614089966\n",
      "Val Loss for batch is  -2.2713446617126465\n",
      "Val Loss for batch is  -2.1929574012756348\n",
      "Val Loss for batch is  -3.5649189949035645\n",
      "|Iter  1453  | Total Val Loss  -10.218051671981812 |\n",
      "Loss for batch is  -1.0899860858917236\n",
      "Loss for batch is  -1.1563278436660767\n",
      "Loss for batch is  -1.0172570943832397\n",
      "Loss for batch is  -2.1012680530548096\n",
      "|Iter  1454  | Total Train Loss  -5.36483907699585 |\n",
      "Val Loss for batch is  -2.212379217147827\n",
      "Val Loss for batch is  -2.3714659214019775\n",
      "Val Loss for batch is  -2.2765023708343506\n",
      "Val Loss for batch is  -3.5634398460388184\n",
      "|Iter  1454  | Total Val Loss  -10.423787355422974 |\n",
      "Loss for batch is  -1.1275721788406372\n",
      "Loss for batch is  -1.2275463342666626\n",
      "Loss for batch is  -1.09685480594635\n",
      "Loss for batch is  -2.056874990463257\n",
      "|Iter  1455  | Total Train Loss  -5.508848309516907 |\n",
      "Val Loss for batch is  -2.2682571411132812\n",
      "Val Loss for batch is  -2.4140586853027344\n",
      "Val Loss for batch is  -2.3546929359436035\n",
      "Val Loss for batch is  -3.5643117427825928\n",
      "|Iter  1455  | Total Val Loss  -10.601320505142212 |\n",
      "Loss for batch is  -1.1485803127288818\n",
      "Loss for batch is  -1.2434550523757935\n",
      "Loss for batch is  -1.1176856756210327\n",
      "Loss for batch is  -2.0996551513671875\n",
      "|Iter  1456  | Total Train Loss  -5.6093761920928955 |\n",
      "Val Loss for batch is  -2.2673511505126953\n",
      "Val Loss for batch is  -2.4527158737182617\n",
      "Val Loss for batch is  -2.3097050189971924\n",
      "Val Loss for batch is  -3.561436176300049\n",
      "|Iter  1456  | Total Val Loss  -10.591208219528198 |\n",
      "Loss for batch is  -1.1638747453689575\n",
      "Loss for batch is  -1.2755584716796875\n",
      "Loss for batch is  -1.13956618309021\n",
      "Loss for batch is  -2.134852409362793\n",
      "|Iter  1457  | Total Train Loss  -5.713851809501648 |\n",
      "Val Loss for batch is  -2.3132882118225098\n",
      "Val Loss for batch is  -2.4750468730926514\n",
      "Val Loss for batch is  -2.3810768127441406\n",
      "Val Loss for batch is  -3.5664896965026855\n",
      "|Iter  1457  | Total Val Loss  -10.735901594161987 |\n",
      "Loss for batch is  -1.1758573055267334\n",
      "Loss for batch is  -1.2911858558654785\n",
      "Loss for batch is  -1.1755584478378296\n",
      "Loss for batch is  -2.1513054370880127\n",
      "|Iter  1458  | Total Train Loss  -5.793907046318054 |\n",
      "Val Loss for batch is  -2.2934632301330566\n",
      "Val Loss for batch is  -2.4971697330474854\n",
      "Val Loss for batch is  -2.3411200046539307\n",
      "Val Loss for batch is  -3.585446357727051\n",
      "|Iter  1458  | Total Val Loss  -10.717199325561523 |\n",
      "Loss for batch is  -1.1856967210769653\n",
      "Loss for batch is  -1.2931246757507324\n",
      "Loss for batch is  -1.1925408840179443\n",
      "Loss for batch is  -2.185457706451416\n",
      "|Iter  1459  | Total Train Loss  -5.856819987297058 |\n",
      "Val Loss for batch is  -2.289320230484009\n",
      "Val Loss for batch is  -2.5098400115966797\n",
      "Val Loss for batch is  -2.3865368366241455\n",
      "Val Loss for batch is  -3.620920181274414\n",
      "|Iter  1459  | Total Val Loss  -10.806617259979248 |\n",
      "Loss for batch is  -1.2102066278457642\n",
      "Loss for batch is  -1.317743182182312\n",
      "Loss for batch is  -1.2019225358963013\n",
      "Loss for batch is  -2.2093920707702637\n",
      "|Iter  1460  | Total Train Loss  -5.939264416694641 |\n",
      "Val Loss for batch is  -2.2755162715911865\n",
      "Val Loss for batch is  -2.511843681335449\n",
      "Val Loss for batch is  -2.4088191986083984\n",
      "Val Loss for batch is  -3.616699457168579\n",
      "|Iter  1460  | Total Val Loss  -10.812878608703613 |\n",
      "Loss for batch is  -1.2086451053619385\n",
      "Loss for batch is  -1.338429570198059\n",
      "Loss for batch is  -1.197786569595337\n",
      "Loss for batch is  -2.239379405975342\n",
      "|Iter  1461  | Total Train Loss  -5.984240651130676 |\n",
      "Val Loss for batch is  -2.2658190727233887\n",
      "Val Loss for batch is  -2.5158276557922363\n",
      "Val Loss for batch is  -2.3525710105895996\n",
      "Val Loss for batch is  -3.6409051418304443\n",
      "|Iter  1461  | Total Val Loss  -10.775122880935669 |\n",
      "Loss for batch is  -1.2277603149414062\n",
      "Loss for batch is  -1.3341940641403198\n",
      "Loss for batch is  -1.2219607830047607\n",
      "Loss for batch is  -2.271883249282837\n",
      "|Iter  1462  | Total Train Loss  -6.055798411369324 |\n",
      "Val Loss for batch is  -2.2446370124816895\n",
      "Val Loss for batch is  -2.5440165996551514\n",
      "Val Loss for batch is  -2.3990910053253174\n",
      "Val Loss for batch is  -3.659911632537842\n",
      "|Iter  1462  | Total Val Loss  -10.84765625 |\n",
      "Loss for batch is  -1.218161940574646\n",
      "Loss for batch is  -1.343111515045166\n",
      "Loss for batch is  -1.2330952882766724\n",
      "Loss for batch is  -2.2708840370178223\n",
      "|Iter  1463  | Total Train Loss  -6.065252780914307 |\n",
      "Val Loss for batch is  -2.29526948928833\n",
      "Val Loss for batch is  -2.545236587524414\n",
      "Val Loss for batch is  -2.41739821434021\n",
      "Val Loss for batch is  -3.6464340686798096\n",
      "|Iter  1463  | Total Val Loss  -10.904338359832764 |\n",
      "Loss for batch is  -1.2280733585357666\n",
      "Loss for batch is  -1.350241780281067\n",
      "Loss for batch is  -1.2528090476989746\n",
      "Loss for batch is  -2.2893834114074707\n",
      "|Iter  1464  | Total Train Loss  -6.120507597923279 |\n",
      "Val Loss for batch is  -2.2379746437072754\n",
      "Val Loss for batch is  -2.536222219467163\n",
      "Val Loss for batch is  -2.3763909339904785\n",
      "Val Loss for batch is  -3.689702272415161\n",
      "|Iter  1464  | Total Val Loss  -10.840290069580078 |\n",
      "Loss for batch is  -1.2295515537261963\n",
      "Loss for batch is  -1.3614916801452637\n",
      "Loss for batch is  -1.2521641254425049\n",
      "Loss for batch is  -2.2817587852478027\n",
      "|Iter  1465  | Total Train Loss  -6.124966144561768 |\n",
      "Val Loss for batch is  -2.2793352603912354\n",
      "Val Loss for batch is  -2.533785820007324\n",
      "Val Loss for batch is  -2.3685643672943115\n",
      "Val Loss for batch is  -3.681103229522705\n",
      "|Iter  1465  | Total Val Loss  -10.862788677215576 |\n",
      "Loss for batch is  -1.2340333461761475\n",
      "Loss for batch is  -1.3524696826934814\n",
      "Loss for batch is  -1.2632026672363281\n",
      "Loss for batch is  -2.3122215270996094\n",
      "|Iter  1466  | Total Train Loss  -6.161927223205566 |\n",
      "Val Loss for batch is  -2.2389872074127197\n",
      "Val Loss for batch is  -2.5206525325775146\n",
      "Val Loss for batch is  -2.3711538314819336\n",
      "Val Loss for batch is  -3.6824846267700195\n",
      "|Iter  1466  | Total Val Loss  -10.813278198242188 |\n",
      "Loss for batch is  -1.2001854181289673\n",
      "Loss for batch is  -1.3684488534927368\n",
      "Loss for batch is  -1.2176374197006226\n",
      "Loss for batch is  -2.28932523727417\n",
      "|Iter  1467  | Total Train Loss  -6.075596928596497 |\n",
      "Val Loss for batch is  -2.19303297996521\n",
      "Val Loss for batch is  -2.5207290649414062\n",
      "Val Loss for batch is  -2.37919545173645\n",
      "Val Loss for batch is  -3.6535136699676514\n",
      "|Iter  1467  | Total Val Loss  -10.746471166610718 |\n",
      "Loss for batch is  -1.193303108215332\n",
      "Loss for batch is  -1.3017232418060303\n",
      "Loss for batch is  -1.2481248378753662\n",
      "Loss for batch is  -2.2560715675354004\n",
      "|Iter  1468  | Total Train Loss  -5.999222755432129 |\n",
      "Val Loss for batch is  -2.172774076461792\n",
      "Val Loss for batch is  -2.5241005420684814\n",
      "Val Loss for batch is  -2.382176637649536\n",
      "Val Loss for batch is  -3.6939849853515625\n",
      "|Iter  1468  | Total Val Loss  -10.773036241531372 |\n",
      "Loss for batch is  -1.169747233390808\n",
      "Loss for batch is  -1.336858868598938\n",
      "Loss for batch is  -1.0857313871383667\n",
      "Loss for batch is  -2.2499923706054688\n",
      "|Iter  1469  | Total Train Loss  -5.8423298597335815 |\n",
      "Val Loss for batch is  -1.8314424753189087\n",
      "Val Loss for batch is  -2.134363889694214\n",
      "Val Loss for batch is  -2.0207324028015137\n",
      "Val Loss for batch is  -3.594168186187744\n",
      "|Iter  1469  | Total Val Loss  -9.58070695400238 |\n",
      "Loss for batch is  -0.8557866811752319\n",
      "Loss for batch is  -1.128705382347107\n",
      "Loss for batch is  -1.1815485954284668\n",
      "Loss for batch is  -1.9188611507415771\n",
      "|Iter  1470  | Total Train Loss  -5.084901809692383 |\n",
      "Val Loss for batch is  -2.131650447845459\n",
      "Val Loss for batch is  -2.4189109802246094\n",
      "Val Loss for batch is  -2.2650890350341797\n",
      "Val Loss for batch is  -3.6033449172973633\n",
      "|Iter  1470  | Total Val Loss  -10.418995380401611 |\n",
      "Loss for batch is  -1.1243165731430054\n",
      "Loss for batch is  -1.2484694719314575\n",
      "Loss for batch is  -0.9675408601760864\n",
      "Loss for batch is  -1.9104597568511963\n",
      "|Iter  1471  | Total Train Loss  -5.250786662101746 |\n",
      "Val Loss for batch is  -1.7627750635147095\n",
      "Val Loss for batch is  -1.9822787046432495\n",
      "Val Loss for batch is  -1.938050627708435\n",
      "Val Loss for batch is  -3.5530850887298584\n",
      "|Iter  1471  | Total Val Loss  -9.236189484596252 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.8027266263961792\n",
      "Loss for batch is  -0.9919761419296265\n",
      "Loss for batch is  -1.0634807348251343\n",
      "Loss for batch is  -2.0788156986236572\n",
      "|Iter  1472  | Total Train Loss  -4.936999201774597 |\n",
      "Val Loss for batch is  -1.9613925218582153\n",
      "Val Loss for batch is  -2.2721052169799805\n",
      "Val Loss for batch is  -2.063225269317627\n",
      "Val Loss for batch is  -3.505725860595703\n",
      "|Iter  1472  | Total Val Loss  -9.802448868751526 |\n",
      "Loss for batch is  -0.9749610424041748\n",
      "Loss for batch is  -1.1818119287490845\n",
      "Loss for batch is  -1.0790989398956299\n",
      "Loss for batch is  -2.0261168479919434\n",
      "|Iter  1473  | Total Train Loss  -5.2619887590408325 |\n",
      "Val Loss for batch is  -2.0894665718078613\n",
      "Val Loss for batch is  -2.3250715732574463\n",
      "Val Loss for batch is  -2.0963656902313232\n",
      "Val Loss for batch is  -3.4694511890411377\n",
      "|Iter  1473  | Total Val Loss  -9.980355024337769 |\n",
      "Loss for batch is  -1.0676332712173462\n",
      "Loss for batch is  -1.1971598863601685\n",
      "Loss for batch is  -1.111363172531128\n",
      "Loss for batch is  -2.17457914352417\n",
      "|Iter  1474  | Total Train Loss  -5.5507354736328125 |\n",
      "Val Loss for batch is  -2.06729793548584\n",
      "Val Loss for batch is  -2.3155431747436523\n",
      "Val Loss for batch is  -2.189767837524414\n",
      "Val Loss for batch is  -3.557316780090332\n",
      "|Iter  1474  | Total Val Loss  -10.129925727844238 |\n",
      "Loss for batch is  -1.071007490158081\n",
      "Loss for batch is  -1.2014139890670776\n",
      "Loss for batch is  -1.120959997177124\n",
      "Loss for batch is  -2.196962833404541\n",
      "|Iter  1475  | Total Train Loss  -5.590344309806824 |\n",
      "Val Loss for batch is  -2.1390066146850586\n",
      "Val Loss for batch is  -2.366332530975342\n",
      "Val Loss for batch is  -2.2550976276397705\n",
      "Val Loss for batch is  -3.5459744930267334\n",
      "|Iter  1475  | Total Val Loss  -10.306411266326904 |\n",
      "Loss for batch is  -1.1110342741012573\n",
      "Loss for batch is  -1.2500903606414795\n",
      "Loss for batch is  -1.164243221282959\n",
      "Loss for batch is  -2.215364933013916\n",
      "|Iter  1476  | Total Train Loss  -5.740732789039612 |\n",
      "Val Loss for batch is  -2.230329751968384\n",
      "Val Loss for batch is  -2.4407331943511963\n",
      "Val Loss for batch is  -2.3102829456329346\n",
      "Val Loss for batch is  -3.6103358268737793\n",
      "|Iter  1476  | Total Val Loss  -10.591681718826294 |\n",
      "Loss for batch is  -1.1414347887039185\n",
      "Loss for batch is  -1.275270938873291\n",
      "Loss for batch is  -1.1902484893798828\n",
      "Loss for batch is  -2.24017596244812\n",
      "|Iter  1477  | Total Train Loss  -5.847130179405212 |\n",
      "Val Loss for batch is  -2.25148868560791\n",
      "Val Loss for batch is  -2.4417665004730225\n",
      "Val Loss for batch is  -2.3217623233795166\n",
      "Val Loss for batch is  -3.635093927383423\n",
      "|Iter  1477  | Total Val Loss  -10.650111436843872 |\n",
      "Loss for batch is  -1.181091547012329\n",
      "Loss for batch is  -1.3038586378097534\n",
      "Loss for batch is  -1.213833212852478\n",
      "Loss for batch is  -2.2509210109710693\n",
      "|Iter  1478  | Total Train Loss  -5.94970440864563 |\n",
      "Val Loss for batch is  -2.2589094638824463\n",
      "Val Loss for batch is  -2.480314254760742\n",
      "Val Loss for batch is  -2.3465514183044434\n",
      "Val Loss for batch is  -3.6384317874908447\n",
      "|Iter  1478  | Total Val Loss  -10.724206924438477 |\n",
      "Loss for batch is  -1.1894105672836304\n",
      "Loss for batch is  -1.3324214220046997\n",
      "Loss for batch is  -1.2353376150131226\n",
      "Loss for batch is  -2.2623167037963867\n",
      "|Iter  1479  | Total Train Loss  -6.019486308097839 |\n",
      "Val Loss for batch is  -2.279909610748291\n",
      "Val Loss for batch is  -2.5160908699035645\n",
      "Val Loss for batch is  -2.2467572689056396\n",
      "Val Loss for batch is  -3.6683526039123535\n",
      "|Iter  1479  | Total Val Loss  -10.711110353469849 |\n",
      "Loss for batch is  -1.2187004089355469\n",
      "Loss for batch is  -1.3289494514465332\n",
      "Loss for batch is  -1.2204805612564087\n",
      "Loss for batch is  -2.286094903945923\n",
      "|Iter  1480  | Total Train Loss  -6.054225325584412 |\n",
      "Val Loss for batch is  -2.271355390548706\n",
      "Val Loss for batch is  -2.4954686164855957\n",
      "Val Loss for batch is  -2.3990988731384277\n",
      "Val Loss for batch is  -3.651031255722046\n",
      "|Iter  1480  | Total Val Loss  -10.816954135894775 |\n",
      "Loss for batch is  -1.2158125638961792\n",
      "Loss for batch is  -1.3597718477249146\n",
      "Loss for batch is  -1.2453486919403076\n",
      "Loss for batch is  -2.29362154006958\n",
      "|Iter  1481  | Total Train Loss  -6.1145546436309814 |\n",
      "Val Loss for batch is  -2.30169415473938\n",
      "Val Loss for batch is  -2.5356786251068115\n",
      "Val Loss for batch is  -2.399641752243042\n",
      "Val Loss for batch is  -3.691598415374756\n",
      "|Iter  1481  | Total Val Loss  -10.92861294746399 |\n",
      "Loss for batch is  -1.2512720823287964\n",
      "Loss for batch is  -1.3721935749053955\n",
      "Loss for batch is  -1.2558753490447998\n",
      "Loss for batch is  -2.3113136291503906\n",
      "|Iter  1482  | Total Train Loss  -6.190654635429382 |\n",
      "Val Loss for batch is  -2.311040163040161\n",
      "Val Loss for batch is  -2.535597801208496\n",
      "Val Loss for batch is  -2.415719509124756\n",
      "Val Loss for batch is  -3.688546895980835\n",
      "|Iter  1482  | Total Val Loss  -10.950904369354248 |\n",
      "Loss for batch is  -1.2580229043960571\n",
      "Loss for batch is  -1.36949622631073\n",
      "Loss for batch is  -1.2649433612823486\n",
      "Loss for batch is  -2.329051971435547\n",
      "|Iter  1483  | Total Train Loss  -6.221514463424683 |\n",
      "Val Loss for batch is  -2.275334119796753\n",
      "Val Loss for batch is  -2.5710043907165527\n",
      "Val Loss for batch is  -2.408475637435913\n",
      "Val Loss for batch is  -3.6996543407440186\n",
      "|Iter  1483  | Total Val Loss  -10.954468488693237 |\n",
      "Loss for batch is  -1.259882926940918\n",
      "Loss for batch is  -1.3842300176620483\n",
      "Loss for batch is  -1.2745164632797241\n",
      "Loss for batch is  -2.3338735103607178\n",
      "|Iter  1484  | Total Train Loss  -6.252502918243408 |\n",
      "Val Loss for batch is  -2.277461290359497\n",
      "Val Loss for batch is  -2.5620405673980713\n",
      "Val Loss for batch is  -2.410484790802002\n",
      "Val Loss for batch is  -3.708866834640503\n",
      "|Iter  1484  | Total Val Loss  -10.958853483200073 |\n",
      "Loss for batch is  -1.2650377750396729\n",
      "Loss for batch is  -1.389668345451355\n",
      "Loss for batch is  -1.2732716798782349\n",
      "Loss for batch is  -2.3437461853027344\n",
      "|Iter  1485  | Total Train Loss  -6.271723985671997 |\n",
      "Val Loss for batch is  -2.207397222518921\n",
      "Val Loss for batch is  -2.5392093658447266\n",
      "Val Loss for batch is  -2.382293939590454\n",
      "Val Loss for batch is  -3.716336250305176\n",
      "|Iter  1485  | Total Val Loss  -10.845236778259277 |\n",
      "Loss for batch is  -1.2332879304885864\n",
      "Loss for batch is  -1.3953715562820435\n",
      "Loss for batch is  -1.2510820627212524\n",
      "Loss for batch is  -2.337921142578125\n",
      "|Iter  1486  | Total Train Loss  -6.217662692070007 |\n",
      "Val Loss for batch is  -2.2532765865325928\n",
      "Val Loss for batch is  -2.5273194313049316\n",
      "Val Loss for batch is  -2.3671886920928955\n",
      "Val Loss for batch is  -3.719996213912964\n",
      "|Iter  1486  | Total Val Loss  -10.867780923843384 |\n",
      "Loss for batch is  -1.2729668617248535\n",
      "Loss for batch is  -1.3789546489715576\n",
      "Loss for batch is  -1.2855348587036133\n",
      "Loss for batch is  -2.3287594318389893\n",
      "|Iter  1487  | Total Train Loss  -6.266215801239014 |\n",
      "Val Loss for batch is  -2.1851935386657715\n",
      "Val Loss for batch is  -2.4671790599823\n",
      "Val Loss for batch is  -2.3272464275360107\n",
      "Val Loss for batch is  -3.7169063091278076\n",
      "|Iter  1487  | Total Val Loss  -10.69652533531189 |\n",
      "Loss for batch is  -1.196006417274475\n",
      "Loss for batch is  -1.37343168258667\n",
      "Loss for batch is  -1.1364349126815796\n",
      "Loss for batch is  -2.3339362144470215\n",
      "|Iter  1488  | Total Train Loss  -6.039809226989746 |\n",
      "Val Loss for batch is  -1.99762761592865\n",
      "Val Loss for batch is  -2.2475104331970215\n",
      "Val Loss for batch is  -2.1303234100341797\n",
      "Val Loss for batch is  -3.636073589324951\n",
      "|Iter  1488  | Total Val Loss  -10.011535048484802 |\n",
      "Loss for batch is  -1.0308051109313965\n",
      "Loss for batch is  -1.3276389837265015\n",
      "Loss for batch is  -1.0125716924667358\n",
      "Loss for batch is  -2.2278122901916504\n",
      "|Iter  1489  | Total Train Loss  -5.598828077316284 |\n",
      "Val Loss for batch is  -2.144423723220825\n",
      "Val Loss for batch is  -2.4464783668518066\n",
      "Val Loss for batch is  -2.2578611373901367\n",
      "Val Loss for batch is  -3.6643049716949463\n",
      "|Iter  1489  | Total Val Loss  -10.513068199157715 |\n",
      "Loss for batch is  -1.176527976989746\n",
      "Loss for batch is  -1.1393266916275024\n",
      "Loss for batch is  -1.093418836593628\n",
      "Loss for batch is  -2.1684200763702393\n",
      "|Iter  1490  | Total Train Loss  -5.577693581581116 |\n",
      "Val Loss for batch is  -1.5286935567855835\n",
      "Val Loss for batch is  -1.9521594047546387\n",
      "Val Loss for batch is  -1.856859803199768\n",
      "Val Loss for batch is  -3.502234697341919\n",
      "|Iter  1490  | Total Val Loss  -8.83994746208191 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -0.6736810207366943\n",
      "Loss for batch is  -1.232593297958374\n",
      "Loss for batch is  -0.8937375545501709\n",
      "Loss for batch is  -1.8193788528442383\n",
      "|Iter  1491  | Total Train Loss  -4.6193907260894775 |\n",
      "Val Loss for batch is  -2.010582685470581\n",
      "Val Loss for batch is  -2.4000205993652344\n",
      "Val Loss for batch is  -2.179947853088379\n",
      "Val Loss for batch is  -3.5830984115600586\n",
      "|Iter  1491  | Total Val Loss  -10.173649549484253 |\n",
      "Loss for batch is  -1.1302244663238525\n",
      "Loss for batch is  -1.1517733335494995\n",
      "Loss for batch is  -0.8864855766296387\n",
      "Loss for batch is  -2.152071475982666\n",
      "|Iter  1492  | Total Train Loss  -5.320554852485657 |\n",
      "Val Loss for batch is  -2.1082136631011963\n",
      "Val Loss for batch is  -2.4157869815826416\n",
      "Val Loss for batch is  -2.211357831954956\n",
      "Val Loss for batch is  -3.572082996368408\n",
      "|Iter  1492  | Total Val Loss  -10.307441473007202 |\n",
      "Loss for batch is  -1.130327820777893\n",
      "Loss for batch is  -1.2243547439575195\n",
      "Loss for batch is  -1.085180401802063\n",
      "Loss for batch is  -2.2222752571105957\n",
      "|Iter  1493  | Total Train Loss  -5.662138223648071 |\n",
      "Val Loss for batch is  -2.086786985397339\n",
      "Val Loss for batch is  -2.423445701599121\n",
      "Val Loss for batch is  -2.175851821899414\n",
      "Val Loss for batch is  -3.592081069946289\n",
      "|Iter  1493  | Total Val Loss  -10.278165578842163 |\n",
      "Loss for batch is  -1.1125904321670532\n",
      "Loss for batch is  -1.246246099472046\n",
      "Loss for batch is  -1.151717185974121\n",
      "Loss for batch is  -2.2362513542175293\n",
      "|Iter  1494  | Total Train Loss  -5.7468050718307495 |\n",
      "Val Loss for batch is  -2.2059006690979004\n",
      "Val Loss for batch is  -2.422111988067627\n",
      "Val Loss for batch is  -2.2688918113708496\n",
      "Val Loss for batch is  -3.6163039207458496\n",
      "|Iter  1494  | Total Val Loss  -10.513208389282227 |\n",
      "Loss for batch is  -1.147654414176941\n",
      "Loss for batch is  -1.275166630744934\n",
      "Loss for batch is  -1.1731728315353394\n",
      "Loss for batch is  -2.240192174911499\n",
      "|Iter  1495  | Total Train Loss  -5.836186051368713 |\n",
      "Val Loss for batch is  -2.2289888858795166\n",
      "Val Loss for batch is  -2.4392807483673096\n",
      "Val Loss for batch is  -2.291750192642212\n",
      "Val Loss for batch is  -3.594064950942993\n",
      "|Iter  1495  | Total Val Loss  -10.554084777832031 |\n",
      "Loss for batch is  -1.1822587251663208\n",
      "Loss for batch is  -1.29938805103302\n",
      "Loss for batch is  -1.2152982950210571\n",
      "Loss for batch is  -2.256502151489258\n",
      "|Iter  1496  | Total Train Loss  -5.953447222709656 |\n",
      "Val Loss for batch is  -2.3022241592407227\n",
      "Val Loss for batch is  -2.4837088584899902\n",
      "Val Loss for batch is  -2.346280574798584\n",
      "Val Loss for batch is  -3.6471939086914062\n",
      "|Iter  1496  | Total Val Loss  -10.779407501220703 |\n",
      "Loss for batch is  -1.1998248100280762\n",
      "Loss for batch is  -1.3458380699157715\n",
      "Loss for batch is  -1.238481044769287\n",
      "Loss for batch is  -2.272639036178589\n",
      "|Iter  1497  | Total Train Loss  -6.056782960891724 |\n",
      "Val Loss for batch is  -2.294321060180664\n",
      "Val Loss for batch is  -2.4707236289978027\n",
      "Val Loss for batch is  -2.3843178749084473\n",
      "Val Loss for batch is  -3.667081594467163\n",
      "|Iter  1497  | Total Val Loss  -10.816444158554077 |\n",
      "Loss for batch is  -1.2228606939315796\n",
      "Loss for batch is  -1.3536994457244873\n",
      "Loss for batch is  -1.248611330986023\n",
      "Loss for batch is  -2.2688043117523193\n",
      "|Iter  1498  | Total Train Loss  -6.093975782394409 |\n",
      "Val Loss for batch is  -2.3360157012939453\n",
      "Val Loss for batch is  -2.542834758758545\n",
      "Val Loss for batch is  -2.4213037490844727\n",
      "Val Loss for batch is  -3.678907632827759\n",
      "|Iter  1498  | Total Val Loss  -10.979061841964722 |\n",
      "Loss for batch is  -1.2490622997283936\n",
      "Loss for batch is  -1.3760557174682617\n",
      "Loss for batch is  -1.2547541856765747\n",
      "Loss for batch is  -2.308238983154297\n",
      "|Iter  1499  | Total Train Loss  -6.188111186027527 |\n",
      "Val Loss for batch is  -2.3503546714782715\n",
      "Val Loss for batch is  -2.5564303398132324\n",
      "Val Loss for batch is  -2.434760808944702\n",
      "Val Loss for batch is  -3.692525863647461\n",
      "|Iter  1499  | Total Val Loss  -11.034071683883667 |\n",
      "Loss for batch is  -1.2596666812896729\n",
      "Loss for batch is  -1.377431035041809\n",
      "Loss for batch is  -1.2636263370513916\n",
      "Loss for batch is  -2.3161725997924805\n",
      "|Iter  1500  | Total Train Loss  -6.216896653175354 |\n",
      "Val Loss for batch is  -2.316986083984375\n",
      "Val Loss for batch is  -2.5624563694000244\n",
      "Val Loss for batch is  -2.4009058475494385\n",
      "Val Loss for batch is  -3.683340549468994\n",
      "|Iter  1500  | Total Val Loss  -10.963688850402832 |\n",
      "Loss for batch is  -1.2674857378005981\n",
      "Loss for batch is  -1.4025367498397827\n",
      "Loss for batch is  -1.2861448526382446\n",
      "Loss for batch is  -2.330207347869873\n",
      "|Iter  1501  | Total Train Loss  -6.2863746881484985 |\n",
      "Val Loss for batch is  -2.314164400100708\n",
      "Val Loss for batch is  -2.583441734313965\n",
      "Val Loss for batch is  -2.4449095726013184\n",
      "Val Loss for batch is  -3.704770088195801\n",
      "|Iter  1501  | Total Val Loss  -11.047285795211792 |\n",
      "Loss for batch is  -1.2891769409179688\n",
      "Loss for batch is  -1.4099493026733398\n",
      "Loss for batch is  -1.2945398092269897\n",
      "Loss for batch is  -2.333998203277588\n",
      "|Iter  1502  | Total Train Loss  -6.327664256095886 |\n",
      "Val Loss for batch is  -2.3358869552612305\n",
      "Val Loss for batch is  -2.5694613456726074\n",
      "Val Loss for batch is  -2.4173519611358643\n",
      "Val Loss for batch is  -3.734325647354126\n",
      "|Iter  1502  | Total Val Loss  -11.057025909423828 |\n",
      "Loss for batch is  -1.2904679775238037\n",
      "Loss for batch is  -1.3993992805480957\n",
      "Loss for batch is  -1.3059091567993164\n",
      "Loss for batch is  -2.3512635231018066\n",
      "|Iter  1503  | Total Train Loss  -6.3470399379730225 |\n",
      "Val Loss for batch is  -2.2624974250793457\n",
      "Val Loss for batch is  -2.529585838317871\n",
      "Val Loss for batch is  -2.4262356758117676\n",
      "Val Loss for batch is  -3.7098548412323\n",
      "|Iter  1503  | Total Val Loss  -10.928173780441284 |\n",
      "Loss for batch is  -1.2631852626800537\n",
      "Loss for batch is  -1.4160569906234741\n",
      "Loss for batch is  -1.2830594778060913\n",
      "Loss for batch is  -2.313512086868286\n",
      "|Iter  1504  | Total Train Loss  -6.275813817977905 |\n",
      "Val Loss for batch is  -2.30430269241333\n",
      "Val Loss for batch is  -2.549457311630249\n",
      "Val Loss for batch is  -2.4073379039764404\n",
      "Val Loss for batch is  -3.736992597579956\n",
      "|Iter  1504  | Total Val Loss  -10.998090505599976 |\n",
      "Loss for batch is  -1.297560691833496\n",
      "Loss for batch is  -1.3816351890563965\n",
      "Loss for batch is  -1.292921781539917\n",
      "Loss for batch is  -2.366975784301758\n",
      "|Iter  1505  | Total Train Loss  -6.339093446731567 |\n",
      "Val Loss for batch is  -2.184211015701294\n",
      "Val Loss for batch is  -2.5298686027526855\n",
      "Val Loss for batch is  -2.1496081352233887\n",
      "Val Loss for batch is  -3.7376251220703125\n",
      "|Iter  1505  | Total Val Loss  -10.60131287574768 |\n",
      "Loss for batch is  -1.244543433189392\n",
      "Loss for batch is  -1.4132416248321533\n",
      "Loss for batch is  -1.289262294769287\n",
      "Loss for batch is  -2.2898950576782227\n",
      "|Iter  1506  | Total Train Loss  -6.236942410469055 |\n",
      "Val Loss for batch is  -2.2742886543273926\n",
      "Val Loss for batch is  -2.5842783451080322\n",
      "Val Loss for batch is  -2.423213243484497\n",
      "Val Loss for batch is  -3.7127115726470947\n",
      "|Iter  1506  | Total Val Loss  -10.994491815567017 |\n",
      "Loss for batch is  -1.2783921957015991\n",
      "Loss for batch is  -1.3772398233413696\n",
      "Loss for batch is  -1.2828738689422607\n",
      "Loss for batch is  -2.369002342224121\n",
      "|Iter  1507  | Total Train Loss  -6.307508230209351 |\n",
      "Val Loss for batch is  -2.16292142868042\n",
      "Val Loss for batch is  -2.4678456783294678\n",
      "Val Loss for batch is  -2.3640120029449463\n",
      "Val Loss for batch is  -3.7192158699035645\n",
      "|Iter  1507  | Total Val Loss  -10.713994979858398 |\n",
      "Loss for batch is  -1.1825709342956543\n",
      "Loss for batch is  -1.3850982189178467\n",
      "Loss for batch is  -1.252368688583374\n",
      "Loss for batch is  -2.226268768310547\n",
      "|Iter  1508  | Total Train Loss  -6.046306610107422 |\n",
      "Val Loss for batch is  -2.2251999378204346\n",
      "Val Loss for batch is  -2.5217127799987793\n",
      "Val Loss for batch is  -2.335230827331543\n",
      "Val Loss for batch is  -3.739790201187134\n",
      "|Iter  1508  | Total Val Loss  -10.82193374633789 |\n",
      "Loss for batch is  -1.276090145111084\n",
      "Loss for batch is  -1.340726375579834\n",
      "Loss for batch is  -1.2538825273513794\n",
      "Loss for batch is  -2.3421947956085205\n",
      "|Iter  1509  | Total Train Loss  -6.212893843650818 |\n",
      "Val Loss for batch is  -2.0956616401672363\n",
      "Val Loss for batch is  -2.2900211811065674\n",
      "Val Loss for batch is  -2.279635429382324\n",
      "Val Loss for batch is  -3.7138962745666504\n",
      "|Iter  1509  | Total Val Loss  -10.379214525222778 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.1049617528915405\n",
      "Loss for batch is  -1.3466236591339111\n",
      "Loss for batch is  -1.1639223098754883\n",
      "Loss for batch is  -2.146148681640625\n",
      "|Iter  1510  | Total Train Loss  -5.761656403541565 |\n",
      "Val Loss for batch is  -2.2398531436920166\n",
      "Val Loss for batch is  -2.532651901245117\n",
      "Val Loss for batch is  -2.3670928478240967\n",
      "Val Loss for batch is  -3.690166473388672\n",
      "|Iter  1510  | Total Val Loss  -10.829764366149902 |\n",
      "Loss for batch is  -1.2501022815704346\n",
      "Loss for batch is  -1.2770590782165527\n",
      "Loss for batch is  -1.175755500793457\n",
      "Loss for batch is  -2.3137216567993164\n",
      "|Iter  1511  | Total Train Loss  -6.016638517379761 |\n",
      "Val Loss for batch is  -2.0223910808563232\n",
      "Val Loss for batch is  -2.267629861831665\n",
      "Val Loss for batch is  -2.1717305183410645\n",
      "Val Loss for batch is  -3.6393017768859863\n",
      "|Iter  1511  | Total Val Loss  -10.101053237915039 |\n",
      "Loss for batch is  -1.0191823244094849\n",
      "Loss for batch is  -1.2391767501831055\n",
      "Loss for batch is  -1.2267593145370483\n",
      "Loss for batch is  -2.1958823204040527\n",
      "|Iter  1512  | Total Train Loss  -5.681000709533691 |\n",
      "Val Loss for batch is  -2.182434320449829\n",
      "Val Loss for batch is  -2.432335376739502\n",
      "Val Loss for batch is  -2.2859346866607666\n",
      "Val Loss for batch is  -3.6132593154907227\n",
      "|Iter  1512  | Total Val Loss  -10.51396369934082 |\n",
      "Loss for batch is  -1.187937617301941\n",
      "Loss for batch is  -1.3728824853897095\n",
      "Loss for batch is  -1.2208964824676514\n",
      "Loss for batch is  -2.2182421684265137\n",
      "|Iter  1513  | Total Train Loss  -5.999958753585815 |\n",
      "Val Loss for batch is  -2.2136828899383545\n",
      "Val Loss for batch is  -2.4726781845092773\n",
      "Val Loss for batch is  -2.377410650253296\n",
      "Val Loss for batch is  -3.698819875717163\n",
      "|Iter  1513  | Total Val Loss  -10.76259160041809 |\n",
      "Loss for batch is  -1.2425293922424316\n",
      "Loss for batch is  -1.3395681381225586\n",
      "Loss for batch is  -1.2050230503082275\n",
      "Loss for batch is  -2.330617666244507\n",
      "|Iter  1514  | Total Train Loss  -6.117738246917725 |\n",
      "Val Loss for batch is  -2.252786874771118\n",
      "Val Loss for batch is  -2.433098077774048\n",
      "Val Loss for batch is  -2.3340864181518555\n",
      "Val Loss for batch is  -3.6785011291503906\n",
      "|Iter  1514  | Total Val Loss  -10.698472499847412 |\n",
      "Loss for batch is  -1.2140581607818604\n",
      "Loss for batch is  -1.3213553428649902\n",
      "Loss for batch is  -1.270695686340332\n",
      "Loss for batch is  -2.3269317150115967\n",
      "|Iter  1515  | Total Train Loss  -6.133040904998779 |\n",
      "Val Loss for batch is  -2.218475103378296\n",
      "Val Loss for batch is  -2.4753925800323486\n",
      "Val Loss for batch is  -2.3261027336120605\n",
      "Val Loss for batch is  -3.699442148208618\n",
      "|Iter  1515  | Total Val Loss  -10.719412565231323 |\n",
      "Loss for batch is  -1.2175885438919067\n",
      "Loss for batch is  -1.3870062828063965\n",
      "Loss for batch is  -1.252915620803833\n",
      "Loss for batch is  -2.324369430541992\n",
      "|Iter  1516  | Total Train Loss  -6.181879878044128 |\n",
      "Val Loss for batch is  -2.2969229221343994\n",
      "Val Loss for batch is  -2.5383660793304443\n",
      "Val Loss for batch is  -2.4257407188415527\n",
      "Val Loss for batch is  -3.727259397506714\n",
      "|Iter  1516  | Total Val Loss  -10.98828911781311 |\n",
      "Loss for batch is  -1.2845276594161987\n",
      "Loss for batch is  -1.3987458944320679\n",
      "Loss for batch is  -1.2752060890197754\n",
      "Loss for batch is  -2.3691065311431885\n",
      "|Iter  1517  | Total Train Loss  -6.3275861740112305 |\n",
      "Val Loss for batch is  -2.3000071048736572\n",
      "Val Loss for batch is  -2.525862216949463\n",
      "Val Loss for batch is  -2.36077880859375\n",
      "Val Loss for batch is  -3.7363908290863037\n",
      "|Iter  1517  | Total Val Loss  -10.923038959503174 |\n",
      "Loss for batch is  -1.296732783317566\n",
      "Loss for batch is  -1.4051920175552368\n",
      "Loss for batch is  -1.3165347576141357\n",
      "Loss for batch is  -2.372326135635376\n",
      "|Iter  1518  | Total Train Loss  -6.3907856941223145 |\n",
      "Val Loss for batch is  -2.2950680255889893\n",
      "Val Loss for batch is  -2.540550947189331\n",
      "Val Loss for batch is  -2.411045789718628\n",
      "Val Loss for batch is  -3.7507851123809814\n",
      "|Iter  1518  | Total Val Loss  -10.99744987487793 |\n",
      "Loss for batch is  -1.2916938066482544\n",
      "Loss for batch is  -1.4261763095855713\n",
      "Loss for batch is  -1.3106822967529297\n",
      "Loss for batch is  -2.3725829124450684\n",
      "|Iter  1519  | Total Train Loss  -6.401135325431824 |\n",
      "Val Loss for batch is  -2.262688159942627\n",
      "Val Loss for batch is  -2.5880582332611084\n",
      "Val Loss for batch is  -2.409296989440918\n",
      "Val Loss for batch is  -3.725327968597412\n",
      "|Iter  1519  | Total Val Loss  -10.985371351242065 |\n",
      "Loss for batch is  -1.309575080871582\n",
      "Loss for batch is  -1.4278945922851562\n",
      "Loss for batch is  -1.3133560419082642\n",
      "Loss for batch is  -2.3832197189331055\n",
      "|Iter  1520  | Total Train Loss  -6.434045433998108 |\n",
      "Val Loss for batch is  -2.3473193645477295\n",
      "Val Loss for batch is  -2.580651044845581\n",
      "Val Loss for batch is  -2.424506664276123\n",
      "Val Loss for batch is  -3.732893705368042\n",
      "|Iter  1520  | Total Val Loss  -11.085370779037476 |\n",
      "Loss for batch is  -1.3285350799560547\n",
      "Loss for batch is  -1.4248720407485962\n",
      "Loss for batch is  -1.3123208284378052\n",
      "Loss for batch is  -2.392592430114746\n",
      "|Iter  1521  | Total Train Loss  -6.458320379257202 |\n",
      "Val Loss for batch is  -2.2531723976135254\n",
      "Val Loss for batch is  -2.536149263381958\n",
      "Val Loss for batch is  -2.378135919570923\n",
      "Val Loss for batch is  -3.7508623600006104\n",
      "|Iter  1521  | Total Val Loss  -10.918319940567017 |\n",
      "Loss for batch is  -1.2695146799087524\n",
      "Loss for batch is  -1.4500622749328613\n",
      "Loss for batch is  -1.279114842414856\n",
      "Loss for batch is  -2.3494954109191895\n",
      "|Iter  1522  | Total Train Loss  -6.348187208175659 |\n",
      "Val Loss for batch is  -2.289991617202759\n",
      "Val Loss for batch is  -2.5334465503692627\n",
      "Val Loss for batch is  -2.388883590698242\n",
      "Val Loss for batch is  -3.7491817474365234\n",
      "|Iter  1522  | Total Val Loss  -10.961503505706787 |\n",
      "Loss for batch is  -1.3126074075698853\n",
      "Loss for batch is  -1.3527870178222656\n",
      "Loss for batch is  -1.3274046182632446\n",
      "Loss for batch is  -2.375749111175537\n",
      "|Iter  1523  | Total Train Loss  -6.368548154830933 |\n",
      "Val Loss for batch is  -2.1223855018615723\n",
      "Val Loss for batch is  -2.498518943786621\n",
      "Val Loss for batch is  -2.2722437381744385\n",
      "Val Loss for batch is  -3.7119476795196533\n",
      "|Iter  1523  | Total Val Loss  -10.605095863342285 |\n",
      "Loss for batch is  -1.1986507177352905\n",
      "Loss for batch is  -1.4333680868148804\n",
      "Loss for batch is  -1.1966205835342407\n",
      "Loss for batch is  -2.317042112350464\n",
      "|Iter  1524  | Total Train Loss  -6.1456815004348755 |\n",
      "Val Loss for batch is  -2.15456223487854\n",
      "Val Loss for batch is  -2.4677932262420654\n",
      "Val Loss for batch is  -2.257554769515991\n",
      "Val Loss for batch is  -3.733322858810425\n",
      "|Iter  1524  | Total Val Loss  -10.613233089447021 |\n",
      "Loss for batch is  -1.1712729930877686\n",
      "Loss for batch is  -1.3335319757461548\n",
      "Loss for batch is  -1.29658043384552\n",
      "Loss for batch is  -2.2209410667419434\n",
      "|Iter  1525  | Total Train Loss  -6.022326469421387 |\n",
      "Val Loss for batch is  -2.1447465419769287\n",
      "Val Loss for batch is  -2.4882524013519287\n",
      "Val Loss for batch is  -2.3225152492523193\n",
      "Val Loss for batch is  -3.738480806350708\n",
      "|Iter  1525  | Total Val Loss  -10.693994998931885 |\n",
      "Loss for batch is  -1.2494148015975952\n",
      "Loss for batch is  -1.3618874549865723\n",
      "Loss for batch is  -1.1944990158081055\n",
      "Loss for batch is  -2.300544500350952\n",
      "|Iter  1526  | Total Train Loss  -6.106345772743225 |\n",
      "Val Loss for batch is  -2.083371877670288\n",
      "Val Loss for batch is  -2.3174664974212646\n",
      "Val Loss for batch is  -2.211851119995117\n",
      "Val Loss for batch is  -3.7286367416381836\n",
      "|Iter  1526  | Total Val Loss  -10.341326236724854 |\n",
      "Loss for batch is  -1.084364652633667\n",
      "Loss for batch is  -1.2726951837539673\n",
      "Loss for batch is  -1.273537278175354\n",
      "Loss for batch is  -2.0594449043273926\n",
      "|Iter  1527  | Total Train Loss  -5.690042018890381 |\n",
      "Val Loss for batch is  -2.179548501968384\n",
      "Val Loss for batch is  -2.4566879272460938\n",
      "Val Loss for batch is  -2.331817626953125\n",
      "Val Loss for batch is  -3.670741081237793\n",
      "|Iter  1527  | Total Val Loss  -10.638795137405396 |\n",
      "Loss for batch is  -1.2442752122879028\n",
      "Loss for batch is  -1.3588142395019531\n",
      "Loss for batch is  -1.1500256061553955\n",
      "Loss for batch is  -2.289717197418213\n",
      "|Iter  1528  | Total Train Loss  -6.042832255363464 |\n",
      "Val Loss for batch is  -2.2504682540893555\n",
      "Val Loss for batch is  -2.471362829208374\n",
      "Val Loss for batch is  -2.2358272075653076\n",
      "Val Loss for batch is  -3.7318620681762695\n",
      "|Iter  1528  | Total Val Loss  -10.689520359039307 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.2632060050964355\n",
      "Loss for batch is  -1.2606528997421265\n",
      "Loss for batch is  -1.2552896738052368\n",
      "Loss for batch is  -2.3593738079071045\n",
      "|Iter  1529  | Total Train Loss  -6.138522386550903 |\n",
      "Val Loss for batch is  -2.168121576309204\n",
      "Val Loss for batch is  -2.469193935394287\n",
      "Val Loss for batch is  -2.3320505619049072\n",
      "Val Loss for batch is  -3.7021710872650146\n",
      "|Iter  1529  | Total Val Loss  -10.671537160873413 |\n",
      "Loss for batch is  -1.2240456342697144\n",
      "Loss for batch is  -1.3903011083602905\n",
      "Loss for batch is  -1.3001083135604858\n",
      "Loss for batch is  -2.350025177001953\n",
      "|Iter  1530  | Total Train Loss  -6.264480233192444 |\n",
      "Val Loss for batch is  -2.190676689147949\n",
      "Val Loss for batch is  -2.490708827972412\n",
      "Val Loss for batch is  -2.3525912761688232\n",
      "Val Loss for batch is  -3.732062816619873\n",
      "|Iter  1530  | Total Val Loss  -10.766039609909058 |\n",
      "Loss for batch is  -1.2510573863983154\n",
      "Loss for batch is  -1.4013205766677856\n",
      "Loss for batch is  -1.2697522640228271\n",
      "Loss for batch is  -2.3612513542175293\n",
      "|Iter  1531  | Total Train Loss  -6.2833815813064575 |\n",
      "Val Loss for batch is  -2.252908706665039\n",
      "Val Loss for batch is  -2.514571189880371\n",
      "Val Loss for batch is  -2.351604461669922\n",
      "Val Loss for batch is  -3.7644364833831787\n",
      "|Iter  1531  | Total Val Loss  -10.88352084159851 |\n",
      "Loss for batch is  -1.300763487815857\n",
      "Loss for batch is  -1.4209628105163574\n",
      "Loss for batch is  -1.304052710533142\n",
      "Loss for batch is  -2.377626419067383\n",
      "|Iter  1532  | Total Train Loss  -6.403405427932739 |\n",
      "Val Loss for batch is  -2.359787940979004\n",
      "Val Loss for batch is  -2.5405526161193848\n",
      "Val Loss for batch is  -2.4192464351654053\n",
      "Val Loss for batch is  -3.7423319816589355\n",
      "|Iter  1532  | Total Val Loss  -11.06191897392273 |\n",
      "Loss for batch is  -1.3115386962890625\n",
      "Loss for batch is  -1.4248180389404297\n",
      "Loss for batch is  -1.3237967491149902\n",
      "Loss for batch is  -2.4001221656799316\n",
      "|Iter  1533  | Total Train Loss  -6.460275650024414 |\n",
      "Val Loss for batch is  -2.336463212966919\n",
      "Val Loss for batch is  -2.5776045322418213\n",
      "Val Loss for batch is  -2.3926525115966797\n",
      "Val Loss for batch is  -3.758957624435425\n",
      "|Iter  1533  | Total Val Loss  -11.065677881240845 |\n",
      "Loss for batch is  -1.3226159811019897\n",
      "Loss for batch is  -1.4411835670471191\n",
      "Loss for batch is  -1.3293979167938232\n",
      "Loss for batch is  -2.4116439819335938\n",
      "|Iter  1534  | Total Train Loss  -6.504841446876526 |\n",
      "Val Loss for batch is  -2.3318886756896973\n",
      "Val Loss for batch is  -2.538259744644165\n",
      "Val Loss for batch is  -2.4572105407714844\n",
      "Val Loss for batch is  -3.747386932373047\n",
      "|Iter  1534  | Total Val Loss  -11.074745893478394 |\n",
      "Loss for batch is  -1.3183239698410034\n",
      "Loss for batch is  -1.4523974657058716\n",
      "Loss for batch is  -1.324907898902893\n",
      "Loss for batch is  -2.3927786350250244\n",
      "|Iter  1535  | Total Train Loss  -6.4884079694747925 |\n",
      "Val Loss for batch is  -2.315528154373169\n",
      "Val Loss for batch is  -2.5898027420043945\n",
      "Val Loss for batch is  -2.4415557384490967\n",
      "Val Loss for batch is  -3.7734453678131104\n",
      "|Iter  1535  | Total Val Loss  -11.12033200263977 |\n",
      "Loss for batch is  -1.3367897272109985\n",
      "Loss for batch is  -1.4340792894363403\n",
      "Loss for batch is  -1.3363454341888428\n",
      "Loss for batch is  -2.4169487953186035\n",
      "|Iter  1536  | Total Train Loss  -6.524163246154785 |\n",
      "Val Loss for batch is  -2.28010892868042\n",
      "Val Loss for batch is  -2.551971197128296\n",
      "Val Loss for batch is  -2.4132776260375977\n",
      "Val Loss for batch is  -3.7478716373443604\n",
      "|Iter  1536  | Total Val Loss  -10.993229389190674 |\n",
      "Loss for batch is  -1.312531590461731\n",
      "Loss for batch is  -1.44614839553833\n",
      "Loss for batch is  -1.3436490297317505\n",
      "Loss for batch is  -2.403444290161133\n",
      "|Iter  1537  | Total Train Loss  -6.505773305892944 |\n",
      "Val Loss for batch is  -2.3197712898254395\n",
      "Val Loss for batch is  -2.58823561668396\n",
      "Val Loss for batch is  -2.4544618129730225\n",
      "Val Loss for batch is  -3.7803080081939697\n",
      "|Iter  1537  | Total Val Loss  -11.142776727676392 |\n",
      "Loss for batch is  -1.3449918031692505\n",
      "Loss for batch is  -1.4516230821609497\n",
      "Loss for batch is  -1.3410444259643555\n",
      "Loss for batch is  -2.408137083053589\n",
      "|Iter  1538  | Total Train Loss  -6.5457963943481445 |\n",
      "Val Loss for batch is  -2.27246356010437\n",
      "Val Loss for batch is  -2.5783615112304688\n",
      "Val Loss for batch is  -2.3887436389923096\n",
      "Val Loss for batch is  -3.7800755500793457\n",
      "|Iter  1538  | Total Val Loss  -11.019644260406494 |\n",
      "Loss for batch is  -1.313961386680603\n",
      "Loss for batch is  -1.4575762748718262\n",
      "Loss for batch is  -1.311179280281067\n",
      "Loss for batch is  -2.4155352115631104\n",
      "|Iter  1539  | Total Train Loss  -6.4982521533966064 |\n",
      "Val Loss for batch is  -2.254425525665283\n",
      "Val Loss for batch is  -2.5275557041168213\n",
      "Val Loss for batch is  -2.3853416442871094\n",
      "Val Loss for batch is  -3.797682523727417\n",
      "|Iter  1539  | Total Val Loss  -10.96500539779663 |\n",
      "Loss for batch is  -1.3043910264968872\n",
      "Loss for batch is  -1.45687997341156\n",
      "Loss for batch is  -1.3413654565811157\n",
      "Loss for batch is  -2.3697762489318848\n",
      "|Iter  1540  | Total Train Loss  -6.472412705421448 |\n",
      "Val Loss for batch is  -2.257087230682373\n",
      "Val Loss for batch is  -2.537433624267578\n",
      "Val Loss for batch is  -2.4052493572235107\n",
      "Val Loss for batch is  -3.7754619121551514\n",
      "|Iter  1540  | Total Val Loss  -10.975232124328613 |\n",
      "Loss for batch is  -1.3044699430465698\n",
      "Loss for batch is  -1.3972992897033691\n",
      "Loss for batch is  -1.2996370792388916\n",
      "Loss for batch is  -2.418266534805298\n",
      "|Iter  1541  | Total Train Loss  -6.419672846794128 |\n",
      "Val Loss for batch is  -2.0098235607147217\n",
      "Val Loss for batch is  -2.4708645343780518\n",
      "Val Loss for batch is  -2.049381732940674\n",
      "Val Loss for batch is  -3.72287917137146\n",
      "|Iter  1541  | Total Val Loss  -10.252948999404907 |\n",
      "Loss for batch is  -1.1387699842453003\n",
      "Loss for batch is  -1.446610689163208\n",
      "Loss for batch is  -1.2703819274902344\n",
      "Loss for batch is  -2.182999610900879\n",
      "|Iter  1542  | Total Train Loss  -6.038762211799622 |\n",
      "Val Loss for batch is  -2.1820147037506104\n",
      "Val Loss for batch is  -2.5371220111846924\n",
      "Val Loss for batch is  -2.373264789581299\n",
      "Val Loss for batch is  -3.786857843399048\n",
      "|Iter  1542  | Total Val Loss  -10.87925934791565 |\n",
      "Loss for batch is  -1.2992167472839355\n",
      "Loss for batch is  -1.270463228225708\n",
      "Loss for batch is  -1.2445378303527832\n",
      "Loss for batch is  -2.408097267150879\n",
      "|Iter  1543  | Total Train Loss  -6.222315073013306 |\n",
      "Val Loss for batch is  -2.025412082672119\n",
      "Val Loss for batch is  -2.386888027191162\n",
      "Val Loss for batch is  -2.2503597736358643\n",
      "Val Loss for batch is  -3.717463254928589\n",
      "|Iter  1543  | Total Val Loss  -10.380123138427734 |\n",
      "Loss for batch is  -1.1146897077560425\n",
      "Loss for batch is  -1.3558096885681152\n",
      "Loss for batch is  -1.2685638666152954\n",
      "Loss for batch is  -2.04512619972229\n",
      "|Iter  1544  | Total Train Loss  -5.784189462661743 |\n",
      "Val Loss for batch is  -2.200096368789673\n",
      "Val Loss for batch is  -2.497899293899536\n",
      "Val Loss for batch is  -2.2863502502441406\n",
      "Val Loss for batch is  -3.75152325630188\n",
      "|Iter  1544  | Total Val Loss  -10.73586916923523 |\n",
      "Loss for batch is  -1.2666765451431274\n",
      "Loss for batch is  -1.331585168838501\n",
      "Loss for batch is  -1.2115432024002075\n",
      "Loss for batch is  -2.376176357269287\n",
      "|Iter  1545  | Total Train Loss  -6.185981273651123 |\n",
      "Val Loss for batch is  -2.2270379066467285\n",
      "Val Loss for batch is  -2.4587833881378174\n",
      "Val Loss for batch is  -2.1929471492767334\n",
      "Val Loss for batch is  -3.739368200302124\n",
      "|Iter  1545  | Total Val Loss  -10.618136644363403 |\n",
      "Loss for batch is  -1.264643669128418\n",
      "Loss for batch is  -1.3264304399490356\n",
      "Loss for batch is  -1.271187424659729\n",
      "Loss for batch is  -2.339165210723877\n",
      "|Iter  1546  | Total Train Loss  -6.20142674446106 |\n",
      "Val Loss for batch is  -2.177978277206421\n",
      "Val Loss for batch is  -2.442225217819214\n",
      "Val Loss for batch is  -2.329087018966675\n",
      "Val Loss for batch is  -3.670952081680298\n",
      "|Iter  1546  | Total Val Loss  -10.620242595672607 |\n",
      "Loss for batch is  -1.2565890550613403\n",
      "Loss for batch is  -1.4145656824111938\n",
      "Loss for batch is  -1.3053662776947021\n",
      "Loss for batch is  -2.3646044731140137\n",
      "|Iter  1547  | Total Train Loss  -6.34112548828125 |\n",
      "Val Loss for batch is  -2.199559211730957\n",
      "Val Loss for batch is  -2.5137903690338135\n",
      "Val Loss for batch is  -2.3721063137054443\n",
      "Val Loss for batch is  -3.774282693862915\n",
      "|Iter  1547  | Total Val Loss  -10.85973858833313 |\n",
      "Loss for batch is  -1.3064080476760864\n",
      "Loss for batch is  -1.4260642528533936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.2961572408676147\n",
      "Loss for batch is  -2.3937463760375977\n",
      "|Iter  1548  | Total Train Loss  -6.422375917434692 |\n",
      "Val Loss for batch is  -2.3496358394622803\n",
      "Val Loss for batch is  -2.5267295837402344\n",
      "Val Loss for batch is  -2.3432695865631104\n",
      "Val Loss for batch is  -3.7691168785095215\n",
      "|Iter  1548  | Total Val Loss  -10.988751888275146 |\n",
      "Loss for batch is  -1.3207021951675415\n",
      "Loss for batch is  -1.4349255561828613\n",
      "Loss for batch is  -1.3251906633377075\n",
      "Loss for batch is  -2.4118149280548096\n",
      "|Iter  1549  | Total Train Loss  -6.49263334274292 |\n",
      "Val Loss for batch is  -2.308248281478882\n",
      "Val Loss for batch is  -2.538506269454956\n",
      "Val Loss for batch is  -2.385593891143799\n",
      "Val Loss for batch is  -3.8118860721588135\n",
      "|Iter  1549  | Total Val Loss  -11.04423451423645 |\n",
      "Loss for batch is  -1.348480463027954\n",
      "Loss for batch is  -1.4504737854003906\n",
      "Loss for batch is  -1.3283953666687012\n",
      "Loss for batch is  -2.4217019081115723\n",
      "|Iter  1550  | Total Train Loss  -6.549051523208618 |\n",
      "Val Loss for batch is  -2.3072073459625244\n",
      "Val Loss for batch is  -2.568887710571289\n",
      "Val Loss for batch is  -2.453112840652466\n",
      "Val Loss for batch is  -3.7912983894348145\n",
      "|Iter  1550  | Total Val Loss  -11.120506286621094 |\n",
      "Loss for batch is  -1.3460503816604614\n",
      "Loss for batch is  -1.4547674655914307\n",
      "Loss for batch is  -1.3539056777954102\n",
      "Loss for batch is  -2.4219768047332764\n",
      "|Iter  1551  | Total Train Loss  -6.576700329780579 |\n",
      "Val Loss for batch is  -2.292567014694214\n",
      "Val Loss for batch is  -2.577869176864624\n",
      "Val Loss for batch is  -2.4341866970062256\n",
      "Val Loss for batch is  -3.777470827102661\n",
      "|Iter  1551  | Total Val Loss  -11.082093715667725 |\n",
      "Loss for batch is  -1.344027042388916\n",
      "Loss for batch is  -1.4782471656799316\n",
      "Loss for batch is  -1.3381057977676392\n",
      "Loss for batch is  -2.417510509490967\n",
      "|Iter  1552  | Total Train Loss  -6.577890515327454 |\n",
      "Val Loss for batch is  -2.338493824005127\n",
      "Val Loss for batch is  -2.5700857639312744\n",
      "Val Loss for batch is  -2.430110216140747\n",
      "Val Loss for batch is  -3.802856206893921\n",
      "|Iter  1552  | Total Val Loss  -11.14154601097107 |\n",
      "Loss for batch is  -1.3558907508850098\n",
      "Loss for batch is  -1.4718527793884277\n",
      "Loss for batch is  -1.3542087078094482\n",
      "Loss for batch is  -2.4305596351623535\n",
      "|Iter  1553  | Total Train Loss  -6.612511873245239 |\n",
      "Val Loss for batch is  -2.3262152671813965\n",
      "Val Loss for batch is  -2.591057300567627\n",
      "Val Loss for batch is  -2.4306719303131104\n",
      "Val Loss for batch is  -3.820202112197876\n",
      "|Iter  1553  | Total Val Loss  -11.16814661026001 |\n",
      "Loss for batch is  -1.357872486114502\n",
      "Loss for batch is  -1.466667890548706\n",
      "Loss for batch is  -1.375920057296753\n",
      "Loss for batch is  -2.4333696365356445\n",
      "|Iter  1554  | Total Train Loss  -6.6338300704956055 |\n",
      "Val Loss for batch is  -2.2872941493988037\n",
      "Val Loss for batch is  -2.608600616455078\n",
      "Val Loss for batch is  -2.4055020809173584\n",
      "Val Loss for batch is  -3.8118503093719482\n",
      "|Iter  1554  | Total Val Loss  -11.113247156143188 |\n",
      "Loss for batch is  -1.3396532535552979\n",
      "Loss for batch is  -1.4820950031280518\n",
      "Loss for batch is  -1.3382165431976318\n",
      "Loss for batch is  -2.4516005516052246\n",
      "|Iter  1555  | Total Train Loss  -6.611565351486206 |\n",
      "Val Loss for batch is  -2.257627487182617\n",
      "Val Loss for batch is  -2.5544931888580322\n",
      "Val Loss for batch is  -2.3608558177948\n",
      "Val Loss for batch is  -3.793008327484131\n",
      "|Iter  1555  | Total Val Loss  -10.96598482131958 |\n",
      "Loss for batch is  -1.3396552801132202\n",
      "Loss for batch is  -1.4479665756225586\n",
      "Loss for batch is  -1.352426528930664\n",
      "Loss for batch is  -2.4170873165130615\n",
      "|Iter  1556  | Total Train Loss  -6.557135701179504 |\n",
      "Val Loss for batch is  -2.2722184658050537\n",
      "Val Loss for batch is  -2.544445753097534\n",
      "Val Loss for batch is  -2.4237844944000244\n",
      "Val Loss for batch is  -3.8212082386016846\n",
      "|Iter  1556  | Total Val Loss  -11.061656951904297 |\n",
      "Loss for batch is  -1.3424781560897827\n",
      "Loss for batch is  -1.4769933223724365\n",
      "Loss for batch is  -1.3329094648361206\n",
      "Loss for batch is  -2.4275007247924805\n",
      "|Iter  1557  | Total Train Loss  -6.57988166809082 |\n",
      "Val Loss for batch is  -2.1981258392333984\n",
      "Val Loss for batch is  -2.3824985027313232\n",
      "Val Loss for batch is  -2.3310067653656006\n",
      "Val Loss for batch is  -3.815274477005005\n",
      "|Iter  1557  | Total Val Loss  -10.726905584335327 |\n",
      "Loss for batch is  -1.275900959968567\n",
      "Loss for batch is  -1.4414000511169434\n",
      "Loss for batch is  -1.2779409885406494\n",
      "Loss for batch is  -2.413834571838379\n",
      "|Iter  1558  | Total Train Loss  -6.409076571464539 |\n",
      "Val Loss for batch is  -2.2185323238372803\n",
      "Val Loss for batch is  -2.5275723934173584\n",
      "Val Loss for batch is  -2.3398871421813965\n",
      "Val Loss for batch is  -3.808307409286499\n",
      "|Iter  1558  | Total Val Loss  -10.894299268722534 |\n",
      "Loss for batch is  -1.3098361492156982\n",
      "Loss for batch is  -1.4217274188995361\n",
      "Loss for batch is  -1.3186428546905518\n",
      "Loss for batch is  -2.4175832271575928\n",
      "|Iter  1559  | Total Train Loss  -6.467789649963379 |\n",
      "Val Loss for batch is  -2.165881872177124\n",
      "Val Loss for batch is  -2.425790786743164\n",
      "Val Loss for batch is  -2.380934953689575\n",
      "Val Loss for batch is  -3.8042876720428467\n",
      "|Iter  1559  | Total Val Loss  -10.77689528465271 |\n",
      "Loss for batch is  -1.2503050565719604\n",
      "Loss for batch is  -1.4229099750518799\n",
      "Loss for batch is  -1.2751681804656982\n",
      "Loss for batch is  -2.4020042419433594\n",
      "|Iter  1560  | Total Train Loss  -6.350387454032898 |\n",
      "Val Loss for batch is  -2.2220356464385986\n",
      "Val Loss for batch is  -2.532456636428833\n",
      "Val Loss for batch is  -2.360856056213379\n",
      "Val Loss for batch is  -3.780623197555542\n",
      "|Iter  1560  | Total Val Loss  -10.895971536636353 |\n",
      "Loss for batch is  -1.3045920133590698\n",
      "Loss for batch is  -1.4353125095367432\n",
      "Loss for batch is  -1.3306784629821777\n",
      "Loss for batch is  -2.4010262489318848\n",
      "|Iter  1561  | Total Train Loss  -6.4716092348098755 |\n",
      "Val Loss for batch is  -2.2017245292663574\n",
      "Val Loss for batch is  -2.4519259929656982\n",
      "Val Loss for batch is  -2.311279535293579\n",
      "Val Loss for batch is  -3.787405014038086\n",
      "|Iter  1561  | Total Val Loss  -10.75233507156372 |\n",
      "Loss for batch is  -1.2748432159423828\n",
      "Loss for batch is  -1.451373815536499\n",
      "Loss for batch is  -1.2711840867996216\n",
      "Loss for batch is  -2.401238441467285\n",
      "|Iter  1562  | Total Train Loss  -6.398639559745789 |\n",
      "Val Loss for batch is  -2.257668972015381\n",
      "Val Loss for batch is  -2.5659384727478027\n",
      "Val Loss for batch is  -2.385345220565796\n",
      "Val Loss for batch is  -3.7733302116394043\n",
      "|Iter  1562  | Total Val Loss  -10.982282876968384 |\n",
      "Loss for batch is  -1.3309334516525269\n",
      "Loss for batch is  -1.4446076154708862\n",
      "Loss for batch is  -1.3063896894454956\n",
      "Loss for batch is  -2.425312042236328\n",
      "|Iter  1563  | Total Train Loss  -6.507242798805237 |\n",
      "Val Loss for batch is  -2.236118793487549\n",
      "Val Loss for batch is  -2.5088138580322266\n",
      "Val Loss for batch is  -2.344257354736328\n",
      "Val Loss for batch is  -3.7831881046295166\n",
      "|Iter  1563  | Total Val Loss  -10.87237811088562 |\n",
      "Loss for batch is  -1.3016585111618042\n",
      "Loss for batch is  -1.4304778575897217\n",
      "Loss for batch is  -1.3337640762329102\n",
      "Loss for batch is  -2.4212450981140137\n",
      "|Iter  1564  | Total Train Loss  -6.48714554309845 |\n",
      "Val Loss for batch is  -2.2693958282470703\n",
      "Val Loss for batch is  -2.5278353691101074\n",
      "Val Loss for batch is  -2.367997646331787\n",
      "Val Loss for batch is  -3.7915260791778564\n",
      "|Iter  1564  | Total Val Loss  -10.956754922866821 |\n",
      "Loss for batch is  -1.322234034538269\n",
      "Loss for batch is  -1.4668108224868774\n",
      "Loss for batch is  -1.3480170965194702\n",
      "Loss for batch is  -2.4231581687927246\n",
      "|Iter  1565  | Total Train Loss  -6.560220122337341 |\n",
      "Val Loss for batch is  -2.227334499359131\n",
      "Val Loss for batch is  -2.553136110305786\n",
      "Val Loss for batch is  -2.376502752304077\n",
      "Val Loss for batch is  -3.830730438232422\n",
      "|Iter  1565  | Total Val Loss  -10.987703800201416 |\n",
      "Loss for batch is  -1.3466562032699585\n",
      "Loss for batch is  -1.4741342067718506\n",
      "Loss for batch is  -1.3650182485580444\n",
      "Loss for batch is  -2.4434561729431152\n",
      "|Iter  1566  | Total Train Loss  -6.629264831542969 |\n",
      "Val Loss for batch is  -2.296428680419922\n",
      "Val Loss for batch is  -2.575172185897827\n",
      "Val Loss for batch is  -2.3962342739105225\n",
      "Val Loss for batch is  -3.8314154148101807\n",
      "|Iter  1566  | Total Val Loss  -11.099250555038452 |\n",
      "Loss for batch is  -1.3671977519989014\n",
      "Loss for batch is  -1.4659512042999268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.3581267595291138\n",
      "Loss for batch is  -2.462218761444092\n",
      "|Iter  1567  | Total Train Loss  -6.653494477272034 |\n",
      "Val Loss for batch is  -2.2769174575805664\n",
      "Val Loss for batch is  -2.5396816730499268\n",
      "Val Loss for batch is  -2.389158010482788\n",
      "Val Loss for batch is  -3.8305561542510986\n",
      "|Iter  1567  | Total Val Loss  -11.03631329536438 |\n",
      "Loss for batch is  -1.3493257761001587\n",
      "Loss for batch is  -1.4655362367630005\n",
      "Loss for batch is  -1.3784154653549194\n",
      "Loss for batch is  -2.4369497299194336\n",
      "|Iter  1568  | Total Train Loss  -6.630227208137512 |\n",
      "Val Loss for batch is  -2.328974962234497\n",
      "Val Loss for batch is  -2.5883970260620117\n",
      "Val Loss for batch is  -2.4249186515808105\n",
      "Val Loss for batch is  -3.826815366744995\n",
      "|Iter  1568  | Total Val Loss  -11.169106006622314 |\n",
      "Loss for batch is  -1.3614068031311035\n",
      "Loss for batch is  -1.484062671661377\n",
      "Loss for batch is  -1.3558635711669922\n",
      "Loss for batch is  -2.4606680870056152\n",
      "|Iter  1569  | Total Train Loss  -6.662001132965088 |\n",
      "Val Loss for batch is  -2.2677838802337646\n",
      "Val Loss for batch is  -2.5528764724731445\n",
      "Val Loss for batch is  -2.379798412322998\n",
      "Val Loss for batch is  -3.825990915298462\n",
      "|Iter  1569  | Total Val Loss  -11.02644968032837 |\n",
      "Loss for batch is  -1.336476445198059\n",
      "Loss for batch is  -1.4646602869033813\n",
      "Loss for batch is  -1.3874701261520386\n",
      "Loss for batch is  -2.4182066917419434\n",
      "|Iter  1570  | Total Train Loss  -6.606813549995422 |\n",
      "Val Loss for batch is  -2.2649929523468018\n",
      "Val Loss for batch is  -2.5380842685699463\n",
      "Val Loss for batch is  -2.400096893310547\n",
      "Val Loss for batch is  -3.833747625350952\n",
      "|Iter  1570  | Total Val Loss  -11.036921739578247 |\n",
      "Loss for batch is  -1.3328020572662354\n",
      "Loss for batch is  -1.4918992519378662\n",
      "Loss for batch is  -1.2930196523666382\n",
      "Loss for batch is  -2.4280738830566406\n",
      "|Iter  1571  | Total Train Loss  -6.54579484462738 |\n",
      "Val Loss for batch is  -2.22871732711792\n",
      "Val Loss for batch is  -2.5177054405212402\n",
      "Val Loss for batch is  -2.362313985824585\n",
      "Val Loss for batch is  -3.838094711303711\n",
      "|Iter  1571  | Total Val Loss  -10.946831464767456 |\n",
      "Loss for batch is  -1.2958706617355347\n",
      "Loss for batch is  -1.4198689460754395\n",
      "Loss for batch is  -1.375433325767517\n",
      "Loss for batch is  -2.3963170051574707\n",
      "|Iter  1572  | Total Train Loss  -6.487489938735962 |\n",
      "Val Loss for batch is  -2.1896371841430664\n",
      "Val Loss for batch is  -2.4679126739501953\n",
      "Val Loss for batch is  -2.366598129272461\n",
      "Val Loss for batch is  -3.7699050903320312\n",
      "|Iter  1572  | Total Val Loss  -10.794053077697754 |\n",
      "Loss for batch is  -1.2705682516098022\n",
      "Loss for batch is  -1.481763243675232\n",
      "Loss for batch is  -1.2697925567626953\n",
      "Loss for batch is  -2.3611655235290527\n",
      "|Iter  1573  | Total Train Loss  -6.383289575576782 |\n",
      "Val Loss for batch is  -2.2996346950531006\n",
      "Val Loss for batch is  -2.5690064430236816\n",
      "Val Loss for batch is  -2.366206645965576\n",
      "Val Loss for batch is  -3.8091681003570557\n",
      "|Iter  1573  | Total Val Loss  -11.044015884399414 |\n",
      "Loss for batch is  -1.3361775875091553\n",
      "Loss for batch is  -1.4112634658813477\n",
      "Loss for batch is  -1.3546029329299927\n",
      "Loss for batch is  -2.4510765075683594\n",
      "|Iter  1574  | Total Train Loss  -6.553120493888855 |\n",
      "Val Loss for batch is  -2.1196789741516113\n",
      "Val Loss for batch is  -2.4712178707122803\n",
      "Val Loss for batch is  -2.3587937355041504\n",
      "Val Loss for batch is  -3.801382064819336\n",
      "|Iter  1574  | Total Val Loss  -10.751072645187378 |\n",
      "Loss for batch is  -1.2721158266067505\n",
      "Loss for batch is  -1.4525244235992432\n",
      "Loss for batch is  -1.311914324760437\n",
      "Loss for batch is  -2.3693008422851562\n",
      "|Iter  1575  | Total Train Loss  -6.405855417251587 |\n",
      "Val Loss for batch is  -2.278247594833374\n",
      "Val Loss for batch is  -2.5382769107818604\n",
      "Val Loss for batch is  -2.4049758911132812\n",
      "Val Loss for batch is  -3.8006272315979004\n",
      "|Iter  1575  | Total Val Loss  -11.022127628326416 |\n",
      "Loss for batch is  -1.3570219278335571\n",
      "Loss for batch is  -1.4267021417617798\n",
      "Loss for batch is  -1.3435331583023071\n",
      "Loss for batch is  -2.457699775695801\n",
      "|Iter  1576  | Total Train Loss  -6.584957003593445 |\n",
      "Val Loss for batch is  -2.2187068462371826\n",
      "Val Loss for batch is  -2.4926819801330566\n",
      "Val Loss for batch is  -2.3945538997650146\n",
      "Val Loss for batch is  -3.7632575035095215\n",
      "|Iter  1576  | Total Val Loss  -10.869200229644775 |\n",
      "Loss for batch is  -1.3061480522155762\n",
      "Loss for batch is  -1.4575272798538208\n",
      "Loss for batch is  -1.3650643825531006\n",
      "Loss for batch is  -2.44565749168396\n",
      "|Iter  1577  | Total Train Loss  -6.5743972063064575 |\n",
      "Val Loss for batch is  -2.3183248043060303\n",
      "Val Loss for batch is  -2.539689540863037\n",
      "Val Loss for batch is  -2.3824522495269775\n",
      "Val Loss for batch is  -3.8349342346191406\n",
      "|Iter  1577  | Total Val Loss  -11.075400829315186 |\n",
      "Loss for batch is  -1.3611855506896973\n",
      "Loss for batch is  -1.4912110567092896\n",
      "Loss for batch is  -1.360534906387329\n",
      "Loss for batch is  -2.4776463508605957\n",
      "|Iter  1578  | Total Train Loss  -6.690577864646912 |\n",
      "Val Loss for batch is  -2.3621628284454346\n",
      "Val Loss for batch is  -2.560732126235962\n",
      "Val Loss for batch is  -2.427668809890747\n",
      "Val Loss for batch is  -3.8461880683898926\n",
      "|Iter  1578  | Total Val Loss  -11.196751832962036 |\n",
      "Loss for batch is  -1.3765437602996826\n",
      "Loss for batch is  -1.4718323945999146\n",
      "Loss for batch is  -1.385577917098999\n",
      "Loss for batch is  -2.4880211353302\n",
      "|Iter  1579  | Total Train Loss  -6.721975207328796 |\n",
      "Val Loss for batch is  -2.3023130893707275\n",
      "Val Loss for batch is  -2.591083288192749\n",
      "Val Loss for batch is  -2.428056478500366\n",
      "Val Loss for batch is  -3.842180013656616\n",
      "|Iter  1579  | Total Val Loss  -11.163632869720459 |\n",
      "Loss for batch is  -1.3541843891143799\n",
      "Loss for batch is  -1.5068563222885132\n",
      "Loss for batch is  -1.3529646396636963\n",
      "Loss for batch is  -2.487851619720459\n",
      "|Iter  1580  | Total Train Loss  -6.701856970787048 |\n",
      "Val Loss for batch is  -2.353698492050171\n",
      "Val Loss for batch is  -2.606950283050537\n",
      "Val Loss for batch is  -2.448596477508545\n",
      "Val Loss for batch is  -3.8416554927825928\n",
      "|Iter  1580  | Total Val Loss  -11.250900745391846 |\n",
      "Loss for batch is  -1.3985332250595093\n",
      "Loss for batch is  -1.4938507080078125\n",
      "Loss for batch is  -1.3955243825912476\n",
      "Loss for batch is  -2.4964094161987305\n",
      "|Iter  1581  | Total Train Loss  -6.7843177318573 |\n",
      "Val Loss for batch is  -2.2733421325683594\n",
      "Val Loss for batch is  -2.5368568897247314\n",
      "Val Loss for batch is  -2.418592929840088\n",
      "Val Loss for batch is  -3.8400440216064453\n",
      "|Iter  1581  | Total Val Loss  -11.068835973739624 |\n",
      "Loss for batch is  -1.3665803670883179\n",
      "Loss for batch is  -1.5075352191925049\n",
      "Loss for batch is  -1.3737095594406128\n",
      "Loss for batch is  -2.489778995513916\n",
      "|Iter  1582  | Total Train Loss  -6.737604141235352 |\n",
      "Val Loss for batch is  -2.343154191970825\n",
      "Val Loss for batch is  -2.5960566997528076\n",
      "Val Loss for batch is  -2.44590163230896\n",
      "Val Loss for batch is  -3.863753080368042\n",
      "|Iter  1582  | Total Val Loss  -11.248865604400635 |\n",
      "Loss for batch is  -1.4084067344665527\n",
      "Loss for batch is  -1.5075620412826538\n",
      "Loss for batch is  -1.3915687799453735\n",
      "Loss for batch is  -2.5058305263519287\n",
      "|Iter  1583  | Total Train Loss  -6.813368082046509 |\n",
      "Val Loss for batch is  -2.291576862335205\n",
      "Val Loss for batch is  -2.5952847003936768\n",
      "Val Loss for batch is  -2.3787434101104736\n",
      "Val Loss for batch is  -3.8795268535614014\n",
      "|Iter  1583  | Total Val Loss  -11.145131826400757 |\n",
      "Loss for batch is  -1.38181471824646\n",
      "Loss for batch is  -1.5134243965148926\n",
      "Loss for batch is  -1.368963360786438\n",
      "Loss for batch is  -2.485496997833252\n",
      "|Iter  1584  | Total Train Loss  -6.7496994733810425 |\n",
      "Val Loss for batch is  -2.213489055633545\n",
      "Val Loss for batch is  -2.622593641281128\n",
      "Val Loss for batch is  -2.4779107570648193\n",
      "Val Loss for batch is  -3.8494672775268555\n",
      "|Iter  1584  | Total Val Loss  -11.163460731506348 |\n",
      "Loss for batch is  -1.4044833183288574\n",
      "Loss for batch is  -1.4893834590911865\n",
      "Loss for batch is  -1.4072476625442505\n",
      "Loss for batch is  -2.4983484745025635\n",
      "|Iter  1585  | Total Train Loss  -6.799462914466858 |\n",
      "Val Loss for batch is  -2.2074661254882812\n",
      "Val Loss for batch is  -2.5699398517608643\n",
      "Val Loss for batch is  -2.3904306888580322\n",
      "Val Loss for batch is  -3.8354930877685547\n",
      "|Iter  1585  | Total Val Loss  -11.003329753875732 |\n",
      "Loss for batch is  -1.3255764245986938\n",
      "Loss for batch is  -1.5183727741241455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.3274141550064087\n",
      "Loss for batch is  -2.456270694732666\n",
      "|Iter  1586  | Total Train Loss  -6.627634048461914 |\n",
      "Val Loss for batch is  -2.2489073276519775\n",
      "Val Loss for batch is  -2.5133018493652344\n",
      "Val Loss for batch is  -2.3510143756866455\n",
      "Val Loss for batch is  -3.8739190101623535\n",
      "|Iter  1586  | Total Val Loss  -10.987142562866211 |\n",
      "Loss for batch is  -1.3587981462478638\n",
      "Loss for batch is  -1.4486000537872314\n",
      "Loss for batch is  -1.40083909034729\n",
      "Loss for batch is  -2.4596433639526367\n",
      "|Iter  1587  | Total Train Loss  -6.667880654335022 |\n",
      "Val Loss for batch is  -2.168677806854248\n",
      "Val Loss for batch is  -2.4125967025756836\n",
      "Val Loss for batch is  -2.2736268043518066\n",
      "Val Loss for batch is  -3.8366217613220215\n",
      "|Iter  1587  | Total Val Loss  -10.69152307510376 |\n",
      "Loss for batch is  -1.242060899734497\n",
      "Loss for batch is  -1.5089040994644165\n",
      "Loss for batch is  -1.2370243072509766\n",
      "Loss for batch is  -2.3928656578063965\n",
      "|Iter  1588  | Total Train Loss  -6.380854964256287 |\n",
      "Val Loss for batch is  -2.20709228515625\n",
      "Val Loss for batch is  -2.470414638519287\n",
      "Val Loss for batch is  -2.4069154262542725\n",
      "Val Loss for batch is  -3.856299638748169\n",
      "|Iter  1588  | Total Val Loss  -10.940721988677979 |\n",
      "Loss for batch is  -1.342764139175415\n",
      "Loss for batch is  -1.3517792224884033\n",
      "Loss for batch is  -1.369592308998108\n",
      "Loss for batch is  -2.4496188163757324\n",
      "|Iter  1589  | Total Train Loss  -6.513754487037659 |\n",
      "Val Loss for batch is  -2.1012868881225586\n",
      "Val Loss for batch is  -2.3938610553741455\n",
      "Val Loss for batch is  -2.269804000854492\n",
      "Val Loss for batch is  -3.795917272567749\n",
      "|Iter  1589  | Total Val Loss  -10.560869216918945 |\n",
      "Loss for batch is  -1.1991022825241089\n",
      "Loss for batch is  -1.4690088033676147\n",
      "Loss for batch is  -1.2825897932052612\n",
      "Loss for batch is  -2.3372726440429688\n",
      "|Iter  1590  | Total Train Loss  -6.287973523139954 |\n",
      "Val Loss for batch is  -2.282564878463745\n",
      "Val Loss for batch is  -2.554819345474243\n",
      "Val Loss for batch is  -2.3754656314849854\n",
      "Val Loss for batch is  -3.835901975631714\n",
      "|Iter  1590  | Total Val Loss  -11.048751831054688 |\n",
      "Loss for batch is  -1.3671880960464478\n",
      "Loss for batch is  -1.4359716176986694\n",
      "Loss for batch is  -1.3449243307113647\n",
      "Loss for batch is  -2.456503391265869\n",
      "|Iter  1591  | Total Train Loss  -6.604587435722351 |\n",
      "Val Loss for batch is  -2.214803457260132\n",
      "Val Loss for batch is  -2.5394222736358643\n",
      "Val Loss for batch is  -2.320342540740967\n",
      "Val Loss for batch is  -3.8043243885040283\n",
      "|Iter  1591  | Total Val Loss  -10.878892660140991 |\n",
      "Loss for batch is  -1.3300162553787231\n",
      "Loss for batch is  -1.4728163480758667\n",
      "Loss for batch is  -1.3647700548171997\n",
      "Loss for batch is  -2.4767088890075684\n",
      "|Iter  1592  | Total Train Loss  -6.644311547279358 |\n",
      "Val Loss for batch is  -2.253248929977417\n",
      "Val Loss for batch is  -2.489717960357666\n",
      "Val Loss for batch is  -2.3526949882507324\n",
      "Val Loss for batch is  -3.8379838466644287\n",
      "|Iter  1592  | Total Val Loss  -10.933645725250244 |\n",
      "Loss for batch is  -1.3362857103347778\n",
      "Loss for batch is  -1.496800422668457\n",
      "Loss for batch is  -1.3656253814697266\n",
      "Loss for batch is  -2.4712681770324707\n",
      "|Iter  1593  | Total Train Loss  -6.669979691505432 |\n",
      "Val Loss for batch is  -2.2903335094451904\n",
      "Val Loss for batch is  -2.5684967041015625\n",
      "Val Loss for batch is  -2.394836902618408\n",
      "Val Loss for batch is  -3.8738014698028564\n",
      "|Iter  1593  | Total Val Loss  -11.127468585968018 |\n",
      "Loss for batch is  -1.3697580099105835\n",
      "Loss for batch is  -1.5046718120574951\n",
      "Loss for batch is  -1.3764171600341797\n",
      "Loss for batch is  -2.4639525413513184\n",
      "|Iter  1594  | Total Train Loss  -6.714799523353577 |\n",
      "Val Loss for batch is  -2.3292596340179443\n",
      "Val Loss for batch is  -2.5775444507598877\n",
      "Val Loss for batch is  -2.4070403575897217\n",
      "Val Loss for batch is  -3.854564905166626\n",
      "|Iter  1594  | Total Val Loss  -11.16840934753418 |\n",
      "Loss for batch is  -1.376575231552124\n",
      "Loss for batch is  -1.4850093126296997\n",
      "Loss for batch is  -1.3860970735549927\n",
      "Loss for batch is  -2.486738681793213\n",
      "|Iter  1595  | Total Train Loss  -6.734420299530029 |\n",
      "Val Loss for batch is  -2.2659926414489746\n",
      "Val Loss for batch is  -2.530388116836548\n",
      "Val Loss for batch is  -2.398913621902466\n",
      "Val Loss for batch is  -3.8426406383514404\n",
      "|Iter  1595  | Total Val Loss  -11.037935018539429 |\n",
      "Loss for batch is  -1.3658576011657715\n",
      "Loss for batch is  -1.5081541538238525\n",
      "Loss for batch is  -1.385304570198059\n",
      "Loss for batch is  -2.4654242992401123\n",
      "|Iter  1596  | Total Train Loss  -6.724740624427795 |\n",
      "Val Loss for batch is  -2.3377721309661865\n",
      "Val Loss for batch is  -2.5786826610565186\n",
      "Val Loss for batch is  -2.415499210357666\n",
      "Val Loss for batch is  -3.853738307952881\n",
      "|Iter  1596  | Total Val Loss  -11.185692310333252 |\n",
      "Loss for batch is  -1.4094775915145874\n",
      "Loss for batch is  -1.5106620788574219\n",
      "Loss for batch is  -1.3997021913528442\n",
      "Loss for batch is  -2.4851584434509277\n",
      "|Iter  1597  | Total Train Loss  -6.805000305175781 |\n",
      "Val Loss for batch is  -2.3219244480133057\n",
      "Val Loss for batch is  -2.579958438873291\n",
      "Val Loss for batch is  -2.416858196258545\n",
      "Val Loss for batch is  -3.8297650814056396\n",
      "|Iter  1597  | Total Val Loss  -11.148506164550781 |\n",
      "Loss for batch is  -1.3808223009109497\n",
      "Loss for batch is  -1.5145087242126465\n",
      "Loss for batch is  -1.4000678062438965\n",
      "Loss for batch is  -2.4923782348632812\n",
      "|Iter  1598  | Total Train Loss  -6.787777066230774 |\n",
      "Val Loss for batch is  -2.294353723526001\n",
      "Val Loss for batch is  -2.576809883117676\n",
      "Val Loss for batch is  -2.4505412578582764\n",
      "Val Loss for batch is  -3.883526563644409\n",
      "|Iter  1598  | Total Val Loss  -11.205231428146362 |\n",
      "Loss for batch is  -1.394181728363037\n",
      "Loss for batch is  -1.5115187168121338\n",
      "Loss for batch is  -1.4129207134246826\n",
      "Loss for batch is  -2.485426425933838\n",
      "|Iter  1599  | Total Train Loss  -6.804047584533691 |\n",
      "Val Loss for batch is  -2.3261263370513916\n",
      "Val Loss for batch is  -2.5540921688079834\n",
      "Val Loss for batch is  -2.4486865997314453\n",
      "Val Loss for batch is  -3.85693359375\n",
      "|Iter  1599  | Total Val Loss  -11.18583869934082 |\n",
      "Loss for batch is  -1.4093014001846313\n",
      "Loss for batch is  -1.5257049798965454\n",
      "Loss for batch is  -1.4043161869049072\n",
      "Loss for batch is  -2.5131425857543945\n",
      "|Iter  1600  | Total Train Loss  -6.8524651527404785 |\n",
      "Val Loss for batch is  -2.2530040740966797\n",
      "Val Loss for batch is  -2.5656962394714355\n",
      "Val Loss for batch is  -2.393308162689209\n",
      "Val Loss for batch is  -3.888967275619507\n",
      "|Iter  1600  | Total Val Loss  -11.100975751876831 |\n",
      "Loss for batch is  -1.386157751083374\n",
      "Loss for batch is  -1.514212965965271\n",
      "Loss for batch is  -1.4011733531951904\n",
      "Loss for batch is  -2.5140419006347656\n",
      "|Iter  1601  | Total Train Loss  -6.815585970878601 |\n",
      "Val Loss for batch is  -2.262462854385376\n",
      "Val Loss for batch is  -2.5652074813842773\n",
      "Val Loss for batch is  -2.4187755584716797\n",
      "Val Loss for batch is  -3.8601560592651367\n",
      "|Iter  1601  | Total Val Loss  -11.10660195350647 |\n",
      "Loss for batch is  -1.38152015209198\n",
      "Loss for batch is  -1.5029624700546265\n",
      "Loss for batch is  -1.4010721445083618\n",
      "Loss for batch is  -2.5094289779663086\n",
      "|Iter  1602  | Total Train Loss  -6.794983744621277 |\n",
      "Val Loss for batch is  -2.2173917293548584\n",
      "Val Loss for batch is  -2.605642080307007\n",
      "Val Loss for batch is  -2.4385197162628174\n",
      "Val Loss for batch is  -3.8292346000671387\n",
      "|Iter  1602  | Total Val Loss  -11.090788125991821 |\n",
      "Loss for batch is  -1.3483880758285522\n",
      "Loss for batch is  -1.5234923362731934\n",
      "Loss for batch is  -1.3762530088424683\n",
      "Loss for batch is  -2.4908668994903564\n",
      "|Iter  1603  | Total Train Loss  -6.73900032043457 |\n",
      "Val Loss for batch is  -2.338751792907715\n",
      "Val Loss for batch is  -2.6168577671051025\n",
      "Val Loss for batch is  -2.4413630962371826\n",
      "Val Loss for batch is  -3.881301164627075\n",
      "|Iter  1603  | Total Val Loss  -11.278273820877075 |\n",
      "Loss for batch is  -1.4160677194595337\n",
      "Loss for batch is  -1.48650324344635\n",
      "Loss for batch is  -1.4199496507644653\n",
      "Loss for batch is  -2.506150722503662\n",
      "|Iter  1604  | Total Train Loss  -6.828671336174011 |\n",
      "Val Loss for batch is  -2.23183536529541\n",
      "Val Loss for batch is  -2.4827044010162354\n",
      "Val Loss for batch is  -2.417729377746582\n",
      "Val Loss for batch is  -3.856795072555542\n",
      "|Iter  1604  | Total Val Loss  -10.98906421661377 |\n",
      "Loss for batch is  -1.351248860359192\n",
      "Loss for batch is  -1.519832730293274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.3901994228363037\n",
      "Loss for batch is  -2.412364959716797\n",
      "|Iter  1605  | Total Train Loss  -6.673645973205566 |\n",
      "Val Loss for batch is  -2.247014284133911\n",
      "Val Loss for batch is  -2.545269727706909\n",
      "Val Loss for batch is  -2.3782296180725098\n",
      "Val Loss for batch is  -3.8702943325042725\n",
      "|Iter  1605  | Total Val Loss  -11.040807962417603 |\n",
      "Loss for batch is  -1.3950707912445068\n",
      "Loss for batch is  -1.476583480834961\n",
      "Loss for batch is  -1.3956615924835205\n",
      "Loss for batch is  -2.4892523288726807\n",
      "|Iter  1606  | Total Train Loss  -6.756568193435669 |\n",
      "Val Loss for batch is  -2.2069034576416016\n",
      "Val Loss for batch is  -2.5800681114196777\n",
      "Val Loss for batch is  -2.394054889678955\n",
      "Val Loss for batch is  -3.8460938930511475\n",
      "|Iter  1606  | Total Val Loss  -11.027120351791382 |\n",
      "Loss for batch is  -1.3341031074523926\n",
      "Loss for batch is  -1.4970921277999878\n",
      "Loss for batch is  -1.372760534286499\n",
      "Loss for batch is  -2.4479756355285645\n",
      "|Iter  1607  | Total Train Loss  -6.651931405067444 |\n",
      "Val Loss for batch is  -2.2361996173858643\n",
      "Val Loss for batch is  -2.548569679260254\n",
      "Val Loss for batch is  -2.360405445098877\n",
      "Val Loss for batch is  -3.8822765350341797\n",
      "|Iter  1607  | Total Val Loss  -11.027451276779175 |\n",
      "Loss for batch is  -1.3702329397201538\n",
      "Loss for batch is  -1.4530683755874634\n",
      "Loss for batch is  -1.4064363241195679\n",
      "Loss for batch is  -2.449472665786743\n",
      "|Iter  1608  | Total Train Loss  -6.679210305213928 |\n",
      "Val Loss for batch is  -2.2040464878082275\n",
      "Val Loss for batch is  -2.533194065093994\n",
      "Val Loss for batch is  -2.3418147563934326\n",
      "Val Loss for batch is  -3.847785711288452\n",
      "|Iter  1608  | Total Val Loss  -10.926841020584106 |\n",
      "Loss for batch is  -1.299708604812622\n",
      "Loss for batch is  -1.5189131498336792\n",
      "Loss for batch is  -1.2871830463409424\n",
      "Loss for batch is  -2.480161190032959\n",
      "|Iter  1609  | Total Train Loss  -6.585965991020203 |\n",
      "Val Loss for batch is  -2.1892223358154297\n",
      "Val Loss for batch is  -2.535369396209717\n",
      "Val Loss for batch is  -2.352874517440796\n",
      "Val Loss for batch is  -3.8778154850006104\n",
      "|Iter  1609  | Total Val Loss  -10.955281734466553 |\n",
      "Loss for batch is  -1.3858720064163208\n",
      "Loss for batch is  -1.4073278903961182\n",
      "Loss for batch is  -1.3976744413375854\n",
      "Loss for batch is  -2.4879775047302246\n",
      "|Iter  1610  | Total Train Loss  -6.678851842880249 |\n",
      "Val Loss for batch is  -2.133502960205078\n",
      "Val Loss for batch is  -2.5249180793762207\n",
      "Val Loss for batch is  -2.2235524654388428\n",
      "Val Loss for batch is  -3.8226969242095947\n",
      "|Iter  1610  | Total Val Loss  -10.704670429229736 |\n",
      "Loss for batch is  -1.283133625984192\n",
      "Loss for batch is  -1.5348705053329468\n",
      "Loss for batch is  -1.3809306621551514\n",
      "Loss for batch is  -2.482700824737549\n",
      "|Iter  1611  | Total Train Loss  -6.681635618209839 |\n",
      "Val Loss for batch is  -2.2392802238464355\n",
      "Val Loss for batch is  -2.5622408390045166\n",
      "Val Loss for batch is  -2.352591037750244\n",
      "Val Loss for batch is  -3.8816447257995605\n",
      "|Iter  1611  | Total Val Loss  -11.035756826400757 |\n",
      "Loss for batch is  -1.3966374397277832\n",
      "Loss for batch is  -1.540595531463623\n",
      "Loss for batch is  -1.3953584432601929\n",
      "Loss for batch is  -2.5219779014587402\n",
      "|Iter  1612  | Total Train Loss  -6.854569315910339 |\n",
      "Val Loss for batch is  -2.29164457321167\n",
      "Val Loss for batch is  -2.603822946548462\n",
      "Val Loss for batch is  -2.4470291137695312\n",
      "Val Loss for batch is  -3.887974262237549\n",
      "|Iter  1612  | Total Val Loss  -11.230470895767212 |\n",
      "Loss for batch is  -1.4195677042007446\n",
      "Loss for batch is  -1.4853010177612305\n",
      "Loss for batch is  -1.413938283920288\n",
      "Loss for batch is  -2.5193827152252197\n",
      "|Iter  1613  | Total Train Loss  -6.838189721107483 |\n",
      "Val Loss for batch is  -2.2365005016326904\n",
      "Val Loss for batch is  -2.508126735687256\n",
      "Val Loss for batch is  -2.4140286445617676\n",
      "Val Loss for batch is  -3.8629655838012695\n",
      "|Iter  1613  | Total Val Loss  -11.021621465682983 |\n",
      "Loss for batch is  -1.358762264251709\n",
      "Loss for batch is  -1.5214916467666626\n",
      "Loss for batch is  -1.3890233039855957\n",
      "Loss for batch is  -2.487748861312866\n",
      "|Iter  1614  | Total Train Loss  -6.7570260763168335 |\n",
      "Val Loss for batch is  -2.2965071201324463\n",
      "Val Loss for batch is  -2.5682179927825928\n",
      "Val Loss for batch is  -2.3744776248931885\n",
      "Val Loss for batch is  -3.8925421237945557\n",
      "|Iter  1614  | Total Val Loss  -11.131744861602783 |\n",
      "Loss for batch is  -1.4093356132507324\n",
      "Loss for batch is  -1.4961950778961182\n",
      "Loss for batch is  -1.405129075050354\n",
      "Loss for batch is  -2.5328714847564697\n",
      "|Iter  1615  | Total Train Loss  -6.843531250953674 |\n",
      "Val Loss for batch is  -2.2652201652526855\n",
      "Val Loss for batch is  -2.565396308898926\n",
      "Val Loss for batch is  -2.3814685344696045\n",
      "Val Loss for batch is  -3.8800089359283447\n",
      "|Iter  1615  | Total Val Loss  -11.09209394454956 |\n",
      "Loss for batch is  -1.3831523656845093\n",
      "Loss for batch is  -1.5370432138442993\n",
      "Loss for batch is  -1.4342210292816162\n",
      "Loss for batch is  -2.5193562507629395\n",
      "|Iter  1616  | Total Train Loss  -6.873772859573364 |\n",
      "Val Loss for batch is  -2.340249538421631\n",
      "Val Loss for batch is  -2.5886621475219727\n",
      "Val Loss for batch is  -2.438739061355591\n",
      "Val Loss for batch is  -3.876570701599121\n",
      "|Iter  1616  | Total Val Loss  -11.244221448898315 |\n",
      "Loss for batch is  -1.430655837059021\n",
      "Loss for batch is  -1.551447868347168\n",
      "Loss for batch is  -1.420121192932129\n",
      "Loss for batch is  -2.5427613258361816\n",
      "|Iter  1617  | Total Train Loss  -6.9449862241744995 |\n",
      "Val Loss for batch is  -2.330901861190796\n",
      "Val Loss for batch is  -2.6124773025512695\n",
      "Val Loss for batch is  -2.4683520793914795\n",
      "Val Loss for batch is  -3.899059772491455\n",
      "|Iter  1617  | Total Val Loss  -11.310791015625 |\n",
      "Loss for batch is  -1.4405617713928223\n",
      "Loss for batch is  -1.5403450727462769\n",
      "Loss for batch is  -1.4451035261154175\n",
      "Loss for batch is  -2.544199228286743\n",
      "|Iter  1618  | Total Train Loss  -6.97020959854126 |\n",
      "Val Loss for batch is  -2.3245766162872314\n",
      "Val Loss for batch is  -2.579206943511963\n",
      "Val Loss for batch is  -2.4527978897094727\n",
      "Val Loss for batch is  -3.8647451400756836\n",
      "|Iter  1618  | Total Val Loss  -11.22132658958435 |\n",
      "Loss for batch is  -1.4224684238433838\n",
      "Loss for batch is  -1.5597937107086182\n",
      "Loss for batch is  -1.4398430585861206\n",
      "Loss for batch is  -2.5363969802856445\n",
      "|Iter  1619  | Total Train Loss  -6.958502173423767 |\n",
      "Val Loss for batch is  -2.31431245803833\n",
      "Val Loss for batch is  -2.6209404468536377\n",
      "Val Loss for batch is  -2.4490342140197754\n",
      "Val Loss for batch is  -3.9013497829437256\n",
      "|Iter  1619  | Total Val Loss  -11.285636901855469 |\n",
      "Loss for batch is  -1.4465616941452026\n",
      "Loss for batch is  -1.5612270832061768\n",
      "Loss for batch is  -1.456107258796692\n",
      "Loss for batch is  -2.553678512573242\n",
      "|Iter  1620  | Total Train Loss  -7.0175745487213135 |\n",
      "Val Loss for batch is  -2.3103339672088623\n",
      "Val Loss for batch is  -2.634557008743286\n",
      "Val Loss for batch is  -2.334371566772461\n",
      "Val Loss for batch is  -3.8916704654693604\n",
      "|Iter  1620  | Total Val Loss  -11.17093300819397 |\n",
      "Loss for batch is  -1.4408038854599\n",
      "Loss for batch is  -1.5653789043426514\n",
      "Loss for batch is  -1.4582158327102661\n",
      "Loss for batch is  -2.554734230041504\n",
      "|Iter  1621  | Total Train Loss  -7.019132852554321 |\n",
      "Val Loss for batch is  -2.280726671218872\n",
      "Val Loss for batch is  -2.6114110946655273\n",
      "Val Loss for batch is  -2.4613823890686035\n",
      "Val Loss for batch is  -3.916703701019287\n",
      "|Iter  1621  | Total Val Loss  -11.27022385597229 |\n",
      "Loss for batch is  -1.4455980062484741\n",
      "Loss for batch is  -1.5726351737976074\n",
      "Loss for batch is  -1.4411773681640625\n",
      "Loss for batch is  -2.553774833679199\n",
      "|Iter  1622  | Total Train Loss  -7.013185381889343 |\n",
      "Val Loss for batch is  -2.277230739593506\n",
      "Val Loss for batch is  -2.6425182819366455\n",
      "Val Loss for batch is  -2.4335520267486572\n",
      "Val Loss for batch is  -3.9021105766296387\n",
      "|Iter  1622  | Total Val Loss  -11.255411624908447 |\n",
      "Loss for batch is  -1.456352710723877\n",
      "Loss for batch is  -1.5503207445144653\n",
      "Loss for batch is  -1.4522918462753296\n",
      "Loss for batch is  -2.546433210372925\n",
      "|Iter  1623  | Total Train Loss  -7.005398511886597 |\n",
      "Val Loss for batch is  -2.2867841720581055\n",
      "Val Loss for batch is  -2.6276967525482178\n",
      "Val Loss for batch is  -2.431352138519287\n",
      "Val Loss for batch is  -3.852031946182251\n",
      "|Iter  1623  | Total Val Loss  -11.197865009307861 |\n",
      "Loss for batch is  -1.4295008182525635\n",
      "Loss for batch is  -1.5711857080459595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.436983346939087\n",
      "Loss for batch is  -2.530158519744873\n",
      "|Iter  1624  | Total Train Loss  -6.967828392982483 |\n",
      "Val Loss for batch is  -2.293882369995117\n",
      "Val Loss for batch is  -2.5992112159729004\n",
      "Val Loss for batch is  -2.4582364559173584\n",
      "Val Loss for batch is  -3.919644594192505\n",
      "|Iter  1624  | Total Val Loss  -11.27097463607788 |\n",
      "Loss for batch is  -1.444299340248108\n",
      "Loss for batch is  -1.5282291173934937\n",
      "Loss for batch is  -1.4475959539413452\n",
      "Loss for batch is  -2.540980815887451\n",
      "|Iter  1625  | Total Train Loss  -6.961105227470398 |\n",
      "Val Loss for batch is  -2.224156379699707\n",
      "Val Loss for batch is  -2.565884590148926\n",
      "Val Loss for batch is  -2.395240306854248\n",
      "Val Loss for batch is  -3.9189906120300293\n",
      "|Iter  1625  | Total Val Loss  -11.10427188873291 |\n",
      "Loss for batch is  -1.4050477743148804\n",
      "Loss for batch is  -1.555114984512329\n",
      "Loss for batch is  -1.4460649490356445\n",
      "Loss for batch is  -2.4969968795776367\n",
      "|Iter  1626  | Total Train Loss  -6.903224587440491 |\n",
      "Val Loss for batch is  -2.275571346282959\n",
      "Val Loss for batch is  -2.5674760341644287\n",
      "Val Loss for batch is  -2.4187543392181396\n",
      "Val Loss for batch is  -3.857724905014038\n",
      "|Iter  1626  | Total Val Loss  -11.119526624679565 |\n",
      "Loss for batch is  -1.418769359588623\n",
      "Loss for batch is  -1.554334044456482\n",
      "Loss for batch is  -1.4122624397277832\n",
      "Loss for batch is  -2.5308079719543457\n",
      "|Iter  1627  | Total Train Loss  -6.916173815727234 |\n",
      "Val Loss for batch is  -2.2300429344177246\n",
      "Val Loss for batch is  -2.528542995452881\n",
      "Val Loss for batch is  -2.3428664207458496\n",
      "Val Loss for batch is  -3.870276927947998\n",
      "|Iter  1627  | Total Val Loss  -10.971729278564453 |\n",
      "Loss for batch is  -1.381964921951294\n",
      "Loss for batch is  -1.5211901664733887\n",
      "Loss for batch is  -1.4389506578445435\n",
      "Loss for batch is  -2.4987382888793945\n",
      "|Iter  1628  | Total Train Loss  -6.840844035148621 |\n",
      "Val Loss for batch is  -2.214571714401245\n",
      "Val Loss for batch is  -2.542940616607666\n",
      "Val Loss for batch is  -2.4231185913085938\n",
      "Val Loss for batch is  -3.868406295776367\n",
      "|Iter  1628  | Total Val Loss  -11.049037218093872 |\n",
      "Loss for batch is  -1.379543662071228\n",
      "Loss for batch is  -1.5406032800674438\n",
      "Loss for batch is  -1.3532565832138062\n",
      "Loss for batch is  -2.5046849250793457\n",
      "|Iter  1629  | Total Train Loss  -6.778088450431824 |\n",
      "Val Loss for batch is  -2.189690351486206\n",
      "Val Loss for batch is  -2.515639305114746\n",
      "Val Loss for batch is  -2.390617847442627\n",
      "Val Loss for batch is  -3.882816791534424\n",
      "|Iter  1629  | Total Val Loss  -10.978764295578003 |\n",
      "Loss for batch is  -1.345909595489502\n",
      "Loss for batch is  -1.4641785621643066\n",
      "Loss for batch is  -1.407477617263794\n",
      "Loss for batch is  -2.4929418563842773\n",
      "|Iter  1630  | Total Train Loss  -6.71050763130188 |\n",
      "Val Loss for batch is  -2.2243502140045166\n",
      "Val Loss for batch is  -2.521644353866577\n",
      "Val Loss for batch is  -2.3700153827667236\n",
      "Val Loss for batch is  -3.8452394008636475\n",
      "|Iter  1630  | Total Val Loss  -10.961249351501465 |\n",
      "Loss for batch is  -1.372123122215271\n",
      "Loss for batch is  -1.5376665592193604\n",
      "Loss for batch is  -1.359917402267456\n",
      "Loss for batch is  -2.482316017150879\n",
      "|Iter  1631  | Total Train Loss  -6.752023100852966 |\n",
      "Val Loss for batch is  -2.241624116897583\n",
      "Val Loss for batch is  -2.5624892711639404\n",
      "Val Loss for batch is  -2.404818296432495\n",
      "Val Loss for batch is  -3.8655776977539062\n",
      "|Iter  1631  | Total Val Loss  -11.074509382247925 |\n",
      "Loss for batch is  -1.4085309505462646\n",
      "Loss for batch is  -1.4756298065185547\n",
      "Loss for batch is  -1.4257394075393677\n",
      "Loss for batch is  -2.521231174468994\n",
      "|Iter  1632  | Total Train Loss  -6.831131339073181 |\n",
      "Val Loss for batch is  -2.2018022537231445\n",
      "Val Loss for batch is  -2.5075271129608154\n",
      "Val Loss for batch is  -2.3395614624023438\n",
      "Val Loss for batch is  -3.8982975482940674\n",
      "|Iter  1632  | Total Val Loss  -10.947188377380371 |\n",
      "Loss for batch is  -1.3739677667617798\n",
      "Loss for batch is  -1.533333420753479\n",
      "Loss for batch is  -1.3931926488876343\n",
      "Loss for batch is  -2.4871015548706055\n",
      "|Iter  1633  | Total Train Loss  -6.7875953912734985 |\n",
      "Val Loss for batch is  -2.2696053981781006\n",
      "Val Loss for batch is  -2.5983948707580566\n",
      "Val Loss for batch is  -2.3694372177124023\n",
      "Val Loss for batch is  -3.885472059249878\n",
      "|Iter  1633  | Total Val Loss  -11.122909545898438 |\n",
      "Loss for batch is  -1.421628713607788\n",
      "Loss for batch is  -1.5112367868423462\n",
      "Loss for batch is  -1.4207923412322998\n",
      "Loss for batch is  -2.556711435317993\n",
      "|Iter  1634  | Total Train Loss  -6.910369277000427 |\n",
      "Val Loss for batch is  -2.200770616531372\n",
      "Val Loss for batch is  -2.550682544708252\n",
      "Val Loss for batch is  -2.4100046157836914\n",
      "Val Loss for batch is  -3.894216537475586\n",
      "|Iter  1634  | Total Val Loss  -11.055674314498901 |\n",
      "Loss for batch is  -1.4111932516098022\n",
      "Loss for batch is  -1.5508309602737427\n",
      "Loss for batch is  -1.4549174308776855\n",
      "Loss for batch is  -2.5360870361328125\n",
      "|Iter  1635  | Total Train Loss  -6.953028678894043 |\n",
      "Val Loss for batch is  -2.288869857788086\n",
      "Val Loss for batch is  -2.5743610858917236\n",
      "Val Loss for batch is  -2.4298899173736572\n",
      "Val Loss for batch is  -3.834306240081787\n",
      "|Iter  1635  | Total Val Loss  -11.127427101135254 |\n",
      "Loss for batch is  -1.4331600666046143\n",
      "Loss for batch is  -1.5697541236877441\n",
      "Loss for batch is  -1.4357086420059204\n",
      "Loss for batch is  -2.565403461456299\n",
      "|Iter  1636  | Total Train Loss  -7.004026293754578 |\n",
      "Val Loss for batch is  -2.3121681213378906\n",
      "Val Loss for batch is  -2.5890018939971924\n",
      "Val Loss for batch is  -2.446868658065796\n",
      "Val Loss for batch is  -3.9268434047698975\n",
      "|Iter  1636  | Total Val Loss  -11.274882078170776 |\n",
      "Loss for batch is  -1.458031177520752\n",
      "Loss for batch is  -1.5656379461288452\n",
      "Loss for batch is  -1.4475396871566772\n",
      "Loss for batch is  -2.563438892364502\n",
      "|Iter  1637  | Total Train Loss  -7.034647703170776 |\n",
      "Val Loss for batch is  -2.2937259674072266\n",
      "Val Loss for batch is  -2.6330618858337402\n",
      "Val Loss for batch is  -2.4304752349853516\n",
      "Val Loss for batch is  -3.914504051208496\n",
      "|Iter  1637  | Total Val Loss  -11.271767139434814 |\n",
      "Loss for batch is  -1.4447969198226929\n",
      "Loss for batch is  -1.5697380304336548\n",
      "Loss for batch is  -1.4578267335891724\n",
      "Loss for batch is  -2.568434238433838\n",
      "|Iter  1638  | Total Train Loss  -7.040795922279358 |\n",
      "Val Loss for batch is  -2.3091483116149902\n",
      "Val Loss for batch is  -2.622058868408203\n",
      "Val Loss for batch is  -2.4214084148406982\n",
      "Val Loss for batch is  -3.906205892562866\n",
      "|Iter  1638  | Total Val Loss  -11.258821487426758 |\n",
      "Loss for batch is  -1.4629154205322266\n",
      "Loss for batch is  -1.5817806720733643\n",
      "Loss for batch is  -1.4641759395599365\n",
      "Loss for batch is  -2.5778045654296875\n",
      "|Iter  1639  | Total Train Loss  -7.086676597595215 |\n",
      "Val Loss for batch is  -2.323286771774292\n",
      "Val Loss for batch is  -2.5911288261413574\n",
      "Val Loss for batch is  -2.425872564315796\n",
      "Val Loss for batch is  -3.9264421463012695\n",
      "|Iter  1639  | Total Val Loss  -11.266730308532715 |\n",
      "Loss for batch is  -1.4634146690368652\n",
      "Loss for batch is  -1.58979070186615\n",
      "Loss for batch is  -1.4616813659667969\n",
      "Loss for batch is  -2.5790834426879883\n",
      "|Iter  1640  | Total Train Loss  -7.0939701795578 |\n",
      "Val Loss for batch is  -2.3119497299194336\n",
      "Val Loss for batch is  -2.609773874282837\n",
      "Val Loss for batch is  -2.465899705886841\n",
      "Val Loss for batch is  -3.929173707962036\n",
      "|Iter  1640  | Total Val Loss  -11.316797018051147 |\n",
      "Loss for batch is  -1.4681700468063354\n",
      "Loss for batch is  -1.5649086236953735\n",
      "Loss for batch is  -1.467573881149292\n",
      "Loss for batch is  -2.562596082687378\n",
      "|Iter  1641  | Total Train Loss  -7.063248634338379 |\n",
      "Val Loss for batch is  -2.306015968322754\n",
      "Val Loss for batch is  -2.601832866668701\n",
      "Val Loss for batch is  -2.4229607582092285\n",
      "Val Loss for batch is  -3.929039716720581\n",
      "|Iter  1641  | Total Val Loss  -11.259849309921265 |\n",
      "Loss for batch is  -1.4642059803009033\n",
      "Loss for batch is  -1.5810132026672363\n",
      "Loss for batch is  -1.4489154815673828\n",
      "Loss for batch is  -2.5803773403167725\n",
      "|Iter  1642  | Total Train Loss  -7.074512004852295 |\n",
      "Val Loss for batch is  -2.2937562465667725\n",
      "Val Loss for batch is  -2.623732805252075\n",
      "Val Loss for batch is  -2.4490721225738525\n",
      "Val Loss for batch is  -3.9266152381896973\n",
      "|Iter  1642  | Total Val Loss  -11.293176412582397 |\n",
      "Loss for batch is  -1.4526081085205078\n",
      "Loss for batch is  -1.566313624382019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.4579113721847534\n",
      "Loss for batch is  -2.5531210899353027\n",
      "|Iter  1643  | Total Train Loss  -7.029954195022583 |\n",
      "Val Loss for batch is  -2.2756433486938477\n",
      "Val Loss for batch is  -2.6118528842926025\n",
      "Val Loss for batch is  -2.4074466228485107\n",
      "Val Loss for batch is  -3.929948091506958\n",
      "|Iter  1643  | Total Val Loss  -11.224890947341919 |\n",
      "Loss for batch is  -1.4568336009979248\n",
      "Loss for batch is  -1.5665078163146973\n",
      "Loss for batch is  -1.4295848608016968\n",
      "Loss for batch is  -2.5846543312072754\n",
      "|Iter  1644  | Total Train Loss  -7.037580609321594 |\n",
      "Val Loss for batch is  -2.218733072280884\n",
      "Val Loss for batch is  -2.553638458251953\n",
      "Val Loss for batch is  -2.3834614753723145\n",
      "Val Loss for batch is  -3.91931414604187\n",
      "|Iter  1644  | Total Val Loss  -11.075147151947021 |\n",
      "Loss for batch is  -1.388404130935669\n",
      "Loss for batch is  -1.5745491981506348\n",
      "Loss for batch is  -1.4434860944747925\n",
      "Loss for batch is  -2.5136868953704834\n",
      "|Iter  1645  | Total Train Loss  -6.92012631893158 |\n",
      "Val Loss for batch is  -2.2693653106689453\n",
      "Val Loss for batch is  -2.601919412612915\n",
      "Val Loss for batch is  -2.4338600635528564\n",
      "Val Loss for batch is  -3.929546356201172\n",
      "|Iter  1645  | Total Val Loss  -11.234691143035889 |\n",
      "Loss for batch is  -1.4723551273345947\n",
      "Loss for batch is  -1.5627540349960327\n",
      "Loss for batch is  -1.4361807107925415\n",
      "Loss for batch is  -2.59242844581604\n",
      "|Iter  1646  | Total Train Loss  -7.063718318939209 |\n",
      "Val Loss for batch is  -2.232771396636963\n",
      "Val Loss for batch is  -2.575451135635376\n",
      "Val Loss for batch is  -2.3472087383270264\n",
      "Val Loss for batch is  -3.928121566772461\n",
      "|Iter  1646  | Total Val Loss  -11.083552837371826 |\n",
      "Loss for batch is  -1.4159351587295532\n",
      "Loss for batch is  -1.5726685523986816\n",
      "Loss for batch is  -1.467280626296997\n",
      "Loss for batch is  -2.540433883666992\n",
      "|Iter  1647  | Total Train Loss  -6.996318221092224 |\n",
      "Val Loss for batch is  -2.3121204376220703\n",
      "Val Loss for batch is  -2.600123405456543\n",
      "Val Loss for batch is  -2.4207708835601807\n",
      "Val Loss for batch is  -3.9181597232818604\n",
      "|Iter  1647  | Total Val Loss  -11.251174449920654 |\n",
      "Loss for batch is  -1.466755986213684\n",
      "Loss for batch is  -1.5824116468429565\n",
      "Loss for batch is  -1.4417827129364014\n",
      "Loss for batch is  -2.587796688079834\n",
      "|Iter  1648  | Total Train Loss  -7.078747034072876 |\n",
      "Val Loss for batch is  -2.2924022674560547\n",
      "Val Loss for batch is  -2.5551204681396484\n",
      "Val Loss for batch is  -2.4264068603515625\n",
      "Val Loss for batch is  -3.933100700378418\n",
      "|Iter  1648  | Total Val Loss  -11.207030296325684 |\n",
      "Loss for batch is  -1.4672925472259521\n",
      "Loss for batch is  -1.5453158617019653\n",
      "Loss for batch is  -1.4795935153961182\n",
      "Loss for batch is  -2.5633764266967773\n",
      "|Iter  1649  | Total Train Loss  -7.055578351020813 |\n",
      "Val Loss for batch is  -2.300055503845215\n",
      "Val Loss for batch is  -2.580747127532959\n",
      "Val Loss for batch is  -2.4356863498687744\n",
      "Val Loss for batch is  -3.902316093444824\n",
      "|Iter  1649  | Total Val Loss  -11.218805074691772 |\n",
      "Loss for batch is  -1.4264792203903198\n",
      "Loss for batch is  -1.572380542755127\n",
      "Loss for batch is  -1.4053330421447754\n",
      "Loss for batch is  -2.5616719722747803\n",
      "|Iter  1650  | Total Train Loss  -6.965864777565002 |\n",
      "Val Loss for batch is  -2.277092933654785\n",
      "Val Loss for batch is  -2.5682177543640137\n",
      "Val Loss for batch is  -2.411346673965454\n",
      "Val Loss for batch is  -3.937608480453491\n",
      "|Iter  1650  | Total Val Loss  -11.194265842437744 |\n",
      "Loss for batch is  -1.4682399034500122\n",
      "Loss for batch is  -1.5021981000900269\n",
      "Loss for batch is  -1.4693565368652344\n",
      "Loss for batch is  -2.5782198905944824\n",
      "|Iter  1651  | Total Train Loss  -7.018014430999756 |\n",
      "Val Loss for batch is  -2.2397918701171875\n",
      "Val Loss for batch is  -2.597745180130005\n",
      "Val Loss for batch is  -2.3074584007263184\n",
      "Val Loss for batch is  -3.91947865486145\n",
      "|Iter  1651  | Total Val Loss  -11.064474105834961 |\n",
      "Loss for batch is  -1.4369409084320068\n",
      "Loss for batch is  -1.583992600440979\n",
      "Loss for batch is  -1.4577152729034424\n",
      "Loss for batch is  -2.569051742553711\n",
      "|Iter  1652  | Total Train Loss  -7.047700524330139 |\n",
      "Val Loss for batch is  -2.231696844100952\n",
      "Val Loss for batch is  -2.6179795265197754\n",
      "Val Loss for batch is  -2.4377853870391846\n",
      "Val Loss for batch is  -3.9314932823181152\n",
      "|Iter  1652  | Total Val Loss  -11.218955039978027 |\n",
      "Loss for batch is  -1.4682053327560425\n",
      "Loss for batch is  -1.5816867351531982\n",
      "Loss for batch is  -1.4372633695602417\n",
      "Loss for batch is  -2.592970132827759\n",
      "|Iter  1653  | Total Train Loss  -7.080125570297241 |\n",
      "Val Loss for batch is  -2.2094130516052246\n",
      "Val Loss for batch is  -2.5758206844329834\n",
      "Val Loss for batch is  -2.3920133113861084\n",
      "Val Loss for batch is  -3.8840928077697754\n",
      "|Iter  1653  | Total Val Loss  -11.061339855194092 |\n",
      "Loss for batch is  -1.4294781684875488\n",
      "Loss for batch is  -1.5706316232681274\n",
      "Loss for batch is  -1.4630513191223145\n",
      "Loss for batch is  -2.5555357933044434\n",
      "|Iter  1654  | Total Train Loss  -7.018696904182434 |\n",
      "Val Loss for batch is  -2.272951364517212\n",
      "Val Loss for batch is  -2.5852582454681396\n",
      "Val Loss for batch is  -2.4253687858581543\n",
      "Val Loss for batch is  -3.93133544921875\n",
      "|Iter  1654  | Total Val Loss  -11.214913845062256 |\n",
      "Loss for batch is  -1.4337427616119385\n",
      "Loss for batch is  -1.589866280555725\n",
      "Loss for batch is  -1.3754760026931763\n",
      "Loss for batch is  -2.597250461578369\n",
      "|Iter  1655  | Total Train Loss  -6.996335506439209 |\n",
      "Val Loss for batch is  -2.262244701385498\n",
      "Val Loss for batch is  -2.6098270416259766\n",
      "Val Loss for batch is  -2.384195566177368\n",
      "Val Loss for batch is  -3.9301774501800537\n",
      "|Iter  1655  | Total Val Loss  -11.186444759368896 |\n",
      "Loss for batch is  -1.4587960243225098\n",
      "Loss for batch is  -1.563981533050537\n",
      "Loss for batch is  -1.4801026582717896\n",
      "Loss for batch is  -2.5750889778137207\n",
      "|Iter  1656  | Total Train Loss  -7.077969193458557 |\n",
      "Val Loss for batch is  -2.2802369594573975\n",
      "Val Loss for batch is  -2.6346235275268555\n",
      "Val Loss for batch is  -2.3737268447875977\n",
      "Val Loss for batch is  -3.932574510574341\n",
      "|Iter  1656  | Total Val Loss  -11.221161842346191 |\n",
      "Loss for batch is  -1.4583638906478882\n",
      "Loss for batch is  -1.5974665880203247\n",
      "Loss for batch is  -1.4772902727127075\n",
      "Loss for batch is  -2.603694200515747\n",
      "|Iter  1657  | Total Train Loss  -7.1368149518966675 |\n",
      "Val Loss for batch is  -2.317340850830078\n",
      "Val Loss for batch is  -2.6038308143615723\n",
      "Val Loss for batch is  -2.4439609050750732\n",
      "Val Loss for batch is  -3.9658117294311523\n",
      "|Iter  1657  | Total Val Loss  -11.330944299697876 |\n",
      "Loss for batch is  -1.4662702083587646\n",
      "Loss for batch is  -1.5908373594284058\n",
      "Loss for batch is  -1.4553672075271606\n",
      "Loss for batch is  -2.601513385772705\n",
      "|Iter  1658  | Total Train Loss  -7.113988161087036 |\n",
      "Val Loss for batch is  -2.2294371128082275\n",
      "Val Loss for batch is  -2.587015151977539\n",
      "Val Loss for batch is  -2.3978347778320312\n",
      "Val Loss for batch is  -3.949312448501587\n",
      "|Iter  1658  | Total Val Loss  -11.163599491119385 |\n",
      "Loss for batch is  -1.4488714933395386\n",
      "Loss for batch is  -1.5946930646896362\n",
      "Loss for batch is  -1.484574317932129\n",
      "Loss for batch is  -2.5893192291259766\n",
      "|Iter  1659  | Total Train Loss  -7.11745810508728 |\n",
      "Val Loss for batch is  -2.3194522857666016\n",
      "Val Loss for batch is  -2.523869752883911\n",
      "Val Loss for batch is  -2.4362990856170654\n",
      "Val Loss for batch is  -3.9321329593658447\n",
      "|Iter  1659  | Total Val Loss  -11.211754083633423 |\n",
      "Loss for batch is  -1.4722262620925903\n",
      "Loss for batch is  -1.598522424697876\n",
      "Loss for batch is  -1.4748289585113525\n",
      "Loss for batch is  -2.5940229892730713\n",
      "|Iter  1660  | Total Train Loss  -7.13960063457489 |\n",
      "Val Loss for batch is  -2.2916488647460938\n",
      "Val Loss for batch is  -2.5964794158935547\n",
      "Val Loss for batch is  -2.467817783355713\n",
      "Val Loss for batch is  -3.956810474395752\n",
      "|Iter  1660  | Total Val Loss  -11.312756538391113 |\n",
      "Loss for batch is  -1.4644217491149902\n",
      "Loss for batch is  -1.5721124410629272\n",
      "Loss for batch is  -1.4778112173080444\n",
      "Loss for batch is  -2.5747194290161133\n",
      "|Iter  1661  | Total Train Loss  -7.089064836502075 |\n",
      "Val Loss for batch is  -2.178426742553711\n",
      "Val Loss for batch is  -2.5828332901000977\n",
      "Val Loss for batch is  -2.378107786178589\n",
      "Val Loss for batch is  -3.9394400119781494\n",
      "|Iter  1661  | Total Val Loss  -11.078807830810547 |\n",
      "Loss for batch is  -1.4397050142288208\n",
      "Loss for batch is  -1.593429684638977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.4310346841812134\n",
      "Loss for batch is  -2.543313980102539\n",
      "|Iter  1662  | Total Train Loss  -7.00748336315155 |\n",
      "Val Loss for batch is  -2.1924972534179688\n",
      "Val Loss for batch is  -2.586991310119629\n",
      "Val Loss for batch is  -2.3741612434387207\n",
      "Val Loss for batch is  -3.9164769649505615\n",
      "|Iter  1662  | Total Val Loss  -11.07012677192688 |\n",
      "Loss for batch is  -1.445149540901184\n",
      "Loss for batch is  -1.5347864627838135\n",
      "Loss for batch is  -1.4496674537658691\n",
      "Loss for batch is  -2.5817196369171143\n",
      "|Iter  1663  | Total Train Loss  -7.011323094367981 |\n",
      "Val Loss for batch is  -2.1195735931396484\n",
      "Val Loss for batch is  -2.474778890609741\n",
      "Val Loss for batch is  -2.3879470825195312\n",
      "Val Loss for batch is  -3.912062168121338\n",
      "|Iter  1663  | Total Val Loss  -10.894361734390259 |\n",
      "Loss for batch is  -1.37810218334198\n",
      "Loss for batch is  -1.57606840133667\n",
      "Loss for batch is  -1.4372029304504395\n",
      "Loss for batch is  -2.5127594470977783\n",
      "|Iter  1664  | Total Train Loss  -6.904132962226868 |\n",
      "Val Loss for batch is  -2.3049559593200684\n",
      "Val Loss for batch is  -2.610614538192749\n",
      "Val Loss for batch is  -2.423063278198242\n",
      "Val Loss for batch is  -3.9129507541656494\n",
      "|Iter  1664  | Total Val Loss  -11.251584529876709 |\n",
      "Loss for batch is  -1.4719117879867554\n",
      "Loss for batch is  -1.5650583505630493\n",
      "Loss for batch is  -1.4247182607650757\n",
      "Loss for batch is  -2.6032190322875977\n",
      "|Iter  1665  | Total Train Loss  -7.064907431602478 |\n",
      "Val Loss for batch is  -2.154527425765991\n",
      "Val Loss for batch is  -2.5677170753479004\n",
      "Val Loss for batch is  -2.3630008697509766\n",
      "Val Loss for batch is  -3.941249132156372\n",
      "|Iter  1665  | Total Val Loss  -11.02649450302124 |\n",
      "Loss for batch is  -1.4119352102279663\n",
      "Loss for batch is  -1.567728877067566\n",
      "Loss for batch is  -1.4777452945709229\n",
      "Loss for batch is  -2.5233118534088135\n",
      "|Iter  1666  | Total Train Loss  -6.9807212352752686 |\n",
      "Val Loss for batch is  -2.2795188426971436\n",
      "Val Loss for batch is  -2.5713860988616943\n",
      "Val Loss for batch is  -2.398219347000122\n",
      "Val Loss for batch is  -3.9260671138763428\n",
      "|Iter  1666  | Total Val Loss  -11.175191402435303 |\n",
      "Loss for batch is  -1.4714561700820923\n",
      "Loss for batch is  -1.6050150394439697\n",
      "Loss for batch is  -1.4432029724121094\n",
      "Loss for batch is  -2.576436758041382\n",
      "|Iter  1667  | Total Train Loss  -7.096110939979553 |\n",
      "Val Loss for batch is  -2.3263845443725586\n",
      "Val Loss for batch is  -2.6166462898254395\n",
      "Val Loss for batch is  -2.441479444503784\n",
      "Val Loss for batch is  -3.955646276473999\n",
      "|Iter  1667  | Total Val Loss  -11.340156555175781 |\n",
      "Loss for batch is  -1.487422227859497\n",
      "Loss for batch is  -1.5598558187484741\n",
      "Loss for batch is  -1.4836431741714478\n",
      "Loss for batch is  -2.5993847846984863\n",
      "|Iter  1668  | Total Train Loss  -7.130306005477905 |\n",
      "Val Loss for batch is  -2.2563328742980957\n",
      "Val Loss for batch is  -2.6049230098724365\n",
      "Val Loss for batch is  -2.4270412921905518\n",
      "Val Loss for batch is  -3.9282186031341553\n",
      "|Iter  1668  | Total Val Loss  -11.21651577949524 |\n",
      "Loss for batch is  -1.4540528059005737\n",
      "Loss for batch is  -1.5992714166641235\n",
      "Loss for batch is  -1.4811546802520752\n",
      "Loss for batch is  -2.5523681640625\n",
      "|Iter  1669  | Total Train Loss  -7.0868470668792725 |\n",
      "Val Loss for batch is  -2.2649834156036377\n",
      "Val Loss for batch is  -2.620490550994873\n",
      "Val Loss for batch is  -2.4546091556549072\n",
      "Val Loss for batch is  -3.935203790664673\n",
      "|Iter  1669  | Total Val Loss  -11.27528691291809 |\n",
      "Loss for batch is  -1.4855982065200806\n",
      "Loss for batch is  -1.5951732397079468\n",
      "Loss for batch is  -1.469545602798462\n",
      "Loss for batch is  -2.612116813659668\n",
      "|Iter  1670  | Total Train Loss  -7.162433862686157 |\n",
      "Val Loss for batch is  -2.3284873962402344\n",
      "Val Loss for batch is  -2.6454336643218994\n",
      "Val Loss for batch is  -2.430600643157959\n",
      "Val Loss for batch is  -3.958834648132324\n",
      "|Iter  1670  | Total Val Loss  -11.363356351852417 |\n",
      "Loss for batch is  -1.48514986038208\n",
      "Loss for batch is  -1.598806619644165\n",
      "Loss for batch is  -1.5034589767456055\n",
      "Loss for batch is  -2.5945186614990234\n",
      "|Iter  1671  | Total Train Loss  -7.181934118270874 |\n",
      "Val Loss for batch is  -2.3116862773895264\n",
      "Val Loss for batch is  -2.6030426025390625\n",
      "Val Loss for batch is  -2.43296217918396\n",
      "Val Loss for batch is  -3.943713903427124\n",
      "|Iter  1671  | Total Val Loss  -11.291404962539673 |\n",
      "Loss for batch is  -1.4842277765274048\n",
      "Loss for batch is  -1.612173318862915\n",
      "Loss for batch is  -1.4815218448638916\n",
      "Loss for batch is  -2.5887410640716553\n",
      "|Iter  1672  | Total Train Loss  -7.166664004325867 |\n",
      "Val Loss for batch is  -2.3036117553710938\n",
      "Val Loss for batch is  -2.6305599212646484\n",
      "Val Loss for batch is  -2.469015121459961\n",
      "Val Loss for batch is  -3.893730401992798\n",
      "|Iter  1672  | Total Val Loss  -11.296917200088501 |\n",
      "Loss for batch is  -1.4963504076004028\n",
      "Loss for batch is  -1.6046487092971802\n",
      "Loss for batch is  -1.4750964641571045\n",
      "Loss for batch is  -2.6184191703796387\n",
      "|Iter  1673  | Total Train Loss  -7.194514751434326 |\n",
      "Val Loss for batch is  -2.2608911991119385\n",
      "Val Loss for batch is  -2.6098954677581787\n",
      "Val Loss for batch is  -2.4661269187927246\n",
      "Val Loss for batch is  -3.966294050216675\n",
      "|Iter  1673  | Total Val Loss  -11.303207635879517 |\n",
      "Loss for batch is  -1.4866691827774048\n",
      "Loss for batch is  -1.5864924192428589\n",
      "Loss for batch is  -1.4974430799484253\n",
      "Loss for batch is  -2.5977437496185303\n",
      "|Iter  1674  | Total Train Loss  -7.168348431587219 |\n",
      "Val Loss for batch is  -2.2344794273376465\n",
      "Val Loss for batch is  -2.6027424335479736\n",
      "Val Loss for batch is  -2.4662277698516846\n",
      "Val Loss for batch is  -3.9459972381591797\n",
      "|Iter  1674  | Total Val Loss  -11.249446868896484 |\n",
      "Loss for batch is  -1.4726961851119995\n",
      "Loss for batch is  -1.6151123046875\n",
      "Loss for batch is  -1.472476840019226\n",
      "Loss for batch is  -2.593648910522461\n",
      "|Iter  1675  | Total Train Loss  -7.1539342403411865 |\n",
      "Val Loss for batch is  -2.326887607574463\n",
      "Val Loss for batch is  -2.6484220027923584\n",
      "Val Loss for batch is  -2.477301597595215\n",
      "Val Loss for batch is  -3.9735143184661865\n",
      "|Iter  1675  | Total Val Loss  -11.426125526428223 |\n",
      "Loss for batch is  -1.4982315301895142\n",
      "Loss for batch is  -1.5996458530426025\n",
      "Loss for batch is  -1.461928129196167\n",
      "Loss for batch is  -2.6145055294036865\n",
      "|Iter  1676  | Total Train Loss  -7.17431104183197 |\n",
      "Val Loss for batch is  -2.2758193016052246\n",
      "Val Loss for batch is  -2.6096251010894775\n",
      "Val Loss for batch is  -2.426828384399414\n",
      "Val Loss for batch is  -3.9582359790802\n",
      "|Iter  1676  | Total Val Loss  -11.270508766174316 |\n",
      "Loss for batch is  -1.4776877164840698\n",
      "Loss for batch is  -1.585842251777649\n",
      "Loss for batch is  -1.498002290725708\n",
      "Loss for batch is  -2.5986135005950928\n",
      "|Iter  1677  | Total Train Loss  -7.1601457595825195 |\n",
      "Val Loss for batch is  -2.2650668621063232\n",
      "Val Loss for batch is  -2.6066627502441406\n",
      "Val Loss for batch is  -2.41261887550354\n",
      "Val Loss for batch is  -3.936328411102295\n",
      "|Iter  1677  | Total Val Loss  -11.220676898956299 |\n",
      "Loss for batch is  -1.4603755474090576\n",
      "Loss for batch is  -1.6108481884002686\n",
      "Loss for batch is  -1.4577876329421997\n",
      "Loss for batch is  -2.6075186729431152\n",
      "|Iter  1678  | Total Train Loss  -7.136530041694641 |\n",
      "Val Loss for batch is  -2.320662498474121\n",
      "Val Loss for batch is  -2.628119945526123\n",
      "Val Loss for batch is  -2.4444541931152344\n",
      "Val Loss for batch is  -3.972747325897217\n",
      "|Iter  1678  | Total Val Loss  -11.365983963012695 |\n",
      "Loss for batch is  -1.495981216430664\n",
      "Loss for batch is  -1.594819188117981\n",
      "Loss for batch is  -1.4900286197662354\n",
      "Loss for batch is  -2.622380256652832\n",
      "|Iter  1679  | Total Train Loss  -7.203209280967712 |\n",
      "Val Loss for batch is  -2.2462947368621826\n",
      "Val Loss for batch is  -2.6185829639434814\n",
      "Val Loss for batch is  -2.3363826274871826\n",
      "Val Loss for batch is  -3.952868938446045\n",
      "|Iter  1679  | Total Val Loss  -11.154129266738892 |\n",
      "Loss for batch is  -1.4809147119522095\n",
      "Loss for batch is  -1.6119256019592285\n",
      "Loss for batch is  -1.49937903881073\n",
      "Loss for batch is  -2.6187894344329834\n",
      "|Iter  1680  | Total Train Loss  -7.211008787155151 |\n",
      "Val Loss for batch is  -2.209373712539673\n",
      "Val Loss for batch is  -2.597529888153076\n",
      "Val Loss for batch is  -2.3842031955718994\n",
      "Val Loss for batch is  -3.960026741027832\n",
      "|Iter  1680  | Total Val Loss  -11.15113353729248 |\n",
      "Loss for batch is  -1.4705946445465088\n",
      "Loss for batch is  -1.6125589609146118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.4915831089019775\n",
      "Loss for batch is  -2.602994918823242\n",
      "|Iter  1681  | Total Train Loss  -7.17773163318634 |\n",
      "Val Loss for batch is  -2.2804830074310303\n",
      "Val Loss for batch is  -2.5609943866729736\n",
      "Val Loss for batch is  -2.3478381633758545\n",
      "Val Loss for batch is  -3.9432754516601562\n",
      "|Iter  1681  | Total Val Loss  -11.132591009140015 |\n",
      "Loss for batch is  -1.4929900169372559\n",
      "Loss for batch is  -1.6230570077896118\n",
      "Loss for batch is  -1.5006269216537476\n",
      "Loss for batch is  -2.5992016792297363\n",
      "|Iter  1682  | Total Train Loss  -7.215875625610352 |\n",
      "Val Loss for batch is  -2.2745893001556396\n",
      "Val Loss for batch is  -2.654402256011963\n",
      "Val Loss for batch is  -2.4316327571868896\n",
      "Val Loss for batch is  -3.96781849861145\n",
      "|Iter  1682  | Total Val Loss  -11.328442811965942 |\n",
      "Loss for batch is  -1.4944090843200684\n",
      "Loss for batch is  -1.6102707386016846\n",
      "Loss for batch is  -1.4838789701461792\n",
      "Loss for batch is  -2.631070852279663\n",
      "|Iter  1683  | Total Train Loss  -7.219629645347595 |\n",
      "Val Loss for batch is  -2.261448621749878\n",
      "Val Loss for batch is  -2.6202757358551025\n",
      "Val Loss for batch is  -2.41792631149292\n",
      "Val Loss for batch is  -3.9628796577453613\n",
      "|Iter  1683  | Total Val Loss  -11.262530326843262 |\n",
      "Loss for batch is  -1.4763379096984863\n",
      "Loss for batch is  -1.6063923835754395\n",
      "Loss for batch is  -1.5112944841384888\n",
      "Loss for batch is  -2.610163688659668\n",
      "|Iter  1684  | Total Train Loss  -7.2041884660720825 |\n",
      "Val Loss for batch is  -2.2748825550079346\n",
      "Val Loss for batch is  -2.6237144470214844\n",
      "Val Loss for batch is  -2.4253053665161133\n",
      "Val Loss for batch is  -3.961735248565674\n",
      "|Iter  1684  | Total Val Loss  -11.285637617111206 |\n",
      "Loss for batch is  -1.4999898672103882\n",
      "Loss for batch is  -1.6221895217895508\n",
      "Loss for batch is  -1.484163761138916\n",
      "Loss for batch is  -2.6315712928771973\n",
      "|Iter  1685  | Total Train Loss  -7.237914443016052 |\n",
      "Val Loss for batch is  -2.251039505004883\n",
      "Val Loss for batch is  -2.622720718383789\n",
      "Val Loss for batch is  -2.393871784210205\n",
      "Val Loss for batch is  -3.9811899662017822\n",
      "|Iter  1685  | Total Val Loss  -11.24882197380066 |\n",
      "Loss for batch is  -1.4928539991378784\n",
      "Loss for batch is  -1.6199367046356201\n",
      "Loss for batch is  -1.5122188329696655\n",
      "Loss for batch is  -2.620570659637451\n",
      "|Iter  1686  | Total Train Loss  -7.245580196380615 |\n",
      "Val Loss for batch is  -2.2352774143218994\n",
      "Val Loss for batch is  -2.611222743988037\n",
      "Val Loss for batch is  -2.4353580474853516\n",
      "Val Loss for batch is  -3.9829812049865723\n",
      "|Iter  1686  | Total Val Loss  -11.26483941078186 |\n",
      "Loss for batch is  -1.4955544471740723\n",
      "Loss for batch is  -1.6212198734283447\n",
      "Loss for batch is  -1.490469217300415\n",
      "Loss for batch is  -2.634918451309204\n",
      "|Iter  1687  | Total Train Loss  -7.242161989212036 |\n",
      "Val Loss for batch is  -2.2714414596557617\n",
      "Val Loss for batch is  -2.6193878650665283\n",
      "Val Loss for batch is  -2.4044041633605957\n",
      "Val Loss for batch is  -3.981019973754883\n",
      "|Iter  1687  | Total Val Loss  -11.276253461837769 |\n",
      "Loss for batch is  -1.4936765432357788\n",
      "Loss for batch is  -1.612078070640564\n",
      "Loss for batch is  -1.5134451389312744\n",
      "Loss for batch is  -2.621497631072998\n",
      "|Iter  1688  | Total Train Loss  -7.240697383880615 |\n",
      "Val Loss for batch is  -2.252324104309082\n",
      "Val Loss for batch is  -2.654139995574951\n",
      "Val Loss for batch is  -2.2957746982574463\n",
      "Val Loss for batch is  -3.9691812992095947\n",
      "|Iter  1688  | Total Val Loss  -11.171420097351074 |\n",
      "Loss for batch is  -1.4955623149871826\n",
      "Loss for batch is  -1.6233842372894287\n",
      "Loss for batch is  -1.5072029829025269\n",
      "Loss for batch is  -2.618950366973877\n",
      "|Iter  1689  | Total Train Loss  -7.245099902153015 |\n",
      "Val Loss for batch is  -2.2723546028137207\n",
      "Val Loss for batch is  -2.6076977252960205\n",
      "Val Loss for batch is  -2.44687819480896\n",
      "Val Loss for batch is  -3.971430540084839\n",
      "|Iter  1689  | Total Val Loss  -11.29836106300354 |\n",
      "Loss for batch is  -1.4952565431594849\n",
      "Loss for batch is  -1.618746280670166\n",
      "Loss for batch is  -1.5162289142608643\n",
      "Loss for batch is  -2.6346123218536377\n",
      "|Iter  1690  | Total Train Loss  -7.264844059944153 |\n",
      "Val Loss for batch is  -2.275857448577881\n",
      "Val Loss for batch is  -2.6386473178863525\n",
      "Val Loss for batch is  -2.452423572540283\n",
      "Val Loss for batch is  -3.9808590412139893\n",
      "|Iter  1690  | Total Val Loss  -11.347787380218506 |\n",
      "Loss for batch is  -1.5065101385116577\n",
      "Loss for batch is  -1.6264235973358154\n",
      "Loss for batch is  -1.516523003578186\n",
      "Loss for batch is  -2.6167497634887695\n",
      "|Iter  1691  | Total Train Loss  -7.266206502914429 |\n",
      "Val Loss for batch is  -2.294376850128174\n",
      "Val Loss for batch is  -2.629608154296875\n",
      "Val Loss for batch is  -2.4135491847991943\n",
      "Val Loss for batch is  -3.9703621864318848\n",
      "|Iter  1691  | Total Val Loss  -11.307896375656128 |\n",
      "Loss for batch is  -1.5113239288330078\n",
      "Loss for batch is  -1.631410002708435\n",
      "Loss for batch is  -1.5130490064620972\n",
      "Loss for batch is  -2.6476612091064453\n",
      "|Iter  1692  | Total Train Loss  -7.303444147109985 |\n",
      "Val Loss for batch is  -2.3274316787719727\n",
      "Val Loss for batch is  -2.6523501873016357\n",
      "Val Loss for batch is  -2.476743698120117\n",
      "Val Loss for batch is  -3.980897903442383\n",
      "|Iter  1692  | Total Val Loss  -11.437423467636108 |\n",
      "Loss for batch is  -1.5162031650543213\n",
      "Loss for batch is  -1.634630560874939\n",
      "Loss for batch is  -1.527302861213684\n",
      "Loss for batch is  -2.6359496116638184\n",
      "|Iter  1693  | Total Train Loss  -7.314086198806763 |\n",
      "Val Loss for batch is  -2.278510332107544\n",
      "Val Loss for batch is  -2.632228374481201\n",
      "Val Loss for batch is  -2.4774036407470703\n",
      "Val Loss for batch is  -3.9953153133392334\n",
      "|Iter  1693  | Total Val Loss  -11.383457660675049 |\n",
      "Loss for batch is  -1.5145183801651\n",
      "Loss for batch is  -1.6388119459152222\n",
      "Loss for batch is  -1.5167871713638306\n",
      "Loss for batch is  -2.648519515991211\n",
      "|Iter  1694  | Total Train Loss  -7.318637013435364 |\n",
      "Val Loss for batch is  -2.308090925216675\n",
      "Val Loss for batch is  -2.6593682765960693\n",
      "Val Loss for batch is  -2.474705457687378\n",
      "Val Loss for batch is  -3.9908711910247803\n",
      "|Iter  1694  | Total Val Loss  -11.433035850524902 |\n",
      "Loss for batch is  -1.5212085247039795\n",
      "Loss for batch is  -1.6298730373382568\n",
      "Loss for batch is  -1.5225577354431152\n",
      "Loss for batch is  -2.64394211769104\n",
      "|Iter  1695  | Total Train Loss  -7.317581415176392 |\n",
      "Val Loss for batch is  -2.279681444168091\n",
      "Val Loss for batch is  -2.6491260528564453\n",
      "Val Loss for batch is  -2.4311764240264893\n",
      "Val Loss for batch is  -3.9865050315856934\n",
      "|Iter  1695  | Total Val Loss  -11.346488952636719 |\n",
      "Loss for batch is  -1.5076966285705566\n",
      "Loss for batch is  -1.6374258995056152\n",
      "Loss for batch is  -1.5001640319824219\n",
      "Loss for batch is  -2.6421399116516113\n",
      "|Iter  1696  | Total Train Loss  -7.287426471710205 |\n",
      "Val Loss for batch is  -2.317303419113159\n",
      "Val Loss for batch is  -2.6422476768493652\n",
      "Val Loss for batch is  -2.450618267059326\n",
      "Val Loss for batch is  -3.978325366973877\n",
      "|Iter  1696  | Total Val Loss  -11.388494729995728 |\n",
      "Loss for batch is  -1.5047951936721802\n",
      "Loss for batch is  -1.626598596572876\n",
      "Loss for batch is  -1.526474952697754\n",
      "Loss for batch is  -2.64328670501709\n",
      "|Iter  1697  | Total Train Loss  -7.3011554479599 |\n",
      "Val Loss for batch is  -2.2835779190063477\n",
      "Val Loss for batch is  -2.6077516078948975\n",
      "Val Loss for batch is  -2.429013967514038\n",
      "Val Loss for batch is  -3.9795238971710205\n",
      "|Iter  1697  | Total Val Loss  -11.299867391586304 |\n",
      "Loss for batch is  -1.5143617391586304\n",
      "Loss for batch is  -1.6394360065460205\n",
      "Loss for batch is  -1.509381890296936\n",
      "Loss for batch is  -2.6369972229003906\n",
      "|Iter  1698  | Total Train Loss  -7.3001768589019775 |\n",
      "Val Loss for batch is  -2.2745351791381836\n",
      "Val Loss for batch is  -2.660954236984253\n",
      "Val Loss for batch is  -2.391648054122925\n",
      "Val Loss for batch is  -4.0008697509765625\n",
      "|Iter  1698  | Total Val Loss  -11.328007221221924 |\n",
      "Loss for batch is  -1.525174617767334\n",
      "Loss for batch is  -1.6370277404785156\n",
      "Loss for batch is  -1.5286693572998047\n",
      "Loss for batch is  -2.650397300720215\n",
      "|Iter  1699  | Total Train Loss  -7.341269016265869 |\n",
      "Val Loss for batch is  -2.258396625518799\n",
      "Val Loss for batch is  -2.603297233581543\n",
      "Val Loss for batch is  -2.3695406913757324\n",
      "Val Loss for batch is  -3.9965522289276123\n",
      "|Iter  1699  | Total Val Loss  -11.227786779403687 |\n",
      "Loss for batch is  -1.514078974723816\n",
      "Loss for batch is  -1.6406749486923218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.5272027254104614\n",
      "Loss for batch is  -2.645240306854248\n",
      "|Iter  1700  | Total Train Loss  -7.327196955680847 |\n",
      "Val Loss for batch is  -2.321761131286621\n",
      "Val Loss for batch is  -2.6301429271698\n",
      "Val Loss for batch is  -2.4242403507232666\n",
      "Val Loss for batch is  -3.999940872192383\n",
      "|Iter  1700  | Total Val Loss  -11.37608528137207 |\n",
      "Loss for batch is  -1.526004672050476\n",
      "Loss for batch is  -1.642707109451294\n",
      "Loss for batch is  -1.5249687433242798\n",
      "Loss for batch is  -2.6540870666503906\n",
      "|Iter  1701  | Total Train Loss  -7.34776759147644 |\n",
      "Val Loss for batch is  -2.291093111038208\n",
      "Val Loss for batch is  -2.552609443664551\n",
      "Val Loss for batch is  -2.4253604412078857\n",
      "Val Loss for batch is  -3.9962079524993896\n",
      "|Iter  1701  | Total Val Loss  -11.265270948410034 |\n",
      "Loss for batch is  -1.5079307556152344\n",
      "Loss for batch is  -1.6477742195129395\n",
      "Loss for batch is  -1.5412293672561646\n",
      "Loss for batch is  -2.650693416595459\n",
      "|Iter  1702  | Total Train Loss  -7.347627758979797 |\n",
      "Val Loss for batch is  -2.2799341678619385\n",
      "Val Loss for batch is  -2.6353938579559326\n",
      "Val Loss for batch is  -2.4394662380218506\n",
      "Val Loss for batch is  -3.969794750213623\n",
      "|Iter  1702  | Total Val Loss  -11.324589014053345 |\n",
      "Loss for batch is  -1.525051474571228\n",
      "Loss for batch is  -1.6469238996505737\n",
      "Loss for batch is  -1.5342735052108765\n",
      "Loss for batch is  -2.653277635574341\n",
      "|Iter  1703  | Total Train Loss  -7.359526515007019 |\n",
      "Val Loss for batch is  -2.2327001094818115\n",
      "Val Loss for batch is  -2.6099319458007812\n",
      "Val Loss for batch is  -2.4643850326538086\n",
      "Val Loss for batch is  -4.003987789154053\n",
      "|Iter  1703  | Total Val Loss  -11.311004877090454 |\n",
      "Loss for batch is  -1.5248349905014038\n",
      "Loss for batch is  -1.6514266729354858\n",
      "Loss for batch is  -1.540183186531067\n",
      "Loss for batch is  -2.6593117713928223\n",
      "|Iter  1704  | Total Train Loss  -7.375756621360779 |\n",
      "Val Loss for batch is  -2.2668256759643555\n",
      "Val Loss for batch is  -2.6569066047668457\n",
      "Val Loss for batch is  -2.443795919418335\n",
      "Val Loss for batch is  -4.013815879821777\n",
      "|Iter  1704  | Total Val Loss  -11.381344079971313 |\n",
      "Loss for batch is  -1.5279781818389893\n",
      "Loss for batch is  -1.645347237586975\n",
      "Loss for batch is  -1.5286321640014648\n",
      "Loss for batch is  -2.6547529697418213\n",
      "|Iter  1705  | Total Train Loss  -7.3567105531692505 |\n",
      "Val Loss for batch is  -2.2909464836120605\n",
      "Val Loss for batch is  -2.6495184898376465\n",
      "Val Loss for batch is  -2.422487735748291\n",
      "Val Loss for batch is  -3.998020887374878\n",
      "|Iter  1705  | Total Val Loss  -11.360973596572876 |\n",
      "Loss for batch is  -1.5400358438491821\n",
      "Loss for batch is  -1.650880217552185\n",
      "Loss for batch is  -1.521702527999878\n",
      "Loss for batch is  -2.6601877212524414\n",
      "|Iter  1706  | Total Train Loss  -7.3728063106536865 |\n",
      "Val Loss for batch is  -2.261712074279785\n",
      "Val Loss for batch is  -2.615265369415283\n",
      "Val Loss for batch is  -2.4506914615631104\n",
      "Val Loss for batch is  -3.996737241744995\n",
      "|Iter  1706  | Total Val Loss  -11.324406147003174 |\n",
      "Loss for batch is  -1.5220961570739746\n",
      "Loss for batch is  -1.6479079723358154\n",
      "Loss for batch is  -1.5305430889129639\n",
      "Loss for batch is  -2.6430134773254395\n",
      "|Iter  1707  | Total Train Loss  -7.343560695648193 |\n",
      "Val Loss for batch is  -2.2657201290130615\n",
      "Val Loss for batch is  -2.6285107135772705\n",
      "Val Loss for batch is  -2.453624725341797\n",
      "Val Loss for batch is  -4.009739875793457\n",
      "|Iter  1707  | Total Val Loss  -11.357595443725586 |\n",
      "Loss for batch is  -1.5336600542068481\n",
      "Loss for batch is  -1.6495275497436523\n",
      "Loss for batch is  -1.5124881267547607\n",
      "Loss for batch is  -2.659390449523926\n",
      "|Iter  1708  | Total Train Loss  -7.355066180229187 |\n",
      "Val Loss for batch is  -2.2279720306396484\n",
      "Val Loss for batch is  -2.6112966537475586\n",
      "Val Loss for batch is  -2.3907082080841064\n",
      "Val Loss for batch is  -3.9815399646759033\n",
      "|Iter  1708  | Total Val Loss  -11.211516857147217 |\n",
      "Loss for batch is  -1.5208394527435303\n",
      "Loss for batch is  -1.637844443321228\n",
      "Loss for batch is  -1.5260870456695557\n",
      "Loss for batch is  -2.645981550216675\n",
      "|Iter  1709  | Total Train Loss  -7.330752491950989 |\n",
      "Val Loss for batch is  -2.2888617515563965\n",
      "Val Loss for batch is  -2.6207261085510254\n",
      "Val Loss for batch is  -2.435917377471924\n",
      "Val Loss for batch is  -4.011087894439697\n",
      "|Iter  1709  | Total Val Loss  -11.356593132019043 |\n",
      "Loss for batch is  -1.5189391374588013\n",
      "Loss for batch is  -1.6415343284606934\n",
      "Loss for batch is  -1.5344789028167725\n",
      "Loss for batch is  -2.6504855155944824\n",
      "|Iter  1710  | Total Train Loss  -7.3454378843307495 |\n",
      "Val Loss for batch is  -2.2717535495758057\n",
      "Val Loss for batch is  -2.640639543533325\n",
      "Val Loss for batch is  -2.435436487197876\n",
      "Val Loss for batch is  -4.018764972686768\n",
      "|Iter  1710  | Total Val Loss  -11.366594552993774 |\n",
      "Loss for batch is  -1.5320976972579956\n",
      "Loss for batch is  -1.6484061479568481\n",
      "Loss for batch is  -1.5397499799728394\n",
      "Loss for batch is  -2.664069652557373\n",
      "|Iter  1711  | Total Train Loss  -7.384323477745056 |\n",
      "Val Loss for batch is  -2.2761006355285645\n",
      "Val Loss for batch is  -2.6344385147094727\n",
      "Val Loss for batch is  -2.465655565261841\n",
      "Val Loss for batch is  -3.9913089275360107\n",
      "|Iter  1711  | Total Val Loss  -11.367503643035889 |\n",
      "Loss for batch is  -1.5321186780929565\n",
      "Loss for batch is  -1.6442451477050781\n",
      "Loss for batch is  -1.5334515571594238\n",
      "Loss for batch is  -2.6582841873168945\n",
      "|Iter  1712  | Total Train Loss  -7.368099570274353 |\n",
      "Val Loss for batch is  -2.25681209564209\n",
      "Val Loss for batch is  -2.652442455291748\n",
      "Val Loss for batch is  -2.396026611328125\n",
      "Val Loss for batch is  -3.987558603286743\n",
      "|Iter  1712  | Total Val Loss  -11.292839765548706 |\n",
      "Loss for batch is  -1.520748257637024\n",
      "Loss for batch is  -1.6398375034332275\n",
      "Loss for batch is  -1.530997395515442\n",
      "Loss for batch is  -2.6568551063537598\n",
      "|Iter  1713  | Total Train Loss  -7.348438262939453 |\n",
      "Val Loss for batch is  -2.2586283683776855\n",
      "Val Loss for batch is  -2.6414849758148193\n",
      "Val Loss for batch is  -2.420900344848633\n",
      "Val Loss for batch is  -4.013572692871094\n",
      "|Iter  1713  | Total Val Loss  -11.334586381912231 |\n",
      "Loss for batch is  -1.524501085281372\n",
      "Loss for batch is  -1.6468276977539062\n",
      "Loss for batch is  -1.5220295190811157\n",
      "Loss for batch is  -2.6405839920043945\n",
      "|Iter  1714  | Total Train Loss  -7.333942294120789 |\n",
      "Val Loss for batch is  -2.2733399868011475\n",
      "Val Loss for batch is  -2.6306469440460205\n",
      "Val Loss for batch is  -2.395582437515259\n",
      "Val Loss for batch is  -3.984218120574951\n",
      "|Iter  1714  | Total Val Loss  -11.283787488937378 |\n",
      "Loss for batch is  -1.5298115015029907\n",
      "Loss for batch is  -1.6305131912231445\n",
      "Loss for batch is  -1.5299646854400635\n",
      "Loss for batch is  -2.6702303886413574\n",
      "|Iter  1715  | Total Train Loss  -7.360519766807556 |\n",
      "Val Loss for batch is  -2.1878817081451416\n",
      "Val Loss for batch is  -2.6488702297210693\n",
      "Val Loss for batch is  -2.4429454803466797\n",
      "Val Loss for batch is  -3.9987857341766357\n",
      "|Iter  1715  | Total Val Loss  -11.278483152389526 |\n",
      "Loss for batch is  -1.5301076173782349\n",
      "Loss for batch is  -1.645447850227356\n",
      "Loss for batch is  -1.5295453071594238\n",
      "Loss for batch is  -2.6653974056243896\n",
      "|Iter  1716  | Total Train Loss  -7.370498180389404 |\n",
      "Val Loss for batch is  -2.305311918258667\n",
      "Val Loss for batch is  -2.5500569343566895\n",
      "Val Loss for batch is  -2.2967159748077393\n",
      "Val Loss for batch is  -3.9995477199554443\n",
      "|Iter  1716  | Total Val Loss  -11.15163254737854 |\n",
      "Loss for batch is  -1.52561354637146\n",
      "Loss for batch is  -1.6494890451431274\n",
      "Loss for batch is  -1.5461143255233765\n",
      "Loss for batch is  -2.6505866050720215\n",
      "|Iter  1717  | Total Train Loss  -7.371803522109985 |\n",
      "Val Loss for batch is  -2.282668352127075\n",
      "Val Loss for batch is  -2.6444859504699707\n",
      "Val Loss for batch is  -2.414273262023926\n",
      "Val Loss for batch is  -4.010884761810303\n",
      "|Iter  1717  | Total Val Loss  -11.352312326431274 |\n",
      "Loss for batch is  -1.5254861116409302\n",
      "Loss for batch is  -1.6555736064910889\n",
      "Loss for batch is  -1.5190937519073486\n",
      "Loss for batch is  -2.67191743850708\n",
      "|Iter  1718  | Total Train Loss  -7.372070908546448 |\n",
      "Val Loss for batch is  -2.284916400909424\n",
      "Val Loss for batch is  -2.6324968338012695\n",
      "Val Loss for batch is  -2.350454092025757\n",
      "Val Loss for batch is  -4.011404991149902\n",
      "|Iter  1718  | Total Val Loss  -11.279272317886353 |\n",
      "Loss for batch is  -1.5166999101638794\n",
      "Loss for batch is  -1.6600306034088135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.5379530191421509\n",
      "Loss for batch is  -2.6596109867095947\n",
      "|Iter  1719  | Total Train Loss  -7.3742945194244385 |\n",
      "Val Loss for batch is  -2.2696781158447266\n",
      "Val Loss for batch is  -2.645425319671631\n",
      "Val Loss for batch is  -2.451738119125366\n",
      "Val Loss for batch is  -3.998380184173584\n",
      "|Iter  1719  | Total Val Loss  -11.365221738815308 |\n",
      "Loss for batch is  -1.5313217639923096\n",
      "Loss for batch is  -1.6584827899932861\n",
      "Loss for batch is  -1.5317786931991577\n",
      "Loss for batch is  -2.6685476303100586\n",
      "|Iter  1720  | Total Train Loss  -7.390130877494812 |\n",
      "Val Loss for batch is  -2.320502519607544\n",
      "Val Loss for batch is  -2.631331443786621\n",
      "Val Loss for batch is  -2.448673725128174\n",
      "Val Loss for batch is  -4.005094051361084\n",
      "|Iter  1720  | Total Val Loss  -11.405601739883423 |\n",
      "Loss for batch is  -1.5458987951278687\n",
      "Loss for batch is  -1.6447311639785767\n",
      "Loss for batch is  -1.536170482635498\n",
      "Loss for batch is  -2.6709625720977783\n",
      "|Iter  1721  | Total Train Loss  -7.397763013839722 |\n",
      "Val Loss for batch is  -2.1867311000823975\n",
      "Val Loss for batch is  -2.6110739707946777\n",
      "Val Loss for batch is  -2.4359023571014404\n",
      "Val Loss for batch is  -4.0032958984375\n",
      "|Iter  1721  | Total Val Loss  -11.237003326416016 |\n",
      "Loss for batch is  -1.5202867984771729\n",
      "Loss for batch is  -1.6511064767837524\n",
      "Loss for batch is  -1.5440669059753418\n",
      "Loss for batch is  -2.6585254669189453\n",
      "|Iter  1722  | Total Train Loss  -7.373985648155212 |\n",
      "Val Loss for batch is  -2.3173587322235107\n",
      "Val Loss for batch is  -2.485466718673706\n",
      "Val Loss for batch is  -2.439727783203125\n",
      "Val Loss for batch is  -3.9785494804382324\n",
      "|Iter  1722  | Total Val Loss  -11.221102714538574 |\n",
      "Loss for batch is  -1.5184513330459595\n",
      "Loss for batch is  -1.663038730621338\n",
      "Loss for batch is  -1.534535527229309\n",
      "Loss for batch is  -2.668287992477417\n",
      "|Iter  1723  | Total Train Loss  -7.384313583374023 |\n",
      "Val Loss for batch is  -2.2987661361694336\n",
      "Val Loss for batch is  -2.6043429374694824\n",
      "Val Loss for batch is  -2.4390347003936768\n",
      "Val Loss for batch is  -4.010111331939697\n",
      "|Iter  1723  | Total Val Loss  -11.35225510597229 |\n",
      "Loss for batch is  -1.5440962314605713\n",
      "Loss for batch is  -1.660786509513855\n",
      "Loss for batch is  -1.5459282398223877\n",
      "Loss for batch is  -2.678227424621582\n",
      "|Iter  1724  | Total Train Loss  -7.429038405418396 |\n",
      "Val Loss for batch is  -2.3236334323883057\n",
      "Val Loss for batch is  -2.6313440799713135\n",
      "Val Loss for batch is  -2.4356160163879395\n",
      "Val Loss for batch is  -3.9921352863311768\n",
      "|Iter  1724  | Total Val Loss  -11.382728815078735 |\n",
      "Loss for batch is  -1.5432988405227661\n",
      "Loss for batch is  -1.6604666709899902\n",
      "Loss for batch is  -1.5468143224716187\n",
      "Loss for batch is  -2.68107271194458\n",
      "|Iter  1725  | Total Train Loss  -7.431652545928955 |\n",
      "Val Loss for batch is  -2.245968818664551\n",
      "Val Loss for batch is  -2.618746042251587\n",
      "Val Loss for batch is  -2.4108712673187256\n",
      "Val Loss for batch is  -4.012554168701172\n",
      "|Iter  1725  | Total Val Loss  -11.288140296936035 |\n",
      "Loss for batch is  -1.5399730205535889\n",
      "Loss for batch is  -1.6517887115478516\n",
      "Loss for batch is  -1.5484631061553955\n",
      "Loss for batch is  -2.6569995880126953\n",
      "|Iter  1726  | Total Train Loss  -7.397224426269531 |\n",
      "Val Loss for batch is  -2.2615954875946045\n",
      "Val Loss for batch is  -2.6219005584716797\n",
      "Val Loss for batch is  -2.3961853981018066\n",
      "Val Loss for batch is  -4.027833461761475\n",
      "|Iter  1726  | Total Val Loss  -11.307514905929565 |\n",
      "Loss for batch is  -1.5224902629852295\n",
      "Loss for batch is  -1.660817265510559\n",
      "Loss for batch is  -1.531941533088684\n",
      "Loss for batch is  -2.634636402130127\n",
      "|Iter  1727  | Total Train Loss  -7.3498854637146 |\n",
      "Val Loss for batch is  -2.293304681777954\n",
      "Val Loss for batch is  -2.627840757369995\n",
      "Val Loss for batch is  -2.4986274242401123\n",
      "Val Loss for batch is  -4.02215576171875\n",
      "|Iter  1727  | Total Val Loss  -11.441928625106812 |\n",
      "Loss for batch is  -1.5405900478363037\n",
      "Loss for batch is  -1.6289762258529663\n",
      "Loss for batch is  -1.527711272239685\n",
      "Loss for batch is  -2.6817238330841064\n",
      "|Iter  1728  | Total Train Loss  -7.3790013790130615 |\n",
      "Val Loss for batch is  -2.257998466491699\n",
      "Val Loss for batch is  -2.644432783126831\n",
      "Val Loss for batch is  -2.4550540447235107\n",
      "Val Loss for batch is  -4.025334358215332\n",
      "|Iter  1728  | Total Val Loss  -11.382819652557373 |\n",
      "Loss for batch is  -1.5374175310134888\n",
      "Loss for batch is  -1.6450787782669067\n",
      "Loss for batch is  -1.5516971349716187\n",
      "Loss for batch is  -2.6535542011260986\n",
      "|Iter  1729  | Total Train Loss  -7.387747645378113 |\n",
      "Val Loss for batch is  -2.204885959625244\n",
      "Val Loss for batch is  -2.6092355251312256\n",
      "Val Loss for batch is  -2.40004825592041\n",
      "Val Loss for batch is  -4.010921478271484\n",
      "|Iter  1729  | Total Val Loss  -11.225091218948364 |\n",
      "Loss for batch is  -1.5268975496292114\n",
      "Loss for batch is  -1.6647968292236328\n",
      "Loss for batch is  -1.5306769609451294\n",
      "Loss for batch is  -2.6719934940338135\n",
      "|Iter  1730  | Total Train Loss  -7.394364833831787 |\n",
      "Val Loss for batch is  -2.285245895385742\n",
      "Val Loss for batch is  -2.6302738189697266\n",
      "Val Loss for batch is  -2.443263292312622\n",
      "Val Loss for batch is  -4.03626823425293\n",
      "|Iter  1730  | Total Val Loss  -11.39505124092102 |\n",
      "Loss for batch is  -1.5445934534072876\n",
      "Loss for batch is  -1.6611872911453247\n",
      "Loss for batch is  -1.5449695587158203\n",
      "Loss for batch is  -2.6778736114501953\n",
      "|Iter  1731  | Total Train Loss  -7.428623914718628 |\n",
      "Val Loss for batch is  -2.3002490997314453\n",
      "Val Loss for batch is  -2.669506549835205\n",
      "Val Loss for batch is  -2.433842182159424\n",
      "Val Loss for batch is  -4.018648147583008\n",
      "|Iter  1731  | Total Val Loss  -11.422245979309082 |\n",
      "Loss for batch is  -1.5492467880249023\n",
      "Loss for batch is  -1.6622473001480103\n",
      "Loss for batch is  -1.5397802591323853\n",
      "Loss for batch is  -2.678412914276123\n",
      "|Iter  1732  | Total Train Loss  -7.429687261581421 |\n",
      "Val Loss for batch is  -2.1759836673736572\n",
      "Val Loss for batch is  -2.627859354019165\n",
      "Val Loss for batch is  -2.427358627319336\n",
      "Val Loss for batch is  -3.983612537384033\n",
      "|Iter  1732  | Total Val Loss  -11.214814186096191 |\n",
      "Loss for batch is  -1.532598614692688\n",
      "Loss for batch is  -1.6583690643310547\n",
      "Loss for batch is  -1.5478780269622803\n",
      "Loss for batch is  -2.6585779190063477\n",
      "|Iter  1733  | Total Train Loss  -7.397423624992371 |\n",
      "Val Loss for batch is  -2.283066987991333\n",
      "Val Loss for batch is  -2.664828062057495\n",
      "Val Loss for batch is  -2.3783881664276123\n",
      "Val Loss for batch is  -3.9890334606170654\n",
      "|Iter  1733  | Total Val Loss  -11.315316677093506 |\n",
      "Loss for batch is  -1.5413461923599243\n",
      "Loss for batch is  -1.6658943891525269\n",
      "Loss for batch is  -1.5271432399749756\n",
      "Loss for batch is  -2.668842315673828\n",
      "|Iter  1734  | Total Train Loss  -7.403226137161255 |\n",
      "Val Loss for batch is  -2.2970521450042725\n",
      "Val Loss for batch is  -2.6322083473205566\n",
      "Val Loss for batch is  -2.399132251739502\n",
      "Val Loss for batch is  -4.022556781768799\n",
      "|Iter  1734  | Total Val Loss  -11.35094952583313 |\n",
      "Loss for batch is  -1.5356309413909912\n",
      "Loss for batch is  -1.6703505516052246\n",
      "Loss for batch is  -1.5303337574005127\n",
      "Loss for batch is  -2.6771135330200195\n",
      "|Iter  1735  | Total Train Loss  -7.413428783416748 |\n",
      "Val Loss for batch is  -2.2762057781219482\n",
      "Val Loss for batch is  -2.652090072631836\n",
      "Val Loss for batch is  -2.385953187942505\n",
      "Val Loss for batch is  -4.022611141204834\n",
      "|Iter  1735  | Total Val Loss  -11.336860179901123 |\n",
      "Loss for batch is  -1.5440806150436401\n",
      "Loss for batch is  -1.6685433387756348\n",
      "Loss for batch is  -1.5521292686462402\n",
      "Loss for batch is  -2.6792969703674316\n",
      "|Iter  1736  | Total Train Loss  -7.444050192832947 |\n",
      "Val Loss for batch is  -2.2416863441467285\n",
      "Val Loss for batch is  -2.614079713821411\n",
      "Val Loss for batch is  -2.388634204864502\n",
      "Val Loss for batch is  -3.991102695465088\n",
      "|Iter  1736  | Total Val Loss  -11.23550295829773 |\n",
      "Loss for batch is  -1.5283377170562744\n",
      "Loss for batch is  -1.6671218872070312\n",
      "Loss for batch is  -1.5353333950042725\n",
      "Loss for batch is  -2.6538853645324707\n",
      "|Iter  1737  | Total Train Loss  -7.384678363800049 |\n",
      "Val Loss for batch is  -2.2555010318756104\n",
      "Val Loss for batch is  -2.646592855453491\n",
      "Val Loss for batch is  -2.4264931678771973\n",
      "Val Loss for batch is  -4.027585506439209\n",
      "|Iter  1737  | Total Val Loss  -11.356172561645508 |\n",
      "Loss for batch is  -1.5418872833251953\n",
      "Loss for batch is  -1.6657599210739136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.5283011198043823\n",
      "Loss for batch is  -2.6902682781219482\n",
      "|Iter  1738  | Total Train Loss  -7.4262166023254395 |\n",
      "Val Loss for batch is  -2.282075881958008\n",
      "Val Loss for batch is  -2.6515920162200928\n",
      "Val Loss for batch is  -2.433675765991211\n",
      "Val Loss for batch is  -4.020251750946045\n",
      "|Iter  1738  | Total Val Loss  -11.387595415115356 |\n",
      "Loss for batch is  -1.5525285005569458\n",
      "Loss for batch is  -1.6436165571212769\n",
      "Loss for batch is  -1.5489948987960815\n",
      "Loss for batch is  -2.6812968254089355\n",
      "|Iter  1739  | Total Train Loss  -7.42643678188324 |\n",
      "Val Loss for batch is  -2.2703537940979004\n",
      "Val Loss for batch is  -2.632129669189453\n",
      "Val Loss for batch is  -2.467191457748413\n",
      "Val Loss for batch is  -4.038518905639648\n",
      "|Iter  1739  | Total Val Loss  -11.408193826675415 |\n",
      "Loss for batch is  -1.5527832508087158\n",
      "Loss for batch is  -1.6764873266220093\n",
      "Loss for batch is  -1.5614460706710815\n",
      "Loss for batch is  -2.6900792121887207\n",
      "|Iter  1740  | Total Train Loss  -7.480795860290527 |\n",
      "Val Loss for batch is  -2.2800846099853516\n",
      "Val Loss for batch is  -2.633841037750244\n",
      "Val Loss for batch is  -2.4709155559539795\n",
      "Val Loss for batch is  -4.010224342346191\n",
      "|Iter  1740  | Total Val Loss  -11.395065546035767 |\n",
      "Loss for batch is  -1.5574216842651367\n",
      "Loss for batch is  -1.6790235042572021\n",
      "Loss for batch is  -1.5556886196136475\n",
      "Loss for batch is  -2.6906204223632812\n",
      "|Iter  1741  | Total Train Loss  -7.482754230499268 |\n",
      "Val Loss for batch is  -2.31917667388916\n",
      "Val Loss for batch is  -2.5684807300567627\n",
      "Val Loss for batch is  -2.4224133491516113\n",
      "Val Loss for batch is  -3.9778800010681152\n",
      "|Iter  1741  | Total Val Loss  -11.28795075416565 |\n",
      "Loss for batch is  -1.5619393587112427\n",
      "Loss for batch is  -1.6706441640853882\n",
      "Loss for batch is  -1.5612236261367798\n",
      "Loss for batch is  -2.678128719329834\n",
      "|Iter  1742  | Total Train Loss  -7.471935868263245 |\n",
      "Val Loss for batch is  -2.271310806274414\n",
      "Val Loss for batch is  -2.6644136905670166\n",
      "Val Loss for batch is  -2.458974838256836\n",
      "Val Loss for batch is  -3.999709367752075\n",
      "|Iter  1742  | Total Val Loss  -11.394408702850342 |\n",
      "Loss for batch is  -1.5552958250045776\n",
      "Loss for batch is  -1.6751854419708252\n",
      "Loss for batch is  -1.563523530960083\n",
      "Loss for batch is  -2.681544303894043\n",
      "|Iter  1743  | Total Train Loss  -7.475549101829529 |\n",
      "Val Loss for batch is  -2.3227221965789795\n",
      "Val Loss for batch is  -2.635683536529541\n",
      "Val Loss for batch is  -2.440000057220459\n",
      "Val Loss for batch is  -4.022875785827637\n",
      "|Iter  1743  | Total Val Loss  -11.421281576156616 |\n",
      "Loss for batch is  -1.556593656539917\n",
      "Loss for batch is  -1.67146897315979\n",
      "Loss for batch is  -1.5603300333023071\n",
      "Loss for batch is  -2.6884193420410156\n",
      "|Iter  1744  | Total Train Loss  -7.47681200504303 |\n",
      "Val Loss for batch is  -2.2197651863098145\n",
      "Val Loss for batch is  -2.6165008544921875\n",
      "Val Loss for batch is  -2.421808958053589\n",
      "Val Loss for batch is  -4.02021598815918\n",
      "|Iter  1744  | Total Val Loss  -11.27829098701477 |\n",
      "Loss for batch is  -1.5620993375778198\n",
      "Loss for batch is  -1.678611397743225\n",
      "Loss for batch is  -1.5504460334777832\n",
      "Loss for batch is  -2.6918976306915283\n",
      "|Iter  1745  | Total Train Loss  -7.4830543994903564 |\n",
      "Val Loss for batch is  -2.29379940032959\n",
      "Val Loss for batch is  -2.648736000061035\n",
      "Val Loss for batch is  -2.4378042221069336\n",
      "Val Loss for batch is  -4.035540580749512\n",
      "|Iter  1745  | Total Val Loss  -11.41588020324707 |\n",
      "Loss for batch is  -1.5551936626434326\n",
      "Loss for batch is  -1.6752772331237793\n",
      "Loss for batch is  -1.5614991188049316\n",
      "Loss for batch is  -2.695913553237915\n",
      "|Iter  1746  | Total Train Loss  -7.487883567810059 |\n",
      "Val Loss for batch is  -2.198533535003662\n",
      "Val Loss for batch is  -2.6549875736236572\n",
      "Val Loss for batch is  -2.4348456859588623\n",
      "Val Loss for batch is  -4.033381938934326\n",
      "|Iter  1746  | Total Val Loss  -11.321748733520508 |\n",
      "Loss for batch is  -1.5676075220108032\n",
      "Loss for batch is  -1.6803522109985352\n",
      "Loss for batch is  -1.571265697479248\n",
      "Loss for batch is  -2.69602632522583\n",
      "|Iter  1747  | Total Train Loss  -7.5152517557144165 |\n",
      "Val Loss for batch is  -2.2418735027313232\n",
      "Val Loss for batch is  -2.6651203632354736\n",
      "Val Loss for batch is  -2.4480931758880615\n",
      "Val Loss for batch is  -4.023370265960693\n",
      "|Iter  1747  | Total Val Loss  -11.378457307815552 |\n",
      "Loss for batch is  -1.5658875703811646\n",
      "Loss for batch is  -1.6825926303863525\n",
      "Loss for batch is  -1.5675901174545288\n",
      "Loss for batch is  -2.6994543075561523\n",
      "|Iter  1748  | Total Train Loss  -7.515524625778198 |\n",
      "Val Loss for batch is  -2.279996156692505\n",
      "Val Loss for batch is  -2.605847120285034\n",
      "Val Loss for batch is  -2.444173574447632\n",
      "Val Loss for batch is  -4.027169227600098\n",
      "|Iter  1748  | Total Val Loss  -11.357186079025269 |\n",
      "Loss for batch is  -1.560318112373352\n",
      "Loss for batch is  -1.683477520942688\n",
      "Loss for batch is  -1.5621472597122192\n",
      "Loss for batch is  -2.702566385269165\n",
      "|Iter  1749  | Total Train Loss  -7.508509278297424 |\n",
      "Val Loss for batch is  -2.287794351577759\n",
      "Val Loss for batch is  -2.621748685836792\n",
      "Val Loss for batch is  -2.4502201080322266\n",
      "Val Loss for batch is  -4.031932830810547\n",
      "|Iter  1749  | Total Val Loss  -11.391695976257324 |\n",
      "Loss for batch is  -1.5674922466278076\n",
      "Loss for batch is  -1.6765445470809937\n",
      "Loss for batch is  -1.569348931312561\n",
      "Loss for batch is  -2.7004499435424805\n",
      "|Iter  1750  | Total Train Loss  -7.513835668563843 |\n",
      "Val Loss for batch is  -2.2426974773406982\n",
      "Val Loss for batch is  -2.6262688636779785\n",
      "Val Loss for batch is  -2.4278945922851562\n",
      "Val Loss for batch is  -4.041080474853516\n",
      "|Iter  1750  | Total Val Loss  -11.337941408157349 |\n",
      "Loss for batch is  -1.5671868324279785\n",
      "Loss for batch is  -1.6859627962112427\n",
      "Loss for batch is  -1.570965051651001\n",
      "Loss for batch is  -2.705010414123535\n",
      "|Iter  1751  | Total Train Loss  -7.529125094413757 |\n",
      "Val Loss for batch is  -2.306878089904785\n",
      "Val Loss for batch is  -2.6311566829681396\n",
      "Val Loss for batch is  -2.4663867950439453\n",
      "Val Loss for batch is  -4.036011695861816\n",
      "|Iter  1751  | Total Val Loss  -11.440433263778687 |\n",
      "Loss for batch is  -1.5687545537948608\n",
      "Loss for batch is  -1.6790932416915894\n",
      "Loss for batch is  -1.568961501121521\n",
      "Loss for batch is  -2.6986896991729736\n",
      "|Iter  1752  | Total Train Loss  -7.515498995780945 |\n",
      "Val Loss for batch is  -2.236252784729004\n",
      "Val Loss for batch is  -2.6678950786590576\n",
      "Val Loss for batch is  -2.4388155937194824\n",
      "Val Loss for batch is  -4.023918151855469\n",
      "|Iter  1752  | Total Val Loss  -11.366881608963013 |\n",
      "Loss for batch is  -1.5685793161392212\n",
      "Loss for batch is  -1.6933015584945679\n",
      "Loss for batch is  -1.5761902332305908\n",
      "Loss for batch is  -2.6877293586730957\n",
      "|Iter  1753  | Total Train Loss  -7.525800466537476 |\n",
      "Val Loss for batch is  -2.314262628555298\n",
      "Val Loss for batch is  -2.6404831409454346\n",
      "Val Loss for batch is  -2.4618051052093506\n",
      "Val Loss for batch is  -4.034149646759033\n",
      "|Iter  1753  | Total Val Loss  -11.450700521469116 |\n",
      "Loss for batch is  -1.5740344524383545\n",
      "Loss for batch is  -1.6778336763381958\n",
      "Loss for batch is  -1.5703518390655518\n",
      "Loss for batch is  -2.7055821418762207\n",
      "|Iter  1754  | Total Train Loss  -7.527802109718323 |\n",
      "Val Loss for batch is  -2.2873377799987793\n",
      "Val Loss for batch is  -2.4585156440734863\n",
      "Val Loss for batch is  -2.3673810958862305\n",
      "Val Loss for batch is  -4.038542747497559\n",
      "|Iter  1754  | Total Val Loss  -11.151777267456055 |\n",
      "Loss for batch is  -1.5744044780731201\n",
      "Loss for batch is  -1.6759443283081055\n",
      "Loss for batch is  -1.5746798515319824\n",
      "Loss for batch is  -2.705787181854248\n",
      "|Iter  1755  | Total Train Loss  -7.530815839767456 |\n",
      "Val Loss for batch is  -2.2601730823516846\n",
      "Val Loss for batch is  -2.6447970867156982\n",
      "Val Loss for batch is  -2.417386054992676\n",
      "Val Loss for batch is  -4.029120445251465\n",
      "|Iter  1755  | Total Val Loss  -11.351476669311523 |\n",
      "Loss for batch is  -1.555816888809204\n",
      "Loss for batch is  -1.682035207748413\n",
      "Loss for batch is  -1.5686813592910767\n",
      "Loss for batch is  -2.7010293006896973\n",
      "|Iter  1756  | Total Train Loss  -7.507562756538391 |\n",
      "Val Loss for batch is  -2.271871328353882\n",
      "Val Loss for batch is  -2.5978713035583496\n",
      "Val Loss for batch is  -2.413041353225708\n",
      "Val Loss for batch is  -4.023351669311523\n",
      "|Iter  1756  | Total Val Loss  -11.306135654449463 |\n",
      "Loss for batch is  -1.5514352321624756\n",
      "Loss for batch is  -1.6794527769088745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.564359426498413\n",
      "Loss for batch is  -2.695838689804077\n",
      "|Iter  1757  | Total Train Loss  -7.49108612537384 |\n",
      "Val Loss for batch is  -2.263909101486206\n",
      "Val Loss for batch is  -2.6235511302948\n",
      "Val Loss for batch is  -2.398224353790283\n",
      "Val Loss for batch is  -4.037977695465088\n",
      "|Iter  1757  | Total Val Loss  -11.323662281036377 |\n",
      "Loss for batch is  -1.5471186637878418\n",
      "Loss for batch is  -1.681663155555725\n",
      "Loss for batch is  -1.5616105794906616\n",
      "Loss for batch is  -2.6987900733947754\n",
      "|Iter  1758  | Total Train Loss  -7.489182472229004 |\n",
      "Val Loss for batch is  -2.2583441734313965\n",
      "Val Loss for batch is  -2.6068310737609863\n",
      "Val Loss for batch is  -2.4366674423217773\n",
      "Val Loss for batch is  -4.029792308807373\n",
      "|Iter  1758  | Total Val Loss  -11.331634998321533 |\n",
      "Loss for batch is  -1.5620852708816528\n",
      "Loss for batch is  -1.6783616542816162\n",
      "Loss for batch is  -1.5729349851608276\n",
      "Loss for batch is  -2.703042507171631\n",
      "|Iter  1759  | Total Train Loss  -7.5164244174957275 |\n",
      "Val Loss for batch is  -2.2572031021118164\n",
      "Val Loss for batch is  -2.6481282711029053\n",
      "Val Loss for batch is  -2.4574379920959473\n",
      "Val Loss for batch is  -4.029870986938477\n",
      "|Iter  1759  | Total Val Loss  -11.392640352249146 |\n",
      "Loss for batch is  -1.56132173538208\n",
      "Loss for batch is  -1.6878019571304321\n",
      "Loss for batch is  -1.5797557830810547\n",
      "Loss for batch is  -2.7064037322998047\n",
      "|Iter  1760  | Total Train Loss  -7.535283207893372 |\n",
      "Val Loss for batch is  -2.2331063747406006\n",
      "Val Loss for batch is  -2.648118734359741\n",
      "Val Loss for batch is  -2.420870065689087\n",
      "Val Loss for batch is  -4.005532741546631\n",
      "|Iter  1760  | Total Val Loss  -11.30762791633606 |\n",
      "Loss for batch is  -1.5621352195739746\n",
      "Loss for batch is  -1.6896291971206665\n",
      "Loss for batch is  -1.5780261754989624\n",
      "Loss for batch is  -2.6947216987609863\n",
      "|Iter  1761  | Total Train Loss  -7.52451229095459 |\n",
      "Val Loss for batch is  -2.261242628097534\n",
      "Val Loss for batch is  -2.6283061504364014\n",
      "Val Loss for batch is  -2.428168535232544\n",
      "Val Loss for batch is  -4.01924467086792\n",
      "|Iter  1761  | Total Val Loss  -11.3369619846344 |\n",
      "Loss for batch is  -1.5680012702941895\n",
      "Loss for batch is  -1.6877721548080444\n",
      "Loss for batch is  -1.5775325298309326\n",
      "Loss for batch is  -2.7096052169799805\n",
      "|Iter  1762  | Total Train Loss  -7.542911171913147 |\n",
      "Val Loss for batch is  -2.203176975250244\n",
      "Val Loss for batch is  -2.637993097305298\n",
      "Val Loss for batch is  -2.4399571418762207\n",
      "Val Loss for batch is  -4.028115749359131\n",
      "|Iter  1762  | Total Val Loss  -11.309242963790894 |\n",
      "Loss for batch is  -1.5722572803497314\n",
      "Loss for batch is  -1.6926624774932861\n",
      "Loss for batch is  -1.5755194425582886\n",
      "Loss for batch is  -2.7123241424560547\n",
      "|Iter  1763  | Total Train Loss  -7.552763342857361 |\n",
      "Val Loss for batch is  -2.2788472175598145\n",
      "Val Loss for batch is  -2.654524087905884\n",
      "Val Loss for batch is  -2.44964599609375\n",
      "Val Loss for batch is  -4.058784008026123\n",
      "|Iter  1763  | Total Val Loss  -11.441801309585571 |\n",
      "Loss for batch is  -1.5647153854370117\n",
      "Loss for batch is  -1.6920795440673828\n",
      "Loss for batch is  -1.573692798614502\n",
      "Loss for batch is  -2.7134671211242676\n",
      "|Iter  1764  | Total Train Loss  -7.543954849243164 |\n",
      "Val Loss for batch is  -2.273207664489746\n",
      "Val Loss for batch is  -2.6501386165618896\n",
      "Val Loss for batch is  -2.452984571456909\n",
      "Val Loss for batch is  -4.029247283935547\n",
      "|Iter  1764  | Total Val Loss  -11.405578136444092 |\n",
      "Loss for batch is  -1.577278971672058\n",
      "Loss for batch is  -1.6928353309631348\n",
      "Loss for batch is  -1.5784416198730469\n",
      "Loss for batch is  -2.7018089294433594\n",
      "|Iter  1765  | Total Train Loss  -7.550364851951599 |\n",
      "Val Loss for batch is  -2.2743215560913086\n",
      "Val Loss for batch is  -2.6300196647644043\n",
      "Val Loss for batch is  -2.4324162006378174\n",
      "Val Loss for batch is  -4.032952308654785\n",
      "|Iter  1765  | Total Val Loss  -11.369709730148315 |\n",
      "Loss for batch is  -1.5776216983795166\n",
      "Loss for batch is  -1.6884970664978027\n",
      "Loss for batch is  -1.5793753862380981\n",
      "Loss for batch is  -2.709449291229248\n",
      "|Iter  1766  | Total Train Loss  -7.5549434423446655 |\n",
      "Val Loss for batch is  -2.2352428436279297\n",
      "Val Loss for batch is  -2.6072561740875244\n",
      "Val Loss for batch is  -2.331442356109619\n",
      "Val Loss for batch is  -4.036905765533447\n",
      "|Iter  1766  | Total Val Loss  -11.21084713935852 |\n",
      "Loss for batch is  -1.5775487422943115\n",
      "Loss for batch is  -1.6876647472381592\n",
      "Loss for batch is  -1.5781137943267822\n",
      "Loss for batch is  -2.7087440490722656\n",
      "|Iter  1767  | Total Train Loss  -7.5520713329315186 |\n",
      "Val Loss for batch is  -2.243666172027588\n",
      "Val Loss for batch is  -2.664383888244629\n",
      "Val Loss for batch is  -2.4305615425109863\n",
      "Val Loss for batch is  -4.02191686630249\n",
      "|Iter  1767  | Total Val Loss  -11.360528469085693 |\n",
      "Loss for batch is  -1.5777137279510498\n",
      "Loss for batch is  -1.6924587488174438\n",
      "Loss for batch is  -1.5756970643997192\n",
      "Loss for batch is  -2.7060818672180176\n",
      "|Iter  1768  | Total Train Loss  -7.5519514083862305 |\n",
      "Val Loss for batch is  -2.271557569503784\n",
      "Val Loss for batch is  -2.6166467666625977\n",
      "Val Loss for batch is  -2.412186622619629\n",
      "Val Loss for batch is  -4.014955997467041\n",
      "|Iter  1768  | Total Val Loss  -11.315346956253052 |\n",
      "Loss for batch is  -1.5599591732025146\n",
      "Loss for batch is  -1.6919337511062622\n",
      "Loss for batch is  -1.5745525360107422\n",
      "Loss for batch is  -2.706364631652832\n",
      "|Iter  1769  | Total Train Loss  -7.532810091972351 |\n",
      "Val Loss for batch is  -2.252624988555908\n",
      "Val Loss for batch is  -2.6354920864105225\n",
      "Val Loss for batch is  -2.4547555446624756\n",
      "Val Loss for batch is  -4.041985511779785\n",
      "|Iter  1769  | Total Val Loss  -11.384858131408691 |\n",
      "Loss for batch is  -1.5645359754562378\n",
      "Loss for batch is  -1.696618914604187\n",
      "Loss for batch is  -1.5721280574798584\n",
      "Loss for batch is  -2.7011568546295166\n",
      "|Iter  1770  | Total Train Loss  -7.5344398021698 |\n",
      "Val Loss for batch is  -2.1982004642486572\n",
      "Val Loss for batch is  -2.6331655979156494\n",
      "Val Loss for batch is  -1.9177244901657104\n",
      "Val Loss for batch is  -4.020592212677002\n",
      "|Iter  1770  | Total Val Loss  -10.769682765007019 |\n",
      "Loss for batch is  -1.5753440856933594\n",
      "Loss for batch is  -1.686983585357666\n",
      "Loss for batch is  -1.5797652006149292\n",
      "Loss for batch is  -2.7138185501098633\n",
      "|Iter  1771  | Total Train Loss  -7.555911421775818 |\n",
      "Val Loss for batch is  -2.2467708587646484\n",
      "Val Loss for batch is  -2.6467318534851074\n",
      "Val Loss for batch is  -2.4628701210021973\n",
      "Val Loss for batch is  -4.044879913330078\n",
      "|Iter  1771  | Total Val Loss  -11.401252746582031 |\n",
      "Loss for batch is  -1.5739763975143433\n",
      "Loss for batch is  -1.6890007257461548\n",
      "Loss for batch is  -1.5776042938232422\n",
      "Loss for batch is  -2.710602283477783\n",
      "|Iter  1772  | Total Train Loss  -7.551183700561523 |\n",
      "Val Loss for batch is  -2.2326085567474365\n",
      "Val Loss for batch is  -2.661205291748047\n",
      "Val Loss for batch is  -2.4448912143707275\n",
      "Val Loss for batch is  -4.039336204528809\n",
      "|Iter  1772  | Total Val Loss  -11.37804126739502 |\n",
      "Loss for batch is  -1.5670864582061768\n",
      "Loss for batch is  -1.6908403635025024\n",
      "Loss for batch is  -1.5849497318267822\n",
      "Loss for batch is  -2.7041783332824707\n",
      "|Iter  1773  | Total Train Loss  -7.547054886817932 |\n",
      "Val Loss for batch is  -2.268075466156006\n",
      "Val Loss for batch is  -2.6557729244232178\n",
      "Val Loss for batch is  -2.450697898864746\n",
      "Val Loss for batch is  -4.057050704956055\n",
      "|Iter  1773  | Total Val Loss  -11.431596994400024 |\n",
      "Loss for batch is  -1.5777775049209595\n",
      "Loss for batch is  -1.698280692100525\n",
      "Loss for batch is  -1.5784013271331787\n",
      "Loss for batch is  -2.7128052711486816\n",
      "|Iter  1774  | Total Train Loss  -7.567264795303345 |\n",
      "Val Loss for batch is  -2.267496347427368\n",
      "Val Loss for batch is  -2.642843008041382\n",
      "Val Loss for batch is  -2.383291244506836\n",
      "Val Loss for batch is  -4.038290023803711\n",
      "|Iter  1774  | Total Val Loss  -11.331920623779297 |\n",
      "Loss for batch is  -1.568393349647522\n",
      "Loss for batch is  -1.6944842338562012\n",
      "Loss for batch is  -1.5887759923934937\n",
      "Loss for batch is  -2.7025039196014404\n",
      "|Iter  1775  | Total Train Loss  -7.554157495498657 |\n",
      "Val Loss for batch is  -2.154810905456543\n",
      "Val Loss for batch is  -2.58923077583313\n",
      "Val Loss for batch is  -2.410712242126465\n",
      "Val Loss for batch is  -4.0109100341796875\n",
      "|Iter  1775  | Total Val Loss  -11.165663957595825 |\n",
      "Loss for batch is  -1.5619961023330688\n",
      "Loss for batch is  -1.6902490854263306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.5749930143356323\n",
      "Loss for batch is  -2.710481643676758\n",
      "|Iter  1776  | Total Train Loss  -7.5377198457717896 |\n",
      "Val Loss for batch is  -2.1879260540008545\n",
      "Val Loss for batch is  -2.6264145374298096\n",
      "Val Loss for batch is  -2.418201208114624\n",
      "Val Loss for batch is  -4.035373687744141\n",
      "|Iter  1776  | Total Val Loss  -11.267915487289429 |\n",
      "Loss for batch is  -1.5675910711288452\n",
      "Loss for batch is  -1.6984783411026\n",
      "Loss for batch is  -1.5765900611877441\n",
      "Loss for batch is  -2.70143461227417\n",
      "|Iter  1777  | Total Train Loss  -7.544094085693359 |\n",
      "Val Loss for batch is  -2.2477121353149414\n",
      "Val Loss for batch is  -2.6408889293670654\n",
      "Val Loss for batch is  -2.431631565093994\n",
      "Val Loss for batch is  -4.053372859954834\n",
      "|Iter  1777  | Total Val Loss  -11.373605489730835 |\n",
      "Loss for batch is  -1.5735058784484863\n",
      "Loss for batch is  -1.698334813117981\n",
      "Loss for batch is  -1.5716755390167236\n",
      "Loss for batch is  -2.712982416152954\n",
      "|Iter  1778  | Total Train Loss  -7.556498646736145 |\n",
      "Val Loss for batch is  -2.2752370834350586\n",
      "Val Loss for batch is  -2.649191379547119\n",
      "Val Loss for batch is  -2.419877767562866\n",
      "Val Loss for batch is  -4.056792736053467\n",
      "|Iter  1778  | Total Val Loss  -11.40109896659851 |\n",
      "Loss for batch is  -1.5814299583435059\n",
      "Loss for batch is  -1.6944408416748047\n",
      "Loss for batch is  -1.5939563512802124\n",
      "Loss for batch is  -2.719092845916748\n",
      "|Iter  1779  | Total Train Loss  -7.588919997215271 |\n",
      "Val Loss for batch is  -2.2684168815612793\n",
      "Val Loss for batch is  -2.624459743499756\n",
      "Val Loss for batch is  -2.4621710777282715\n",
      "Val Loss for batch is  -4.057372570037842\n",
      "|Iter  1779  | Total Val Loss  -11.412420272827148 |\n",
      "Loss for batch is  -1.5721098184585571\n",
      "Loss for batch is  -1.699285864830017\n",
      "Loss for batch is  -1.5923893451690674\n",
      "Loss for batch is  -2.70566987991333\n",
      "|Iter  1780  | Total Train Loss  -7.569454908370972 |\n",
      "Val Loss for batch is  -2.194664478302002\n",
      "Val Loss for batch is  -2.6358416080474854\n",
      "Val Loss for batch is  -2.3996551036834717\n",
      "Val Loss for batch is  -4.013185501098633\n",
      "|Iter  1780  | Total Val Loss  -11.243346691131592 |\n",
      "Loss for batch is  -1.5783535242080688\n",
      "Loss for batch is  -1.6986616849899292\n",
      "Loss for batch is  -1.591888666152954\n",
      "Loss for batch is  -2.720630645751953\n",
      "|Iter  1781  | Total Train Loss  -7.589534521102905 |\n",
      "Val Loss for batch is  -2.2535479068756104\n",
      "Val Loss for batch is  -2.6271703243255615\n",
      "Val Loss for batch is  -2.3957021236419678\n",
      "Val Loss for batch is  -4.035089492797852\n",
      "|Iter  1781  | Total Val Loss  -11.311509847640991 |\n",
      "Loss for batch is  -1.579637050628662\n",
      "Loss for batch is  -1.693847417831421\n",
      "Loss for batch is  -1.5899208784103394\n",
      "Loss for batch is  -2.717050313949585\n",
      "|Iter  1782  | Total Train Loss  -7.580455660820007 |\n",
      "Val Loss for batch is  -2.290104866027832\n",
      "Val Loss for batch is  -2.6318397521972656\n",
      "Val Loss for batch is  -2.406845808029175\n",
      "Val Loss for batch is  -4.004732131958008\n",
      "|Iter  1782  | Total Val Loss  -11.33352255821228 |\n",
      "Loss for batch is  -1.5768656730651855\n",
      "Loss for batch is  -1.7061702013015747\n",
      "Loss for batch is  -1.5901432037353516\n",
      "Loss for batch is  -2.7270054817199707\n",
      "|Iter  1783  | Total Train Loss  -7.6001845598220825 |\n",
      "Val Loss for batch is  -2.163255453109741\n",
      "Val Loss for batch is  -2.638688802719116\n",
      "Val Loss for batch is  -2.466681718826294\n",
      "Val Loss for batch is  -4.02034854888916\n",
      "|Iter  1783  | Total Val Loss  -11.288974523544312 |\n",
      "Loss for batch is  -1.5856976509094238\n",
      "Loss for batch is  -1.7067005634307861\n",
      "Loss for batch is  -1.597620964050293\n",
      "Loss for batch is  -2.7130911350250244\n",
      "|Iter  1784  | Total Train Loss  -7.603110313415527 |\n",
      "Val Loss for batch is  -2.243967056274414\n",
      "Val Loss for batch is  -2.6599209308624268\n",
      "Val Loss for batch is  -2.438589334487915\n",
      "Val Loss for batch is  -4.009764194488525\n",
      "|Iter  1784  | Total Val Loss  -11.352241516113281 |\n",
      "Loss for batch is  -1.5823873281478882\n",
      "Loss for batch is  -1.7015563249588013\n",
      "Loss for batch is  -1.59630286693573\n",
      "Loss for batch is  -2.724834442138672\n",
      "|Iter  1785  | Total Train Loss  -7.605080962181091 |\n",
      "Val Loss for batch is  -2.2676239013671875\n",
      "Val Loss for batch is  -2.5916261672973633\n",
      "Val Loss for batch is  -2.2853009700775146\n",
      "Val Loss for batch is  -4.039259433746338\n",
      "|Iter  1785  | Total Val Loss  -11.183810472488403 |\n",
      "Loss for batch is  -1.5886149406433105\n",
      "Loss for batch is  -1.7032151222229004\n",
      "Loss for batch is  -1.5874558687210083\n",
      "Loss for batch is  -2.724543571472168\n",
      "|Iter  1786  | Total Train Loss  -7.603829503059387 |\n",
      "Val Loss for batch is  -2.2969400882720947\n",
      "Val Loss for batch is  -2.6330859661102295\n",
      "Val Loss for batch is  -2.356949806213379\n",
      "Val Loss for batch is  -4.0034894943237305\n",
      "|Iter  1786  | Total Val Loss  -11.290465354919434 |\n",
      "Loss for batch is  -1.5903573036193848\n",
      "Loss for batch is  -1.6983656883239746\n",
      "Loss for batch is  -1.5942450761795044\n",
      "Loss for batch is  -2.7178447246551514\n",
      "|Iter  1787  | Total Train Loss  -7.600812792778015 |\n",
      "Val Loss for batch is  -2.260436534881592\n",
      "Val Loss for batch is  -2.625143051147461\n",
      "Val Loss for batch is  -2.3276419639587402\n",
      "Val Loss for batch is  -4.035725116729736\n",
      "|Iter  1787  | Total Val Loss  -11.24894666671753 |\n",
      "Loss for batch is  -1.5734816789627075\n",
      "Loss for batch is  -1.6947978734970093\n",
      "Loss for batch is  -1.5884538888931274\n",
      "Loss for batch is  -2.7176547050476074\n",
      "|Iter  1788  | Total Train Loss  -7.574388146400452 |\n",
      "Val Loss for batch is  -2.2818801403045654\n",
      "Val Loss for batch is  -2.584836721420288\n",
      "Val Loss for batch is  -2.444316864013672\n",
      "Val Loss for batch is  -4.037295818328857\n",
      "|Iter  1788  | Total Val Loss  -11.348329544067383 |\n",
      "Loss for batch is  -1.5723572969436646\n",
      "Loss for batch is  -1.705654501914978\n",
      "Loss for batch is  -1.5941004753112793\n",
      "Loss for batch is  -2.7205567359924316\n",
      "|Iter  1789  | Total Train Loss  -7.5926690101623535 |\n",
      "Val Loss for batch is  -2.1905975341796875\n",
      "Val Loss for batch is  -2.6597063541412354\n",
      "Val Loss for batch is  -2.4272611141204834\n",
      "Val Loss for batch is  -4.033661842346191\n",
      "|Iter  1789  | Total Val Loss  -11.311226844787598 |\n",
      "Loss for batch is  -1.5645548105239868\n",
      "Loss for batch is  -1.7012563943862915\n",
      "Loss for batch is  -1.5940269231796265\n",
      "Loss for batch is  -2.721369743347168\n",
      "|Iter  1790  | Total Train Loss  -7.581207871437073 |\n",
      "Val Loss for batch is  -2.253052234649658\n",
      "Val Loss for batch is  -2.6469290256500244\n",
      "Val Loss for batch is  -2.431509256362915\n",
      "Val Loss for batch is  -4.051637172698975\n",
      "|Iter  1790  | Total Val Loss  -11.383127689361572 |\n",
      "Loss for batch is  -1.5693163871765137\n",
      "Loss for batch is  -1.6971027851104736\n",
      "Loss for batch is  -1.5947802066802979\n",
      "Loss for batch is  -2.7170493602752686\n",
      "|Iter  1791  | Total Train Loss  -7.578248739242554 |\n",
      "Val Loss for batch is  -2.232370138168335\n",
      "Val Loss for batch is  -2.614306926727295\n",
      "Val Loss for batch is  -2.3936681747436523\n",
      "Val Loss for batch is  -4.010227680206299\n",
      "|Iter  1791  | Total Val Loss  -11.250572919845581 |\n",
      "Loss for batch is  -1.556282877922058\n",
      "Loss for batch is  -1.694890022277832\n",
      "Loss for batch is  -1.5915402173995972\n",
      "Loss for batch is  -2.696621894836426\n",
      "|Iter  1792  | Total Train Loss  -7.539335012435913 |\n",
      "Val Loss for batch is  -2.2710680961608887\n",
      "Val Loss for batch is  -2.6197686195373535\n",
      "Val Loss for batch is  -2.3729379177093506\n",
      "Val Loss for batch is  -4.0182600021362305\n",
      "|Iter  1792  | Total Val Loss  -11.282034635543823 |\n",
      "Loss for batch is  -1.565998911857605\n",
      "Loss for batch is  -1.701934576034546\n",
      "Loss for batch is  -1.5888938903808594\n",
      "Loss for batch is  -2.7116456031799316\n",
      "|Iter  1793  | Total Train Loss  -7.568472981452942 |\n",
      "Val Loss for batch is  -2.2699544429779053\n",
      "Val Loss for batch is  -2.576632261276245\n",
      "Val Loss for batch is  -2.432018995285034\n",
      "Val Loss for batch is  -4.035885334014893\n",
      "|Iter  1793  | Total Val Loss  -11.314491033554077 |\n",
      "Loss for batch is  -1.5662704706192017\n",
      "Loss for batch is  -1.6995373964309692\n",
      "Loss for batch is  -1.5784363746643066\n",
      "Loss for batch is  -2.7201361656188965\n",
      "|Iter  1794  | Total Train Loss  -7.564380407333374 |\n",
      "Val Loss for batch is  -2.226158857345581\n",
      "Val Loss for batch is  -2.645977258682251\n",
      "Val Loss for batch is  -2.4116389751434326\n",
      "Val Loss for batch is  -4.0442609786987305\n",
      "|Iter  1794  | Total Val Loss  -11.328036069869995 |\n",
      "Loss for batch is  -1.5691578388214111\n",
      "Loss for batch is  -1.6859291791915894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.5884768962860107\n",
      "Loss for batch is  -2.725714683532715\n",
      "|Iter  1795  | Total Train Loss  -7.569278597831726 |\n",
      "Val Loss for batch is  -2.2155039310455322\n",
      "Val Loss for batch is  -2.5781543254852295\n",
      "Val Loss for batch is  -2.395460844039917\n",
      "Val Loss for batch is  -4.043829441070557\n",
      "|Iter  1795  | Total Val Loss  -11.232948541641235 |\n",
      "Loss for batch is  -1.5675973892211914\n",
      "Loss for batch is  -1.6800386905670166\n",
      "Loss for batch is  -1.5802452564239502\n",
      "Loss for batch is  -2.72617244720459\n",
      "|Iter  1796  | Total Train Loss  -7.554053783416748 |\n",
      "Val Loss for batch is  -2.194350481033325\n",
      "Val Loss for batch is  -2.62451434135437\n",
      "Val Loss for batch is  -2.388484239578247\n",
      "Val Loss for batch is  -3.996356248855591\n",
      "|Iter  1796  | Total Val Loss  -11.203705310821533 |\n",
      "Loss for batch is  -1.5499850511550903\n",
      "Loss for batch is  -1.6902235746383667\n",
      "Loss for batch is  -1.5935356616973877\n",
      "Loss for batch is  -2.7289366722106934\n",
      "|Iter  1797  | Total Train Loss  -7.562680959701538 |\n",
      "Val Loss for batch is  -2.2402267456054688\n",
      "Val Loss for batch is  -2.562127113342285\n",
      "Val Loss for batch is  -2.381134033203125\n",
      "Val Loss for batch is  -4.029425621032715\n",
      "|Iter  1797  | Total Val Loss  -11.212913513183594 |\n",
      "Loss for batch is  -1.5517481565475464\n",
      "Loss for batch is  -1.6815910339355469\n",
      "Loss for batch is  -1.6011481285095215\n",
      "Loss for batch is  -2.7253165245056152\n",
      "|Iter  1798  | Total Train Loss  -7.55980384349823 |\n",
      "Val Loss for batch is  -2.277064323425293\n",
      "Val Loss for batch is  -2.6431827545166016\n",
      "Val Loss for batch is  -2.4210522174835205\n",
      "Val Loss for batch is  -3.983009099960327\n",
      "|Iter  1798  | Total Val Loss  -11.324308395385742 |\n",
      "Loss for batch is  -1.5488080978393555\n",
      "Loss for batch is  -1.7001659870147705\n",
      "Loss for batch is  -1.602913737297058\n",
      "Loss for batch is  -2.7249207496643066\n",
      "|Iter  1799  | Total Train Loss  -7.576808571815491 |\n",
      "Val Loss for batch is  -2.197251081466675\n",
      "Val Loss for batch is  -2.6137678623199463\n",
      "Val Loss for batch is  -2.4119763374328613\n",
      "Val Loss for batch is  -4.04153299331665\n",
      "|Iter  1799  | Total Val Loss  -11.264528274536133 |\n",
      "Loss for batch is  -1.5580964088439941\n",
      "Loss for batch is  -1.6942899227142334\n",
      "Loss for batch is  -1.6021102666854858\n",
      "Loss for batch is  -2.725168228149414\n",
      "|Iter  1800  | Total Train Loss  -7.579664826393127 |\n",
      "Val Loss for batch is  -2.2921814918518066\n",
      "Val Loss for batch is  -2.61791729927063\n",
      "Val Loss for batch is  -2.4426848888397217\n",
      "Val Loss for batch is  -4.036099910736084\n",
      "|Iter  1800  | Total Val Loss  -11.388883590698242 |\n",
      "Loss for batch is  -1.5689959526062012\n",
      "Loss for batch is  -1.6948912143707275\n",
      "Loss for batch is  -1.6028183698654175\n",
      "Loss for batch is  -2.7313194274902344\n",
      "|Iter  1801  | Total Train Loss  -7.598024964332581 |\n",
      "Val Loss for batch is  -2.222299814224243\n",
      "Val Loss for batch is  -2.633256196975708\n",
      "Val Loss for batch is  -2.3454513549804688\n",
      "Val Loss for batch is  -4.0416717529296875\n",
      "|Iter  1801  | Total Val Loss  -11.242679119110107 |\n",
      "Loss for batch is  -1.5678647756576538\n",
      "Loss for batch is  -1.7018345594406128\n",
      "Loss for batch is  -1.604635238647461\n",
      "Loss for batch is  -2.726598024368286\n",
      "|Iter  1802  | Total Train Loss  -7.600932598114014 |\n",
      "Val Loss for batch is  -2.247835636138916\n",
      "Val Loss for batch is  -2.6245265007019043\n",
      "Val Loss for batch is  -2.348280906677246\n",
      "Val Loss for batch is  -3.9449045658111572\n",
      "|Iter  1802  | Total Val Loss  -11.165547609329224 |\n",
      "Loss for batch is  -1.5688319206237793\n",
      "Loss for batch is  -1.685595154762268\n",
      "Loss for batch is  -1.6051002740859985\n",
      "Loss for batch is  -2.716127634048462\n",
      "|Iter  1803  | Total Train Loss  -7.575654983520508 |\n",
      "Val Loss for batch is  -2.2289257049560547\n",
      "Val Loss for batch is  -2.56455135345459\n",
      "Val Loss for batch is  -2.3812479972839355\n",
      "Val Loss for batch is  -4.057070732116699\n",
      "|Iter  1803  | Total Val Loss  -11.23179578781128 |\n",
      "Loss for batch is  -1.5608155727386475\n",
      "Loss for batch is  -1.6928932666778564\n",
      "Loss for batch is  -1.586556077003479\n",
      "Loss for batch is  -2.7286715507507324\n",
      "|Iter  1804  | Total Train Loss  -7.568936467170715 |\n",
      "Val Loss for batch is  -2.251647710800171\n",
      "Val Loss for batch is  -2.6247828006744385\n",
      "Val Loss for batch is  -2.4283807277679443\n",
      "Val Loss for batch is  -4.041256427764893\n",
      "|Iter  1804  | Total Val Loss  -11.346067667007446 |\n",
      "Loss for batch is  -1.58469557762146\n",
      "Loss for batch is  -1.71050226688385\n",
      "Loss for batch is  -1.6032580137252808\n",
      "Loss for batch is  -2.737673282623291\n",
      "|Iter  1805  | Total Train Loss  -7.636129140853882 |\n",
      "Val Loss for batch is  -2.229584217071533\n",
      "Val Loss for batch is  -2.620159149169922\n",
      "Val Loss for batch is  -2.4419684410095215\n",
      "Val Loss for batch is  -4.072939872741699\n",
      "|Iter  1805  | Total Val Loss  -11.364651679992676 |\n",
      "Loss for batch is  -1.5890010595321655\n",
      "Loss for batch is  -1.6947593688964844\n",
      "Loss for batch is  -1.6023366451263428\n",
      "Loss for batch is  -2.7365353107452393\n",
      "|Iter  1806  | Total Train Loss  -7.622632384300232 |\n",
      "Val Loss for batch is  -2.210142135620117\n",
      "Val Loss for batch is  -2.6185483932495117\n",
      "Val Loss for batch is  -2.382601737976074\n",
      "Val Loss for batch is  -4.050599575042725\n",
      "|Iter  1806  | Total Val Loss  -11.261891841888428 |\n",
      "Loss for batch is  -1.577687382698059\n",
      "Loss for batch is  -1.702880620956421\n",
      "Loss for batch is  -1.5947779417037964\n",
      "Loss for batch is  -2.7318317890167236\n",
      "|Iter  1807  | Total Train Loss  -7.607177734375 |\n",
      "Val Loss for batch is  -2.2970409393310547\n",
      "Val Loss for batch is  -2.623009204864502\n",
      "Val Loss for batch is  -2.4587199687957764\n",
      "Val Loss for batch is  -4.065614223480225\n",
      "|Iter  1807  | Total Val Loss  -11.444384336471558 |\n",
      "Loss for batch is  -1.5806530714035034\n",
      "Loss for batch is  -1.7113096714019775\n",
      "Loss for batch is  -1.5991144180297852\n",
      "Loss for batch is  -2.7341558933258057\n",
      "|Iter  1808  | Total Train Loss  -7.625233054161072 |\n",
      "Val Loss for batch is  -2.277876377105713\n",
      "Val Loss for batch is  -2.6329751014709473\n",
      "Val Loss for batch is  -2.421147108078003\n",
      "Val Loss for batch is  -4.060132026672363\n",
      "|Iter  1808  | Total Val Loss  -11.392130613327026 |\n",
      "Loss for batch is  -1.5932714939117432\n",
      "Loss for batch is  -1.7061114311218262\n",
      "Loss for batch is  -1.6043012142181396\n",
      "Loss for batch is  -2.737670660018921\n",
      "|Iter  1809  | Total Train Loss  -7.64135479927063 |\n",
      "Val Loss for batch is  -2.2724297046661377\n",
      "Val Loss for batch is  -2.632923126220703\n",
      "Val Loss for batch is  -2.400536298751831\n",
      "Val Loss for batch is  -4.068727016448975\n",
      "|Iter  1809  | Total Val Loss  -11.374616146087646 |\n",
      "Loss for batch is  -1.5848331451416016\n",
      "Loss for batch is  -1.7060999870300293\n",
      "Loss for batch is  -1.608611822128296\n",
      "Loss for batch is  -2.7356982231140137\n",
      "|Iter  1810  | Total Train Loss  -7.63524317741394 |\n",
      "Val Loss for batch is  -2.2872262001037598\n",
      "Val Loss for batch is  -2.6531271934509277\n",
      "Val Loss for batch is  -2.4679689407348633\n",
      "Val Loss for batch is  -4.064763069152832\n",
      "|Iter  1810  | Total Val Loss  -11.473085403442383 |\n",
      "Loss for batch is  -1.5882885456085205\n",
      "Loss for batch is  -1.6912379264831543\n",
      "Loss for batch is  -1.6111836433410645\n",
      "Loss for batch is  -2.7334840297698975\n",
      "|Iter  1811  | Total Train Loss  -7.624194145202637 |\n",
      "Val Loss for batch is  -2.238859176635742\n",
      "Val Loss for batch is  -2.6306607723236084\n",
      "Val Loss for batch is  -2.4112350940704346\n",
      "Val Loss for batch is  -4.056777000427246\n",
      "|Iter  1811  | Total Val Loss  -11.337532043457031 |\n",
      "Loss for batch is  -1.5923329591751099\n",
      "Loss for batch is  -1.7053776979446411\n",
      "Loss for batch is  -1.607688546180725\n",
      "Loss for batch is  -2.738532781600952\n",
      "|Iter  1812  | Total Train Loss  -7.643931984901428 |\n",
      "Val Loss for batch is  -2.285414934158325\n",
      "Val Loss for batch is  -2.5721235275268555\n",
      "Val Loss for batch is  -2.437830924987793\n",
      "Val Loss for batch is  -4.081854820251465\n",
      "|Iter  1812  | Total Val Loss  -11.377224206924438 |\n",
      "Loss for batch is  -1.5932179689407349\n",
      "Loss for batch is  -1.7162599563598633\n",
      "Loss for batch is  -1.6089986562728882\n",
      "Loss for batch is  -2.7415852546691895\n",
      "|Iter  1813  | Total Train Loss  -7.660061836242676 |\n",
      "Val Loss for batch is  -2.29242205619812\n",
      "Val Loss for batch is  -2.608161687850952\n",
      "Val Loss for batch is  -2.441220760345459\n",
      "Val Loss for batch is  -4.063251495361328\n",
      "|Iter  1813  | Total Val Loss  -11.40505599975586 |\n",
      "Loss for batch is  -1.601853370666504\n",
      "Loss for batch is  -1.7205190658569336\n",
      "Loss for batch is  -1.6123552322387695\n",
      "Loss for batch is  -2.737442970275879\n",
      "|Iter  1814  | Total Train Loss  -7.672170639038086 |\n",
      "Val Loss for batch is  -2.249901533126831\n",
      "Val Loss for batch is  -2.640620470046997\n",
      "Val Loss for batch is  -2.4138965606689453\n",
      "Val Loss for batch is  -4.055902481079102\n",
      "|Iter  1814  | Total Val Loss  -11.360321044921875 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6038713455200195\n",
      "Loss for batch is  -1.7177482843399048\n",
      "Loss for batch is  -1.6143567562103271\n",
      "Loss for batch is  -2.7410988807678223\n",
      "|Iter  1815  | Total Train Loss  -7.677075266838074 |\n",
      "Val Loss for batch is  -2.2260231971740723\n",
      "Val Loss for batch is  -2.666454315185547\n",
      "Val Loss for batch is  -2.4356918334960938\n",
      "Val Loss for batch is  -4.059474468231201\n",
      "|Iter  1815  | Total Val Loss  -11.387643814086914 |\n",
      "Loss for batch is  -1.6079546213150024\n",
      "Loss for batch is  -1.7220293283462524\n",
      "Loss for batch is  -1.6100308895111084\n",
      "Loss for batch is  -2.7406716346740723\n",
      "|Iter  1816  | Total Train Loss  -7.6806864738464355 |\n",
      "Val Loss for batch is  -2.2686493396759033\n",
      "Val Loss for batch is  -2.660874605178833\n",
      "Val Loss for batch is  -2.415400743484497\n",
      "Val Loss for batch is  -4.074408054351807\n",
      "|Iter  1816  | Total Val Loss  -11.41933274269104 |\n",
      "Loss for batch is  -1.608214020729065\n",
      "Loss for batch is  -1.7161974906921387\n",
      "Loss for batch is  -1.6141152381896973\n",
      "Loss for batch is  -2.742906093597412\n",
      "|Iter  1817  | Total Train Loss  -7.681432843208313 |\n",
      "Val Loss for batch is  -2.2506825923919678\n",
      "Val Loss for batch is  -2.640177011489868\n",
      "Val Loss for batch is  -2.4708805084228516\n",
      "Val Loss for batch is  -4.056448936462402\n",
      "|Iter  1817  | Total Val Loss  -11.41818904876709 |\n",
      "Loss for batch is  -1.6076598167419434\n",
      "Loss for batch is  -1.719417929649353\n",
      "Loss for batch is  -1.6098392009735107\n",
      "Loss for batch is  -2.744635820388794\n",
      "|Iter  1818  | Total Train Loss  -7.681552767753601 |\n",
      "Val Loss for batch is  -2.273489475250244\n",
      "Val Loss for batch is  -2.6671388149261475\n",
      "Val Loss for batch is  -2.419654369354248\n",
      "Val Loss for batch is  -4.042551040649414\n",
      "|Iter  1818  | Total Val Loss  -11.402833700180054 |\n",
      "Loss for batch is  -1.6047422885894775\n",
      "Loss for batch is  -1.7204316854476929\n",
      "Loss for batch is  -1.6147806644439697\n",
      "Loss for batch is  -2.7417802810668945\n",
      "|Iter  1819  | Total Train Loss  -7.681734919548035 |\n",
      "Val Loss for batch is  -2.281177520751953\n",
      "Val Loss for batch is  -2.634153127670288\n",
      "Val Loss for batch is  -2.157034397125244\n",
      "Val Loss for batch is  -4.040161609649658\n",
      "|Iter  1819  | Total Val Loss  -11.112526655197144 |\n",
      "Loss for batch is  -1.5960506200790405\n",
      "Loss for batch is  -1.7192180156707764\n",
      "Loss for batch is  -1.6114449501037598\n",
      "Loss for batch is  -2.742671012878418\n",
      "|Iter  1820  | Total Train Loss  -7.669384598731995 |\n",
      "Val Loss for batch is  -2.261275291442871\n",
      "Val Loss for batch is  -2.672558546066284\n",
      "Val Loss for batch is  -2.4671318531036377\n",
      "Val Loss for batch is  -4.051273345947266\n",
      "|Iter  1820  | Total Val Loss  -11.452239036560059 |\n",
      "Loss for batch is  -1.6030582189559937\n",
      "Loss for batch is  -1.724920392036438\n",
      "Loss for batch is  -1.6126585006713867\n",
      "Loss for batch is  -2.736037254333496\n",
      "|Iter  1821  | Total Train Loss  -7.6766743659973145 |\n",
      "Val Loss for batch is  -2.2755274772644043\n",
      "Val Loss for batch is  -2.651144504547119\n",
      "Val Loss for batch is  -2.4504594802856445\n",
      "Val Loss for batch is  -4.056280136108398\n",
      "|Iter  1821  | Total Val Loss  -11.433411598205566 |\n",
      "Loss for batch is  -1.6015578508377075\n",
      "Loss for batch is  -1.717661738395691\n",
      "Loss for batch is  -1.6117405891418457\n",
      "Loss for batch is  -2.7416763305664062\n",
      "|Iter  1822  | Total Train Loss  -7.67263650894165 |\n",
      "Val Loss for batch is  -2.196216583251953\n",
      "Val Loss for batch is  -2.5587120056152344\n",
      "Val Loss for batch is  -2.453195571899414\n",
      "Val Loss for batch is  -4.053983688354492\n",
      "|Iter  1822  | Total Val Loss  -11.262107849121094 |\n",
      "Loss for batch is  -1.5946519374847412\n",
      "Loss for batch is  -1.7150535583496094\n",
      "Loss for batch is  -1.6076549291610718\n",
      "Loss for batch is  -2.7448954582214355\n",
      "|Iter  1823  | Total Train Loss  -7.662255883216858 |\n",
      "Val Loss for batch is  -2.2640719413757324\n",
      "Val Loss for batch is  -2.6523823738098145\n",
      "Val Loss for batch is  -2.4458694458007812\n",
      "Val Loss for batch is  -4.060363292694092\n",
      "|Iter  1823  | Total Val Loss  -11.42268705368042 |\n",
      "Loss for batch is  -1.6006088256835938\n",
      "Loss for batch is  -1.7129508256912231\n",
      "Loss for batch is  -1.613521695137024\n",
      "Loss for batch is  -2.738025665283203\n",
      "|Iter  1824  | Total Train Loss  -7.665107011795044 |\n",
      "Val Loss for batch is  -2.06624698638916\n",
      "Val Loss for batch is  -2.650296926498413\n",
      "Val Loss for batch is  -2.440669536590576\n",
      "Val Loss for batch is  -4.0692572593688965\n",
      "|Iter  1824  | Total Val Loss  -11.226470708847046 |\n",
      "Loss for batch is  -1.6072521209716797\n",
      "Loss for batch is  -1.7207081317901611\n",
      "Loss for batch is  -1.6123775243759155\n",
      "Loss for batch is  -2.748757839202881\n",
      "|Iter  1825  | Total Train Loss  -7.689095616340637 |\n",
      "Val Loss for batch is  -2.2782180309295654\n",
      "Val Loss for batch is  -2.636733055114746\n",
      "Val Loss for batch is  -2.436875581741333\n",
      "Val Loss for batch is  -4.072581768035889\n",
      "|Iter  1825  | Total Val Loss  -11.424408435821533 |\n",
      "Loss for batch is  -1.6027861833572388\n",
      "Loss for batch is  -1.720937967300415\n",
      "Loss for batch is  -1.6222425699234009\n",
      "Loss for batch is  -2.7454781532287598\n",
      "|Iter  1826  | Total Train Loss  -7.6914448738098145 |\n",
      "Val Loss for batch is  -2.1060330867767334\n",
      "Val Loss for batch is  -2.65126895904541\n",
      "Val Loss for batch is  -2.4706480503082275\n",
      "Val Loss for batch is  -4.0629777908325195\n",
      "|Iter  1826  | Total Val Loss  -11.29092788696289 |\n",
      "Loss for batch is  -1.605101227760315\n",
      "Loss for batch is  -1.7228344678878784\n",
      "Loss for batch is  -1.6203348636627197\n",
      "Loss for batch is  -2.7502005100250244\n",
      "|Iter  1827  | Total Train Loss  -7.6984710693359375 |\n",
      "Val Loss for batch is  -2.2540440559387207\n",
      "Val Loss for batch is  -2.608253240585327\n",
      "Val Loss for batch is  -2.4384288787841797\n",
      "Val Loss for batch is  -4.071463108062744\n",
      "|Iter  1827  | Total Val Loss  -11.372189283370972 |\n",
      "Loss for batch is  -1.6130785942077637\n",
      "Loss for batch is  -1.7234749794006348\n",
      "Loss for batch is  -1.6176092624664307\n",
      "Loss for batch is  -2.748983860015869\n",
      "|Iter  1828  | Total Train Loss  -7.703146696090698 |\n",
      "Val Loss for batch is  -2.2511115074157715\n",
      "Val Loss for batch is  -2.648203134536743\n",
      "Val Loss for batch is  -2.4135141372680664\n",
      "Val Loss for batch is  -4.037553787231445\n",
      "|Iter  1828  | Total Val Loss  -11.350382566452026 |\n",
      "Loss for batch is  -1.6088935136795044\n",
      "Loss for batch is  -1.7309849262237549\n",
      "Loss for batch is  -1.6183819770812988\n",
      "Loss for batch is  -2.7463550567626953\n",
      "|Iter  1829  | Total Train Loss  -7.704615473747253 |\n",
      "Val Loss for batch is  -2.2944934368133545\n",
      "Val Loss for batch is  -2.619202136993408\n",
      "Val Loss for batch is  -2.44608211517334\n",
      "Val Loss for batch is  -4.071106433868408\n",
      "|Iter  1829  | Total Val Loss  -11.43088412284851 |\n",
      "Loss for batch is  -1.6082127094268799\n",
      "Loss for batch is  -1.731143593788147\n",
      "Loss for batch is  -1.618707537651062\n",
      "Loss for batch is  -2.7499895095825195\n",
      "|Iter  1830  | Total Train Loss  -7.708053350448608 |\n",
      "Val Loss for batch is  -2.25748348236084\n",
      "Val Loss for batch is  -2.633021354675293\n",
      "Val Loss for batch is  -2.460195541381836\n",
      "Val Loss for batch is  -4.092194080352783\n",
      "|Iter  1830  | Total Val Loss  -11.442894458770752 |\n",
      "Loss for batch is  -1.6155455112457275\n",
      "Loss for batch is  -1.7086466550827026\n",
      "Loss for batch is  -1.6145391464233398\n",
      "Loss for batch is  -2.75105619430542\n",
      "|Iter  1831  | Total Train Loss  -7.68978750705719 |\n",
      "Val Loss for batch is  -2.218687057495117\n",
      "Val Loss for batch is  -2.6649394035339355\n",
      "Val Loss for batch is  -2.445401668548584\n",
      "Val Loss for batch is  -4.07511043548584\n",
      "|Iter  1831  | Total Val Loss  -11.404138565063477 |\n",
      "Loss for batch is  -1.6177268028259277\n",
      "Loss for batch is  -1.7168370485305786\n",
      "Loss for batch is  -1.6110392808914185\n",
      "Loss for batch is  -2.7512378692626953\n",
      "|Iter  1832  | Total Train Loss  -7.69684100151062 |\n",
      "Val Loss for batch is  -2.2577946186065674\n",
      "Val Loss for batch is  -2.6642508506774902\n",
      "Val Loss for batch is  -2.4461920261383057\n",
      "Val Loss for batch is  -4.033871173858643\n",
      "|Iter  1832  | Total Val Loss  -11.402108669281006 |\n",
      "Loss for batch is  -1.6132540702819824\n",
      "Loss for batch is  -1.720787763595581\n",
      "Loss for batch is  -1.6147449016571045\n",
      "Loss for batch is  -2.7480263710021973\n",
      "|Iter  1833  | Total Train Loss  -7.696813106536865 |\n",
      "Val Loss for batch is  -2.276862859725952\n",
      "Val Loss for batch is  -2.644253730773926\n",
      "Val Loss for batch is  -2.4326348304748535\n",
      "Val Loss for batch is  -4.063508033752441\n",
      "|Iter  1833  | Total Val Loss  -11.417259454727173 |\n",
      "Loss for batch is  -1.6153234243392944\n",
      "Loss for batch is  -1.7251590490341187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6194194555282593\n",
      "Loss for batch is  -2.7502341270446777\n",
      "|Iter  1834  | Total Train Loss  -7.71013605594635 |\n",
      "Val Loss for batch is  -2.263777494430542\n",
      "Val Loss for batch is  -2.6635148525238037\n",
      "Val Loss for batch is  -2.4648046493530273\n",
      "Val Loss for batch is  -4.05649995803833\n",
      "|Iter  1834  | Total Val Loss  -11.448596954345703 |\n",
      "Loss for batch is  -1.607139229774475\n",
      "Loss for batch is  -1.7209136486053467\n",
      "Loss for batch is  -1.607802152633667\n",
      "Loss for batch is  -2.7502400875091553\n",
      "|Iter  1835  | Total Train Loss  -7.686095118522644 |\n",
      "Val Loss for batch is  -2.2258784770965576\n",
      "Val Loss for batch is  -2.6432785987854004\n",
      "Val Loss for batch is  -2.4443769454956055\n",
      "Val Loss for batch is  -4.06057596206665\n",
      "|Iter  1835  | Total Val Loss  -11.374109983444214 |\n",
      "Loss for batch is  -1.6192171573638916\n",
      "Loss for batch is  -1.7290714979171753\n",
      "Loss for batch is  -1.6085050106048584\n",
      "Loss for batch is  -2.7494044303894043\n",
      "|Iter  1836  | Total Train Loss  -7.70619809627533 |\n",
      "Val Loss for batch is  -2.2683663368225098\n",
      "Val Loss for batch is  -2.6145248413085938\n",
      "Val Loss for batch is  -2.4261651039123535\n",
      "Val Loss for batch is  -4.064792633056641\n",
      "|Iter  1836  | Total Val Loss  -11.373848915100098 |\n",
      "Loss for batch is  -1.6171798706054688\n",
      "Loss for batch is  -1.729487419128418\n",
      "Loss for batch is  -1.616815209388733\n",
      "Loss for batch is  -2.7518415451049805\n",
      "|Iter  1837  | Total Train Loss  -7.7153240442276 |\n",
      "Val Loss for batch is  -2.2510986328125\n",
      "Val Loss for batch is  -2.6302525997161865\n",
      "Val Loss for batch is  -2.4250519275665283\n",
      "Val Loss for batch is  -4.073139667510986\n",
      "|Iter  1837  | Total Val Loss  -11.379542827606201 |\n",
      "Loss for batch is  -1.6250331401824951\n",
      "Loss for batch is  -1.7283064126968384\n",
      "Loss for batch is  -1.6231975555419922\n",
      "Loss for batch is  -2.75372314453125\n",
      "|Iter  1838  | Total Train Loss  -7.730260252952576 |\n",
      "Val Loss for batch is  -2.2901089191436768\n",
      "Val Loss for batch is  -2.6659274101257324\n",
      "Val Loss for batch is  -2.4503800868988037\n",
      "Val Loss for batch is  -4.071946144104004\n",
      "|Iter  1838  | Total Val Loss  -11.478362560272217 |\n",
      "Loss for batch is  -1.6220283508300781\n",
      "Loss for batch is  -1.7335904836654663\n",
      "Loss for batch is  -1.6210873126983643\n",
      "Loss for batch is  -2.7492027282714844\n",
      "|Iter  1839  | Total Train Loss  -7.725908875465393 |\n",
      "Val Loss for batch is  -2.2705581188201904\n",
      "Val Loss for batch is  -2.6497926712036133\n",
      "Val Loss for batch is  -2.447242021560669\n",
      "Val Loss for batch is  -4.073153018951416\n",
      "|Iter  1839  | Total Val Loss  -11.440745830535889 |\n",
      "Loss for batch is  -1.6218960285186768\n",
      "Loss for batch is  -1.7365821599960327\n",
      "Loss for batch is  -1.6246047019958496\n",
      "Loss for batch is  -2.758145809173584\n",
      "|Iter  1840  | Total Train Loss  -7.741228699684143 |\n",
      "Val Loss for batch is  -2.2396087646484375\n",
      "Val Loss for batch is  -2.6668145656585693\n",
      "Val Loss for batch is  -2.472001552581787\n",
      "Val Loss for batch is  -4.084153175354004\n",
      "|Iter  1840  | Total Val Loss  -11.462578058242798 |\n",
      "Loss for batch is  -1.621825098991394\n",
      "Loss for batch is  -1.7286714315414429\n",
      "Loss for batch is  -1.6231112480163574\n",
      "Loss for batch is  -2.754157304763794\n",
      "|Iter  1841  | Total Train Loss  -7.727765083312988 |\n",
      "Val Loss for batch is  -2.2626583576202393\n",
      "Val Loss for batch is  -2.6511998176574707\n",
      "Val Loss for batch is  -2.4066193103790283\n",
      "Val Loss for batch is  -4.078893184661865\n",
      "|Iter  1841  | Total Val Loss  -11.399370670318604 |\n",
      "Loss for batch is  -1.6220881938934326\n",
      "Loss for batch is  -1.7392295598983765\n",
      "Loss for batch is  -1.6164326667785645\n",
      "Loss for batch is  -2.75533390045166\n",
      "|Iter  1842  | Total Train Loss  -7.733084321022034 |\n",
      "Val Loss for batch is  -2.2522363662719727\n",
      "Val Loss for batch is  -2.658081293106079\n",
      "Val Loss for batch is  -2.424593210220337\n",
      "Val Loss for batch is  -4.048284530639648\n",
      "|Iter  1842  | Total Val Loss  -11.383195400238037 |\n",
      "Loss for batch is  -1.6251598596572876\n",
      "Loss for batch is  -1.7350677251815796\n",
      "Loss for batch is  -1.6207356452941895\n",
      "Loss for batch is  -2.757214069366455\n",
      "|Iter  1843  | Total Train Loss  -7.738177299499512 |\n",
      "Val Loss for batch is  -2.198392391204834\n",
      "Val Loss for batch is  -2.6358654499053955\n",
      "Val Loss for batch is  -2.460202217102051\n",
      "Val Loss for batch is  -4.052793979644775\n",
      "|Iter  1843  | Total Val Loss  -11.347254037857056 |\n",
      "Loss for batch is  -1.6152656078338623\n",
      "Loss for batch is  -1.729176640510559\n",
      "Loss for batch is  -1.6206090450286865\n",
      "Loss for batch is  -2.7550840377807617\n",
      "|Iter  1844  | Total Train Loss  -7.72013533115387 |\n",
      "Val Loss for batch is  -2.317387580871582\n",
      "Val Loss for batch is  -2.661053419113159\n",
      "Val Loss for batch is  -2.4471256732940674\n",
      "Val Loss for batch is  -4.074971675872803\n",
      "|Iter  1844  | Total Val Loss  -11.500538349151611 |\n",
      "Loss for batch is  -1.624802589416504\n",
      "Loss for batch is  -1.7381129264831543\n",
      "Loss for batch is  -1.6286107301712036\n",
      "Loss for batch is  -2.756493091583252\n",
      "|Iter  1845  | Total Train Loss  -7.748019337654114 |\n",
      "Val Loss for batch is  -2.1755871772766113\n",
      "Val Loss for batch is  -2.6374876499176025\n",
      "Val Loss for batch is  -2.4727869033813477\n",
      "Val Loss for batch is  -4.098517417907715\n",
      "|Iter  1845  | Total Val Loss  -11.384379148483276 |\n",
      "Loss for batch is  -1.627712607383728\n",
      "Loss for batch is  -1.7338367700576782\n",
      "Loss for batch is  -1.6335963010787964\n",
      "Loss for batch is  -2.7563695907592773\n",
      "|Iter  1846  | Total Train Loss  -7.75151526927948 |\n",
      "Val Loss for batch is  -2.2061264514923096\n",
      "Val Loss for batch is  -2.6656973361968994\n",
      "Val Loss for batch is  -2.435351610183716\n",
      "Val Loss for batch is  -4.077030658721924\n",
      "|Iter  1846  | Total Val Loss  -11.384206056594849 |\n",
      "Loss for batch is  -1.6248832941055298\n",
      "Loss for batch is  -1.734739065170288\n",
      "Loss for batch is  -1.6216574907302856\n",
      "Loss for batch is  -2.7588951587677\n",
      "|Iter  1847  | Total Train Loss  -7.740175008773804 |\n",
      "Val Loss for batch is  -2.259629011154175\n",
      "Val Loss for batch is  -2.6468288898468018\n",
      "Val Loss for batch is  -2.4457621574401855\n",
      "Val Loss for batch is  -4.075516223907471\n",
      "|Iter  1847  | Total Val Loss  -11.427736282348633 |\n",
      "Loss for batch is  -1.630864143371582\n",
      "Loss for batch is  -1.7373608350753784\n",
      "Loss for batch is  -1.6295560598373413\n",
      "Loss for batch is  -2.7568774223327637\n",
      "|Iter  1848  | Total Train Loss  -7.754658460617065 |\n",
      "Val Loss for batch is  -2.2680511474609375\n",
      "Val Loss for batch is  -2.6665585041046143\n",
      "Val Loss for batch is  -2.466127395629883\n",
      "Val Loss for batch is  -4.074373245239258\n",
      "|Iter  1848  | Total Val Loss  -11.475110292434692 |\n",
      "Loss for batch is  -1.6291534900665283\n",
      "Loss for batch is  -1.7383551597595215\n",
      "Loss for batch is  -1.6293213367462158\n",
      "Loss for batch is  -2.7552695274353027\n",
      "|Iter  1849  | Total Train Loss  -7.752099514007568 |\n",
      "Val Loss for batch is  -2.2977936267852783\n",
      "Val Loss for batch is  -2.52518892288208\n",
      "Val Loss for batch is  -2.4561378955841064\n",
      "Val Loss for batch is  -4.076918601989746\n",
      "|Iter  1849  | Total Val Loss  -11.356039047241211 |\n",
      "Loss for batch is  -1.623679757118225\n",
      "Loss for batch is  -1.7394533157348633\n",
      "Loss for batch is  -1.6262860298156738\n",
      "Loss for batch is  -2.753688335418701\n",
      "|Iter  1850  | Total Train Loss  -7.743107438087463 |\n",
      "Val Loss for batch is  -2.2900032997131348\n",
      "Val Loss for batch is  -2.6450207233428955\n",
      "Val Loss for batch is  -2.432908058166504\n",
      "Val Loss for batch is  -4.076230525970459\n",
      "|Iter  1850  | Total Val Loss  -11.444162607192993 |\n",
      "Loss for batch is  -1.6287274360656738\n",
      "Loss for batch is  -1.7384682893753052\n",
      "Loss for batch is  -1.6290661096572876\n",
      "Loss for batch is  -2.7602081298828125\n",
      "|Iter  1851  | Total Train Loss  -7.756469964981079 |\n",
      "Val Loss for batch is  -2.215620994567871\n",
      "Val Loss for batch is  -2.6639015674591064\n",
      "Val Loss for batch is  -2.45493483543396\n",
      "Val Loss for batch is  -4.08273983001709\n",
      "|Iter  1851  | Total Val Loss  -11.417197227478027 |\n",
      "Loss for batch is  -1.6297550201416016\n",
      "Loss for batch is  -1.7405165433883667\n",
      "Loss for batch is  -1.6328119039535522\n",
      "Loss for batch is  -2.757735252380371\n",
      "|Iter  1852  | Total Train Loss  -7.760818719863892 |\n",
      "Val Loss for batch is  -2.191669225692749\n",
      "Val Loss for batch is  -2.6958694458007812\n",
      "Val Loss for batch is  -2.464742422103882\n",
      "Val Loss for batch is  -4.080540180206299\n",
      "|Iter  1852  | Total Val Loss  -11.432821273803711 |\n",
      "Loss for batch is  -1.628037691116333\n",
      "Loss for batch is  -1.7392683029174805\n",
      "Loss for batch is  -1.6284230947494507\n",
      "Loss for batch is  -2.7616631984710693\n",
      "|Iter  1853  | Total Train Loss  -7.7573922872543335 |\n",
      "Val Loss for batch is  -2.2593188285827637\n",
      "Val Loss for batch is  -2.6522769927978516\n",
      "Val Loss for batch is  -2.445169687271118\n",
      "Val Loss for batch is  -4.095318794250488\n",
      "|Iter  1853  | Total Val Loss  -11.452084302902222 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.624353289604187\n",
      "Loss for batch is  -1.7369211912155151\n",
      "Loss for batch is  -1.6285343170166016\n",
      "Loss for batch is  -2.7567214965820312\n",
      "|Iter  1854  | Total Train Loss  -7.746530294418335 |\n",
      "Val Loss for batch is  -2.3028171062469482\n",
      "Val Loss for batch is  -2.6483988761901855\n",
      "Val Loss for batch is  -2.4748106002807617\n",
      "Val Loss for batch is  -4.076131820678711\n",
      "|Iter  1854  | Total Val Loss  -11.502158403396606 |\n",
      "Loss for batch is  -1.6239243745803833\n",
      "Loss for batch is  -1.7407066822052002\n",
      "Loss for batch is  -1.6345070600509644\n",
      "Loss for batch is  -2.7601399421691895\n",
      "|Iter  1855  | Total Train Loss  -7.759278059005737 |\n",
      "Val Loss for batch is  -2.2499356269836426\n",
      "Val Loss for batch is  -2.6631782054901123\n",
      "Val Loss for batch is  -2.291109561920166\n",
      "Val Loss for batch is  -3.9935925006866455\n",
      "|Iter  1855  | Total Val Loss  -11.197815895080566 |\n",
      "Loss for batch is  -1.6300076246261597\n",
      "Loss for batch is  -1.733793020248413\n",
      "Loss for batch is  -1.6354684829711914\n",
      "Loss for batch is  -2.761931896209717\n",
      "|Iter  1856  | Total Train Loss  -7.761201024055481 |\n",
      "Val Loss for batch is  -2.2749710083007812\n",
      "Val Loss for batch is  -2.641643524169922\n",
      "Val Loss for batch is  -2.4424750804901123\n",
      "Val Loss for batch is  -4.05460786819458\n",
      "|Iter  1856  | Total Val Loss  -11.413697481155396 |\n",
      "Loss for batch is  -1.6267420053482056\n",
      "Loss for batch is  -1.7402184009552002\n",
      "Loss for batch is  -1.6269620656967163\n",
      "Loss for batch is  -2.7607431411743164\n",
      "|Iter  1857  | Total Train Loss  -7.7546656131744385 |\n",
      "Val Loss for batch is  -2.3439550399780273\n",
      "Val Loss for batch is  -2.6742780208587646\n",
      "Val Loss for batch is  -2.462952136993408\n",
      "Val Loss for batch is  -4.077975749969482\n",
      "|Iter  1857  | Total Val Loss  -11.559160947799683 |\n",
      "Loss for batch is  -1.6286897659301758\n",
      "Loss for batch is  -1.744375228881836\n",
      "Loss for batch is  -1.6305814981460571\n",
      "Loss for batch is  -2.759922504425049\n",
      "|Iter  1858  | Total Train Loss  -7.763568997383118 |\n",
      "Val Loss for batch is  -2.254972457885742\n",
      "Val Loss for batch is  -2.655327320098877\n",
      "Val Loss for batch is  -2.4739739894866943\n",
      "Val Loss for batch is  -4.075240135192871\n",
      "|Iter  1858  | Total Val Loss  -11.459513902664185 |\n",
      "Loss for batch is  -1.6301295757293701\n",
      "Loss for batch is  -1.7446752786636353\n",
      "Loss for batch is  -1.6330994367599487\n",
      "Loss for batch is  -2.7636542320251465\n",
      "|Iter  1859  | Total Train Loss  -7.771558523178101 |\n",
      "Val Loss for batch is  -2.2367920875549316\n",
      "Val Loss for batch is  -2.6595921516418457\n",
      "Val Loss for batch is  -2.4444055557250977\n",
      "Val Loss for batch is  -4.076229572296143\n",
      "|Iter  1859  | Total Val Loss  -11.417019367218018 |\n",
      "Loss for batch is  -1.6261217594146729\n",
      "Loss for batch is  -1.747495412826538\n",
      "Loss for batch is  -1.6325352191925049\n",
      "Loss for batch is  -2.7602930068969727\n",
      "|Iter  1860  | Total Train Loss  -7.7664453983306885 |\n",
      "Val Loss for batch is  -2.1445610523223877\n",
      "Val Loss for batch is  -2.6415371894836426\n",
      "Val Loss for batch is  -2.3666436672210693\n",
      "Val Loss for batch is  -4.072659969329834\n",
      "|Iter  1860  | Total Val Loss  -11.225401878356934 |\n",
      "Loss for batch is  -1.628902554512024\n",
      "Loss for batch is  -1.7362399101257324\n",
      "Loss for batch is  -1.6322039365768433\n",
      "Loss for batch is  -2.7556285858154297\n",
      "|Iter  1861  | Total Train Loss  -7.752974987030029 |\n",
      "Val Loss for batch is  -2.3024702072143555\n",
      "Val Loss for batch is  -2.5618975162506104\n",
      "Val Loss for batch is  -2.4805843830108643\n",
      "Val Loss for batch is  -4.0778326988220215\n",
      "|Iter  1861  | Total Val Loss  -11.422784805297852 |\n",
      "Loss for batch is  -1.6225250959396362\n",
      "Loss for batch is  -1.7443844079971313\n",
      "Loss for batch is  -1.6287555694580078\n",
      "Loss for batch is  -2.7548274993896484\n",
      "|Iter  1862  | Total Train Loss  -7.750492572784424 |\n",
      "Val Loss for batch is  -2.241210699081421\n",
      "Val Loss for batch is  -2.553692102432251\n",
      "Val Loss for batch is  -2.429863214492798\n",
      "Val Loss for batch is  -4.077556610107422\n",
      "|Iter  1862  | Total Val Loss  -11.302322626113892 |\n",
      "Loss for batch is  -1.627668857574463\n",
      "Loss for batch is  -1.7420746088027954\n",
      "Loss for batch is  -1.6340926885604858\n",
      "Loss for batch is  -2.762434959411621\n",
      "|Iter  1863  | Total Train Loss  -7.766271114349365 |\n",
      "Val Loss for batch is  -2.2873291969299316\n",
      "Val Loss for batch is  -2.6472253799438477\n",
      "Val Loss for batch is  -2.463338613510132\n",
      "Val Loss for batch is  -4.076353549957275\n",
      "|Iter  1863  | Total Val Loss  -11.474246740341187 |\n",
      "Loss for batch is  -1.625507116317749\n",
      "Loss for batch is  -1.7448928356170654\n",
      "Loss for batch is  -1.6263631582260132\n",
      "Loss for batch is  -2.7615609169006348\n",
      "|Iter  1864  | Total Train Loss  -7.758324027061462 |\n",
      "Val Loss for batch is  -2.2246713638305664\n",
      "Val Loss for batch is  -2.6602392196655273\n",
      "Val Loss for batch is  -2.4620277881622314\n",
      "Val Loss for batch is  -4.088491916656494\n",
      "|Iter  1864  | Total Val Loss  -11.43543028831482 |\n",
      "Loss for batch is  -1.6295783519744873\n",
      "Loss for batch is  -1.7473219633102417\n",
      "Loss for batch is  -1.6350455284118652\n",
      "Loss for batch is  -2.758211612701416\n",
      "|Iter  1865  | Total Train Loss  -7.77015745639801 |\n",
      "Val Loss for batch is  -2.2245917320251465\n",
      "Val Loss for batch is  -2.6396262645721436\n",
      "Val Loss for batch is  -2.4013755321502686\n",
      "Val Loss for batch is  -4.097084999084473\n",
      "|Iter  1865  | Total Val Loss  -11.362678527832031 |\n",
      "Loss for batch is  -1.6313797235488892\n",
      "Loss for batch is  -1.7472765445709229\n",
      "Loss for batch is  -1.6358007192611694\n",
      "Loss for batch is  -2.76173734664917\n",
      "|Iter  1866  | Total Train Loss  -7.776194334030151 |\n",
      "Val Loss for batch is  -2.2111358642578125\n",
      "Val Loss for batch is  -2.66076922416687\n",
      "Val Loss for batch is  -2.4501094818115234\n",
      "Val Loss for batch is  -4.053735733032227\n",
      "|Iter  1866  | Total Val Loss  -11.375750303268433 |\n",
      "Loss for batch is  -1.633670687675476\n",
      "Loss for batch is  -1.7476242780685425\n",
      "Loss for batch is  -1.6337411403656006\n",
      "Loss for batch is  -2.7643213272094727\n",
      "|Iter  1867  | Total Train Loss  -7.779357433319092 |\n",
      "Val Loss for batch is  -2.214887857437134\n",
      "Val Loss for batch is  -2.6469874382019043\n",
      "Val Loss for batch is  -2.3926150798797607\n",
      "Val Loss for batch is  -4.066017150878906\n",
      "|Iter  1867  | Total Val Loss  -11.320507526397705 |\n",
      "Loss for batch is  -1.6338895559310913\n",
      "Loss for batch is  -1.7482906579971313\n",
      "Loss for batch is  -1.6371562480926514\n",
      "Loss for batch is  -2.7681217193603516\n",
      "|Iter  1868  | Total Train Loss  -7.787458181381226 |\n",
      "Val Loss for batch is  -2.2683377265930176\n",
      "Val Loss for batch is  -2.6376590728759766\n",
      "Val Loss for batch is  -2.4151065349578857\n",
      "Val Loss for batch is  -4.071803569793701\n",
      "|Iter  1868  | Total Val Loss  -11.392906904220581 |\n",
      "Loss for batch is  -1.6288247108459473\n",
      "Loss for batch is  -1.748464584350586\n",
      "Loss for batch is  -1.6343117952346802\n",
      "Loss for batch is  -2.7657318115234375\n",
      "|Iter  1869  | Total Train Loss  -7.777332901954651 |\n",
      "Val Loss for batch is  -2.2925822734832764\n",
      "Val Loss for batch is  -2.6563615798950195\n",
      "Val Loss for batch is  -2.42020320892334\n",
      "Val Loss for batch is  -4.093782901763916\n",
      "|Iter  1869  | Total Val Loss  -11.462929964065552 |\n",
      "Loss for batch is  -1.6327263116836548\n",
      "Loss for batch is  -1.749891757965088\n",
      "Loss for batch is  -1.6405959129333496\n",
      "Loss for batch is  -2.764784336090088\n",
      "|Iter  1870  | Total Train Loss  -7.78799831867218 |\n",
      "Val Loss for batch is  -2.3088676929473877\n",
      "Val Loss for batch is  -2.653320074081421\n",
      "Val Loss for batch is  -2.4538588523864746\n",
      "Val Loss for batch is  -4.075581073760986\n",
      "|Iter  1870  | Total Val Loss  -11.49162769317627 |\n",
      "Loss for batch is  -1.628419041633606\n",
      "Loss for batch is  -1.7492234706878662\n",
      "Loss for batch is  -1.6353431940078735\n",
      "Loss for batch is  -2.759737730026245\n",
      "|Iter  1871  | Total Train Loss  -7.772723436355591 |\n",
      "Val Loss for batch is  -2.2727370262145996\n",
      "Val Loss for batch is  -2.6068761348724365\n",
      "Val Loss for batch is  -2.467181444168091\n",
      "Val Loss for batch is  -4.097890853881836\n",
      "|Iter  1871  | Total Val Loss  -11.444685459136963 |\n",
      "Loss for batch is  -1.6365716457366943\n",
      "Loss for batch is  -1.7460544109344482\n",
      "Loss for batch is  -1.6364344358444214\n",
      "Loss for batch is  -2.75978422164917\n",
      "|Iter  1872  | Total Train Loss  -7.778844714164734 |\n",
      "Val Loss for batch is  -2.2492663860321045\n",
      "Val Loss for batch is  -2.6562399864196777\n",
      "Val Loss for batch is  -2.4216835498809814\n",
      "Val Loss for batch is  -4.0923542976379395\n",
      "|Iter  1872  | Total Val Loss  -11.419544219970703 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6384700536727905\n",
      "Loss for batch is  -1.7445662021636963\n",
      "Loss for batch is  -1.6366162300109863\n",
      "Loss for batch is  -2.7709479331970215\n",
      "|Iter  1873  | Total Train Loss  -7.790600419044495 |\n",
      "Val Loss for batch is  -2.252610921859741\n",
      "Val Loss for batch is  -2.6636409759521484\n",
      "Val Loss for batch is  -2.418867349624634\n",
      "Val Loss for batch is  -4.096118450164795\n",
      "|Iter  1873  | Total Val Loss  -11.431237697601318 |\n",
      "Loss for batch is  -1.6289938688278198\n",
      "Loss for batch is  -1.7485750913619995\n",
      "Loss for batch is  -1.6394898891448975\n",
      "Loss for batch is  -2.768162488937378\n",
      "|Iter  1874  | Total Train Loss  -7.785221338272095 |\n",
      "Val Loss for batch is  -2.1976699829101562\n",
      "Val Loss for batch is  -2.650298595428467\n",
      "Val Loss for batch is  -2.4024035930633545\n",
      "Val Loss for batch is  -4.0420708656311035\n",
      "|Iter  1874  | Total Val Loss  -11.292443037033081 |\n",
      "Loss for batch is  -1.6359368562698364\n",
      "Loss for batch is  -1.742792010307312\n",
      "Loss for batch is  -1.6399495601654053\n",
      "Loss for batch is  -2.7693400382995605\n",
      "|Iter  1875  | Total Train Loss  -7.788018465042114 |\n",
      "Val Loss for batch is  -2.2762091159820557\n",
      "Val Loss for batch is  -2.6604011058807373\n",
      "Val Loss for batch is  -2.39174485206604\n",
      "Val Loss for batch is  -4.088103771209717\n",
      "|Iter  1875  | Total Val Loss  -11.41645884513855 |\n",
      "Loss for batch is  -1.636031150817871\n",
      "Loss for batch is  -1.7500526905059814\n",
      "Loss for batch is  -1.6415579319000244\n",
      "Loss for batch is  -2.7668631076812744\n",
      "|Iter  1876  | Total Train Loss  -7.794504880905151 |\n",
      "Val Loss for batch is  -2.2474498748779297\n",
      "Val Loss for batch is  -2.6616063117980957\n",
      "Val Loss for batch is  -2.463916063308716\n",
      "Val Loss for batch is  -4.095240592956543\n",
      "|Iter  1876  | Total Val Loss  -11.468212842941284 |\n",
      "Loss for batch is  -1.635875940322876\n",
      "Loss for batch is  -1.7493947744369507\n",
      "Loss for batch is  -1.6470532417297363\n",
      "Loss for batch is  -2.768846273422241\n",
      "|Iter  1877  | Total Train Loss  -7.801170229911804 |\n",
      "Val Loss for batch is  -2.2360353469848633\n",
      "Val Loss for batch is  -2.669398546218872\n",
      "Val Loss for batch is  -2.452643632888794\n",
      "Val Loss for batch is  -4.103947162628174\n",
      "|Iter  1877  | Total Val Loss  -11.462024688720703 |\n",
      "Loss for batch is  -1.6378875970840454\n",
      "Loss for batch is  -1.749129295349121\n",
      "Loss for batch is  -1.640151858329773\n",
      "Loss for batch is  -2.766101121902466\n",
      "|Iter  1878  | Total Train Loss  -7.793269872665405 |\n",
      "Val Loss for batch is  -2.1957600116729736\n",
      "Val Loss for batch is  -2.6896238327026367\n",
      "Val Loss for batch is  -2.4704813957214355\n",
      "Val Loss for batch is  -4.079195022583008\n",
      "|Iter  1878  | Total Val Loss  -11.435060262680054 |\n",
      "Loss for batch is  -1.6417057514190674\n",
      "Loss for batch is  -1.750396490097046\n",
      "Loss for batch is  -1.6464545726776123\n",
      "Loss for batch is  -2.768986940383911\n",
      "|Iter  1879  | Total Train Loss  -7.807543754577637 |\n",
      "Val Loss for batch is  -2.230384588241577\n",
      "Val Loss for batch is  -2.672631025314331\n",
      "Val Loss for batch is  -2.4162001609802246\n",
      "Val Loss for batch is  -4.07119607925415\n",
      "|Iter  1879  | Total Val Loss  -11.390411853790283 |\n",
      "Loss for batch is  -1.6393136978149414\n",
      "Loss for batch is  -1.7554895877838135\n",
      "Loss for batch is  -1.6435084342956543\n",
      "Loss for batch is  -2.76812744140625\n",
      "|Iter  1880  | Total Train Loss  -7.806439161300659 |\n",
      "Val Loss for batch is  -2.3131182193756104\n",
      "Val Loss for batch is  -2.6227405071258545\n",
      "Val Loss for batch is  -2.4666435718536377\n",
      "Val Loss for batch is  -4.090205192565918\n",
      "|Iter  1880  | Total Val Loss  -11.49270749092102 |\n",
      "Loss for batch is  -1.639074444770813\n",
      "Loss for batch is  -1.7531291246414185\n",
      "Loss for batch is  -1.643051266670227\n",
      "Loss for batch is  -2.7739572525024414\n",
      "|Iter  1881  | Total Train Loss  -7.8092120885849 |\n",
      "Val Loss for batch is  -2.2985081672668457\n",
      "Val Loss for batch is  -2.642096519470215\n",
      "Val Loss for batch is  -2.4638664722442627\n",
      "Val Loss for batch is  -4.09006929397583\n",
      "|Iter  1881  | Total Val Loss  -11.494540452957153 |\n",
      "Loss for batch is  -1.6406095027923584\n",
      "Loss for batch is  -1.752254605293274\n",
      "Loss for batch is  -1.6443606615066528\n",
      "Loss for batch is  -2.766934394836426\n",
      "|Iter  1882  | Total Train Loss  -7.804159164428711 |\n",
      "Val Loss for batch is  -2.2440614700317383\n",
      "Val Loss for batch is  -2.6340553760528564\n",
      "Val Loss for batch is  -2.44516921043396\n",
      "Val Loss for batch is  -4.066659450531006\n",
      "|Iter  1882  | Total Val Loss  -11.38994550704956 |\n",
      "Loss for batch is  -1.636810064315796\n",
      "Loss for batch is  -1.750177025794983\n",
      "Loss for batch is  -1.6416594982147217\n",
      "Loss for batch is  -2.7691502571105957\n",
      "|Iter  1883  | Total Train Loss  -7.797796845436096 |\n",
      "Val Loss for batch is  -2.2338011264801025\n",
      "Val Loss for batch is  -2.641399621963501\n",
      "Val Loss for batch is  -2.4239163398742676\n",
      "Val Loss for batch is  -4.0880279541015625\n",
      "|Iter  1883  | Total Val Loss  -11.387145042419434 |\n",
      "Loss for batch is  -1.6404544115066528\n",
      "Loss for batch is  -1.7500802278518677\n",
      "Loss for batch is  -1.642259120941162\n",
      "Loss for batch is  -2.7678442001342773\n",
      "|Iter  1884  | Total Train Loss  -7.80063796043396 |\n",
      "Val Loss for batch is  -2.2998650074005127\n",
      "Val Loss for batch is  -2.6543350219726562\n",
      "Val Loss for batch is  -2.3955798149108887\n",
      "Val Loss for batch is  -4.080123424530029\n",
      "|Iter  1884  | Total Val Loss  -11.429903268814087 |\n",
      "Loss for batch is  -1.6429675817489624\n",
      "Loss for batch is  -1.752348780632019\n",
      "Loss for batch is  -1.6404041051864624\n",
      "Loss for batch is  -2.7705860137939453\n",
      "|Iter  1885  | Total Train Loss  -7.806306481361389 |\n",
      "Val Loss for batch is  -2.269951820373535\n",
      "Val Loss for batch is  -2.6410627365112305\n",
      "Val Loss for batch is  -2.4395532608032227\n",
      "Val Loss for batch is  -4.0845112800598145\n",
      "|Iter  1885  | Total Val Loss  -11.435079097747803 |\n",
      "Loss for batch is  -1.6414549350738525\n",
      "Loss for batch is  -1.7528412342071533\n",
      "Loss for batch is  -1.636277437210083\n",
      "Loss for batch is  -2.771942615509033\n",
      "|Iter  1886  | Total Train Loss  -7.802516222000122 |\n",
      "Val Loss for batch is  -2.2204482555389404\n",
      "Val Loss for batch is  -2.656665563583374\n",
      "Val Loss for batch is  -2.4271106719970703\n",
      "Val Loss for batch is  -4.099242687225342\n",
      "|Iter  1886  | Total Val Loss  -11.403467178344727 |\n",
      "Loss for batch is  -1.6317508220672607\n",
      "Loss for batch is  -1.7542939186096191\n",
      "Loss for batch is  -1.6422042846679688\n",
      "Loss for batch is  -2.7693755626678467\n",
      "|Iter  1887  | Total Train Loss  -7.797624588012695 |\n",
      "Val Loss for batch is  -2.2760446071624756\n",
      "Val Loss for batch is  -2.6329269409179688\n",
      "Val Loss for batch is  -2.4433951377868652\n",
      "Val Loss for batch is  -4.0806756019592285\n",
      "|Iter  1887  | Total Val Loss  -11.433042287826538 |\n",
      "Loss for batch is  -1.635232925415039\n",
      "Loss for batch is  -1.755215048789978\n",
      "Loss for batch is  -1.6411954164505005\n",
      "Loss for batch is  -2.7712271213531494\n",
      "|Iter  1888  | Total Train Loss  -7.802870512008667 |\n",
      "Val Loss for batch is  -2.313206911087036\n",
      "Val Loss for batch is  -2.6759603023529053\n",
      "Val Loss for batch is  -2.454054832458496\n",
      "Val Loss for batch is  -4.0946502685546875\n",
      "|Iter  1888  | Total Val Loss  -11.537872314453125 |\n",
      "Loss for batch is  -1.6421089172363281\n",
      "Loss for batch is  -1.7515662908554077\n",
      "Loss for batch is  -1.6404881477355957\n",
      "Loss for batch is  -2.7688372135162354\n",
      "|Iter  1889  | Total Train Loss  -7.803000569343567 |\n",
      "Val Loss for batch is  -2.284381866455078\n",
      "Val Loss for batch is  -2.673088788986206\n",
      "Val Loss for batch is  -2.4566967487335205\n",
      "Val Loss for batch is  -4.076216220855713\n",
      "|Iter  1889  | Total Val Loss  -11.490383625030518 |\n",
      "Loss for batch is  -1.6406159400939941\n",
      "Loss for batch is  -1.7522073984146118\n",
      "Loss for batch is  -1.6405203342437744\n",
      "Loss for batch is  -2.771045207977295\n",
      "|Iter  1890  | Total Train Loss  -7.804388880729675 |\n",
      "Val Loss for batch is  -2.254138708114624\n",
      "Val Loss for batch is  -2.683314800262451\n",
      "Val Loss for batch is  -2.4371910095214844\n",
      "Val Loss for batch is  -4.061434268951416\n",
      "|Iter  1890  | Total Val Loss  -11.436078786849976 |\n",
      "Loss for batch is  -1.64313542842865\n",
      "Loss for batch is  -1.7531945705413818\n",
      "Loss for batch is  -1.640354871749878\n",
      "Loss for batch is  -2.7701358795166016\n",
      "|Iter  1891  | Total Train Loss  -7.806820750236511 |\n",
      "Val Loss for batch is  -2.2946436405181885\n",
      "Val Loss for batch is  -2.6602838039398193\n",
      "Val Loss for batch is  -2.4653961658477783\n",
      "Val Loss for batch is  -4.073742866516113\n",
      "|Iter  1891  | Total Val Loss  -11.4940664768219 |\n",
      "Loss for batch is  -1.6424404382705688\n",
      "Loss for batch is  -1.7564818859100342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6447169780731201\n",
      "Loss for batch is  -2.7637994289398193\n",
      "|Iter  1892  | Total Train Loss  -7.8074387311935425 |\n",
      "Val Loss for batch is  -2.290609359741211\n",
      "Val Loss for batch is  -2.6789369583129883\n",
      "Val Loss for batch is  -2.4029295444488525\n",
      "Val Loss for batch is  -4.073372840881348\n",
      "|Iter  1892  | Total Val Loss  -11.4458487033844 |\n",
      "Loss for batch is  -1.641341209411621\n",
      "Loss for batch is  -1.7512898445129395\n",
      "Loss for batch is  -1.6426188945770264\n",
      "Loss for batch is  -2.774543046951294\n",
      "|Iter  1893  | Total Train Loss  -7.809792995452881 |\n",
      "Val Loss for batch is  -2.2337753772735596\n",
      "Val Loss for batch is  -2.6684114933013916\n",
      "Val Loss for batch is  -2.422534227371216\n",
      "Val Loss for batch is  -4.088662624359131\n",
      "|Iter  1893  | Total Val Loss  -11.413383722305298 |\n",
      "Loss for batch is  -1.6391322612762451\n",
      "Loss for batch is  -1.755897879600525\n",
      "Loss for batch is  -1.6385719776153564\n",
      "Loss for batch is  -2.770251512527466\n",
      "|Iter  1894  | Total Train Loss  -7.803853631019592 |\n",
      "Val Loss for batch is  -2.3188819885253906\n",
      "Val Loss for batch is  -2.586574077606201\n",
      "Val Loss for batch is  -2.467177391052246\n",
      "Val Loss for batch is  -4.094903945922852\n",
      "|Iter  1894  | Total Val Loss  -11.46753740310669 |\n",
      "Loss for batch is  -1.6461067199707031\n",
      "Loss for batch is  -1.7518216371536255\n",
      "Loss for batch is  -1.6463017463684082\n",
      "Loss for batch is  -2.768009662628174\n",
      "|Iter  1895  | Total Train Loss  -7.812239766120911 |\n",
      "Val Loss for batch is  -2.298717498779297\n",
      "Val Loss for batch is  -2.6452248096466064\n",
      "Val Loss for batch is  -2.4471447467803955\n",
      "Val Loss for batch is  -4.071363925933838\n",
      "|Iter  1895  | Total Val Loss  -11.462450981140137 |\n",
      "Loss for batch is  -1.6430840492248535\n",
      "Loss for batch is  -1.7542071342468262\n",
      "Loss for batch is  -1.6481239795684814\n",
      "Loss for batch is  -2.7732186317443848\n",
      "|Iter  1896  | Total Train Loss  -7.818633794784546 |\n",
      "Val Loss for batch is  -2.209603786468506\n",
      "Val Loss for batch is  -2.6143758296966553\n",
      "Val Loss for batch is  -2.445321798324585\n",
      "Val Loss for batch is  -4.076253890991211\n",
      "|Iter  1896  | Total Val Loss  -11.345555305480957 |\n",
      "Loss for batch is  -1.647853970527649\n",
      "Loss for batch is  -1.7549779415130615\n",
      "Loss for batch is  -1.6462390422821045\n",
      "Loss for batch is  -2.773723602294922\n",
      "|Iter  1897  | Total Train Loss  -7.822794556617737 |\n",
      "Val Loss for batch is  -2.244868040084839\n",
      "Val Loss for batch is  -2.682318687438965\n",
      "Val Loss for batch is  -2.4031147956848145\n",
      "Val Loss for batch is  -4.085121154785156\n",
      "|Iter  1897  | Total Val Loss  -11.415422677993774 |\n",
      "Loss for batch is  -1.643465280532837\n",
      "Loss for batch is  -1.75770103931427\n",
      "Loss for batch is  -1.6420243978500366\n",
      "Loss for batch is  -2.771806478500366\n",
      "|Iter  1898  | Total Train Loss  -7.81499719619751 |\n",
      "Val Loss for batch is  -2.2488279342651367\n",
      "Val Loss for batch is  -2.6293585300445557\n",
      "Val Loss for batch is  -2.436826705932617\n",
      "Val Loss for batch is  -4.1008734703063965\n",
      "|Iter  1898  | Total Val Loss  -11.415886640548706 |\n",
      "Loss for batch is  -1.642547607421875\n",
      "Loss for batch is  -1.758344292640686\n",
      "Loss for batch is  -1.6453438997268677\n",
      "Loss for batch is  -2.7752463817596436\n",
      "|Iter  1899  | Total Train Loss  -7.821482181549072 |\n",
      "Val Loss for batch is  -2.2630374431610107\n",
      "Val Loss for batch is  -2.661088228225708\n",
      "Val Loss for batch is  -2.4574615955352783\n",
      "Val Loss for batch is  -4.078708648681641\n",
      "|Iter  1899  | Total Val Loss  -11.460295915603638 |\n",
      "Loss for batch is  -1.6415163278579712\n",
      "Loss for batch is  -1.7600136995315552\n",
      "Loss for batch is  -1.6467384099960327\n",
      "Loss for batch is  -2.7744197845458984\n",
      "|Iter  1900  | Total Train Loss  -7.8226882219314575 |\n",
      "Val Loss for batch is  -2.275787591934204\n",
      "Val Loss for batch is  -2.628060817718506\n",
      "Val Loss for batch is  -2.4661946296691895\n",
      "Val Loss for batch is  -4.084256649017334\n",
      "|Iter  1900  | Total Val Loss  -11.454299688339233 |\n",
      "Loss for batch is  -1.6473459005355835\n",
      "Loss for batch is  -1.7550818920135498\n",
      "Loss for batch is  -1.6503829956054688\n",
      "Loss for batch is  -2.767594814300537\n",
      "|Iter  1901  | Total Train Loss  -7.820405602455139 |\n",
      "Val Loss for batch is  -2.260493278503418\n",
      "Val Loss for batch is  -2.6758322715759277\n",
      "Val Loss for batch is  -2.4256789684295654\n",
      "Val Loss for batch is  -4.101541996002197\n",
      "|Iter  1901  | Total Val Loss  -11.463546514511108 |\n",
      "Loss for batch is  -1.6471222639083862\n",
      "Loss for batch is  -1.756516695022583\n",
      "Loss for batch is  -1.648436427116394\n",
      "Loss for batch is  -2.775177478790283\n",
      "|Iter  1902  | Total Train Loss  -7.8272528648376465 |\n",
      "Val Loss for batch is  -2.277076482772827\n",
      "Val Loss for batch is  -2.66687273979187\n",
      "Val Loss for batch is  -2.4585180282592773\n",
      "Val Loss for batch is  -4.097780704498291\n",
      "|Iter  1902  | Total Val Loss  -11.500247955322266 |\n",
      "Loss for batch is  -1.644485592842102\n",
      "Loss for batch is  -1.7596415281295776\n",
      "Loss for batch is  -1.6483263969421387\n",
      "Loss for batch is  -2.768321990966797\n",
      "|Iter  1903  | Total Train Loss  -7.820775508880615 |\n",
      "Val Loss for batch is  -2.2791941165924072\n",
      "Val Loss for batch is  -2.6484756469726562\n",
      "Val Loss for batch is  -2.4638335704803467\n",
      "Val Loss for batch is  -4.093326568603516\n",
      "|Iter  1903  | Total Val Loss  -11.484829902648926 |\n",
      "Loss for batch is  -1.649032711982727\n",
      "Loss for batch is  -1.7575517892837524\n",
      "Loss for batch is  -1.6517918109893799\n",
      "Loss for batch is  -2.7709503173828125\n",
      "|Iter  1904  | Total Train Loss  -7.829326629638672 |\n",
      "Val Loss for batch is  -2.2536110877990723\n",
      "Val Loss for batch is  -2.637345790863037\n",
      "Val Loss for batch is  -2.4389259815216064\n",
      "Val Loss for batch is  -4.09427547454834\n",
      "|Iter  1904  | Total Val Loss  -11.424158334732056 |\n",
      "Loss for batch is  -1.6445764303207397\n",
      "Loss for batch is  -1.7591017484664917\n",
      "Loss for batch is  -1.64851975440979\n",
      "Loss for batch is  -2.7722480297088623\n",
      "|Iter  1905  | Total Train Loss  -7.824445962905884 |\n",
      "Val Loss for batch is  -2.294114828109741\n",
      "Val Loss for batch is  -2.6375198364257812\n",
      "Val Loss for batch is  -2.332646131515503\n",
      "Val Loss for batch is  -4.085646152496338\n",
      "|Iter  1905  | Total Val Loss  -11.349926948547363 |\n",
      "Loss for batch is  -1.6533082723617554\n",
      "Loss for batch is  -1.7620424032211304\n",
      "Loss for batch is  -1.6472736597061157\n",
      "Loss for batch is  -2.777087450027466\n",
      "|Iter  1906  | Total Train Loss  -7.839711785316467 |\n",
      "Val Loss for batch is  -2.2660810947418213\n",
      "Val Loss for batch is  -2.652672052383423\n",
      "Val Loss for batch is  -2.460282325744629\n",
      "Val Loss for batch is  -4.074154853820801\n",
      "|Iter  1906  | Total Val Loss  -11.453190326690674 |\n",
      "Loss for batch is  -1.6469297409057617\n",
      "Loss for batch is  -1.752649188041687\n",
      "Loss for batch is  -1.648645281791687\n",
      "Loss for batch is  -2.7757132053375244\n",
      "|Iter  1907  | Total Train Loss  -7.82393741607666 |\n",
      "Val Loss for batch is  -2.2797653675079346\n",
      "Val Loss for batch is  -2.6649768352508545\n",
      "Val Loss for batch is  -2.4327290058135986\n",
      "Val Loss for batch is  -4.083561420440674\n",
      "|Iter  1907  | Total Val Loss  -11.461032629013062 |\n",
      "Loss for batch is  -1.6479506492614746\n",
      "Loss for batch is  -1.7606550455093384\n",
      "Loss for batch is  -1.645897388458252\n",
      "Loss for batch is  -2.77764892578125\n",
      "|Iter  1908  | Total Train Loss  -7.832152009010315 |\n",
      "Val Loss for batch is  -2.2373900413513184\n",
      "Val Loss for batch is  -2.6605472564697266\n",
      "Val Loss for batch is  -2.48953914642334\n",
      "Val Loss for batch is  -4.098669052124023\n",
      "|Iter  1908  | Total Val Loss  -11.486145496368408 |\n",
      "Loss for batch is  -1.650164008140564\n",
      "Loss for batch is  -1.7609455585479736\n",
      "Loss for batch is  -1.6526377201080322\n",
      "Loss for batch is  -2.772312641143799\n",
      "|Iter  1909  | Total Train Loss  -7.836059927940369 |\n",
      "Val Loss for batch is  -1.9706637859344482\n",
      "Val Loss for batch is  -2.640655755996704\n",
      "Val Loss for batch is  -2.4378890991210938\n",
      "Val Loss for batch is  -4.091744899749756\n",
      "|Iter  1909  | Total Val Loss  -11.140953540802002 |\n",
      "Loss for batch is  -1.6432327032089233\n",
      "Loss for batch is  -1.7630928754806519\n",
      "Loss for batch is  -1.6563962697982788\n",
      "Loss for batch is  -2.774677038192749\n",
      "|Iter  1910  | Total Train Loss  -7.837398886680603 |\n",
      "Val Loss for batch is  -2.283140182495117\n",
      "Val Loss for batch is  -2.6583969593048096\n",
      "Val Loss for batch is  -2.4311745166778564\n",
      "Val Loss for batch is  -4.073300838470459\n",
      "|Iter  1910  | Total Val Loss  -11.446012496948242 |\n",
      "Loss for batch is  -1.6467057466506958\n",
      "Loss for batch is  -1.7601958513259888\n",
      "Loss for batch is  -1.6520159244537354\n",
      "Loss for batch is  -2.776754379272461\n",
      "|Iter  1911  | Total Train Loss  -7.835671901702881 |\n",
      "Val Loss for batch is  -2.309295892715454\n",
      "Val Loss for batch is  -2.6572985649108887\n",
      "Val Loss for batch is  -2.4631855487823486\n",
      "Val Loss for batch is  -4.067492961883545\n",
      "|Iter  1911  | Total Val Loss  -11.497272968292236 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6470381021499634\n",
      "Loss for batch is  -1.761529564857483\n",
      "Loss for batch is  -1.6491705179214478\n",
      "Loss for batch is  -2.7756428718566895\n",
      "|Iter  1912  | Total Train Loss  -7.8333810567855835 |\n",
      "Val Loss for batch is  -2.2563538551330566\n",
      "Val Loss for batch is  -2.664687395095825\n",
      "Val Loss for batch is  -2.4634718894958496\n",
      "Val Loss for batch is  -4.094520568847656\n",
      "|Iter  1912  | Total Val Loss  -11.479033708572388 |\n",
      "Loss for batch is  -1.652542233467102\n",
      "Loss for batch is  -1.7594736814498901\n",
      "Loss for batch is  -1.6520928144454956\n",
      "Loss for batch is  -2.7763919830322266\n",
      "|Iter  1913  | Total Train Loss  -7.840500712394714 |\n",
      "Val Loss for batch is  -2.2652201652526855\n",
      "Val Loss for batch is  -2.6628735065460205\n",
      "Val Loss for batch is  -2.4147982597351074\n",
      "Val Loss for batch is  -4.080690383911133\n",
      "|Iter  1913  | Total Val Loss  -11.423582315444946 |\n",
      "Loss for batch is  -1.6485240459442139\n",
      "Loss for batch is  -1.7614492177963257\n",
      "Loss for batch is  -1.6491711139678955\n",
      "Loss for batch is  -2.7760448455810547\n",
      "|Iter  1914  | Total Train Loss  -7.83518922328949 |\n",
      "Val Loss for batch is  -2.2689263820648193\n",
      "Val Loss for batch is  -2.683136224746704\n",
      "Val Loss for batch is  -2.430912971496582\n",
      "Val Loss for batch is  -4.0860443115234375\n",
      "|Iter  1914  | Total Val Loss  -11.469019889831543 |\n",
      "Loss for batch is  -1.6519688367843628\n",
      "Loss for batch is  -1.763946294784546\n",
      "Loss for batch is  -1.6525239944458008\n",
      "Loss for batch is  -2.77184796333313\n",
      "|Iter  1915  | Total Train Loss  -7.840287089347839 |\n",
      "Val Loss for batch is  -2.2839159965515137\n",
      "Val Loss for batch is  -2.687474012374878\n",
      "Val Loss for batch is  -2.437879800796509\n",
      "Val Loss for batch is  -4.0889081954956055\n",
      "|Iter  1915  | Total Val Loss  -11.498178005218506 |\n",
      "Loss for batch is  -1.6510767936706543\n",
      "Loss for batch is  -1.7621965408325195\n",
      "Loss for batch is  -1.6530890464782715\n",
      "Loss for batch is  -2.773702621459961\n",
      "|Iter  1916  | Total Train Loss  -7.840065002441406 |\n",
      "Val Loss for batch is  -2.2643496990203857\n",
      "Val Loss for batch is  -2.644784927368164\n",
      "Val Loss for batch is  -2.4109833240509033\n",
      "Val Loss for batch is  -4.084939479827881\n",
      "|Iter  1916  | Total Val Loss  -11.405057430267334 |\n",
      "Loss for batch is  -1.6526193618774414\n",
      "Loss for batch is  -1.7565873861312866\n",
      "Loss for batch is  -1.6529290676116943\n",
      "Loss for batch is  -2.7759876251220703\n",
      "|Iter  1917  | Total Train Loss  -7.838123440742493 |\n",
      "Val Loss for batch is  -2.2960872650146484\n",
      "Val Loss for batch is  -2.674715280532837\n",
      "Val Loss for batch is  -2.219467878341675\n",
      "Val Loss for batch is  -4.088902473449707\n",
      "|Iter  1917  | Total Val Loss  -11.279172897338867 |\n",
      "Loss for batch is  -1.6480458974838257\n",
      "Loss for batch is  -1.7593320608139038\n",
      "Loss for batch is  -1.6549887657165527\n",
      "Loss for batch is  -2.7756292819976807\n",
      "|Iter  1918  | Total Train Loss  -7.837996006011963 |\n",
      "Val Loss for batch is  -2.2677624225616455\n",
      "Val Loss for batch is  -2.66931414604187\n",
      "Val Loss for batch is  -2.349888563156128\n",
      "Val Loss for batch is  -4.077099800109863\n",
      "|Iter  1918  | Total Val Loss  -11.364064931869507 |\n",
      "Loss for batch is  -1.6506775617599487\n",
      "Loss for batch is  -1.7612650394439697\n",
      "Loss for batch is  -1.6554354429244995\n",
      "Loss for batch is  -2.7792887687683105\n",
      "|Iter  1919  | Total Train Loss  -7.8466668128967285 |\n",
      "Val Loss for batch is  -2.2289891242980957\n",
      "Val Loss for batch is  -2.6288836002349854\n",
      "Val Loss for batch is  -2.4627044200897217\n",
      "Val Loss for batch is  -4.080913543701172\n",
      "|Iter  1919  | Total Val Loss  -11.401490688323975 |\n",
      "Loss for batch is  -1.6512879133224487\n",
      "Loss for batch is  -1.7641537189483643\n",
      "Loss for batch is  -1.6496115922927856\n",
      "Loss for batch is  -2.7809200286865234\n",
      "|Iter  1920  | Total Train Loss  -7.845973253250122 |\n",
      "Val Loss for batch is  -2.204223394393921\n",
      "Val Loss for batch is  -2.6347849369049072\n",
      "Val Loss for batch is  -2.4608094692230225\n",
      "Val Loss for batch is  -4.097548484802246\n",
      "|Iter  1920  | Total Val Loss  -11.397366285324097 |\n",
      "Loss for batch is  -1.6506918668746948\n",
      "Loss for batch is  -1.7644416093826294\n",
      "Loss for batch is  -1.6558969020843506\n",
      "Loss for batch is  -2.7781286239624023\n",
      "|Iter  1921  | Total Train Loss  -7.849159002304077 |\n",
      "Val Loss for batch is  -2.1870062351226807\n",
      "Val Loss for batch is  -2.6591949462890625\n",
      "Val Loss for batch is  -2.4613444805145264\n",
      "Val Loss for batch is  -4.066388130187988\n",
      "|Iter  1921  | Total Val Loss  -11.373933792114258 |\n",
      "Loss for batch is  -1.6493794918060303\n",
      "Loss for batch is  -1.7620033025741577\n",
      "Loss for batch is  -1.6554970741271973\n",
      "Loss for batch is  -2.777285099029541\n",
      "|Iter  1922  | Total Train Loss  -7.844164967536926 |\n",
      "Val Loss for batch is  -2.257023334503174\n",
      "Val Loss for batch is  -2.665374279022217\n",
      "Val Loss for batch is  -2.4436020851135254\n",
      "Val Loss for batch is  -4.080278396606445\n",
      "|Iter  1922  | Total Val Loss  -11.446278095245361 |\n",
      "Loss for batch is  -1.652099609375\n",
      "Loss for batch is  -1.7648621797561646\n",
      "Loss for batch is  -1.6568242311477661\n",
      "Loss for batch is  -2.775049924850464\n",
      "|Iter  1923  | Total Train Loss  -7.8488359451293945 |\n",
      "Val Loss for batch is  -2.2245094776153564\n",
      "Val Loss for batch is  -2.6590960025787354\n",
      "Val Loss for batch is  -2.419708251953125\n",
      "Val Loss for batch is  -4.080594062805176\n",
      "|Iter  1923  | Total Val Loss  -11.383907794952393 |\n",
      "Loss for batch is  -1.6556795835494995\n",
      "Loss for batch is  -1.7606258392333984\n",
      "Loss for batch is  -1.6530259847640991\n",
      "Loss for batch is  -2.781531810760498\n",
      "|Iter  1924  | Total Train Loss  -7.850863218307495 |\n",
      "Val Loss for batch is  -2.2732479572296143\n",
      "Val Loss for batch is  -2.6506316661834717\n",
      "Val Loss for batch is  -2.4113364219665527\n",
      "Val Loss for batch is  -4.0796732902526855\n",
      "|Iter  1924  | Total Val Loss  -11.414889335632324 |\n",
      "Loss for batch is  -1.6454612016677856\n",
      "Loss for batch is  -1.7641706466674805\n",
      "Loss for batch is  -1.655256986618042\n",
      "Loss for batch is  -2.779355764389038\n",
      "|Iter  1925  | Total Train Loss  -7.844244599342346 |\n",
      "Val Loss for batch is  -2.232426881790161\n",
      "Val Loss for batch is  -2.650059223175049\n",
      "Val Loss for batch is  -2.4534363746643066\n",
      "Val Loss for batch is  -4.088995456695557\n",
      "|Iter  1925  | Total Val Loss  -11.424917936325073 |\n",
      "Loss for batch is  -1.653250813484192\n",
      "Loss for batch is  -1.7618591785430908\n",
      "Loss for batch is  -1.6521949768066406\n",
      "Loss for batch is  -2.7807297706604004\n",
      "|Iter  1926  | Total Train Loss  -7.848034739494324 |\n",
      "Val Loss for batch is  -2.1962127685546875\n",
      "Val Loss for batch is  -2.6591432094573975\n",
      "Val Loss for batch is  -2.4607315063476562\n",
      "Val Loss for batch is  -4.094972610473633\n",
      "|Iter  1926  | Total Val Loss  -11.411060094833374 |\n",
      "Loss for batch is  -1.652612328529358\n",
      "Loss for batch is  -1.7657020092010498\n",
      "Loss for batch is  -1.6559017896652222\n",
      "Loss for batch is  -2.7789855003356934\n",
      "|Iter  1927  | Total Train Loss  -7.853201627731323 |\n",
      "Val Loss for batch is  -2.214576244354248\n",
      "Val Loss for batch is  -2.6581833362579346\n",
      "Val Loss for batch is  -2.4698660373687744\n",
      "Val Loss for batch is  -4.0768232345581055\n",
      "|Iter  1927  | Total Val Loss  -11.419448852539062 |\n",
      "Loss for batch is  -1.6546289920806885\n",
      "Loss for batch is  -1.7636669874191284\n",
      "Loss for batch is  -1.6528334617614746\n",
      "Loss for batch is  -2.7799131870269775\n",
      "|Iter  1928  | Total Train Loss  -7.851042628288269 |\n",
      "Val Loss for batch is  -2.2277474403381348\n",
      "Val Loss for batch is  -2.6747963428497314\n",
      "Val Loss for batch is  -2.3952693939208984\n",
      "Val Loss for batch is  -4.093838214874268\n",
      "|Iter  1928  | Total Val Loss  -11.391651391983032 |\n",
      "Loss for batch is  -1.654849648475647\n",
      "Loss for batch is  -1.7627114057540894\n",
      "Loss for batch is  -1.6549633741378784\n",
      "Loss for batch is  -2.776480197906494\n",
      "|Iter  1929  | Total Train Loss  -7.849004626274109 |\n",
      "Val Loss for batch is  -2.2589111328125\n",
      "Val Loss for batch is  -2.6759519577026367\n",
      "Val Loss for batch is  -2.390287160873413\n",
      "Val Loss for batch is  -4.091294288635254\n",
      "|Iter  1929  | Total Val Loss  -11.416444540023804 |\n",
      "Loss for batch is  -1.6575826406478882\n",
      "Loss for batch is  -1.7670224905014038\n",
      "Loss for batch is  -1.6498647928237915\n",
      "Loss for batch is  -2.780681610107422\n",
      "|Iter  1930  | Total Train Loss  -7.855151534080505 |\n",
      "Val Loss for batch is  -2.19291090965271\n",
      "Val Loss for batch is  -2.6618146896362305\n",
      "Val Loss for batch is  -2.451960802078247\n",
      "Val Loss for batch is  -4.0854926109313965\n",
      "|Iter  1930  | Total Val Loss  -11.392179012298584 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6523778438568115\n",
      "Loss for batch is  -1.764020562171936\n",
      "Loss for batch is  -1.660471796989441\n",
      "Loss for batch is  -2.777590751647949\n",
      "|Iter  1931  | Total Train Loss  -7.854460954666138 |\n",
      "Val Loss for batch is  -2.2797231674194336\n",
      "Val Loss for batch is  -2.6226394176483154\n",
      "Val Loss for batch is  -2.3904523849487305\n",
      "Val Loss for batch is  -4.097983360290527\n",
      "|Iter  1931  | Total Val Loss  -11.390798330307007 |\n",
      "Loss for batch is  -1.6555278301239014\n",
      "Loss for batch is  -1.766535997390747\n",
      "Loss for batch is  -1.6574230194091797\n",
      "Loss for batch is  -2.7756457328796387\n",
      "|Iter  1932  | Total Train Loss  -7.855132579803467 |\n",
      "Val Loss for batch is  -2.1961193084716797\n",
      "Val Loss for batch is  -2.6633009910583496\n",
      "Val Loss for batch is  -2.435312509536743\n",
      "Val Loss for batch is  -4.096872806549072\n",
      "|Iter  1932  | Total Val Loss  -11.391605615615845 |\n",
      "Loss for batch is  -1.6556944847106934\n",
      "Loss for batch is  -1.767214059829712\n",
      "Loss for batch is  -1.6506586074829102\n",
      "Loss for batch is  -2.77846622467041\n",
      "|Iter  1933  | Total Train Loss  -7.852033376693726 |\n",
      "Val Loss for batch is  -2.256866931915283\n",
      "Val Loss for batch is  -2.6625993251800537\n",
      "Val Loss for batch is  -2.4081459045410156\n",
      "Val Loss for batch is  -4.078301429748535\n",
      "|Iter  1933  | Total Val Loss  -11.405913591384888 |\n",
      "Loss for batch is  -1.6557151079177856\n",
      "Loss for batch is  -1.760921597480774\n",
      "Loss for batch is  -1.6600722074508667\n",
      "Loss for batch is  -2.779249668121338\n",
      "|Iter  1934  | Total Train Loss  -7.855958580970764 |\n",
      "Val Loss for batch is  -2.299600124359131\n",
      "Val Loss for batch is  -2.659522771835327\n",
      "Val Loss for batch is  -2.2789435386657715\n",
      "Val Loss for batch is  -4.066772937774658\n",
      "|Iter  1934  | Total Val Loss  -11.304839372634888 |\n",
      "Loss for batch is  -1.6541407108306885\n",
      "Loss for batch is  -1.7653281688690186\n",
      "Loss for batch is  -1.6481592655181885\n",
      "Loss for batch is  -2.7811741828918457\n",
      "|Iter  1935  | Total Train Loss  -7.848802328109741 |\n",
      "Val Loss for batch is  -2.249671220779419\n",
      "Val Loss for batch is  -2.6514391899108887\n",
      "Val Loss for batch is  -2.390894889831543\n",
      "Val Loss for batch is  -4.086586952209473\n",
      "|Iter  1935  | Total Val Loss  -11.378592252731323 |\n",
      "Loss for batch is  -1.6576838493347168\n",
      "Loss for batch is  -1.7663469314575195\n",
      "Loss for batch is  -1.6573421955108643\n",
      "Loss for batch is  -2.780822277069092\n",
      "|Iter  1936  | Total Train Loss  -7.862195253372192 |\n",
      "Val Loss for batch is  -2.318585157394409\n",
      "Val Loss for batch is  -2.661367416381836\n",
      "Val Loss for batch is  -2.356628894805908\n",
      "Val Loss for batch is  -4.0887579917907715\n",
      "|Iter  1936  | Total Val Loss  -11.425339460372925 |\n",
      "Loss for batch is  -1.6503950357437134\n",
      "Loss for batch is  -1.7670000791549683\n",
      "Loss for batch is  -1.6583316326141357\n",
      "Loss for batch is  -2.78041410446167\n",
      "|Iter  1937  | Total Train Loss  -7.856140851974487 |\n",
      "Val Loss for batch is  -2.2525196075439453\n",
      "Val Loss for batch is  -2.6703553199768066\n",
      "Val Loss for batch is  -2.4026753902435303\n",
      "Val Loss for batch is  -4.0659661293029785\n",
      "|Iter  1937  | Total Val Loss  -11.39151644706726 |\n",
      "Loss for batch is  -1.6586650609970093\n",
      "Loss for batch is  -1.7688578367233276\n",
      "Loss for batch is  -1.659448504447937\n",
      "Loss for batch is  -2.779813289642334\n",
      "|Iter  1938  | Total Train Loss  -7.866784691810608 |\n",
      "Val Loss for batch is  -2.2623939514160156\n",
      "Val Loss for batch is  -2.6695353984832764\n",
      "Val Loss for batch is  -2.44089412689209\n",
      "Val Loss for batch is  -4.079172611236572\n",
      "|Iter  1938  | Total Val Loss  -11.451996088027954 |\n",
      "Loss for batch is  -1.655107855796814\n",
      "Loss for batch is  -1.7620259523391724\n",
      "Loss for batch is  -1.6568479537963867\n",
      "Loss for batch is  -2.781362533569336\n",
      "|Iter  1939  | Total Train Loss  -7.855344295501709 |\n",
      "Val Loss for batch is  -2.2706120014190674\n",
      "Val Loss for batch is  -2.6708180904388428\n",
      "Val Loss for batch is  -2.4579203128814697\n",
      "Val Loss for batch is  -4.086458206176758\n",
      "|Iter  1939  | Total Val Loss  -11.485808610916138 |\n",
      "Loss for batch is  -1.6584320068359375\n",
      "Loss for batch is  -1.768633484840393\n",
      "Loss for batch is  -1.6583844423294067\n",
      "Loss for batch is  -2.7816991806030273\n",
      "|Iter  1940  | Total Train Loss  -7.867149114608765 |\n",
      "Val Loss for batch is  -2.233057975769043\n",
      "Val Loss for batch is  -2.5394763946533203\n",
      "Val Loss for batch is  -2.4513211250305176\n",
      "Val Loss for batch is  -4.0954508781433105\n",
      "|Iter  1940  | Total Val Loss  -11.319306373596191 |\n",
      "Loss for batch is  -1.6578223705291748\n",
      "Loss for batch is  -1.7696741819381714\n",
      "Loss for batch is  -1.6566805839538574\n",
      "Loss for batch is  -2.7815933227539062\n",
      "|Iter  1941  | Total Train Loss  -7.86577045917511 |\n",
      "Val Loss for batch is  -2.2709474563598633\n",
      "Val Loss for batch is  -2.666231393814087\n",
      "Val Loss for batch is  -2.373095750808716\n",
      "Val Loss for batch is  -4.081037521362305\n",
      "|Iter  1941  | Total Val Loss  -11.39131212234497 |\n",
      "Loss for batch is  -1.6604821681976318\n",
      "Loss for batch is  -1.7658493518829346\n",
      "Loss for batch is  -1.6558339595794678\n",
      "Loss for batch is  -2.7809438705444336\n",
      "|Iter  1942  | Total Train Loss  -7.863109350204468 |\n",
      "Val Loss for batch is  -2.2698752880096436\n",
      "Val Loss for batch is  -2.671121835708618\n",
      "Val Loss for batch is  -2.4552767276763916\n",
      "Val Loss for batch is  -4.088743209838867\n",
      "|Iter  1942  | Total Val Loss  -11.48501706123352 |\n",
      "Loss for batch is  -1.6591145992279053\n",
      "Loss for batch is  -1.7671175003051758\n",
      "Loss for batch is  -1.6572237014770508\n",
      "Loss for batch is  -2.77890682220459\n",
      "|Iter  1943  | Total Train Loss  -7.862362623214722 |\n",
      "Val Loss for batch is  -2.2266108989715576\n",
      "Val Loss for batch is  -2.653837203979492\n",
      "Val Loss for batch is  -2.4374563694000244\n",
      "Val Loss for batch is  -4.057877063751221\n",
      "|Iter  1943  | Total Val Loss  -11.375781536102295 |\n",
      "Loss for batch is  -1.6564924716949463\n",
      "Loss for batch is  -1.7647901773452759\n",
      "Loss for batch is  -1.6609688997268677\n",
      "Loss for batch is  -2.7832937240600586\n",
      "|Iter  1944  | Total Train Loss  -7.865545272827148 |\n",
      "Val Loss for batch is  -2.2524852752685547\n",
      "Val Loss for batch is  -2.6573781967163086\n",
      "Val Loss for batch is  -2.4050745964050293\n",
      "Val Loss for batch is  -4.10090446472168\n",
      "|Iter  1944  | Total Val Loss  -11.415842533111572 |\n",
      "Loss for batch is  -1.6528563499450684\n",
      "Loss for batch is  -1.765885591506958\n",
      "Loss for batch is  -1.6562120914459229\n",
      "Loss for batch is  -2.7777156829833984\n",
      "|Iter  1945  | Total Train Loss  -7.852669715881348 |\n",
      "Val Loss for batch is  -2.185811758041382\n",
      "Val Loss for batch is  -2.599377155303955\n",
      "Val Loss for batch is  -2.4793174266815186\n",
      "Val Loss for batch is  -4.09813117980957\n",
      "|Iter  1945  | Total Val Loss  -11.362637519836426 |\n",
      "Loss for batch is  -1.6566861867904663\n",
      "Loss for batch is  -1.7654658555984497\n",
      "Loss for batch is  -1.6587773561477661\n",
      "Loss for batch is  -2.78092622756958\n",
      "|Iter  1946  | Total Train Loss  -7.861855626106262 |\n",
      "Val Loss for batch is  -2.2324843406677246\n",
      "Val Loss for batch is  -2.6607275009155273\n",
      "Val Loss for batch is  -2.46042799949646\n",
      "Val Loss for batch is  -4.045848846435547\n",
      "|Iter  1946  | Total Val Loss  -11.399488687515259 |\n",
      "Loss for batch is  -1.6612529754638672\n",
      "Loss for batch is  -1.766690731048584\n",
      "Loss for batch is  -1.6498934030532837\n",
      "Loss for batch is  -2.7813448905944824\n",
      "|Iter  1947  | Total Train Loss  -7.859182000160217 |\n",
      "Val Loss for batch is  -2.2484130859375\n",
      "Val Loss for batch is  -2.6529595851898193\n",
      "Val Loss for batch is  -2.465075969696045\n",
      "Val Loss for batch is  -4.087484836578369\n",
      "|Iter  1947  | Total Val Loss  -11.453933477401733 |\n",
      "Loss for batch is  -1.6536937952041626\n",
      "Loss for batch is  -1.7678388357162476\n",
      "Loss for batch is  -1.656658411026001\n",
      "Loss for batch is  -2.7837977409362793\n",
      "|Iter  1948  | Total Train Loss  -7.86198878288269 |\n",
      "Val Loss for batch is  -2.2691726684570312\n",
      "Val Loss for batch is  -2.664141893386841\n",
      "Val Loss for batch is  -2.4368276596069336\n",
      "Val Loss for batch is  -4.09009313583374\n",
      "|Iter  1948  | Total Val Loss  -11.460235357284546 |\n",
      "Loss for batch is  -1.6566741466522217\n",
      "Loss for batch is  -1.7706423997879028\n",
      "Loss for batch is  -1.6617449522018433\n",
      "Loss for batch is  -2.779634952545166\n",
      "|Iter  1949  | Total Train Loss  -7.868696451187134 |\n",
      "Val Loss for batch is  -2.2315146923065186\n",
      "Val Loss for batch is  -2.66050124168396\n",
      "Val Loss for batch is  -2.4442853927612305\n",
      "Val Loss for batch is  -4.106403350830078\n",
      "|Iter  1949  | Total Val Loss  -11.442704677581787 |\n",
      "Loss for batch is  -1.6529875993728638\n",
      "Loss for batch is  -1.7708392143249512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6552910804748535\n",
      "Loss for batch is  -2.783820629119873\n",
      "|Iter  1950  | Total Train Loss  -7.8629385232925415 |\n",
      "Val Loss for batch is  -2.2471892833709717\n",
      "Val Loss for batch is  -2.672490119934082\n",
      "Val Loss for batch is  -2.4335758686065674\n",
      "Val Loss for batch is  -4.050447463989258\n",
      "|Iter  1950  | Total Val Loss  -11.403702735900879 |\n",
      "Loss for batch is  -1.6587235927581787\n",
      "Loss for batch is  -1.769528865814209\n",
      "Loss for batch is  -1.660723328590393\n",
      "Loss for batch is  -2.7815351486206055\n",
      "|Iter  1951  | Total Train Loss  -7.870510935783386 |\n",
      "Val Loss for batch is  -2.217625141143799\n",
      "Val Loss for batch is  -2.6366422176361084\n",
      "Val Loss for batch is  -2.4640440940856934\n",
      "Val Loss for batch is  -4.049383163452148\n",
      "|Iter  1951  | Total Val Loss  -11.367694616317749 |\n",
      "Loss for batch is  -1.6584997177124023\n",
      "Loss for batch is  -1.764519453048706\n",
      "Loss for batch is  -1.6574699878692627\n",
      "Loss for batch is  -2.7851274013519287\n",
      "|Iter  1952  | Total Train Loss  -7.8656165599823 |\n",
      "Val Loss for batch is  -2.1879963874816895\n",
      "Val Loss for batch is  -2.62726092338562\n",
      "Val Loss for batch is  -2.3647589683532715\n",
      "Val Loss for batch is  -4.066817760467529\n",
      "|Iter  1952  | Total Val Loss  -11.24683403968811 |\n",
      "Loss for batch is  -1.6544861793518066\n",
      "Loss for batch is  -1.7652521133422852\n",
      "Loss for batch is  -1.6585540771484375\n",
      "Loss for batch is  -2.7846035957336426\n",
      "|Iter  1953  | Total Train Loss  -7.862895965576172 |\n",
      "Val Loss for batch is  -2.234330892562866\n",
      "Val Loss for batch is  -2.6659233570098877\n",
      "Val Loss for batch is  -2.4520344734191895\n",
      "Val Loss for batch is  -4.075258255004883\n",
      "|Iter  1953  | Total Val Loss  -11.427546977996826 |\n",
      "Loss for batch is  -1.6568766832351685\n",
      "Loss for batch is  -1.7720261812210083\n",
      "Loss for batch is  -1.6586462259292603\n",
      "Loss for batch is  -2.7846527099609375\n",
      "|Iter  1954  | Total Train Loss  -7.8722018003463745 |\n",
      "Val Loss for batch is  -2.2421791553497314\n",
      "Val Loss for batch is  -2.6572277545928955\n",
      "Val Loss for batch is  -2.433894634246826\n",
      "Val Loss for batch is  -4.087211608886719\n",
      "|Iter  1954  | Total Val Loss  -11.420513153076172 |\n",
      "Loss for batch is  -1.6611279249191284\n",
      "Loss for batch is  -1.7672491073608398\n",
      "Loss for batch is  -1.6590489149093628\n",
      "Loss for batch is  -2.7832179069519043\n",
      "|Iter  1955  | Total Train Loss  -7.870643854141235 |\n",
      "Val Loss for batch is  -2.2709763050079346\n",
      "Val Loss for batch is  -2.663614511489868\n",
      "Val Loss for batch is  -2.3918912410736084\n",
      "Val Loss for batch is  -4.079582691192627\n",
      "|Iter  1955  | Total Val Loss  -11.406064748764038 |\n",
      "Loss for batch is  -1.660252332687378\n",
      "Loss for batch is  -1.7668591737747192\n",
      "Loss for batch is  -1.659144639968872\n",
      "Loss for batch is  -2.782778263092041\n",
      "|Iter  1956  | Total Train Loss  -7.86903440952301 |\n",
      "Val Loss for batch is  -2.2532424926757812\n",
      "Val Loss for batch is  -2.6322972774505615\n",
      "Val Loss for batch is  -2.4576520919799805\n",
      "Val Loss for batch is  -4.09262752532959\n",
      "|Iter  1956  | Total Val Loss  -11.435819387435913 |\n",
      "Loss for batch is  -1.6572308540344238\n",
      "Loss for batch is  -1.7671685218811035\n",
      "Loss for batch is  -1.6608703136444092\n",
      "Loss for batch is  -2.780268430709839\n",
      "|Iter  1957  | Total Train Loss  -7.865538120269775 |\n",
      "Val Loss for batch is  -2.2289440631866455\n",
      "Val Loss for batch is  -2.668116569519043\n",
      "Val Loss for batch is  -2.431704521179199\n",
      "Val Loss for batch is  -4.079319477081299\n",
      "|Iter  1957  | Total Val Loss  -11.408084630966187 |\n",
      "Loss for batch is  -1.656949520111084\n",
      "Loss for batch is  -1.770995020866394\n",
      "Loss for batch is  -1.660319447517395\n",
      "Loss for batch is  -2.782651901245117\n",
      "|Iter  1958  | Total Train Loss  -7.87091588973999 |\n",
      "Val Loss for batch is  -2.2726035118103027\n",
      "Val Loss for batch is  -2.645519256591797\n",
      "Val Loss for batch is  -2.4807076454162598\n",
      "Val Loss for batch is  -4.084225177764893\n",
      "|Iter  1958  | Total Val Loss  -11.483055591583252 |\n",
      "Loss for batch is  -1.6567550897598267\n",
      "Loss for batch is  -1.7693604230880737\n",
      "Loss for batch is  -1.6591031551361084\n",
      "Loss for batch is  -2.7828497886657715\n",
      "|Iter  1959  | Total Train Loss  -7.86806845664978 |\n",
      "Val Loss for batch is  -2.24235463142395\n",
      "Val Loss for batch is  -2.634016513824463\n",
      "Val Loss for batch is  -2.4284744262695312\n",
      "Val Loss for batch is  -4.082884788513184\n",
      "|Iter  1959  | Total Val Loss  -11.387730360031128 |\n",
      "Loss for batch is  -1.6568500995635986\n",
      "Loss for batch is  -1.769272804260254\n",
      "Loss for batch is  -1.655620813369751\n",
      "Loss for batch is  -2.782223701477051\n",
      "|Iter  1960  | Total Train Loss  -7.863967418670654 |\n",
      "Val Loss for batch is  -2.1887259483337402\n",
      "Val Loss for batch is  -2.680110454559326\n",
      "Val Loss for batch is  -2.4081637859344482\n",
      "Val Loss for batch is  -4.110350608825684\n",
      "|Iter  1960  | Total Val Loss  -11.387350797653198 |\n",
      "Loss for batch is  -1.655585527420044\n",
      "Loss for batch is  -1.7737947702407837\n",
      "Loss for batch is  -1.663176417350769\n",
      "Loss for batch is  -2.7876129150390625\n",
      "|Iter  1961  | Total Train Loss  -7.880169630050659 |\n",
      "Val Loss for batch is  -2.2839136123657227\n",
      "Val Loss for batch is  -2.682690143585205\n",
      "Val Loss for batch is  -2.3370721340179443\n",
      "Val Loss for batch is  -4.0974321365356445\n",
      "|Iter  1961  | Total Val Loss  -11.401108026504517 |\n",
      "Loss for batch is  -1.658033013343811\n",
      "Loss for batch is  -1.7733460664749146\n",
      "Loss for batch is  -1.6598244905471802\n",
      "Loss for batch is  -2.777092933654785\n",
      "|Iter  1962  | Total Train Loss  -7.868296504020691 |\n",
      "Val Loss for batch is  -2.277405023574829\n",
      "Val Loss for batch is  -2.6921117305755615\n",
      "Val Loss for batch is  -2.399519920349121\n",
      "Val Loss for batch is  -4.083350658416748\n",
      "|Iter  1962  | Total Val Loss  -11.45238733291626 |\n",
      "Loss for batch is  -1.6599503755569458\n",
      "Loss for batch is  -1.7679498195648193\n",
      "Loss for batch is  -1.6641364097595215\n",
      "Loss for batch is  -2.7834231853485107\n",
      "|Iter  1963  | Total Train Loss  -7.875459790229797 |\n",
      "Val Loss for batch is  -2.2509915828704834\n",
      "Val Loss for batch is  -2.6457533836364746\n",
      "Val Loss for batch is  -2.3985724449157715\n",
      "Val Loss for batch is  -4.094483852386475\n",
      "|Iter  1963  | Total Val Loss  -11.389801263809204 |\n",
      "Loss for batch is  -1.6528761386871338\n",
      "Loss for batch is  -1.7685712575912476\n",
      "Loss for batch is  -1.6608343124389648\n",
      "Loss for batch is  -2.7849955558776855\n",
      "|Iter  1964  | Total Train Loss  -7.867277264595032 |\n",
      "Val Loss for batch is  -2.250696897506714\n",
      "Val Loss for batch is  -2.6669256687164307\n",
      "Val Loss for batch is  -2.4464008808135986\n",
      "Val Loss for batch is  -4.086726665496826\n",
      "|Iter  1964  | Total Val Loss  -11.45075011253357 |\n",
      "Loss for batch is  -1.663347840309143\n",
      "Loss for batch is  -1.7720239162445068\n",
      "Loss for batch is  -1.6621800661087036\n",
      "Loss for batch is  -2.7852091789245605\n",
      "|Iter  1965  | Total Train Loss  -7.882761001586914 |\n",
      "Val Loss for batch is  -2.256523847579956\n",
      "Val Loss for batch is  -2.6445982456207275\n",
      "Val Loss for batch is  -2.4600298404693604\n",
      "Val Loss for batch is  -4.087991237640381\n",
      "|Iter  1965  | Total Val Loss  -11.449143171310425 |\n",
      "Loss for batch is  -1.6607873439788818\n",
      "Loss for batch is  -1.7695115804672241\n",
      "Loss for batch is  -1.6627106666564941\n",
      "Loss for batch is  -2.7858798503875732\n",
      "|Iter  1966  | Total Train Loss  -7.878889441490173 |\n",
      "Val Loss for batch is  -2.2581307888031006\n",
      "Val Loss for batch is  -2.6490237712860107\n",
      "Val Loss for batch is  -2.3832616806030273\n",
      "Val Loss for batch is  -4.107420921325684\n",
      "|Iter  1966  | Total Val Loss  -11.397837162017822 |\n",
      "Loss for batch is  -1.6566689014434814\n",
      "Loss for batch is  -1.7722370624542236\n",
      "Loss for batch is  -1.659886121749878\n",
      "Loss for batch is  -2.781395196914673\n",
      "|Iter  1967  | Total Train Loss  -7.870187282562256 |\n",
      "Val Loss for batch is  -2.103872537612915\n",
      "Val Loss for batch is  -2.654806137084961\n",
      "Val Loss for batch is  -2.455630302429199\n",
      "Val Loss for batch is  -4.106560707092285\n",
      "|Iter  1967  | Total Val Loss  -11.32086968421936 |\n",
      "Loss for batch is  -1.6586438417434692\n",
      "Loss for batch is  -1.7696993350982666\n",
      "Loss for batch is  -1.6633405685424805\n",
      "Loss for batch is  -2.7862067222595215\n",
      "|Iter  1968  | Total Train Loss  -7.877890467643738 |\n",
      "Val Loss for batch is  -2.2446954250335693\n",
      "Val Loss for batch is  -2.657557964324951\n",
      "Val Loss for batch is  -2.435137987136841\n",
      "Val Loss for batch is  -4.082796573638916\n",
      "|Iter  1968  | Total Val Loss  -11.420187950134277 |\n",
      "Loss for batch is  -1.6600688695907593\n",
      "Loss for batch is  -1.7713035345077515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6607606410980225\n",
      "Loss for batch is  -2.7810962200164795\n",
      "|Iter  1969  | Total Train Loss  -7.873229265213013 |\n",
      "Val Loss for batch is  -2.0795562267303467\n",
      "Val Loss for batch is  -2.6499931812286377\n",
      "Val Loss for batch is  -2.440166711807251\n",
      "Val Loss for batch is  -4.096907615661621\n",
      "|Iter  1969  | Total Val Loss  -11.266623735427856 |\n",
      "Loss for batch is  -1.661215901374817\n",
      "Loss for batch is  -1.7740733623504639\n",
      "Loss for batch is  -1.6616499423980713\n",
      "Loss for batch is  -2.7833828926086426\n",
      "|Iter  1970  | Total Train Loss  -7.880322098731995 |\n",
      "Val Loss for batch is  -2.214024066925049\n",
      "Val Loss for batch is  -2.661651372909546\n",
      "Val Loss for batch is  -2.451695203781128\n",
      "Val Loss for batch is  -4.074854373931885\n",
      "|Iter  1970  | Total Val Loss  -11.402225017547607 |\n",
      "Loss for batch is  -1.6586782932281494\n",
      "Loss for batch is  -1.773674726486206\n",
      "Loss for batch is  -1.6599116325378418\n",
      "Loss for batch is  -2.7837677001953125\n",
      "|Iter  1971  | Total Train Loss  -7.87603235244751 |\n",
      "Val Loss for batch is  -2.304529905319214\n",
      "Val Loss for batch is  -2.6769511699676514\n",
      "Val Loss for batch is  -2.4561753273010254\n",
      "Val Loss for batch is  -4.0776801109313965\n",
      "|Iter  1971  | Total Val Loss  -11.515336513519287 |\n",
      "Loss for batch is  -1.6552003622055054\n",
      "Loss for batch is  -1.7713204622268677\n",
      "Loss for batch is  -1.6590367555618286\n",
      "Loss for batch is  -2.780020236968994\n",
      "|Iter  1972  | Total Train Loss  -7.865577816963196 |\n",
      "Val Loss for batch is  -2.2137978076934814\n",
      "Val Loss for batch is  -2.6431448459625244\n",
      "Val Loss for batch is  -2.4637858867645264\n",
      "Val Loss for batch is  -4.08841609954834\n",
      "|Iter  1972  | Total Val Loss  -11.409144639968872 |\n",
      "Loss for batch is  -1.6599016189575195\n",
      "Loss for batch is  -1.7688877582550049\n",
      "Loss for batch is  -1.6633349657058716\n",
      "Loss for batch is  -2.7848739624023438\n",
      "|Iter  1973  | Total Train Loss  -7.87699830532074 |\n",
      "Val Loss for batch is  -2.2870218753814697\n",
      "Val Loss for batch is  -2.641404867172241\n",
      "Val Loss for batch is  -2.4386870861053467\n",
      "Val Loss for batch is  -4.07692813873291\n",
      "|Iter  1973  | Total Val Loss  -11.444041967391968 |\n",
      "Loss for batch is  -1.6644831895828247\n",
      "Loss for batch is  -1.7713332176208496\n",
      "Loss for batch is  -1.6638667583465576\n",
      "Loss for batch is  -2.7848095893859863\n",
      "|Iter  1974  | Total Train Loss  -7.884492754936218 |\n",
      "Val Loss for batch is  -2.1965746879577637\n",
      "Val Loss for batch is  -2.624830961227417\n",
      "Val Loss for batch is  -2.440161943435669\n",
      "Val Loss for batch is  -4.091466903686523\n",
      "|Iter  1974  | Total Val Loss  -11.353034496307373 |\n",
      "Loss for batch is  -1.6560752391815186\n",
      "Loss for batch is  -1.7710574865341187\n",
      "Loss for batch is  -1.664408564567566\n",
      "Loss for batch is  -2.7824535369873047\n",
      "|Iter  1975  | Total Train Loss  -7.873994827270508 |\n",
      "Val Loss for batch is  -2.223939895629883\n",
      "Val Loss for batch is  -2.6403892040252686\n",
      "Val Loss for batch is  -2.441601276397705\n",
      "Val Loss for batch is  -4.102715015411377\n",
      "|Iter  1975  | Total Val Loss  -11.408645391464233 |\n",
      "Loss for batch is  -1.6631766557693481\n",
      "Loss for batch is  -1.7756352424621582\n",
      "Loss for batch is  -1.6640045642852783\n",
      "Loss for batch is  -2.787743330001831\n",
      "|Iter  1976  | Total Train Loss  -7.890559792518616 |\n",
      "Val Loss for batch is  -2.263875961303711\n",
      "Val Loss for batch is  -2.6521592140197754\n",
      "Val Loss for batch is  -2.4655845165252686\n",
      "Val Loss for batch is  -4.071659088134766\n",
      "|Iter  1976  | Total Val Loss  -11.45327877998352 |\n",
      "Loss for batch is  -1.6618239879608154\n",
      "Loss for batch is  -1.7718842029571533\n",
      "Loss for batch is  -1.6601204872131348\n",
      "Loss for batch is  -2.7832846641540527\n",
      "|Iter  1977  | Total Train Loss  -7.877113342285156 |\n",
      "Val Loss for batch is  -2.2485032081604004\n",
      "Val Loss for batch is  -2.6439425945281982\n",
      "Val Loss for batch is  -2.4209299087524414\n",
      "Val Loss for batch is  -4.099145412445068\n",
      "|Iter  1977  | Total Val Loss  -11.412521123886108 |\n",
      "Loss for batch is  -1.66515052318573\n",
      "Loss for batch is  -1.770095705986023\n",
      "Loss for batch is  -1.6626440286636353\n",
      "Loss for batch is  -2.783695697784424\n",
      "|Iter  1978  | Total Train Loss  -7.881585955619812 |\n",
      "Val Loss for batch is  -2.201937437057495\n",
      "Val Loss for batch is  -2.5842535495758057\n",
      "Val Loss for batch is  -2.4355380535125732\n",
      "Val Loss for batch is  -4.092670917510986\n",
      "|Iter  1978  | Total Val Loss  -11.31439995765686 |\n",
      "Loss for batch is  -1.6601990461349487\n",
      "Loss for batch is  -1.7716809511184692\n",
      "Loss for batch is  -1.6629196405410767\n",
      "Loss for batch is  -2.779841661453247\n",
      "|Iter  1979  | Total Train Loss  -7.874641299247742 |\n",
      "Val Loss for batch is  -2.2442514896392822\n",
      "Val Loss for batch is  -2.676062822341919\n",
      "Val Loss for batch is  -2.445101737976074\n",
      "Val Loss for batch is  -4.102206707000732\n",
      "|Iter  1979  | Total Val Loss  -11.467622756958008 |\n",
      "Loss for batch is  -1.6587170362472534\n",
      "Loss for batch is  -1.7715543508529663\n",
      "Loss for batch is  -1.6625853776931763\n",
      "Loss for batch is  -2.785926342010498\n",
      "|Iter  1980  | Total Train Loss  -7.878783106803894 |\n",
      "Val Loss for batch is  -2.189362049102783\n",
      "Val Loss for batch is  -2.668344020843506\n",
      "Val Loss for batch is  -2.4351043701171875\n",
      "Val Loss for batch is  -4.089662551879883\n",
      "|Iter  1980  | Total Val Loss  -11.38247299194336 |\n",
      "Loss for batch is  -1.663543939590454\n",
      "Loss for batch is  -1.774248719215393\n",
      "Loss for batch is  -1.6653186082839966\n",
      "Loss for batch is  -2.7839956283569336\n",
      "|Iter  1981  | Total Train Loss  -7.887106895446777 |\n",
      "Val Loss for batch is  -2.280824899673462\n",
      "Val Loss for batch is  -2.6434993743896484\n",
      "Val Loss for batch is  -2.4430882930755615\n",
      "Val Loss for batch is  -4.09201717376709\n",
      "|Iter  1981  | Total Val Loss  -11.459429740905762 |\n",
      "Loss for batch is  -1.6634272336959839\n",
      "Loss for batch is  -1.7723199129104614\n",
      "Loss for batch is  -1.6609753370285034\n",
      "Loss for batch is  -2.784613609313965\n",
      "|Iter  1982  | Total Train Loss  -7.881336092948914 |\n",
      "Val Loss for batch is  -2.2298269271850586\n",
      "Val Loss for batch is  -2.6432647705078125\n",
      "Val Loss for batch is  -2.4535701274871826\n",
      "Val Loss for batch is  -4.085177421569824\n",
      "|Iter  1982  | Total Val Loss  -11.411839246749878 |\n",
      "Loss for batch is  -1.6625045537948608\n",
      "Loss for batch is  -1.7700109481811523\n",
      "Loss for batch is  -1.6644632816314697\n",
      "Loss for batch is  -2.7868733406066895\n",
      "|Iter  1983  | Total Train Loss  -7.883852124214172 |\n",
      "Val Loss for batch is  -2.2455339431762695\n",
      "Val Loss for batch is  -2.109301805496216\n",
      "Val Loss for batch is  -2.4300971031188965\n",
      "Val Loss for batch is  -4.100286483764648\n",
      "|Iter  1983  | Total Val Loss  -10.88521933555603 |\n",
      "Loss for batch is  -1.6665118932724\n",
      "Loss for batch is  -1.7720285654067993\n",
      "Loss for batch is  -1.6644659042358398\n",
      "Loss for batch is  -2.7840991020202637\n",
      "|Iter  1984  | Total Train Loss  -7.887105464935303 |\n",
      "Val Loss for batch is  -2.237247943878174\n",
      "Val Loss for batch is  -2.619290828704834\n",
      "Val Loss for batch is  -2.4671809673309326\n",
      "Val Loss for batch is  -4.105622291564941\n",
      "|Iter  1984  | Total Val Loss  -11.429342031478882 |\n",
      "Loss for batch is  -1.662989616394043\n",
      "Loss for batch is  -1.7694287300109863\n",
      "Loss for batch is  -1.6631591320037842\n",
      "Loss for batch is  -2.780620574951172\n",
      "|Iter  1985  | Total Train Loss  -7.876198053359985 |\n",
      "Val Loss for batch is  -2.2752256393432617\n",
      "Val Loss for batch is  -2.672337770462036\n",
      "Val Loss for batch is  -2.407261848449707\n",
      "Val Loss for batch is  -4.062775611877441\n",
      "|Iter  1985  | Total Val Loss  -11.417600870132446 |\n",
      "Loss for batch is  -1.660189151763916\n",
      "Loss for batch is  -1.7696852684020996\n",
      "Loss for batch is  -1.6661961078643799\n",
      "Loss for batch is  -2.784250020980835\n",
      "|Iter  1986  | Total Train Loss  -7.8803205490112305 |\n",
      "Val Loss for batch is  -2.2347373962402344\n",
      "Val Loss for batch is  -2.6450278759002686\n",
      "Val Loss for batch is  -2.4724605083465576\n",
      "Val Loss for batch is  -4.063392162322998\n",
      "|Iter  1986  | Total Val Loss  -11.415617942810059 |\n",
      "Loss for batch is  -1.6608057022094727\n",
      "Loss for batch is  -1.7705427408218384\n",
      "Loss for batch is  -1.666082501411438\n",
      "Loss for batch is  -2.7818679809570312\n",
      "|Iter  1987  | Total Train Loss  -7.87929892539978 |\n",
      "Val Loss for batch is  -2.225076675415039\n",
      "Val Loss for batch is  -2.6545095443725586\n",
      "Val Loss for batch is  -2.3891773223876953\n",
      "Val Loss for batch is  -4.094651699066162\n",
      "|Iter  1987  | Total Val Loss  -11.363415241241455 |\n",
      "Loss for batch is  -1.6570957899093628\n",
      "Loss for batch is  -1.7668505907058716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch is  -1.6609703302383423\n",
      "Loss for batch is  -2.7840723991394043\n",
      "|Iter  1988  | Total Train Loss  -7.868989109992981 |\n",
      "Val Loss for batch is  -2.2863240242004395\n",
      "Val Loss for batch is  -2.663879156112671\n",
      "Val Loss for batch is  -2.443204879760742\n",
      "Val Loss for batch is  -4.094455242156982\n",
      "|Iter  1988  | Total Val Loss  -11.487863302230835 |\n",
      "Loss for batch is  -1.6605709791183472\n",
      "Loss for batch is  -1.7736443281173706\n",
      "Loss for batch is  -1.6603111028671265\n",
      "Loss for batch is  -2.7858452796936035\n",
      "|Iter  1989  | Total Train Loss  -7.880371689796448 |\n",
      "Val Loss for batch is  -2.226987600326538\n",
      "Val Loss for batch is  -2.648064613342285\n",
      "Val Loss for batch is  -2.4467456340789795\n",
      "Val Loss for batch is  -4.077172756195068\n",
      "|Iter  1989  | Total Val Loss  -11.398970603942871 |\n",
      "Loss for batch is  -1.662467360496521\n",
      "Loss for batch is  -1.7734318971633911\n",
      "Loss for batch is  -1.6658982038497925\n",
      "Loss for batch is  -2.7838754653930664\n",
      "|Iter  1990  | Total Train Loss  -7.885672926902771 |\n",
      "Val Loss for batch is  -2.267822742462158\n",
      "Val Loss for batch is  -2.6451613903045654\n",
      "Val Loss for batch is  -2.4524118900299072\n",
      "Val Loss for batch is  -4.017087459564209\n",
      "|Iter  1990  | Total Val Loss  -11.38248348236084 |\n",
      "Loss for batch is  -1.6621278524398804\n",
      "Loss for batch is  -1.7729600667953491\n",
      "Loss for batch is  -1.6637800931930542\n",
      "Loss for batch is  -2.7873783111572266\n",
      "|Iter  1991  | Total Train Loss  -7.88624632358551 |\n",
      "Val Loss for batch is  -2.2444827556610107\n",
      "Val Loss for batch is  -2.6390762329101562\n",
      "Val Loss for batch is  -2.471710205078125\n",
      "Val Loss for batch is  -4.081372261047363\n",
      "|Iter  1991  | Total Val Loss  -11.436641454696655 |\n",
      "Loss for batch is  -1.6645179986953735\n",
      "Loss for batch is  -1.775286316871643\n",
      "Loss for batch is  -1.6619127988815308\n",
      "Loss for batch is  -2.7836999893188477\n",
      "|Iter  1992  | Total Train Loss  -7.885417103767395 |\n",
      "Val Loss for batch is  -2.259403705596924\n",
      "Val Loss for batch is  -2.6486027240753174\n",
      "Val Loss for batch is  -2.4346446990966797\n",
      "Val Loss for batch is  -4.08419132232666\n",
      "|Iter  1992  | Total Val Loss  -11.426842451095581 |\n",
      "Loss for batch is  -1.6655529737472534\n",
      "Loss for batch is  -1.7743109464645386\n",
      "Loss for batch is  -1.6650060415267944\n",
      "Loss for batch is  -2.7857675552368164\n",
      "|Iter  1993  | Total Train Loss  -7.890637516975403 |\n",
      "Val Loss for batch is  -2.2419521808624268\n",
      "Val Loss for batch is  -2.6682047843933105\n",
      "Val Loss for batch is  -2.4330875873565674\n",
      "Val Loss for batch is  -4.091362476348877\n",
      "|Iter  1993  | Total Val Loss  -11.434607028961182 |\n",
      "Loss for batch is  -1.6622833013534546\n",
      "Loss for batch is  -1.7733279466629028\n",
      "Loss for batch is  -1.6644283533096313\n",
      "Loss for batch is  -2.7804365158081055\n",
      "|Iter  1994  | Total Train Loss  -7.880476117134094 |\n",
      "Val Loss for batch is  -2.2714688777923584\n",
      "Val Loss for batch is  -2.66902756690979\n",
      "Val Loss for batch is  -2.4203267097473145\n",
      "Val Loss for batch is  -4.063155651092529\n",
      "|Iter  1994  | Total Val Loss  -11.423978805541992 |\n",
      "Loss for batch is  -1.658387541770935\n",
      "Loss for batch is  -1.7715448141098022\n",
      "Loss for batch is  -1.6659945249557495\n",
      "Loss for batch is  -2.781644821166992\n",
      "|Iter  1995  | Total Train Loss  -7.877571702003479 |\n",
      "Val Loss for batch is  -2.2315590381622314\n",
      "Val Loss for batch is  -2.699660062789917\n",
      "Val Loss for batch is  -2.4691805839538574\n",
      "Val Loss for batch is  -4.106250762939453\n",
      "|Iter  1995  | Total Val Loss  -11.506650447845459 |\n",
      "Loss for batch is  -1.6609877347946167\n",
      "Loss for batch is  -1.7740648984909058\n",
      "Loss for batch is  -1.6632071733474731\n",
      "Loss for batch is  -2.782942295074463\n",
      "|Iter  1996  | Total Train Loss  -7.8812021017074585 |\n",
      "Val Loss for batch is  -2.26284122467041\n",
      "Val Loss for batch is  -2.660167932510376\n",
      "Val Loss for batch is  -2.4334487915039062\n",
      "Val Loss for batch is  -4.0791916847229\n",
      "|Iter  1996  | Total Val Loss  -11.435649633407593 |\n",
      "Loss for batch is  -1.6627458333969116\n",
      "Loss for batch is  -1.7728055715560913\n",
      "Loss for batch is  -1.6628304719924927\n",
      "Loss for batch is  -2.784964084625244\n",
      "|Iter  1997  | Total Train Loss  -7.88334596157074 |\n",
      "Val Loss for batch is  -2.2071611881256104\n",
      "Val Loss for batch is  -2.652898073196411\n",
      "Val Loss for batch is  -2.4671449661254883\n",
      "Val Loss for batch is  -4.064362525939941\n",
      "|Iter  1997  | Total Val Loss  -11.391566753387451 |\n",
      "Loss for batch is  -1.657246470451355\n",
      "Loss for batch is  -1.7716511487960815\n",
      "Loss for batch is  -1.6651719808578491\n",
      "Loss for batch is  -2.7855820655822754\n",
      "|Iter  1998  | Total Train Loss  -7.879651665687561 |\n",
      "Val Loss for batch is  -2.2532272338867188\n",
      "Val Loss for batch is  -2.66776704788208\n",
      "Val Loss for batch is  -2.450336217880249\n",
      "Val Loss for batch is  -4.093149185180664\n",
      "|Iter  1998  | Total Val Loss  -11.464479684829712 |\n",
      "Loss for batch is  -1.6644352674484253\n",
      "Loss for batch is  -1.772006869316101\n",
      "Loss for batch is  -1.6629730463027954\n",
      "Loss for batch is  -2.7869558334350586\n",
      "|Iter  1999  | Total Train Loss  -7.88637101650238 |\n",
      "Val Loss for batch is  -2.1220996379852295\n",
      "Val Loss for batch is  -2.659846067428589\n",
      "Val Loss for batch is  -2.3370981216430664\n",
      "Val Loss for batch is  -4.080087661743164\n",
      "|Iter  1999  | Total Val Loss  -11.199131488800049 |\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.niters):\n",
    "    total_train_loss = 0\n",
    "    val_loss = 0\n",
    "    test_loss = 0\n",
    "    if epoch == 0:\n",
    "        var_coeff = 0.001\n",
    "    else:\n",
    "        var_coeff = 2*scheduler.get_last_lr()[0]\n",
    "    \n",
    "    for entry,(time_steps,batch) in enumerate(zip(time_loader,Train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        data = batch[0].to(device).view(num_years,1,len(paths_to_data)*(args.scale+1),H,W)\n",
    "        past_sample = vel_train[entry].view(num_years,2*len(paths_to_data)*(args.scale+1),H,W).to(device)\n",
    "        model.update_param([past_sample,const_channels_info.to(device),lat_map.to(device),lon_map.to(device)])\n",
    "        t = time_steps.float().to(device).flatten()\n",
    "       \n",
    "        mean,std,_ = model(t,data)\n",
    "            \n",
    "        loss = nll(mean,std,batch.float().to(device),lat,var_coeff)\n",
    "            \n",
    "        l2_lambda = 0.001\n",
    "        l2_norm = sum(p.pow(2.0).sum()\n",
    "                for p in model.parameters())\n",
    "        loss = loss + l2_lambda * l2_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        print(\"Loss for batch is \",loss.item())\n",
    "        if torch.isnan(loss) : \n",
    "            print(\"Quitting due to Nan loss\")\n",
    "            quit()\n",
    "        total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "    lr_val = scheduler.get_last_lr()[0]\n",
    "    scheduler.step()\n",
    "   \n",
    "    print(\"|Iter \",epoch,\" | Total Train Loss \", total_train_loss,\"|\")\n",
    "        \n",
    "\n",
    "    for entry,(time_steps,batch) in enumerate(zip(time_loader,Val_loader)):\n",
    "        data = batch[0].to(device).view(1,1,len(paths_to_data)*(args.scale+1),H,W)\n",
    "        past_sample = vel_val[entry].view(1,2*len(paths_to_data)*(args.scale+1),H,W).to(device)\n",
    "        model.update_param([past_sample,const_channels_info.to(device),lat_map.to(device),lon_map.to(device)])\n",
    "        t = time_steps.float().to(device).flatten()\n",
    "         \n",
    "        mean,std,_ = model(t,data)\n",
    "               \n",
    "        loss = nll(mean,std,batch.float().to(device),lat,var_coeff)\n",
    "        if torch.isnan(loss) : \n",
    "            print(\"Quitting due to Nan loss\")\n",
    "            quit()\n",
    "      \n",
    "        print(\"Val Loss for batch is \",loss.item())\n",
    "        val_loss = val_loss + loss.item()\n",
    "\n",
    "    print(\"|Iter \",epoch,\" | Total Val Loss \", val_loss,\"|\")\n",
    "\n",
    "\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model,str(cwd) + \"/Models/gradient_modified/\" + \"ClimODE_monthly_avg_\"+args.solver+\"_\"+str(args.spectral)+\"_model_pad2.pt\")\n",
    "\n",
    "    if total_train_loss < train_best_loss:\n",
    "        train_best_loss = total_train_loss\n",
    "        torch.save(model,str(cwd) + \"/Models/gradient_modified/\" + \"ClimODE_monthly_avg_overfit_\"+args.solver+\"_\"+str(args.spectral)+\"_model_pad2_\" + str(epoch) + \".pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23025604",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef2ac3d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:29:05.352198Z",
     "start_time": "2024-10-14T08:29:05.335783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "SOLVERS = [\"dopri8\",\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams', 'fixed_adams',\"adaptive_heun\"]\n",
    "parser = argparse.ArgumentParser('ClimODE')\n",
    "parser.add_argument('--solver', type=str, default=\"euler\", choices=SOLVERS)\n",
    "parser.add_argument('--atol', type=float, default=5e-3)\n",
    "parser.add_argument('--rtol', type=float, default=5e-3)\n",
    "parser.add_argument('--batch_size', type=int, default=3)\n",
    "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
    "parser.add_argument('--teacher', type=int, default=1,choices=[0,1])\n",
    "parser.add_argument('--scale', type=int, default=0)\n",
    "parser.add_argument('--days', type=int, default=3)\n",
    "parser.add_argument('--spectral', type=int, default=0,choices=[0,1])\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "330b9a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:29:16.810285Z",
     "start_time": "2024-10-14T08:29:06.300652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ Data is loading ###########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/groupby.py:668: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  index_grouper = pd.Grouper(\n",
      "/home/namkyeong/anaconda3/envs/clim_ode/lib/python3.9/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data 12\n",
      "Length of validation data 12\n",
      "Length of testing data 12\n",
      "['/home/namkyeong/file_share/clim_ode_Theseus/era5_data/constants/constants_5.625deg.nc']\n",
      "Climate_encoder_free_uncertain_monthly(\n",
      "  (vel_f): Climate_ResNet_2D(\n",
      "    (layer_cnn): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(50, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(50, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (2): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (3): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (2): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (activation_cnn): ModuleList()\n",
      "  )\n",
      "  (vel_att): Self_attn_conv(\n",
      "    (query): Sequential(\n",
      "      (0): boundarypad()\n",
      "      (1): Conv2d(50, 25, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (2): LeakyReLU(negative_slope=0.3)\n",
      "      (3): boundarypad()\n",
      "      (4): Conv2d(25, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): LeakyReLU(negative_slope=0.3)\n",
      "      (6): boundarypad()\n",
      "      (7): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (key): Sequential(\n",
      "      (0): Conv2d(50, 25, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): LeakyReLU(negative_slope=0.3)\n",
      "      (2): Conv2d(25, 6, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): LeakyReLU(negative_slope=0.3)\n",
      "      (4): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (value): Sequential(\n",
      "      (0): Conv2d(50, 25, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): LeakyReLU(negative_slope=0.3)\n",
      "      (2): Conv2d(25, 10, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): LeakyReLU(negative_slope=0.3)\n",
      "      (4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (post_map): Sequential(\n",
      "      (0): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (noise_net): Climate_ResNet_2D(\n",
      "    (layer_cnn): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(29, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(29, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (2): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(64, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (activation): LeakyReLU(negative_slope=0.3)\n",
      "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (shortcut): Identity()\n",
      "          (norm1): Identity()\n",
      "          (norm2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (activation_cnn): ModuleList()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "train_time_scale= slice('2006','2016')\n",
    "val_time_scale = slice('2016','2016')\n",
    "test_time_scale = slice('2017','2018')\n",
    "\n",
    "paths_to_data = [str(cwd) + '/era5_data/geopotential_500/*.nc',str(cwd) + '/era5_data/temperature_850/*.nc',str(cwd) + '/era5_data/2m_temperature/*.nc',str(cwd) + '/era5_data/10m_u_component_of_wind/*.nc',str(cwd) + '/era5_data/10m_v_component_of_wind/*.nc']\n",
    "const_info_path = [str(cwd) + '/era5_data/constants/constants_5.625deg.nc']\n",
    "levels = [\"z\",\"t\",\"t2m\",\"u10\",\"v10\",\"v\",\"u\",\"r\",\"q\"]\n",
    "paths_to_data = paths_to_data[0:5]\n",
    "levels = levels[0:5]\n",
    "assert len(paths_to_data) == len(levels), \"Paths to different type of data must be same as number of types of observations\"\n",
    "print(\"############################ Data is loading ###########################\")\n",
    "Final_train_data = 0\n",
    "Final_val_data = 0\n",
    "Final_test_data = 0\n",
    "max_lev = []\n",
    "min_lev = []\n",
    "for idx,data in enumerate(paths_to_data):\n",
    "    Train_data,Val_data,Test_data,time_steps,lat,lon,mean,std,time_stamp = get_train_test_data_without_scales_batched_monthly(data,train_time_scale,val_time_scale,test_time_scale,levels[idx],args.spectral)  \n",
    "    max_lev.append(mean)\n",
    "    min_lev.append(std)\n",
    "    if idx==0: \n",
    "        Final_train_data = Train_data\n",
    "        Final_val_data = Val_data\n",
    "        Final_test_data = Test_data\n",
    "    else:\n",
    "        Final_train_data = torch.cat([Final_train_data,Train_data],dim=2)\n",
    "        Final_val_data = torch.cat([Final_val_data,Val_data],dim=2)\n",
    "        Final_test_data = torch.cat([Final_test_data,Test_data],dim=2)\n",
    "\n",
    "print(\"Length of training data\",len(Final_train_data))\n",
    "print(\"Length of validation data\",len(Final_val_data))\n",
    "print(\"Length of testing data\",len(Final_test_data))\n",
    "const_channels_info,lat_map,lon_map = add_constant_info(const_info_path)\n",
    "\n",
    "if args.spectral == 1: print(\"############## Running the Model in Spectral Domain ####################\")\n",
    "H,W = Train_data.shape[3],Train_data.shape[4]\n",
    "clim = torch.mean(Final_test_data,dim=0)\n",
    "Test_loader = DataLoader(Final_test_data[2:],batch_size=args.batch_size,shuffle=False)\n",
    "time_loader = DataLoader(time_steps[2:],batch_size=args.batch_size,shuffle=False)\n",
    "time_idx_steps = torch.tensor([i for i in range(12)]).view(-1,1)\n",
    "time_idx = DataLoader(time_idx_steps[2:],batch_size=args.batch_size,shuffle=False,pin_memory=False)\n",
    "\n",
    "otal_time_len = len(time_steps[2:])\n",
    "total_time_steps = time_steps[2:].numpy().flatten().tolist()\n",
    "num_years  = 2\n",
    "\n",
    "vel_test= torch.from_numpy(np.load('test_monthly_vel_gradient_modified_pad2.npy'))\n",
    "model = torch.load(str(cwd) + \"/Models/gradient_modified/ClimODE_monthly_avg_euler_0_model_pad2.pt\",map_location=torch.device('cpu')).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8feedb82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:29:16.820544Z",
     "start_time": "2024-10-14T08:29:16.811772Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_rmsd_mm(Pred,Truth,lat,lon,max_vals,min_vals,H,W,levels):\n",
    "    RMSD_final = []\n",
    "    RMSD_lat_lon = []\n",
    "    true_lat_lon = []\n",
    "    pred_lat_lon = []\n",
    "    for idx,lev in enumerate(levels):\n",
    "        true_idx = idx\n",
    "        das_pred = []\n",
    "        das_true = []\n",
    "        pred_spectral = Pred[idx].detach().cpu().numpy()\n",
    "        true_spectral = Truth[true_idx,:,:].detach().cpu().numpy()\n",
    "\n",
    "        pred = pred_spectral*(max_vals[idx] - min_vals[idx]) + min_vals[idx]\n",
    "\n",
    "        das_pred.append(xr.DataArray(pred.reshape(1,H,W),dims=['time','lat','lon'],coords={'time':[0],'lat':lat,'lon':lon},name=lev))\n",
    "        Pred_xr = xr.merge(das_pred)\n",
    "        \n",
    "        true = true_spectral*(max_vals[idx] - min_vals[idx]) + min_vals[idx]\n",
    "\n",
    "        das_true.append(xr.DataArray(true.reshape(1,H,W),dims=['time','lat','lon'],coords={'time':[0],'lat':lat,'lon':lon},name=lev))\n",
    "        True_xr = xr.merge(das_true)\n",
    "        error = Pred_xr - True_xr\n",
    "        weights_lat = np.cos(np.deg2rad(error.lat))\n",
    "        weights_lat /= weights_lat.mean()\n",
    "        rmse = np.sqrt(((error)**2 * weights_lat).mean(dim=['lat','lon'])).mean(dim=['time'])\n",
    "        lat_lon_rmse = np.sqrt((error)**2)\n",
    "        RMSD_lat_lon.append(lat_lon_rmse[lev].values)\n",
    "        RMSD_final.append(rmse[lev].values.tolist())\n",
    "\n",
    "    return RMSD_final\n",
    "\n",
    "\n",
    "def evaluation_acc_mm(Pred,Truth,lat,lon,max_vals,min_vals,H,W,levels,clim):\n",
    "    ACC_final = []\n",
    "    \n",
    "    for idx,lev in enumerate(levels):\n",
    "        pred_spectral = Pred[idx].detach().cpu().numpy()\n",
    "        true_spectral = Truth[idx,:,:].detach().cpu().numpy()\n",
    "        pred_spectral = pred_spectral - clim[idx]\n",
    "        true_spectral = true_spectral - clim[idx]\n",
    "\n",
    "        pred = pred_spectral*(max_vals[idx] - min_vals[idx]) + min_vals[idx]\n",
    "        true = true_spectral*(max_vals[idx] - min_vals[idx]) + min_vals[idx]\n",
    "\n",
    "        weights_lat = np.cos(np.deg2rad(lat))\n",
    "        weights_lat /= weights_lat.mean()\n",
    "        weights_lat = weights_lat.reshape(len(lat),1)\n",
    "        weights_lat = weights_lat.repeat(len(lon),1)\n",
    "\n",
    "        pred_prime = pred - np.mean(pred)\n",
    "        true_prime = true - np.mean(true)\n",
    "\n",
    "        acc= np.sum(weights_lat * pred_prime * true_prime) / np.sqrt(np.sum(weights_lat * pred_prime**2) * np.sum(weights_lat * true_prime**2))\n",
    "        ACC_final.append(acc)                                                        \n",
    "\n",
    "    \n",
    "    return ACC_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb5c514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:29:17.356657Z",
     "start_time": "2024-10-14T08:29:16.821622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead Time  1 Month  | Observable  z | Mean RMSD  710.6426761491199 | Std RMSD  88.2510208342286\n",
      "Lead Time  1 Month  | Observable  t | Mean RMSD  2.9676447383601423 | Std RMSD  0.14507121891792354\n",
      "Lead Time  1 Month  | Observable  t2m | Mean RMSD  4.238117968350293 | Std RMSD  0.4536048609789241\n",
      "Lead Time  1 Month  | Observable  u10 | Mean RMSD  1.946921546517326 | Std RMSD  0.10503010160636819\n",
      "Lead Time  1 Month  | Observable  v10 | Mean RMSD  1.5733455307975674 | Std RMSD  0.06772849214980023\n",
      "Lead Time  2 Month  | Observable  z | Mean RMSD  915.0024740909117 | Std RMSD  106.99709024351912\n",
      "Lead Time  2 Month  | Observable  t | Mean RMSD  3.2326488060428553 | Std RMSD  0.38952360407959274\n",
      "Lead Time  2 Month  | Observable  t2m | Mean RMSD  4.496102593798292 | Std RMSD  0.43444551453324504\n",
      "Lead Time  2 Month  | Observable  u10 | Mean RMSD  2.066315348241121 | Std RMSD  0.13009083407538177\n",
      "Lead Time  2 Month  | Observable  v10 | Mean RMSD  1.6968007725005079 | Std RMSD  0.17680991047651246\n"
     ]
    }
   ],
   "source": [
    "RMSD = []\n",
    "RMSD_lat_lon= []\n",
    "Pred = []\n",
    "Truth  = []\n",
    "\n",
    "org_time = 1\n",
    "RMSD = []\n",
    "RMSD_lat_lon= []\n",
    "Mean_pred = 0\n",
    "Truth_pred = 0\n",
    "Std_pred = 0\n",
    "Lead_RMSD_arr = {\"z\":[[] for _ in range(args.batch_size-1)],\"t\":[[] for _ in range(args.batch_size-1)],\"t2m\":[[] for _ in range(args.batch_size-1)],\"u10\":[[] for _ in range(args.batch_size-1)],\"v10\":[[] for _ in range(args.batch_size-1)]}\n",
    "Lead_ACC = {\"z\":[[] for _ in range(args.batch_size-1)],\"t\":[[] for _ in range(args.batch_size-1)],\"t2m\":[[] for _ in range(args.batch_size-1)],\"u10\":[[] for _ in range(args.batch_size-1)],\"v10\":[[] for _ in range(args.batch_size-1)]}\n",
    "Lead_CRPS = {\"z\":[[] for _ in range(args.batch_size-1)],\"t\":[[] for _ in range(args.batch_size-1)],\"t2m\":[[] for _ in range(args.batch_size-1)],\"u10\":[[] for _ in range(args.batch_size-1)],\"v10\":[[] for _ in range(args.batch_size-1)]}\n",
    "for entry,(time_steps,batch) in enumerate(zip(time_loader,Test_loader)):\n",
    "        data = batch[0].to(device).view(num_years,1,len(paths_to_data)*(args.scale+1),H,W)\n",
    "        past_sample = vel_test[entry].view(num_years,2*len(paths_to_data)*(args.scale+1),H,W).to(device)\n",
    "        model.update_param([past_sample,const_channels_info.to(device),lat_map.to(device),lon_map.to(device)])\n",
    "        t = time_steps.float().to(device).flatten()\n",
    "        mean_pred,std_pred,_ = model(t,data)\n",
    "\n",
    "        mean_avg = mean_pred.view(-1,len(paths_to_data)*(args.scale+1),H,W)\n",
    "        std_avg = std_pred.view(-1,len(paths_to_data)*(args.scale+1),H,W)\n",
    "\n",
    "        for yr in range(2): \n",
    "            for t_step in range(1,len(time_steps),1):\n",
    "                evaluate_rmsd = evaluation_rmsd_mm(mean_pred[t_step,yr,:,:,:].cpu(),batch[t_step,yr,:,:,:].cpu(),lat,lon,max_lev,min_lev,H,W,levels)\n",
    "                evaluate_acc = evaluation_acc_mm(mean_pred[t_step,yr,:,:,:].cpu(),batch[t_step,yr,:,:,:].cpu(),lat,lon,max_lev,min_lev,H,W,levels,clim[yr,:,:,:].cpu().detach().numpy())\n",
    "                \n",
    "\n",
    "                for idx,lev in enumerate(levels):\n",
    "                    Lead_RMSD_arr[lev][t_step-1].append(evaluate_rmsd[idx]) \n",
    "                    Lead_ACC[lev][t_step-1].append(evaluate_acc[idx])\n",
    "\n",
    "for t_idx in range(args.batch_size-1):\n",
    "    for idx, lev in enumerate(levels):\n",
    "        print(\"Lead Time \",(t_idx+1), \"Month \",\"| Observable \",lev, \"| Mean RMSD \", np.mean(Lead_RMSD_arr[lev][t_idx]), \"| Std RMSD \", np.std(Lead_RMSD_arr[lev][t_idx]))\n",
    "        #print(\"Lead Time \",(t_idx+1), \"Month \",\"| Observable \",lev, \"| Mean ACC \", np.mean(Lead_ACC[lev][t_idx]), \"| Std ACC \", np.std(Lead_ACC[lev][t_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd3555b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:29:19.038032Z",
     "start_time": "2024-10-14T08:29:17.361521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8UAAAKtCAYAAAC+O79tAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOy9eZgdVbX+v6rq1Jl67vSUoTOSMJNoILkhIKgRLiiKIxCVQUSmAJJHZRAICCb3imC+F4EoGsCrXLlwZfhdELwGkBmVQQEhCZA56U6nOz2d02eqqt8fsTupWm93V8IJCSfv53nO83St3rVrD2uvtfeuc/YyPM/zhBBCCCGEEEIIIYQQQgghhBBCCClBzD1dAEIIIYQQQgghhBBCCCGEEEIIIWR3wZfihBBCCCGEEEIIIYQQQgghhBBCSha+FCeEEEIIIYQQQgghhBBCCCGEEFKy8KU4IYQQQgghhBBCCCGEEEIIIYSQkoUvxQkhhBBCCCGEEEIIIYQQQgghhJQsfClOCCGEEEIIIYQQQgghhBBCCCGkZOFLcUIIIYQQQgghhBBCCCGEEEIIISULX4oTQgghhBBCCCGEEEIIIYQQQggpWfhSnBBCCCGEEEIIIYQQQgghhBBCSMnCl+KEfIAce+yxcuyxx+7pYhBCCCkhrr32WjEMY08XgxBCyIeE8ePHy5lnnrmni0EIIeRDwOrVq8UwDLnrrrv2dFEIIYR8wBiGIddee+2eLgYhRYUvxck+w1NPPSWGYQx8bNuWiRMnyumnny7vvffeni4eIYSQYdi0aZNcfvnl8vGPf1wqKirEMAx56qmnBk3//PPPy1FHHSXJZFKamprk4osvlt7e3qKXa0ffYpqmjBo1So477rghy0YIIWT384c//EHOPvtsOeSQQ8SyLBk/fvygaV3XlR/96EcyYcIEicfjcthhh8l//dd/Fb1Md911l89vxONxmTJlisybN09aW1uL/jxCCNlXef755+Xaa6+Vzs5OnzydTsutt94qxx13nIwcOVIqKirkIx/5iNx+++3iOM6eKewQnHnmmT6/UVlZKVOnTpWbbrpJstnsni4eIYTs0wzma4pB/w8g+j/JZFIOOuggueqqq6S7u7vozyNkXyGypwtAyAfNxRdfLEcccYTk83l55ZVX5Oc//7k88sgj8vrrr8uoUaP2dPEIIYQMwvLly+Xf//3fZfLkyXLooYfKCy+8MGja1157TT75yU/KgQceKDfffLOsX79efvzjH8vKlSvl97//fdHL9qlPfUpOP/108TxPVq1aJbfddpt84hOfkEceeUROOOGEoj+PEELI8Nxzzz1y7733ykc/+tFh5/nf//735d/+7d/knHPOkSOOOEIeeughmTt3rhiGIaeeemrRy/aDH/xAJkyYIJlMRp599lm5/fbb5dFHH5U33nhDkslk0Z9HCCH7Gs8//7xcd911cuaZZ0p1dfWA/L333pOLLrpIPvnJT8r8+fOlsrJSHn/8cbngggvkxRdflLvvvnvPFXoQYrGY/OIXvxARkc7OTvmf//kf+c53viN/+ctf5Le//e0eLh0hhOy7DOZrisntt98u5eXl0tvbK3/4wx/khz/8oTzxxBPy3HPP8dRAQnYBvhQn+xxHH320fOlLXxIRkbPOOkumTJkiF198sdx9991yxRVXwHtSqZSUlZV9kMUkhBASYPr06dLe3i61tbVy//33y5e//OVB01555ZVSU1MjTz31lFRWVorItuNizznnHPnDH/4gxx13XFHLNmXKFPna1742cP35z39eDjvsMFm8ePGgL8UzmYxEo1ExTR7cQwghu4OFCxfKHXfcIbZty2c+8xl54403YLoNGzbITTfdJBdeeKH89Kc/FRGRb37zm3LMMcfId7/7Xfnyl78slmUVtWwnnHCCHH744QPPGjFihNx8883y0EMPyWmnnQbv4ZqEEELeP01NTfL666/LwQcfPCA799xz5Rvf+IbceeedcvXVV8t+++23B0uoiUQivrXGBRdcIDNnzpR7771Xbr75ZvjFL8/zJJPJSCKR+CCLSgghpMh86Utfkrq6OhEROe+88+SLX/yi/O53v5MXX3xRZs2aBe9Jp9P8oi0hg8BdWLLH6Y9PNNhnd/OJT3xCRERWrVolItuPJvnHP/4hc+fOlZqaGjnqqKMG0v/617+W6dOnSyKRkNraWjn11FNl3bp1Kt+f//znMmnSJEkkEjJjxgx55plndntdCCHkw8b9998vhmHIn/70J/W/n/3sZ2IYxsBLjIqKCqmtrR02z+7ubvm///s/+drXvjbwQlxE5PTTT5fy8nL57//+7+JVYBAOPfRQqaurG/At/SE8fvvb38pVV10lo0ePlmQyOXDk1UsvvST/+q//KlVVVZJMJuWYY46R5557TuX77LPPyhFHHCHxeFwmTZokP/vZz3Z7XQghZG/izDPPhMeg98/hd2TUqFFi2/aweT700EOSz+flggsuGJAZhiHnn3++rF+/fsiTSYpFcE1y5plnSnl5ubz77rty4oknSkVFhXz1q18VkW1HvS9evFgOPvhgicfj0tjYKOeee65s3brVl6fneXLDDTfImDFjJJlMysc//nF58803d3tdCCFkT3PttdfKd7/7XRERmTBhwsD+0urVq6Wurs73Qryfz3/+8yIi8tZbbw3I+kNePPvss3LxxRdLfX29VFdXy7nnniu5XE46Ozvl9NNPl5qaGqmpqZHvfe974nnebq+faZpy7LHHisi2PTWRbV8A/sxnPiOPP/64HH744ZJIJAbWCp2dnfLtb39bmpubJRaLyX777Sf//u//Lq7r+vLt7OyUM888U6qqqqS6ulrOOOOM3XIkMCGElAJD+ZrdSXDdcOyxx8ohhxwiL7/8snzsYx+TZDIpV155pYiIZLNZWbBggey3334Si8WkublZvve976nwG9lsVi699FKpr6+XiooK+exnPyvr16/frfUgZE/BX4qTPU59fb3853/+p0+Wz+fl0ksvlWg0utuf/+6774qIyIgRI3zyL3/5yzJ58mRZuHDhwKLmhz/8oVx99dXyla98Rb75zW9KW1ub3HLLLfKxj31MXn311YFjUn75y1/KueeeK0ceeaR8+9vflvfee08++9nPSm1trTQ3N+/2OhFCyIeFT3/60wMvqo855hjf/+699145+OCD5ZBDDtmpPF9//XUpFAoDv8DrJxqNyrRp0+TVV1993+Uejq1bt8rWrVvVr0yuv/56iUaj8p3vfEey2axEo1F54okn5IQTTpDp06fLggULxDRNufPOO+UTn/iEPPPMMzJjxoyBeh133HFSX18v1157rRQKBVmwYIE0Njbu9voQQkgp8+qrr0pZWZkceOCBPnm//X311Vd9X5LdHaA1SaFQkOOPP16OOuoo+fGPfzzwa49zzz1X7rrrLjnrrLPk4osvllWrVslPf/pTefXVV+W5554b+CLANddcIzfccIOceOKJcuKJJ8orr7wixx13nORyud1aF0II2dN84QtfkBUrVsh//dd/yU9+8pOBX9jV19cPek9LS4uIyEDaHbnoooukqalJrrvuOnnxxRfl5z//uVRXV8vzzz8vY8eOlYULF8qjjz4qN954oxxyyCFy+umn756K7QDyG8uXL5fTTjtNzj33XDnnnHNk//33l3Q6Lcccc4xs2LBBzj33XBk7dqw8//zzcsUVV8imTZtk8eLFIrLti1Sf+9zn5Nlnn5XzzjtPDjzwQHnggQfkjDPO2O11IYSQDyO74muKAbL/7e3tcsIJJ8ipp54qX/va16SxsVFc15XPfvaz8uyzz8q3vvUtOfDAA+X111+Xn/zkJ7JixQp58MEHB+7/5je/Kb/+9a9l7ty5cuSRR8oTTzwhn/70p3drPQjZY3iE7IVccMEFnmVZ3hNPPFG0PJ988klPRLylS5d6bW1t3saNG71HHnnEGz9+vGcYhveXv/zF8zzPW7BggSci3mmnnea7f/Xq1Z5lWd4Pf/hDn/z111/3IpHIgDyXy3kNDQ3etGnTvGw2O5Du5z//uSci3jHHHFO0OhFCSClw2mmneQ0NDV6hUBiQbdq0yTNN0/vBD34A77nvvvs8EfGefPLJQf/39NNPq/99+ctf9pqamopWds/zPBHxzj77bK+trc3bvHmz99JLL3mf/OQnPRHxbrrpJs/ztvugiRMneul0euBe13W9yZMne8cff7znuu6APJ1OexMmTPA+9alPDchOPvlkLx6Pe2vWrBmQ/eMf//Asy/I4pSOE7CucccYZ3rhx45S8fw4/GJ/+9Kfhff3/mzhxopKnUilPRLzLL798V4uruPPOOz0R8f74xz96bW1t3rp167zf/va33ogRI7xEIuGtX7/e87xt9UTPfuaZZzwR8X7zm9/45I899phPvnnzZi8ajXqf/vSnff7lyiuv9ETEO+OMM4pWJ0II2Ru58cYbPRHxVq1aNWzabDbrHXTQQd6ECRO8fD4/IO+32cG5+qxZszzDMLzzzjtvQFYoFLwxY8YUfc/njDPO8MrKyry2tjavra3Ne+edd7yFCxd6hmF4hx122EC6cePGeSLiPfbYY777r7/+eq+srMxbsWKFT3755Zd7lmV5a9eu9TzP8x588EFPRLwf/ehHvjodffTRnoh4d955Z1HrRQghpcDO+JqdpX99s3z5cq+trc1btWqV97Of/cyLxWJeY2Ojl0qlPM/zvGOOOcYTEW/JkiW++//zP//TM03Te+aZZ3zyJUuWeCLiPffcc57ned5rr73miYh3wQUX+NLNnTvXExFvwYIFRa8bIXsSHp9O9jp+9atfyW233SY/+tGP5OMf/3jR8//GN74h9fX1MmrUKPn0pz8tqVRK7r77bvWLwvPOO893/bvf/U5c15WvfOUrsmXLloFPU1OTTJ48WZ588kkREfnrX/8qmzdvlvPOO8/3S/f+I6gIIYT4OeWUU2Tz5s3y1FNPDcjuv/9+cV1XTjnllJ3Or6+vT0REYrGY+l88Hh/4fzH55S9/KfX19dLQ0CAzZ86U5557TubPny/f/va3fenOOOMMX1y/1157TVauXClz586V9vb2Ad+SSqXkk5/8pDz99NPiuq44jiOPP/64nHzyyTJ27NiB+w888EA5/vjji14fQgjZl+jr6xvUZ/T/v9jMmTNH6uvrpbm5WU499VQpLy+XBx54QEaPHu1Ld/755/uu77vvPqmqqpJPfepTvjXJ9OnTpby8fGBN8sc//lFyuZxcdNFFvmPlg36JEEKIyLx58+Qf//iH/PSnP5VIRB+qefbZZ/ts6cyZM8XzPDn77LMHZJZlyeGHHy7vvfde0cuXSqWkvr5e6uvrZb/99pMrr7xSZs2aJQ888IAv3YQJE9Ta4L777pOjjz5aampqfH5jzpw54jiOPP300yIi8uijj0okEvH5Hcuy5KKLLip6fQghhIRn//33l/r6epkwYYKce+65st9++8kjjzziixkei8XkrLPO8t133333yYEHHigHHHCAz/73H7/ev2549NFHRUTk4osv9t3PdQMpVXh8OtmreO211+S8886T0047TebPnz9k2lwuJx0dHT5ZfX29WJY15H3XXHONHH300WJZltTV1cmBBx4IFz0TJkzwXa9cuVI8z5PJkyfDfPuPKVyzZo2IiEpn27ZMnDhxyLIRQsi+SH8s7XvvvVc++clPisi2o9OnTZsmU6ZM2en8+l86B2MkiYhkMhnfS2lE/9GJ/VRVVQ17z+c+9zmZN2+eGIYhFRUVcvDBB0tZWZlKh3yLiAx5LGFXV5dks1np6+uDPmj//fcfWMQQQgjZeRKJxKA+o///g9HX1yddXV0+WVNT07DPvPXWW2XKlCkSiUSksbFR9t9/fzFN/3fWI5GIjBkzxidbuXKldHV1SUNDA8x38+bNIjL4mqS+vl5qamqGLR8hhOwr3HjjjXLHHXfI9ddfLyeeeCJMs+OXUkVk4AcPwfB4VVVVsnXr1iGf19XV5fuyVTQaldra2iHvicfj8v/9f/+fiGx78TFhwgTlH0T0WkNkm9/4+9//Puhxvjv6jZEjR0p5ebnv//vvv/+QZSOEEBKO3t5e6e3tHbi2LCvUUev/8z//I5WVlWLbtowZM0YmTZqk0owePVqFoV25cqW89dZboey/aZoqX9p/UqrwpTjZa9i6dat88YtflClTpsgvfvGLYdM///zz6pfkq1atkvHjxw9536GHHipz5swZNv/g5pfrumIYhvz+97+HL96DCwdCCCHhiMVicvLJJ8sDDzwgt912m7S2tspzzz0nCxcu3KX8Ro4cKSIimzZtUv/btGmTjBo1KtT9/dx5551y5plnDnnPmDFjdtm3iGzbjJs2bRq8p7y8HL6sIYSQfZEdf6m3I47j7HKeI0eOlCeffFI8z/Pl3+9HhvIb9957r/pVhud5wz5zxowZ6qSqILFYTL0od11XGhoa5De/+Q28Z3fHMCSEkFLirrvukssuu0zOO+88ueqqqwZNN9iPL5B8OB9wySWXyN133z1wfcwxx/hOzBrsObuy1hDZ5jc+9alPyfe+9z14z658CZkQQsjO8+Mf/1iuu+66getx48bJ6tWrh73vYx/72EC88sEYzP4feuihcvPNN8N7gl/sImRfgS/FyV6B67ry1a9+VTo7O+WPf/yj7/iPwZg6dar83//9n08W5lcZu8qkSZPE8zyZMGHCkIuGcePGici2b2P1H0ciIpLP52XVqlUyderU3VZGQgj5sHLKKafI3XffLcuWLZO33npLPM/bpaPTRUQOOeQQiUQi8te//lW+8pWvDMhzuZy89tprPhki6FsOPvjgXSpHGPq/iVtZWTnkRld9fb0kEomBX5bvyPLly3db+QghZG+jpqZGOjs7lbz/l9G7wrRp0+QXv/iFvPXWW3LQQQcNyF966aWB/w/G8ccfr/zG7mTSpEnyxz/+UWbPnj3kL9h3XJPseFpVW1vbsL9iJISQUmCwL1H189BDD8k3v/lN+cIXviC33nrrB1Qqke9973vyta99beB6d5/eMWnSJOnt7R32pfq4ceNk2bJl0tvb6/vRB9cahBAyOMP5mh05/fTT5aijjhq4Hu5EwvfLpEmT5G9/+5t88pOfHLKc48aNE9d15d133/X9Opz2n5QqjClO9gquu+46efzxx+W//uu/4HFPiJqaGpkzZ47v0x/3b3fwhS98QSzLkuuuu05989fzPGlvbxcRkcMPP1zq6+tlyZIlksvlBtLcddddcAOPEELIttiqtbW1cu+998q9994rM2bMCO0PglRVVcmcOXPk17/+tfT09AzI//M//1N6e3vly1/+8rBl2fET/OV4MZk+fbpMmjRJfvzjH/uO0eqnra1NRLb9OuT444+XBx98UNauXTvw/7feeksef/zx3VY+QgjZ25g0aZJ0dXXJ3//+9wHZpk2bVFzVneFzn/uc2LYtt91224DM8zxZsmSJjB49Wo488shB7x05cqTyG7uTr3zlK+I4jlx//fXqf4VCYWC9MWfOHLFtW2655Rbf2mXx4sW7tXyEELK30B/KCO3DPP3003LqqafKxz72MfnNb36jTuXYnRx00EE+nzF9+vTd+ryvfOUr8sILL8A1Q2dnpxQKBREROfHEE6VQKMjtt98+8H/HceSWW27ZreUjhJAPM0P5miATJ0702f/Zs2fv1rJ95StfkQ0bNsgdd9yh/tfX1yepVEpERE444QQREfmP//gPXxquG0ipwl+Kkz3O66+/Ltdff7187GMfk82bN8uvf/1r3/93/AbtnmTSpElyww03yBVXXCGrV6+Wk08+WSoqKmTVqlXywAMPyLe+9S35zne+I7Ztyw033CDnnnuufOITn5BTTjlFVq1aJXfeeSdjihNCyCDYti1f+MIX5Le//a2kUin58Y9/DNPdcMMNIiLy5ptvisi2F93PPvusiIjvyMMf/vCHcuSRR8oxxxwj3/rWt2T9+vVy0003yXHHHSf/+q//uptrEx7TNOUXv/iFnHDCCXLwwQfLWWedJaNHj5YNGzbIk08+KZWVlQPxA6+77jp57LHH5Oijj5YLLrhACoWC3HLLLXLwwQf7Xg4RQkgpc+qpp8pll10mn//85+Xiiy+WdDott99+u0yZMkVeeeUVX9q///3v8vDDD4uIyDvvvCNdXV0DfmTq1Kly0kknici2EBjf/va35cYbb5R8Pi9HHHGEPPjgg/LMM8/Ib37zm0GPzd0THHPMMXLuuefKokWL5LXXXpPjjjtObNuWlStXyn333Sf/7//9P/nSl74k9fX18p3vfEcWLVokn/nMZ+TEE0+UV199VX7/+98Pe/wiIYSUAv0vm7///e/LqaeeKrZty0knnSRbtmyRz372s2IYhnzpS1+S++67z3ffYYcdJocddtieKPJu4bvf/a48/PDD8pnPfEbOPPNMmT59uqRSKXn99dfl/vvvl9WrV0tdXZ2cdNJJMnv2bLn88stl9erVctBBB8nvfvc76erq2tNVIISQvZbBfE3/y/I9yde//nX57//+bznvvPPkySeflNmzZ4vjOPL222/Lf//3f8vjjz8uhx9+uEybNk1OO+00ue2226Srq0uOPPJIWbZsmbzzzjt7ugqE7Bb4Upzscdrb28XzPPnTn/4kf/rTn9T/95aX4iIil19+uUyZMkV+8pOfDMQAaW5uluOOO04++9nPDqT71re+JY7jyI033ijf/e535dBDD5WHH35Yrr766j1VdEII2es55ZRT5Be/+IUYhjHoEedBO7p06dKBv3d8Kf7Rj35U/vjHP8pll10ml156qVRUVMjZZ58tixYt2j2Ffx8ce+yx8sILL8j1118vP/3pT6W3t1eamppk5syZcu655w6kO+yww+Txxx+X+fPnyzXXXCNjxoyR6667TjZt2sSX4oSQfYYRI0bIAw88IPPnz5fvfe97MmHCBFm0aJGsXLlSvRR/5ZVXlN/ovz7jjDMGXoqLiPzbv/2b1NTUyM9+9jO56667ZPLkyfLrX/9a5s6du/srtZMsWbJEpk+fLj/72c/kyiuvlEgkIuPHj5evfe1rvl+c3HDDDRKPx2XJkiXy5JNPysyZM+UPf/iDfPrTn96DpSeEkA+GI444Qq6//npZsmSJPPbYY+K6rqxatUpWr1498KL3wgsvVPctWLCgpF6KJ5NJ+dOf/iQLFy6U++67T371q19JZWWlTJkyRa677jqpqqoSkW1f1n344Yfl29/+tvz6178WwzDks5/9rNx0003ykY98ZA/XghBC9k4G8zV7w0tx0zTlwQcflJ/85Cfyq1/9Sh544AFJJpMyceJEueSSS3zhYZcuXSr19fXym9/8Rh588EH5xCc+IY888gjjjpOSxPCC50ATQgghhBBCCCGEEEIIIYQQQgghJQJjihNCCCGEEEIIIYQQQgghhBBCCClZ+FKcEEIIIYQQQgghhBBCCCGEEEJIycKX4oQQQgghhBBCCCGEEEIIIYQQQkoWvhQnhBBCCCGEEEIIIYQQQgghhBBSsvClOCGEEEIIIYQQQgghhBBCCCGEkJIlsrsyvvXWW+XGG2+UlpYWmTp1qtxyyy0yY8aMYe9zXVc2btwoFRUVYhjG7ioeIYR8IHieJz09PTJq1Cgxze3fQ8pkMpLL5Ya8NxqNSjwe391F3CugzyCEEPqMsNBnEEIIfUZY6DMIIYQ+Iyz0GYQQsg/4DG838Nvf/taLRqPe0qVLvTfffNM755xzvOrqaq+1tXXYe9etW+eJCD/88MNPSX3WrVs3YOf6+vq8pgZr2Huampq8vr6+3WGm9yroM/jhhx9+/B/6jMGhz+CHH3748X/oMwaHPoMffvjhx/+hzxgc+gx++OGHH/+nVH2G4XmeJ0Vm5syZcsQRR8hPf/pTEdn2banm5ma56KKL5PLLLx/y3q6uLqmurpaj5ESJiD0gN+MxldbNZIcti3vUYUrWPVZ/U6FybUbJrL8sHzZ/L6vLYHz0QC1bvgbcrJveiOl6iuf6r01Lp0Ey1wmXLpj/IGXbZVA5YLoQzzT3wLftULks0I4Oau9AeQ0QsQB9g/D9tH8wv7B9jupUKIB7Qdls23eJvhXp9vYqmVlRobPP9CmZ09mtZMGxYtXVqjSFDZuUzKqtUTLURkY8qvNbv9F/22EHqDTu39/23yN5eVYelc7OTqmqqhIRke7ubqmqqpJVL4+TygocxaK7x5UJ09dIV1eXVFZWwjSlQjF8xuzp35FIZLtOeLZuV/PZv/uvK8pVGrdH6+kDK15Xsi9+6cv63kDfi4gYU/3+wNzQptI4W7YomfcvhyqZ2afHo+HqsexG/fqM7vNsoPPvw+54yu6ARMDsQJA7iwBhEOAfXKAHHkiH6h5M50b0fWYB3IfcFPJdwLcE8/MsfR/K3wjZdQZ4ZrDvTAd0lBPuAbAcaK5T8D/Di+h+gmW1dDqrV8/D8rUJnV/e/0y7VfuVvokjlCzapfPvmZBUsop3U0oWxHp3vZK5E0f7rgtOVp7520/oM4Zgd6wzgrZaRKRnYpnvuvyBv4Yqn1mm9eN/Xn1JyT4/Rdv5YmLV1SlZ91Hjlazqb5v9grReE3llekxBTKCfYXwLui+M3RfgfwZ9RuAa+SRgi5AdDgOya6isXjScn3KRPwhRNjMP2j9klQxk+wP3wvZHSxvoz8L5uDBEMnodhp7pgPaOtWudNwL+2MzpudTGY/Xao+kl7VvaPqrXO7Vv6vWO+YKebw4H1xnDUwyfMe2zV4llb99DylYPr/d9dTpNtEfflq3W48xO6XszDdpoOWV+vbfK8yqNCSZnZWV6btNYrgvX0jO8bjghBy1K5YJ7HQfYP9cvc/L6wEsnD3Q9q/2ImQFzyT79TCvjl0XSOnsL/EjKzIF5L9gmCnaLC87wLCRAuUD+8XatG/kyXc98mT+/6ne1vqB5b65K74ugNZAD1l35Cn8fFOK6TrkKLcvrZTpcZ0S7tTDW6ZdZWdAnYJ1hgHmBayN/rNN5gXR5UM9Yl34AqpMXMshp0L93j9dKlGjzP9PJZ+RvD9xAnzEExfAZh8+5UiI7+IytU3Tf1Cz3zyvWn6T1Y8p39f5S+mMHKVnZynadbj89R8nU+MthgnleHtidXCWYrwFVcbWpECfmf4YLfjjqxHXdvYQ2nFZcyxLAn1XE/LKquJ7n1cX0vl9FROcVQQY8BDnXVrKUo2VpRzdaBqQLliNq6XKNjHUp2bh4h5KVW3oOui6r90G25Pzz19UpvZe+obtKl9XU/Rm3tb+xgAHMO34Da4A0sYiek1fHdJ1qo9pxm8DQpx3/u4XuvFbSroyWJW09CUBzrt6cfs8XvHcTmG85jh5kmU1lSoYmWFVv63uD81KgGpJs9euVk8/IK4/+sGR9RtGPT8/lcvLyyy/LFVdcMSAzTVPmzJkjL7zwgkqfzWYlu8OL5Z6enn8WzJaIscNLcUMbChfNWoJpIlpxraiWRUBLWIY2REE8UAbD0vkboPzbvjwRJt0uvhSXveSluBfSiYTZ1d8TR9CgchmgHeFqJ8RL8ZAvS0ITzC9sn0O9QjMdUDYzxEtxoNumqWUeaEcDjMWgzDLBF0rAfRZ4JnwpHiI/0wJf1gk+85/NhdokUe5Johz3db7431faKymaz4jEJLKDvUeb0Waw/6Bf0TqDHH0kTN+LiBFIh3Qe6bcHfJdp6YmkAXyQG3Bo6D4PfAlmt78UD2u+wSb2Lr8URy8livlSHPjx9/VSPJDfHnkpjuZWIR8Q+qW4FPGlOHJxYPwYAb8XscCi10bzQ7AhCdMNP9exkM0Bc0YR+ozB2F3rjKCtFtH6EAmxLhDBvgX6kZD57SpovgN1PDjfARsZHmgfSDFfiqPBDdjll+LoNmBjdv2luC4/fAkc0k/t8ktx1P5hX4rDNVCgDO/npTio5y6/FC+EeylugJc2aB8g6DNMoI9WDIwn8JYM7z2AOcCu2ASuM4akWD7DsuM++1mIDq/3Vgy90NS3WXHdF1YBzDlDvDQwk+AlMBjHlv7ulkTKgO46IWz/+3gpbqB7wSawEXgp7oGX4l4O7bOA9gCLFBOUwwrIwPsH9D1euDYI81LcAHbIQzoE8o/Y6EvSup5uIL8I+JI0mve6EbCPg+qJvowceIYHxg4aKy7aikG/VYmi9gi8FIcvwIv7UjyYzgX1RP30vl6KB/rAimklsqJ4z5w+A1O0dUbAZ6C+idj+F3tmQvdVBKwp4Fw+xDpGRMSKBvaJwJdbkO6iMYr0FL7OCPo4sPT1QN3RS3ETvShH/izwDBv4zyj4EVQ0otPZaA8/DOCleB68AM8XtMwJ8VLcBk4pFtf3xePgyzKWlsVsfW8055dFROuZVQAysJaMAJ+EXoq7IV6KR8Aay47p9ojG9MtztNcV7AM7D8YdHGNgrobKaw9/L5xvgfmQmQADCEylLNTegXFsgfEasfGeVqn6jF0c3YOzZcsWcRxHGhsbffLGxkZpaWlR6RctWiRVVVUDn+bm5mIXiRBC9krynjPkZ1+APoMQQsJBn0GfQQghYaHPoM8ghJCw0GfQZxBCSFhKwWcU/aX4znLFFVdIV1fXwGfdunV7ukiEEPKB4Io35Ido6DMIIfsq9Bk7D30GIWRfhT5j56HPIITsq9Bn7Dz0GYSQfZVS8BlFPz69rq5OLMuS1tZWn7y1tVWamppU+lgsJjEQR3vLN2f4jhprWPrKsM9e84NZStbwsv52Qt3/gljhI3RsBGfqFJ0ueMzbi3/XSVZtVDJJ6jM9vJE6ZoMUwJE6GXDOVhhy+shcFPfac9GxVYF0KLY0Oto0p8vqgfikHoq/HTb2+K6yq0evg+NSDHSkIzqSN8wzwZGRBjzPHzwTHPOo8kJp0PHv6OhKcIQKbMdg3dGxwCiWt63raUTBUTEJHcvS2eIPgOH16RgiCC+jj8w1K3WsP7djqy5Hk/8bo15Kx6bZGQriChilA//bFyiWzygkI77zL+0/vqzSPL7xNd/18aOmhSrjv570VSVzQGw1u1rH03FefdN//S+HqTRWrb7PzYAjz8GRdA4YL2bAjxSqwVHSIeKEioiYeXSOHDpe239zMGa0SPgjbtFRsqhs+Qp/3QvgyEgPHa0Nz28MUTZ0DHjIuV7Y42CDR8niI8RDtiM6RgnEIQzK0NH6YY/HRbFrUR2C6ZyY7vNcOTguCh0/GeZoT9FHInZP1PYFHSHXsb8OatjwivY3hXJtEzoO8JctMUHHrU62+udNBTB2Bv5Hn1E0n9Hz5SN8x+BX/teLKs2V9/nj+P17/nSVJvHgn5XMTen48sjf/Oe655Ts682zlSwMaN7oZfQcpfLJlTpd4F4DHMsG1yIorAU4kg4dIx4mXjg8bjumfZ4TAzYL+MtC3J8uV6nvAycdSve4cN8hD/obGIkixBG62zLbtXQmWjKiE+1RlVB0JLD8C8bLjYCpsJ3SlUd5odAlBeQPArEsszXo2HUwHwIGE8ky/wKONQ3EdM5X6rLmRuhKRT+lOyH3jC7v1v312qai7HBduACxp/xxxw3PFNFLGxGhzxApns9oO1zE3ME0uiCuqVXt73t3i84nMxIMNHCkcqRWzzOcFh1H0uzzGx43Bo7RBsajy9F7U1s3gZiPFohfXeVXuGRcK6BtgTqh41SBDMUZ78sHjnC1dJzQLDhSvTcNwl1VAv9QALGwA/HIrW5wFHtel7WQ1HWyQBzzIJ6JjjEG4xS4z06UIVivRTv8N69vBsdt94FwYMB3wTUWCAthdweOawU+A83vkT/OjNP2tVCh9S8fOP41lwMx6Nfq8eSCI5yN3PDx5kV0GxUqdFs0/x8KewbyQpEOwRouSGyrTpOt9uueg8IM/BP6jOL5jK37R3xHphfAicfRSzf5rk+ve0+l+cPJRytZsgVNbnS/tn9Tr0ca/8NfkN5Rev2K5lgoVngOzIs8W8ucysCx32DMVsR1nWrLtJ2viWlZfVzHBk8EwgeWg7BpFcAYdRS0XUiCyXU36NDOvN+vpsBR6S+9MUnJ7EqdP4olHZyne8BXeg4K2Yfe9wC7kwXvJQI+zmjU7eht1vqfaAFHd7dp3ahcreue7PHLnKS23z3N4B3iKP3MVyfqefrE/fSJDyeN1O/1gvy+5RAlQ33wTmudkhWyug52wq+j6LTxPPLHlWD8d+l0mVpdtljgFUdGF1W9K3Jyg6/ZS8FnFP2X4tFoVKZPny7Lli0bkLmuK8uWLZNZs/RLa0II2VfJe96Qn30B+gxCCAkHfQZ9BiGEhIU+gz6DEELCQp9Bn0EIIWEpBZ9R9F+Ki4jMnz9fzjjjDDn88MNlxowZsnjxYkmlUnLWWWftjscRQsiHEkc8cQY5VmQweSlCn0EIIcNDn7EN+gxCCBke+oxt0GcQQsjw0Gdsgz6DEEKGpxR8xm55KX7KKadIW1ubXHPNNdLS0iLTpk2Txx57TBobG4e/mRBC9hHy3rbPYP/bV6DPIISQ4aHP2AZ9BiGEDA99xjboMwghZHjoM7ZBn0EIIcNTCj5jt7wUFxGZN2+ezJs3b3dlTwghH3oKniH5QYINF8IGIS4R6DMIIWRo6DO2Q59BCCFDQ5+xHfoMQggZGvqM7dBnEELI0JSCz9htL8XfL3W/+LNEjO3B4s0pk1QaZ8W7vusxT2RVGuupV5Qs+8npShZ/b4suxJ9fVyIzmfRfTxyv7+vLKJFXllAyY2ObvtcCQeyNgDLFYzoNIgLyAiBVVV/qQPEA3IKWofKDew00QEy/OnqOEyqv0ATvDbbroPe5QAbuRcU1TP8jozZIpOuE6g5Li9pI5QXKjzDBEyLaRBio3cK0Jai7ZwMT1JPS2QNdtpoa/AJX1zMyfqx+Zk+vlmW07TCrq5SssGGjvwwVE1WancERQxzcs4PKCSb+97USMaPbBfX1Ks3sb5/nu+47X7dx/ZIXlax3QrmSZapMJRvxpy4li4xs8gvWaV/j1mldc6N6bBQq9BhyLV0H0/HbFMPRNsbMhbMLngvsE7ALZg74gwBGHtgrUH4jDdKB4lrpvO/aSej2QW2WqQk39TGCdQd27v3M9VzkLgMyx9Z6ZgA3iMqB0iFZECcK6gnKauVAOXRxJZLWD3Vt/zPQM4NpRECfiIgb0+kKQGYG1Cq+VetZ91itG/EO/czO/eJKlq0FnRC8FSTpnOyf0zk5T+QZnU6EPqOYVNz3F9864+wVq1Sa/9jvAN91fJaeK4Tl8Y2vAWnZsPe1XHKkko1c8rKSGQfqdVJuhF57oJPMrKx/LFg9eh1jbtVzJwFzJ2QnjYQeL16IObkB5q/Ijxh9wEBFtDGK2P50Vi6q0uSqtA3I1ulyeBZY2zj+OiB7CG0wsung3jDY3bodo11aFtuqH2rllQiWzQ00Ua5S558apfvEBNMEA00L+kA5Ao8oANU2Qi538pValq3XBclP9DeI0arX31av7qiuN0YomXOIrlR+tR4XYvjnLOWbdKP1fHaa77qQz4g88N86L6HPKCaJFlOs2Pb+tjK67828X+9z1bqNrZBupHKNHkPlSZ1fpsZfjmwt2CcC4zjarWV1fw9XuEyd33flk2CdBOxCpk4XJF8ObCkay4Hs3CiwYSN0+Ssr0krmurrvXDCRdgLpvFpdrHTb8H5cRESQXQtUwcrrMtjAxrjA/7hgi6nQoI16zvU/I9ECfKVuMsnVaFlyky6HDe41C8Mb59qn1ylZcN9FRMSqrlYyZ+tWJWs/2x/72SrTbVv9rm6fjbP1HMDuBboBhlnQn8W26Ps2TwPtDXQDrT1ssJ7KBGwM8qk94/33uZnBF4P0GcVj1Jy1Einbrih9N49WaSIP+u3Ho4d/TKWJ5vX4sa9qUbL3nhmn7zX02NhwgV/v35q9VKV5qk/raQ5sBixtPVrJ/vLSFCUb8ZJ/XOXLtcHKazciGw/V6RLArvXk9Xyqtc8/2WtJVag0XSk9mRxRofeiswVtFxK2Lodt+Qfg2HLd/l84XK/hHlt9oJI563R5E2N7fNcxUIZsXrdZulcbrEhSG4t4g97c6dnqf/8lGbAv36jXjZkGsDZI6XacfPZ7SrZ/eavv+t10nUpTltN9Z4LJw9/XjVEy5O9f7/GnO6B8k0qzqUf3Sc8mLbMqdb9U1QC9yvvbI5fVfbff2M1K9s57TUoWSYO5wtQeJetd67c51ct1W5S1+HWjkB98n7cUfMZe+1KcEEJKnbxnSn6Qnc8Py3EjhBBCPhjoMwghhISFPoMQQkhY6DMIIYSEpRR8Bl+KE0LIHiInluQEO5Hch+SbVYQQQj4Y6DMIIYSEhT6DEEJIWOgzCCGEhKUUfAZfihNCyB7C8wx4fEv//wghhJB+6DMIIYSEhT6DEEJIWOgzCCGEhKUUfAZfihNCyB4i51liD3LcSO5D4kQIIYR8MNBnEEIICQt9BiGEkLDQZxBCCAlLKfiMvfaleGT0SImYsYHrfL0OYG9WHOy7tjszOqOpBypRbGO3knVP08HqK7p1YPogbosOfC+uq0RGwdHpLEvLCjqIvRfxd5ORzQ1bLhERMYFyIlmovEBZE1p9DFcHDkBDwbB0OTxv+KADhgFyQzJEsO4O6BOQlwf6E96Lyg/aAzxAixwkA88EMtWOoAwwr5BlE0P3nWEHdBTothG1dV6ZmE4X0feiPgj2VGHDRpUm0jxGyZz2DiWzamr0M1Mpnd/4sb5rd51+ptYhQ2QQNciLKXkBY0tE8vgWMghHP7pW4uXb9fDJUw5XaWId/lbtaQb6N+0gJSvEtV2IpsC4mjVVyTZd4fdL1T/VvizWqnXNjWu9SDXpMeQCLx7t9ZfN7gF2AphNK6/HmWtj/QySK4v774sBXwPMiZXTQjOry2uAMeTE/GXL1ujGQH3nWVqGfJcb8FOoDIiw878w+Zl6SgCdqgnLrxOi9nDL/DIPdLkB+i6f1DLPBO0NbLpKA+rkxLXMzIOx2AXmHaC8QVKNulzRHtCOwHVlq3U5KtZqve0Z638Gan87MF7NIQIw0WcUjwlPxSRaHh24vur+uSrNio23+64/svBIlabhBZ132/mzlGz6dVpW+2afkkUO8/sMKxdublmo0P5swzFRJUM+o/Jd/3V5i1b6yAg94K2U1jo3Fm5p6ST8euzaemygsWACP2UCP+La2gdl6vz1ylSDtQhaJiFjjVyc7S+H4QJ7CLIPTYib8zU6Ub5ap+udCOwmMsTomYFknolsMKg7SAfzD7NURXmBZ0a6tL10o6C8BX2vtc7vhAplYG0WAeVIg3XSZj0+kT/rGefPL1urx2Kuwp/GzZgiD+hiiNBnFJORz/dKJLJ9UuaBPYPesQnfdeUaPYnbMlX3afk6rVsVb2xRsnyDXkOMeKrNL7C1Dc43VSvZ2hO0TV91uhJJ4x90eWv+sDIgqNI3gj2n7GidzovodkTrHcPx6328U7eZ1ad1vVBWrWSdE3Ub9Y7X+ZWt99chktbjPQG25XrGa1nFWi3rq/PXvVCu889VA/+WQPuKwBYB32XWZf351wOfYYJnOrptM2OBbQF2LTjJNzNaN9oPGatksY5xShacM4vg+YMbmP4g97bxaK0H8c1AH8fq9hjxKtqTDJZLJ0HlQGsWJENriOQWf8J4m1bI2n/40xQKGVmjsxcR+oxicvW4/5Xyiu26/pVTzlVpfjP7N77rmzcep9K0Z8qUbMVbeo/zz9+4Ucn+5el5SmZs8M9tLp5whErTmU8o2Zfq/qpka7r1Hqrdo8d3utF/nWkEe1NJLZtSp/dtx5dr2ahYp5JtyZf7rjf2Vqo02bQepOmYln2kYYOSnVj7dyWrj/jfMy3rOVileX7LRCXrS+k5Ymw02B8MzHO3dpSrNNKry2/2gb2vjJZlLD0vMOOBOWcMzIVjYA0H5um1I7uUbH1vtZJ15vz615vT7ZPK6Xr2ZcG619FlW/1Oo5KtzYz0Xb/QcZjOf5Se0414RdvL2Bf1/K0HvPcI/pLajur8W3t0H9eP6lSyLbbWbzC9Uvt8HYfoftp6kH8MuxlT5Pc6L5HS8Bl77UtxQggpdRzPFGeQb1Y5Ib4kQgghZN+BPoMQQkhY6DMIIYSEhT6DEEJIWErBZ/ClOCGE7CEKYg36zSr0A1FCCCH7LvQZhBBCwkKfQQghJCz0GYQQQsJSCj5jF8/SJoQQ8n7Je5EhP4QQQkg/9BmEEELCQp9BCCEkLLvDZ9x6660yfvx4icfjMnPmTPnzn/88ZPrFixfL/vvvL4lEQpqbm+XSSy+VTAaESSWEELJHKYV1xt5bSsPwxeY1nntNJQn+GN8ar2PRuG3tSuaAWMFlcR3vAcUeNpP+OAsoNrNZoc/9d7t1HHOUzrBB8JlgzCoUixzE6A4Vz3qQ/ILhBzwHRARwQcw0VI4IUDMUYzQYAwvEovJQLPIISgfiUsdCxDUF8VBRnHEDxPwOxroSETHyAf0ooPuADvVllUzy4Ls2KKZ4sGxZnZeHYtej/MMGxw2UA2oeiLmF4vh6IPa4EQfBZQN1QHHB3Y6tShaMCy4i4rZs1jIw+bYCeqXit4uINaLWn8bNiWgzJCIijmeIM0gbDyYnmEcWHSsRe7ueGPvrNH0j/LaicrUeP/karWsmMH9Vb+uYOOmxOtZf/B5/TJx0vc4r1VitZNmacPFVEZlAbLVMNbDBQL1QDGoLPDOMaqJTdJB9NUIeq4PigAefgWLlovjYqO6hKoWShIwDB2UgJGBQhmwkjDOOChcuvKBuI6QbKC4e6BOEi/og2Hch45ijPs5VIr3S6YJdjGI147bVJDfrwuXLtNJHu/3PSDeimOJ+mZMb/Puqu8Nn3HrrrXLjjTdKS0uLTJ06VW655RaZMWPGoOkXL14st99+u6xdu1bq6urkS1/6kixatEjiyE/vxbx47zSxYtvLHAe6ddDtF/iuxz+pnXn3F2cqWdOfdLr2w0coWcdBOmZfvty/zsjWaj1977rpSuaO1/HJ3Y5wMZDb/dMW6SigWNtonGnD4IHBh2JVq3RIfcH8G649wuS/LWHgGhhhRIi42ih/D8V4RXmFHbsh3CWK5Q3LCosRLna6iueN2h/VPWQccJguzFQBlL9Qpydw9matt/lGbfzztj8/FHfcyIeLS4+aNrufXmd4gdiHB05cr9K8+ZI/HuVQy32uM4rHe19KirmDnzPAfCE4/uJtWtdSk/WC2InrOJhdk3TMS2Syapv8sWUTIKZw7ygdyzK5UecV7dTpuibpdJ0X+hdZTkIroRPVsuB8R0TEArFO8xX63vgW/8BKjdIDLdEG1uVZkFcHGjQgBnrAN6bGgHjWCbC3k9TKkT5Q90tl0m8DyqI6jQmMR29O60s2rycxBeAvI4F44SiNbWlFsy1dzz4Q0zUC7g0Dyj9YVhGRHIhtnsro9sik/TK3T7ePmdZ55arC6WjHcdp+O8GY9mg+j3xqWH8JYuNKYE5npHRbSJXfD7p9lsgg76WL7TPuvfdemT9/vixZskRmzpwpixcvluOPP16WL18uDQ0NKv0999wjl19+uSxdulSOPPJIWbFihZx55pliGIbcfPPNO/38Pcn/9R4scdk+RsxNep2UC/zC8uW/TFZpyiboPadofVrJZjw0X8lQnG4j5teZ/33ho/qZY3qU7NmV++n8M3oM2cD2mzm/7sS2gLFXrfVrxVrtB1fF9XrKimhbEY/69T6V1v4Nxd/emtGxmf/Uo2Ntv7hhnJJls/783Hb9zEivNgLJTmB3csOvqw2QBM030Rwa7e0gX65Ae3xo3WjrPskAn5Gwwfw7sDHkAtuTKwCflwfvgMA8HdnX4Po4W6PLb3fp/NMNumzt67SOGn2gjYJjBex9ZRJ6DPeAOqH1SLZLt3e8I7gJp5+ZrQ8IhzD9pbDO2HtfihNCSImz7RtU+Msa+Q+JEyGEEPLBUGyfsS9vVhFCSKnDdQYhhJCwFNtn3HzzzXLOOefIWWedJSIiS5YskUceeUSWLl0ql19+uUr//PPPy+zZs2Xu3LkiIjJ+/Hg57bTT5KWXXtrpZxNCCNm9lMI6g8enE0LIHiIvpuQ8C37yNM+EEEJ2oNg+Y8fNqoMOOkiWLFkiyWRSli5dCtPvuFk1fvx4Oe644+S0004b9ihEQgghHzxcZxBCCAlLGJ/R3d3t+2TBqZQiIrlcTl5++WWZM2fOgMw0TZkzZ4688MIL8J4jjzxSXn755YF1xXvvvSePPvqonHjiiUWuKSGEkPdLKawzPhylJISQEsQVc8gPIYQQ0k8Yn8HNKkIIISJcZxBCCAlPGJ/R3NwsVVVVA59FixbBvLZs2SKO40hjo/8I7MbGRmlpaYH3zJ07V37wgx/IUUcdJbZty6RJk+TYY4+VK6+8srgVJYQQ8r4phXUGj08nhJA9RN6zJDLocSPhYi0TQgjZNwjjM5qbm33yBQsWyLXXXqvSD7VZ9fbbb8NnzJ07V7Zs2SJHHXWUeJ4nhUJBzjvvPG5WEULIXgjXGYQQQsISxmesW7dOKiu3x12OxUCs5l3kqaeekoULF8ptt90mM2fOlHfeeUcuueQSuf766+Xqq68u2nMIIYS8f0phnbHXvhT3YlHxrOjAtbXfBJ0m7nfA3oZWlaZw+BQls9/eoGTG1h4lcysqlMysrfY/s7tX55VIKJll6yD3XrXO343qLnFjw3eTZ+lvYZgFVyd0tczMFHS6XN53aeRBGqTklh4QXjyqZG5Mt4eb9MsKCZ2XE9eyQlLXPZ/Q8QucwHzNjYAYByjsAUpW0HU38zqdlQte6/siWdAnWZS/TmdlHF02xwtc6/sMV+dv5HVesI/DYISLH+Gauu8M8ExUimB5nTKtZ1Yqp2QOGGNmQk/mTaTLEX95nQp9n9HV57/HyYq0q2QiIpLzImJ5eHwDVSFD0FdnihXd3j8mMFmO7dfLnmbdx/t/900le+O2Q5Wse/8qJUs1AjucD3QkGBqepYWuNpHi2iAdUB/DHfpaRMQAwx0NNPRMaCeD94a0pS6oO0wH6hmcf6GhhELZeCaoKPgioxeQwbA4qPgge9jeoF9Mx5+hUdAPQLqNQP0OjemupBHBbQbbO5wsTP4IR0+5YNsG+6VQpgub08NarD4tQ+PCTumGy1X509ndOq9Mvf/ayeg0A/mF8BncrApHrlrEim+/RnahfJ3/eu1JI1SahWffpWSX332mktkpXQbr43py0PePWt+1G9PlcuNAwXu10zDKgbFwwSAN2B3PQkYs3NwMGQEPOiF0c4g00MCGu9VDdQ+Rv2mD+XdEG3Ur0G5oCu2CMniuNnYwnQNkBf+9sI6oo5AfBP1uIBnqg+AjgR6YIfOCxTX9fRBymSGuo9vWGwfGRSbEWjsOHHkB5A/uRb48GtcLx9ymMt/1395tVmmC6gh9fX9+XGcUjcmHrJNI2XZ/2pjQe0epgn8t2pvX/revoG21OS5cZ7hgXE08ye9H2rNlKk0FUJKIqWUFYItyYAIeLEfe1eupsoheg5ugHCawAahsLalK33XM0uO4KqYnbKgPOvqSSoamkqMTad91ZVRP0Oqi2rkfUrZeyZKmPrUn4/n1pdrSeZWB+xwwiU67up6djq7n230jfdebM3o/Mgf6s8rWda+200qG+m5szK+j1Za+b4Sl91RtQ/dx3NB20waLrBGg3YJUmHo8tQM/+7fsaCVrKegFw+acX0d7g5uPItJTiCuZA8Z1zNR1ioEFYCSQbn26WqXJOH6bk0/lZJ1KtY0wPqOystK3zhiMuro6sSxLWlv9e/Stra3S1NQE77n66qvl61//unzzm98UEZFDDz1UUqmUfOtb35Lvf//7YoK9w72V/113iFjJHXQA+OqzHjzPLwBzm54WPUatHj1GUcsYvcO3FzAdUni1WsnQFCgC1sgWWscGigGGhrhRXfm8rXWxAOavpqUbty8b0Hswz7P69DMTm8EmnGiZE9VeIzitS7boslrBvUHBe/OF2PD7frY2m5KtAe9BQJXy5WB9GQVz8oDMyIO1COg7tE5ywJx8w+o6XbhdBS0R0boIzMmtTGDPpkengft5gPLlQF/QOKvwFzhfBdaWW/Q7jmg3GCtloO/Au+pgvyc3grbIB955ZAe3I6WwzthrX4oTQkip43oG3ODo/x8hhBDSTxifwc0qQgghIlxnEEIICU8xfUY0GpXp06fLsmXL5OSTT96Wh+vKsmXLZN68efCedDqt1hLWP3+o4n1IfnVICCH7CqWwzuBLcUII2UMUvIjkB/lmFTiMgBBCyD5MMX0GN6sIIaS04TqDEEJIWIrtM+bPny9nnHGGHH744TJjxgxZvHixpFIpOeuss0RE5PTTT5fRo0cPxCU/6aST5Oabb5aPfOQjAydSXX311XLSSScNrDcIIYTsHZTCOoMvxQkhZA/hiCHOIGeIDiYnhBCyb1Jsn8HNKkIIKV24ziCEEBKWYvuMU045Rdra2uSaa66RlpYWmTZtmjz22GPS2NgoIiJr1671fdn2qquuEsMw5KqrrpINGzZIfX29nHTSSfLDH/5w1ypECCFkt1EK6wy+FCeEkD1E3jPFQsE+RCTvDREkkBBCyD5HsX0GN6sIIaR04TqDEEJIWHaHz5g3b96gJ1A99dRTvutIJCILFiyQBQsW7NKzCCGEfHCUwjpjr30pvvXwerGi8YHrWJdu0EyNv/GNqdUqTaJdB6t3Dm1WMiuv8/fG1ymZ4fjPAHBiDSpNoUwrRT6p4yy6kXDfnHCiwUKAROBoAgPooJXXCa2slkUyflmw3iIinhmu/AVQd8cG6RLGkNciIvlylL8umwNkbtwv82zQQKhKDhCi9s7pelp9/nutnM7LzOlhaGi1FRPIBFRBpUNlRXkBkA6h/HQh0DNDnp8Bkpl5IAtTB5CXB8qG8kIhMCKBsZIrRwoT9105uYzIG7h4eTcilovNcN79kJw3speQqTHEim3vj3yFbr94u/9a2VYR+fPvDlMyc4ROV4jrvkfzgUg6kA6ojBPTMqSn6F6oPkag7u8jnguyFVAWtBUhH4nazLWBvwHpgjJUTTcK8kIyCxnKXRyDoCBGIZwfEdefDtpg1LbInUFZiI4JOX8Nq1W7aslQ8ztxZNTRzUAU6ANo90FWXp2Wov7sawTPDPg9NIcJjmF3CL3bHT5jX92sytU6Yia2K0GkVxvdrv381yY4B+y7vz1DyVzgf/pGa4Uz1lRr2ZiMP69ePWE2y/WkyI4WlCwP5pdBGyMiajVomGCtYIUzDK6r2xGtiY1AMgPovWHqGw1QfBOUFxF8BqpTBDwzAtKh8gaxQF6IXEE7OA/4EQe1bTCNA9KAvJAMtQfsl130jSGXr7sMyj8R1WOlL6fHVKK6V8l6+vyTs2wWjEXQFoU8mrBoUSSi27tQ7x//TkaP4Xytf6y7fXrsD6TdDT7j1ltvlRtvvFFaWlpk6tSpcsstt8iMGTMGTb948WK5/fbbZe3atVJXVydf+tKXZNGiRRKPxwe9Z2/k38Y/KOUV28dXm5NQaezABLnC0PpnAZ2JA1mdqRcprwKb3u74JxaT7HaVJg8WFTaYJKJ06Jc+6wq1vusmq0ulqTBzuqyubrOoaN+YBAv/MsOv52iZZAEj8FpW79W1FSqVbH2uVskqLP94NEGbuaDNOgp6srfJq9aFC5CO6D7PgwVQGiwck1Z22PxFRCYnWoe83hkOjm1QsjjQ+W7XP9bjoH/rzbSWgbVZ3NDtUW4iWwI2GwM8ltbteFBUj5/guBYRmRZfq2ROYG+g0ynTacB4qjQzSpY0dX+isRIcZ/E6nSYVONq2t8eVP6pU2+DeVPFw/2+EGDu8z6gEa4hsrd9+ZKt1GjOrdR5NwwwwFXDBHpPdHdDBXq2T8fZw621Lm3m4ZxMcQvkyMC8FameAfXg3pRPmOrXtDLaRBdrHzIO5NtgfRGWLb9ayZNvwG9RODNQd7VuEaO9cBWgfUFa0f+LGwPy+QdudsrLAvBSsM9Jd2rfntmq7bIB3IZG0zs8cfFo7JGjvC/UxGivBZ0b6UF5gDNfo/LNg7wj2caC88U2689B4svWSRUzQtujeoPtFbR3vCLzzzH2we1MfNHvtS3FCCCl1PDHEHeTVkvchOW6EEELIBwN9BiGEkLAU22fce++9Mn/+fFmyZInMnDlTFi9eLMcff7wsX75cGhr0y8d77rlHLr/8clm6dKkceeSRsmLFCjnzzDPFMAy5+eabd/r5hBBCdh9cZxBCCAlLKfgMvhQnhJA9RN61xHQHOW7E/XAcN0IIIeSDgT6DEEJIWML4jO7ubp88FotJLAZ+XiYiN998s5xzzjly1llniYjIkiVL5JFHHpGlS5fK5ZdfrtI///zzMnv2bJk7d66IiIwfP15OO+00eemll3a5ToQQQnYPXGcQQggJSyn4DHTyECGEkA+AvGcN+dkVbr31Vhk/frzE43GZOXOm/PnPfx4y/eLFi2X//feXRCIhzc3Ncumll0omo4/xIoQQsmfZHT6DEEJIaRLGZzQ3N0tVVdXAZ9GiRTCvXC4nL7/8ssyZM2dAZpqmzJkzR1544QV4z5FHHikvv/zywFrkvffek0cffVROPPHEIteUEELI+4XrDEIIIWEpBZ/BX4oTQsgewhVT3EG+mzSYfCh4rCEhhJQuxfYZhBBCSpcwPmPdunVSWbk9xvJgvxLfsmWLOI4jjY2NPnljY6O8/fbb8J65c+fKli1b5KijjhLP86RQKMh5550nV1555a5UhxBCyG6E6wxCCCFhKQWfsde+FN88yxUzsf3n9mZZXqWx1vu/eeCANZyV1VUMBpcXETHccOfdG47/upDUweM90PeuDYLMG+BekM6LBGTgPnhcv6OFRl7LzKwusFkI3FdQSWCbeSYoP/iCiBvV6dy4/3gFL+6oNFZSFySZzCpZ3NbpopY/v4gZ7jgH19P1zBS0XvXlbCXLBvSv4OjGcEE/CehiM6gHImIAXTCC9QLlhzoE8FytG+hOL1AHD+lGHgwMVDb0AKTLBWPIaxERzwKZgWd6UaALaEwFs0P3BcaA25cTuRvkJSJ51xQTtHH//3aWfflYw8yogpiJ7ePerNCGvrfaP0YjnXo8ZmPh7Cuyicj2Z+oD92mzBu0+BA0XYBcUyNShOiFbhG4FY02NDTT00JcFkR9EdUJFC4xvA4xHK6obXNlIETGB77KsgE8CtsMFtq6Q1/7BBfYP2clguyETidrMQFmBOiGfEaxX0J4Pmn9I3yWonkER0lFwX8XIHiXLZLTvdfLA1wbyQ77dCzkUcccAgvVCtwV0w+0DRqI/aZF9xr5MpK5PzB3n8HW6c47fz/+i54k1k0PlnU9HlcyOaqcRj2s/VSj4dddI5FQaZIuQLJHU9yIbEJwzu2AcRCNaL7PA1jkh9TCYX9hnmqD8FpIBOx81/fmhtkD5B+8TERg/DdkUnZfWg4yjbVgeHEWXR2uIsLaoSPeJ6DZyQF6oT1A6BCobyi9IT0ZvBMxufE/JXts6RsnSed0HzTWdvuusE27bJJUD49/SOoTGSjBdLoZ8mf8+Jzb46U5hfEZlZaXvpXgxeeqpp2ThwoVy2223ycyZM+Wdd96RSy65RK6//nq5+uqrd8szdxdpNzJoW/ZjBSZBG50KlabC1P01MaJtddbTtmKyrX3GZLvPd90O5mbIrrU5ZUqW8rTuZlwtqzD7lCyIbWgbXG+mlSwLFgdxsFhKBqrVg+bkYBI32d6iZGMjW5VsWnytkv2lb4LvemVfo0rzVneTLivoz/aMbu9NW/3jDq0fTODL4mBekIxq3UB7XbGIX69iltazyqjW0aZ4t5JtLeg61UX0PL020uu7Hmt2qDR5sHmeB2PAFa0bnYVeJcsHVGFNQdu485/5mpJNGduqZBWgPSaVa72aXrbKXy4wxqqtlJK1FKqULG7o/qy29PhJBja2U57WoaaAX+mxBt8D5TqjeBieJ8YONilbq21Wtjqwb4j2ocK9RhDRplqszPDzrpgejuKBhb+TBOmASthpXbhcZWCvAewJob01E5UfPNRD7xYSgTl/GjwUradAnWLtoGyOfmZqpP8Z+XJ9X2IzWLPo1xnSB9alhUAfoHaE+9/o/VQ58LPx4decGbDGFfCOyerRhbNT6F2Uzk69iwImC+oQ2EJx9fQebuME3Rl6t5jXUzrxgD2NbgUNDkS5qsBeIxpPveF01AJLAbDkFLPg1w+ke06giwffmSoNn7HXvhQnhJBSp+BZYg5yrEjB27lYf/3HGl5xxRUDsjDHGv7617+WP//5zzJjxoyBYw2//vWv72qVCCGE7CbC+AxCCCFEpLg+o66uTizLktZW/0uj1tZWaWrSLwZFRK6++mr5+te/Lt/85jdFROTQQw+VVCol3/rWt+T73/++mOaHY8OMEEL2BbjOIIQQEpZS8BlciRBCyB7C9bb9IgZ/tqUJG+tvqGMNW1pa4D1z586VH/zgB3LUUUeJbdsyadIkOfbYY3msISGE7IWE8RmEEEKISHF9RjQalenTp8uyZcu25++6smzZMpk1axa8J51OqxfflrVt88wLfTQLIYSQDwKuMwghhISlFHwGfylOCCF7iKG/WbVNHjbW365QSscaEkJIqRPGZxBCCCEixfcZ8+fPlzPOOEMOP/xwmTFjhixevFhSqdRA2KbTTz9dRo8ePfAF3pNOOkluvvlm+chHPjKwzrj66qvlpJNOGng5TgghZO+A6wxCCCFhKQWfwZfihBCyh8i7lhggbmT//0TCx/rjsYaEEFLahPEZhBBCiEjxfcYpp5wibW1tcs0110hLS4tMmzZNHnvssYFTqtauXetbO1x11VViGIZcddVVsmHDBqmvr5eTTjpJfvjDH+5ahQghhOw2uM4ghBASllLwGXvtS/FpB64Wu2x7hHfT0L+9b2sq9107IJB7pqCrWHB0Os/TAewRRqAcESvcOfnxiI5yn7RzSlYZzShZ1PSHtjeNcM/MOLaS9eb1r0x7c1rWl/ffmytohUZ9Ypq6bCZo2hhoj4Sd912XRXT7jIillKzC1m3mgv4MflOlD7QPkqG8co7WqxwY9NlAOpQXakc70OciIlFLy8ojWSULtlsZSJM0dduiZ2Zc3R5ZV9c92G6pAoh5HdIoIh1F7RZsWzSGC8AmQL0Fsngkr2SRgH5HQozFfCon6wf5nysGrFv//3aGHY81PPnkk7fl8c9jDefNmwfvKaVjDWMj+sRKbi8zskVG0q/3uWpg14DOuK7uC9Q8BvgOQfBOVK73g2HqgkQi/rFsAT+FtAv5UFR3KAv4VaTX6JmwPcC9EVvbJztQz9qytEoD6wTyt4F9rYj6bWd7X1KlSWejSpYD7W0mdT/htvWXF+mZCfocySzQtljmvzcK/DMiB+ZX6JikgqPHWXAehux3cL4lIjKtcYOSIZ+RLuh+6cwkAmVF4xr4GjD/CfbTtnuVSAqBe1Gd1Nixtc8e+F8Rfca+zsSGdomUbded4JxCRKQlU+G7PqChVaWpAvP2npA6iZ4Zs4Yff2iRWRPT9g/NgYLzGAS6z/WADOgcnH+HmIsF1zqD5R8xdDpUJzQ/a8+U+a4PqNL9aUk4H42eGZz7ovKjvkNrDzRnHmzsF4tCyHl6NKCjqFyoP1Gd0Pw7rP4FaUj2KFnM1ONpYkW7krVntX9PBtYBqJ6oj3OxXd8EygXWWHFLr0WC6/t8KifvDpLf7vAZ8+bNG3Rd8dRTT/muI5GILFiwQBYsWLBLz9qb+N+eqRLztrd9TUTvSVRbfjtcYWr/kHK1f1iZSyhZa75KyWxg/9Zma33Xk+KbVZouR+t3XUSPlw6nTMm25CuULG769TJm6HFWYem62yBdMC8R3G4Zzw6k6VNp0qBtHRAtcmO+WsnWZWqV7Im1U3zXaJ4Xj+ryo7lkzNZ1L6zx72WC7pVID7A7Bd1P3WBXt1AB7GvCL/MqdfnNCFgrgHVYRZnuJ7TvF5zzB9dvIiJN5Vof0b5fmaX3sJpiXUoWC+jVhmyNSmPFdDmqYlqv/vL2BCVbUVevZGvq/Tpkim5/NBdEjEl2Khmy2U5A1xLAZwTtRrY3LyIPwudynVE8stWGWLHtbZatAWvAhH+seXGtk0YE3JfR8wyjANa1YOrkxP35OQl9n5UBe9Z66ImnTZEUkkBP44HrmK4T+lGpU65tkZcAhhIRbA8wvQdTUHFB2bIjwHwe1N1w/PcmW8GYAc/M1Ol0PVO0zyh/12/o+xrQhqSWOUld+ViVtt/Id+XzgX2KAlIqoC9gOWuCLQ6ko/mA7zJzIH9t6sQF5jWoe9tkwDdW+fUqUqkVvtCtH2CgsgEVBUsbiXYFhGj7HagQegWBZAU9tVROGrV/MC8nO7jt3x0+49Zbb5Ubb7xRWlpaZOrUqXLLLbfIjBkzBk3f2dkp3//+9+V3v/uddHR0yLhx42Tx4sVy4oknhnreXvtSnBBCSh3HM6UwyEafE2IDMAiPNSSEkNKl2D6DEEJI6UKfQQghJCz0GYQQQsJSbJ9x7733yvz582XJkiUyc+ZMWbx4sRx//PGyfPlyaWhoUOlzuZx86lOfkoaGBrn//vtl9OjRsmbNGqmurg79TL4UJ4SQPURhiONGwv5iZ0d4rCEhhJQuxfYZhBBCShf6DEIIIWGhzyCEEBKWYvuMm2++Wc4555yBH/UtWbJEHnnkEVm6dKlcfvnlKv3SpUulo6NDnn/+ebHtbScKjR8/fqeeyZfihBCyh3DFGPRYER5rSAghZEd2h88ghBBSmtBnEEIICQt9BiGEkLCE8Rnd3d0+eSwWk1gMhNvN5eTll1+WK664YkBmmqbMmTNHXnjhBfiMhx9+WGbNmiUXXnihPPTQQ1JfXy9z586Vyy67LPTJtzwDhRBC9hAF1xzyQwghhPRDn0EIISQs9BmEEELCQp9BCCEkLGF8RnNzs1RVVQ18+kO5BtmyZYs4jjNwym0/jY2N0tLSAu9577335P777xfHceTRRx+Vq6++Wm666Sa54YYbQtdhr/2l+CdGvC2J8u3FKzOzKk1boWLYfNKO/gZC2o0qWd7T3yJAAeOD6WJmQaWpsDJKljRzQKbrhO61xB/p3jacYcslItLtJpSs14krWQ+Q9QbaDeWPCJZVRCRiahki2EaoLWJmPtQzM56tZF2FpO96a+BaRCQPjngogO+ORC3d7+W27k/TCPadLmvC0nUqi4TTDaRX5YF0I6xenT/QPaSPGVe3Y150G6Vcv770OFr3kA45Ib+Xg/Q2mF/aCTeu0ZhFY70q0qdkwf6MG2j8++/r6y3IYyrVNgquKcYgCwwuPHaOxqoeiZRtHw95R/e9aXi+a6tCj0cP6ALSDwfIMvnhXSrKHxGztW4lbW0rbFP7g6DNRbqUKeixjdJ5SoKxAm1rW7pcwfYXwW2biOh6JiPa1o1KdA1bri3ZciXrzGn7lHV03/UF2giV1QB1ikeBnzJ1OtQeeccMpFFJxAI+FfnZWETrEGrbaMAmuiD+T9D2DZYuB3wo8qvBtkR6PDLZrWRTK9Yp2eZcpZJ1F3Qfr7eqfdedWeCnwBhA7Yj0GxEc70iHgrJCNCurBsmPPqN4HFDZKrHy7WM8BdYLQbswMq5tDpqTZ2PhlldZV6fLBuZdaOwhkD1BZUME06G5E5L1OciPhFsvoLlvGNC30RNoLgzm0YdX+ednqG3DrrGCawoRkdp4KpAG+BrQ55Vgvomeifo42N4RYEtz4JkRUM+wtr/c8rctmlejdkR9h+buqI2C61JUJzQvm162Wsk647rvtoA9heB+wda8vg+B2hH1C2yjEHPEYHtnrbz8cZC09BnF43fvTBUrqdejO1LI+8ejuxmkBybdTWqhVantWkWZ3gsoi/nTPZGfrNJk88BWF8L1vwFUMjj3jUS0LseAzAWLCtvSda+Iafu9ucc/n+/p1vbVA3UybVCONPDRti6c0RfY92vVdrmzWpffcHSjeSB/t9o/lq0usA9Vposa7db5W7rJREL8sjcfA3N0kM7N6XRb23QfWH1ArwJVL1ToPtlSpm2waeuSmGC94xR02YJ66xZ0W1TXpJTstWenKJkF+q6wqkbJXq6s9qepBHOwKGhdULa/OeOUzKwA68uIPz+0z2AE2sxNZ0TkQV0Ooc8oJtk6V8z49raPNes92f3qt/iuR8S0Tr64brySZTw9d/LAHi0azF5gc8EAt3kJrfMeevUCbHokBXQwMBQckL9TreeSZkyPoWhMp6su1/Po/ar9bfvntWNVmsJmbcMKI8B8rUz742hEN25fuz+/XJ/2vWZWtw/YyhAjoeuZ/ajf0BtgvFvA95oO2LMBZTPAflXQ9lsd+r5IGvg8kFe+CshqdT3jG/3PSGwG5bLBuzqgo24M2G9gm2tHd/quPz/u7yrNK53NSvbqO9pW56rAflI7aKPAVAQsiaRQBtpMb28K2i6wwVwhCHjdo8anmxl8RziMz1i3bp1UVm5XcvQr8V3FdV1paGiQn//852JZlkyfPl02bNggN954Y+jTcPfal+KEEFLqeDL4UVRhX0YSQgjZN6DPIIQQEhb6DEIIIWGhzyCEEBKWMD6jsrLS91J8MOrq6sSyLGltbfXJW1tbpampCd4zcuRIsW3bd1T6gQceKC0tLZLL5SQaBd80CMCvexFCyB6CR1QRQggJC30GIYSQsNBnEEIICQt9BiGEkLAU02dEo1GZPn26LFu2bEDmuq4sW7ZMZs2aBe+ZPXu2vPPOO+K6238qv2LFChk5cmSoF+Iiu/BS/Omnn5aTTjpJRo0aJYZhyIMPPuj7v+d5cs0118jIkSMlkUjInDlzZOXKlTv7GEIIKXn2hYUHfQYhhBQH+gz6DEIICQt9Bn0GIYSEhT6DPoMQQsJSbJ8xf/58ueOOO+Tuu++Wt956S84//3xJpVJy1llniYjI6aefLldcccVA+vPPP186OjrkkksukRUrVsgjjzwiCxculAsvvDD0M3f6+PRUKiVTp06Vb3zjG/KFL3xB/f9HP/qR/Md//IfcfffdMmHCBLn66qvl+OOPl3/84x8Sjw8dh2lHDolukLLY9kZEsXsz9vDFz4HYGmlXn2FvgoAbKHayE4gTZoHD+8sMHXPCBuVHdUIxp/OBZ6I4cyiGdq2n45a4NognETK/IFEQCw21I8IF38cI07aI4H0iIj2u1jUrcOgPyh/FJw/GdxQJF2MPyVBc8GAMahEc3ztYfhGsV5Wm/xkVJohFDu6LgjrlTN226WDwCxHJmH5ZBsQxR7HzogJiH4Jxh3S+xw3EbIExMHVZUV6ovVHc9eDYRnoQrFMKxMPqx/OMQWNMh409vbfzQfmMymhW7Oj2MVJu6/4bm+jwXY+Pb1FpZifeVbJn0vsp2RupMUrWktGBbFDcyyBxEFu1ytbjFukbyj8YwxnFeUaxSVFcbZSuPq7H9+hEp+8atW2vo/szbEzXaiutZEHbtjGv47shm46emYGxcQPxvYENzoF40yi+N/I3KB1q7zCg+LkxC5QNlCMb0A8UE7gA/GzG2bX44SIiFYHxiWKoTQPxw6cm1ijZe1aDkvU4OkbYiKhfb1uy+hinVCFcjCM0plDbpgv+b6ii+5yAnhVsPYfshz6jeD6jOpKWeGT7uEdjrzoQLy4Y11hE5OiKFUr2vx1TlWxWlfYtaD4SnHMie5gBcxu0poBzyRBz6wyY93Y5Op4yKj+KEY3aNm76/d6MpG6ftKfbOwXWcCPAnLM50qnvDbTb77t1P+0f36RkaE0UN7TfDq4v62xdrmCc6sFA/Y7WO0FQ/6L77JBxwJEsWHcHrOnQ2skJEd92W9nA+jKgQykwFpH/aXd04L0DYhuVbERE91V7wX/vSLtTpUHrTbhnAcYsWtcFQfOVdKDuETCH7Ic+o3g+I/pihVix7emtLIiJGvW3aaoZBXQFMUBTegwl3tE2t2BoWVewGKBbgfqJCWQOiL3poZixgfihLqhmH4pFjsIpA/XN6CEkQRcUAbFJ7Z5wOp1pAPNvEOc1GIs0V6Pvq3xX9x3SDQ8EZ3ftgJ0ExQ8bOxQtH1CcccP1P8Tq04pggLUl2MIStMTN1YBxkfQXGMUdN7u1TbdAnFoT6EsMmcDAragdO8FYjAI9sLvAmAVT9co1/roXQLz2XJVutL4G0HnI3feCtUdQ54FuBF2qkx18b4I+o3g+w6zPiLmD3Zo5Rq9hL2n8o+/679nRKs1zqyYqmdEN9uqhEdAiLxmYwxnIBqD4xzr/WBvYYwLjMVcdiFEMYopbSW34K8q04Tm0Qc/hvtHwrJIdm/APhv1WnaXSeAk9YGIVYE8lqssWs7Wsz/M7qnylzj++BeypjARz8qieC1dV+N8bGKDPy6LaOOXAns3GjbVK5gH7Z2T85Y2CONW5al1PNwr8INKhFq1/NW/580PLnxyIH17QUyTJV+t2rGjqUbJp9X69+nj5P1Sa06r+qmQnPvNd8EzwTq8erLHa/OPYAu1vDr4t5M+rF80xdDonHugDYDaCc0EXTQT7by+yzzjllFOkra1NrrnmGmlpaZFp06bJY489Jo2NjSIisnbtWjF3eEfV3Nwsjz/+uFx66aVy2GGHyejRo+WSSy6Ryy67LPQzd/ql+AknnCAnnHAC/J/nebJ48WK56qqr5HOf+5yIiPzqV7+SxsZGefDBB+XUU0/d2ccRQkjJUvBM7OUFv4T6MEKfQQghxYE+gz6DEELCQp9Bn0EIIWGhz6DPIISQsOwOnzFv3jyZN28e/N9TTz2lZLNmzZIXX3xxl54lUuSY4qtWrZKWlhaZM2fOgKyqqkpmzpwpL7zwArwnm81Kd3e370MIIfsCjmsO+Sl16DMIISQ89Bn0GYQQEhb6DPoMQggJC30GfQYhhISlFHxGUUvZ0tIiIjLw0/Z+GhsbB/4XZNGiRVJVVTXwaW5uLmaRCCFkr6X/uJHBPqUOfQYhhISHPoM+gxBCwkKfQZ9BCCFhoc+gzyCEkLCUgs/Y46/ur7jiCunq6hr4rFunY0gSQkgp4nqGOC7+oDi8hD6DELLvQp+x89BnEEL2Vegzdh76DELIvgp9xs5Dn0EI2VcpBZ+x0zHFh6KpqUlERFpbW2XkyJED8tbWVpk2bRq8JxaLSSwWU/JyMyfl5tDv7KvMPt+1DdrcAvdlvF4ly4u+OQ/OwDcDkeijhgvSgHKE1AcHxLBPB2phiU5kG45+JkiHMA2dLnhvsN4iIlaI+0REHNC2GU/3TLC9M55WzzyQ5UBecTOnZEkz67uuslIqzUg73JCwjYKSlYFnVgR0tCxQhm157XrfIVTfoX4CsrDEQXmTgfaIGmmVBo0LPXowOTAWs6b/GUjPkAyB6mSD0pWZw5c4FTgmxBriHmeIGBxOicRtGopi+oy6WK9E49GB6wPLNqk0B8U3+K4/ntC+ICI671RitZJlPFvJJibalKzcyviuu5yESpN1dV6o//PA1mVdbbO6C3HftZuPqzTILiBZWUTbtcnlm5VsVtk7vuvDYx0qTZcbzu6g8R4HvrbDjfqu1+TqVJqElVey+qju975AXiLa7/VFdT8hIqa2J+WWtv2ovWOm35Yi/+AiWwfarM/R5UW6FiRd0G3Rm9fjIlPQeRXAMUkRYAOjlr+edTHdJ7URLXNBPSdH9Tf3251yJauw/P64DuTfmq9UMkRwjImIFMD4DOpCZy6p0iQDYyyf12OuH/qM4vmMPjcq7g7jIW5qW9Fb8N93YtXfVJoRYC75+REv4woEQHPa4LwxD1cymrDz7zDzaGR3UPt0OlqfYyBdHMyZg+P7mISeN3Y4W5WsByyw600tq7F02Vbl/c+cVbZSpUl7WlfQZDUZ0Ta9yejyXaP2qdVZwbGL1hlo7u4E2iMK+s4E/jPsOgPNO9S6C+gUemYmhJ8VCWfLoqZunwio+6ObD1Wykyfqfl8JOrm94Pcj9ZEelaZWwvkp1B5pF+hagB4wZ7QDdXfAnGPgf/QZIlIcn1G5tiARe3vbe8DudI/zjw2nHOwTZcA8qUfnFe/QYwMMR8nW+O9NtoAxpYeeoCVytFvfG+0F+hViAW9ldSKkcoVyXal0nZb1jvOXze4G7ZhRIvjMWLsWOgmwLkr5G8ks6EZLbAFz3B7dZk5MPzNX5peBJR3Us0ydLmshGU5fol2BfYo+nQYsWQYpG8h/K9Dld/0J7ZQua7wD+DygQwZYSzpR0LaV/sqn63VjFMrBOmmknsMk1uh0mRG6HJkR/rpXaVcjFet1neIdYK5WpmUF7Q7ECiwZ7B4whgPtXchzb2owiukzJjRukUjZdvlHK9aqNFWBOfOC509WaaKbtP5FQFfkm7TuTmzWe1Pr26t918ZaXXY3CvaPge8C0y7J1Gv9cqv84zuS0OO9olwbo0Pq9X7eUdXvKNl+tj6yvjdgtJyMtgFmXJcju0UPtOSYLiVr36r3GmKbA3MAvV0gnVN1P0XKwdopofcDIpa/wdHx1DkHvGcBMgFrishWrWtWxm+LnDjwlTltr5DMygL/oLcVpS9gS9F8JT1SlyPXCPSqXs/TR1Vqfamw/ROId3MNKs3rYN7uoqbNgwIXtBMN+mjUtrF24Av08lL5AhERsPQQJ1CFQhkY6yP9Y9FIg8lVf34l4DOKWsoJEyZIU1OTLFu2bEDW3d0tL730ksyaNauYjyKEkA89rmcM+Sl16DMIISQ89Bn0GYQQEhb6DPoMQggJC30GfQYhhISlFHzGTv9SvLe3V955Z/u3c1atWiWvvfaa1NbWytixY+Xb3/623HDDDTJ58mSZMGGCXH311TJq1Cg5+eSTi1luQgj50OO6hhgudhbuIPIPG/QZhBBSHOgz6DMIISQs9Bn0GYQQEhb6DPoMQggJSyn4jJ1+Kf7Xv/5VPv7xjw9cz58/X0REzjjjDLnrrrvke9/7nqRSKfnWt74lnZ2dctRRR8ljjz0m8Tg4O4IQQvZhHNcUAcfeDPyvBKDPIISQ4kCfQZ9BCCFhoc+gzyCEkLDQZ9BnEEJIWErBZ+z0S/Fjjz1WPG/wOGSGYcgPfvAD+cEPfvC+CkYIIaWO5237DPa/UoA+gxBCigN9Bn0GIYSEhT6DPoMQQsJCn0GfQQghYSkFn7HTL8U/KOosVypA0PodSRr+4uc9EEke3Wfqn/GnXUfJbFPnpyU6L/R9iHxYjQAnDMTFX7a4ocuKsN/HaQVhvtOBugeVzBVUd9Degda1wH2OUQA56YpmPFvJRpgpUA4/pqF7OAp63TR02YLlFxGJBvJD7Yq0FtXJATEZ8iDHYOwG1I4IVDYL1BO1Rxhd0z2CyUOpfmYc9FUQC5Qr7PeV4oa+2Q30iw36KWg3LGBHBvJzDTEG+QbVh+W4kb2FQyvWS6J8u084ILZRpWmyegMS7QLb3T4lixva2tVHepTMgmPDb+tQbJWwYzTv6XK4XkLJElY+kEbrGLJhiNqotptjo+1K1mR1+5+J8jJ1OZKmtgw9bk7J0qC4GwrVvutgW4uINNrdSuYAg4XbaHifhLCBn0LtHUX+LFAOC9i5lBtTsrQbBeVA6cC8puBvj6ip2zEC7JhthZuLxCPaqtdE077rSfHN+j4De4MgyN+je5ttv972WHrsoL7rcpIgna57e75MyYJ6VQD2fkQs67vORbT+D+RHn1E0YmZe4js0JbKvlZHMsPmg+Vre076lKdKpZMimJE2/7qZdrd9I53Og/Gi9gGbCwfGCbAyiVvlUTJmpdbrC9PvaN3K6LaZF9dirA21mosUToC1Qr7ip7UQczULBggfpS7A/q820SoNAfRcFfYd0LWiLomB9lQMVyAC9csEMGT1T6Qdw+A7IKw7sq2ugeYHWl+DcCflUdN/mSIWStTi6PZBND84j25xKlaYM+BrkR3pc7W/EzCpRp6P9SJC8G9j/cAefm9BnFA/DEdlRXbMVYLwEhka0fZjNrH8S6dN90T1R92u8HYzHZr/uJlp1uRLtepAmN+n1jrUV2Cy0h2UFngHm925C2xgHyHLV2l9ma3Q9vSa/P546a41K89Lf9lOy+Gadv2vrOhUSWla23l+vmhV6bFsZbTvcqG4Pw9H5G4G2LST1fQXtBsUBP1h14zp/zwL1zPvbFrgCyQGTAcyaxLbqfoptBe242X+z3aXtppUBDwC650bDbV9H+szANdBjYAONPuAvG8BeYA5tMvmf0b2fThPfHG4nqnwjWIuBOpj54ffDgu7SKoC2/if0GcVjZKJbosnt63M0n3oi7bdZRo/WbzBFFAeMdwG625PV83lnvd+olG3VWfWO0/nna7ROGgmtSwawO/GYP119pV4/1CW0rD6qZWj/ZA0wlL9J7e8vVxrM21u1AYyCsd2Vq1EyZAMMJ7BXPFHvQzWUaz+7qUXnHynXY7sm7vfbazr0fZm03hNye3Q9rZRuDzQXCTa3kQH+GeioCcyMDZaN6N58YOqeqwK+bIT2I9EKvQ5IRnU6tK+VKvjHyt9SY1WaDZlqJXOSYI+sG80BwPudwDhGIbjB0kaqNupn5sv0zT0T9L35Ef6OiVbptcio2i7fdSGVlVU6KxEpDZ+x174UJ4SQUsf1DDGQ9xP88pQQQsi+C30GIYSQsNBnEEIICQt9BiGEkLCUgs/gS3FCCNlTeP/8DPY/QgghpB/6DEIIIWGhzyCEEBIW+gxCCCFhKQGfwZfihBCyh/BcY9BjRbwPyXEjhBBCPhjoMwghhISFPoMQQkhY6DMIIYSEpRR8Bl+KE0LIHsJzTfEGicExmJwQQsi+CX0GIYSQsNBnEEIICQt9BiGEkLCUgs/Ya1+Km//89KNDyYv0uP4g8Zahv4lgi5ZlPZ0bujfv6d/7B9Oh/PMhzwlAz3TAM+0ifsECqSVq26AM3ZcPmb8DmsMCbeQE2jJpFnSakHEJKgxdOssYvl9Qud5P+wfbA7Z1yGMlXKBrxQSVDbVHEhQD6UIQB8jySDdA/qgPwphYC8nAuAs7Lopt1j1v22ew/5HwTI+vlrLE9h6KG1rj2t2E77oto3s05yWVLG5qDa+2UkrW4ySULIgNylVlpZXMAdoWNbRNRPem3ZjvOmPbw5ZLRCTv6RGD6m4ZenRscKp81ylPlwsRB3WyDV2OjYUqJQvWsz7SHeqZqG0RNijbruJ6SNeAhTKCacJN05CtRv2ESJg537UDDG5ZJKdkfY7Wq0o7o2TVEa0Lk+KbfddIz4JzAhGRbjeuZHnQRmVmVskyrr+8SUOnQZRbuk5JU7dHbUTbBDvp16EKkNcIq9d3nY458ttBykKfUTxiZkFiOwxL29O2Odj3b2SaVRpkg9HY68yOUrIDYhuVLBh/C40DE+QfBzILzGRGgPlx8BnxEGlERBxg1xDVoI2C/gb57HcLuhxJMJcvM3Q5WsCkMxqYFVqGHsfIluaAzygD99qBPkCx1GA7AlkU9F0G2DqkCyovZBtA1wVt5GAJlX6EnES7YP6dBPem3aiSxQJrQuQz/mftNCX7yYH3KlmbU6ZkSJejAZ0MOyfAayfkb2JKEg/olWXqhgzqi2kNXi76jOKRrzDFtbfrSWaE1ufUlIBdyGq9stJA6cF4sbI6/0yd7jTP9styVfq+RLu+z7X1HNQCSmEUtDF1E/4x6pTpMesktL0qJPQzs5W6PVww9fW6/M946e2Juqzleixky7QsskGPPQ8YyrJN/o7pHqsL5sZQYbUIyfqa/EIP+DcnCZQDqJAJ9MUC69xIb2Avs1clkUgarCnAZk+0R+tGvAP41bS/D4xcOFvqRcLNMXYcl/3ky/wyR3e5uAndtp4JxoCj29aNDZ8OTZFyejkrjX/VjRvt1O1ogE1DL+BX3ZgeY4WkXzZUnFf6jOJRE01LLLq9b9NACZ/t2M93DabCcGw7SP8KOl37ihE6XeDXm8njW3X+fXq9XR7X85jejK6TAexYJDCXQXObqKUrb4K8OgrlSvb7zFQle6plsj+vWj2mnIIufyQF3il1a1mhTJdt1NEb/HmBOfqK1U1KJjltLLq69Z7k1rYKf7na9Lw91gvKD+w8soloSRH0x7be7hBHq4u4YJsrWw3SgaVHrsavC1651o1ome7PWEzb0rKoThcHDi24ziiPaH3/6wtTdDnSwJ4CG4vaNqjeaFynR+r7BKx7U836AcZIve9UU97nv072qTT1Cb/C5D3dhv2Ugs/Ya1+KE0JIqeO5xqDHinxYjhshhBDywUCfQQghJCz0GYQQQsJCn0EIISQspeAzPhy/ZyeEkBLE84wBR6I+IU9FIIQQsm9An0EIISQs9BmEEELCsjt8xq233irjx4+XeDwuM2fOlD//+c9Dpu/s7JQLL7xQRo4cKbFYTKZMmSKPPvroLj2bEELI7qMU1hn8pTghhOwpPMFHrMkQckIIIfsm9BmEEELCQp9BCCEkLEX2Gffee6/Mnz9flixZIjNnzpTFixfL8ccfL8uXL5eGhgaVPpfLyac+9SlpaGiQ+++/X0aPHi1r1qyR6urqnX84IYSQ3UsJrDP4S3FCCNlTeMbQH0IIIaSf3eAz+AsOQggpUbjOIIQQEpYQPqO7u9v3yWZ17N1+br75ZjnnnHPkrLPOkoMOOkiWLFkiyWRSli5dCtMvXbpUOjo65MEHH5TZs2fL+PHj5ZhjjpGpU3XMaEIIIXuYElhn7LW/FE97nlg7RGa3QZp84NoBkdydkF9PsIxwHZYLPiNkPztIuJsjz6NvPLhA5oQoBio/yivstyxs2G7+HGFe4D4L9DG61wrcmwf1fj/fEkHtqHQ0pMI4wICYoXXZn842dE/Z7+NrO5kQtwbbWgTrC9KDsDoKx1QwL5B/Hoy7sP1uBfMC7Rgs15DldA1cyP7/kdCk3agY7vYe6gFjzfH8PW2BsYFkeS/Y8yL1Vo+S9TgJJct4fu+F8g+WS0QkbuRA2cIZLdvwa11tJNwz3w+dTpnvOuNGVZq4CeoEx5Duu7ynpytmoC1RXmHbO0y6sHkhfckBGaqTE+hQ931MJpHPiJtBrySSD1i2ykgm1H0xIEuCPka6HMwvqD8iIg0RvcmC20y3UcqNKVmw/1wweCbHWpRsXX6Efibod9soKFml5W9LE3i44H0RYwivUWSfsS//gqMh0iWJyHZ9qrbSKk2l6e8/NI43FmqUDOkfyj8OdCaoz0GbMBgW1C2tS8g+6ft0udA6zDbDzMREXDBegvVqAzag2uzT94E6dYRso1wgXVj/EwVtayJ/HCINssuobZHtD/q8ben8dQq7zgijB4PlF7Sd2PeG8z9o0h8z9TPLA7Y0DeYYPzrgf5QM+QxEcK6GQPO+yRFdp3/k40qG2tEWPc6aIl2+6w6nXKUJtmNfROczANcZRSNTY4oV3a776SZgA4LbRKCN0bTOSQJb4QCd6QFrm6h/LPfN7lVp0kfqZ0aj2pZGI3q8bG2vVrLK1/w+Lt6hy5+rAGXVyyTpmaLH0LgJbUrWnfE/c2trpUpjALvj5oCtAy4j0quFrbMC6wywCeIktBHzIiH3WSx/OqtHlzXRomW27mKxsvqZwGVIrCtQp6xOFElp3UDprD5g08GmjeEG7gX7rl5Et78b1XUvlGtb3Ven9TZd78+v+wBtJw0wxoy8LgdqR6MA6hDoT+Rm41u0zMqAOUZO94FRAOvQpL89CgmwBq30ywr5Ifx/CJ/R3NzsEy9YsECuvfZa/dxcTl5++WW54oorBmSmacqcOXPkhRdegI94+OGHZdasWXLhhRfKQw89JPX19TJ37ly57LLLxLLCzVv2FmJmwTef6QIGsDfvt2ueDWxpNbAxCaAfQHdr/qb7sq/BL2vZqNcxRp9u61Rclz/aoscj2mLqHeEff/Gx2nas6dbl2JpJKtn/dhyiZPm1eg3hxoa3w16ZbsdCFvlZ4Fviul9WvTnKdx1p0OuYyBbdZpE0sCebtV0LTIWh7zVzoN5gSKO5CJhaK/vn4hc5ofIqlAEfmgQGttKvH+UVem+qJqnbtjau19r1ce0wE5bemwruf9238iMqjd0D/BTw92F0T0TX3cwB3Rul98O6a/X4rGjQ9axO6HZrSPrXMmOSncMVU3IF4Ov7KYF1xl77UpwQQkodzxv8uzG7+TszhBBCPmQU22fs+AsOEZElS5bII488IkuXLpXLL79cpe//Bcfzzz8vtr1tUT9+/PidfzAhhJDdDtcZhBBCwhLGZ6xbt04qK7d/KSUW018EFRHZsmWLOI4jjY2NPnljY6O8/fbb8J733ntPnnjiCfnqV78qjz76qLzzzjtywQUXSD6flwULFux8hQghhOw2SmGdwePTCSFkT9H/zarBPoQQQkg/IXxG2GMN+3/BMWfOnAHZzvyCo7GxUQ455BBZuHChOE64XwwTQgj5AOE6gxBCSFhC+IzKykrfZ7CX4rv0eNeVhoYG+fnPfy7Tp0+XU045Rb7//e/LkiVLivYMQgghRaIE1hl8KU4IIXsIwxv6QwghhPQTxmc0NzdLVVXVwGfRokUwr6F+wdHSoo+RF9n2C477779fHMeRRx99VK6++mq56aab5IYbbihqPQkhhLx/uM4ghBASlmL6jLq6OrEsS1pbW33y1tZWaWpqgveMHDlSpkyZ4jsq/cADD5SWlhbJ5fSRx4QQQvYcpbDO4PHphBCypyiBGByEEEI+IEL4jLDHGu7S43f4BYdlWTJ9+nTZsGGD3HjjjTzWkBBC9ja4ziCEEBKWIvqMaDQq06dPl2XLlsnJJ5+8LQvXlWXLlsm8efPgPbNnz5Z77rlHXNcV09z2+70VK1bIyJEjJRoFQYoJIYTsOUpgnbHXvhQ3xf8z9gz4loEdoo3RgY4ukOXBgfe7+jP6XMjD88MeNuns4jcswuZvgXYM80zU/ug+1N4uSBdsb3SfJbv+dZNg2VD/orZ4PziyaxlaYb9WA5KFuRe1bVhQv2c8vxD1E9INlFc+ZNWDfRV2nKB0aKwgXQiWLYwNGhJXBu+M99NJ+yDrCzWSyG93aWgcWIFGrbD6VJq4kVeyvKddZdzQWmMautPsgHY5HlAaIHKRhfJ0/iZSlEB+NigrkqGyZTy9AEVtZBsFf16gUo6n65QBbRvMazCC5YBtBrBQP4FnmoH8oqjNDNR5us0cWDZQz0B7uGKpJEE9HgxUzxh6ZqBoLui7vKfL4Ti6Ti7o416JK1lQ/6qslM4f5IX0CvkbVN6gzDFQ/rqsCNSfDhgr+YK/P1GfdDpJ33VfYQj9D+Ez+o8zHI5d/QWHbduD/oLjw7Rh1WB1S1lkez0qzYxKkwr0Keq/+ki3kmVyI3a5XEGbiOwtGmfI/yDiwNYFx1XGs8F9yDcCuwDGRhTMsoJ2rMzUvwAygR/vcvWXPJKmLlsPsMMVgWfkQTsie4LKAee5gXZ0kb8H5ECboT7OAX+g7Bq4Lw/uQ+mQD81BWxoJXOs0OeDbw7ZHEuhCOtCfz7RPVmmOGLNKycL4dhGRaiutZCNM/xzRBH2+sqDHCgLpS3B+KKJtTK3VO2zeKXOIFT/XGUUjPdITM769HwvVut2NvsBYQCofbnovhTiwOxmdYf0r/nStiYRKU9XcpWTVSb0GQrYO4XzcP15G1nSEui8e0bb6L6vGKVlXn56L9fQG6mXpsropPR6NhLYB+Vowz3J02x49zR/3uDun2/bt1gYly20qU7L4Ft3x8TZ/HRLtWhEiGWDDwGZJpE+nMwpoY84vM/PAfoC1jWsDnxHTdt6LgDVEMrC2ieo0hQSYV4N0uUqQf7kSSbbGX0+7U/sp4LowYHxa4EfLZt5fBxuY76Zn9VgpVGp9z1fquY4TAz660l+JXDlosyq/zMkOsV4uss+YP3++nHHGGXL44YfLjBkzZPHixZJKpeSss84SEZHTTz9dRo8ePXCq1fnnny8//elP5ZJLLpGLLrpIVq5cKQsXLpSLL7545x++h2mweyRub9f99bkalWb58tF+QQTZYC0zUB8Cf9NxmL63+f/8Y976i07TPV7n39egB4wLpkAesM3B8rasAuskUKUWoHMV72i7Y4Fy9I303+yWAxuZ1nUqJEAfoHVASt9btt7fCT2GHtsVLcD+ge0HsNyRSMBtFxLAVleAvNBUFW1JgvYOTt3B9B62mZMEProK7CEmtCwR98vK4zoUnGXqwiYjaC0ZznB15gP7MVuSKo1Vruvk2SFfgAH9jm71CyMp3SmpCq1ndaP0nG5EUu+l1cW1rDHm37dojmufFFyDZvLvb29qb4fHpxNCyJ7CG+ZDCCGE9FNEn7HjLzj66f8Fx6xZs+A9s2fPlnfeeUdcd/sqh7/gIISQvRSuMwghhISlyD7jlFNOkR//+MdyzTXXyLRp0+S1116Txx57bCB009q1a2XTpk0D6Zubm+Xxxx+Xv/zlL3LYYYfJxRdfLJdccolcfvnl77tqhBBCikwJrDP4UpwQQvYQhmsM+SGEEEL6KbbPmD9/vtxxxx1y9913y1tvvSXnn3+++gXHFVdcMZD+/PPPl46ODrnkkktkxYoV8sgjj8jChQvlwgsvLFodCSGEFIfdsc649dZbZfz48RKPx2XmzJny5z//ecj0nZ2dcuGFF8rIkSMlFovJlClT5NFHH92lZxNCCNl97A6fMW/ePFmzZo1ks1l56aWXZObMmQP/e+qpp+Suu+7ypZ81a5a8+OKLkslk5N1335Urr7zSd0IVIYSQvYNSeJ+x1x6fTgghJc9Q36D6kHyzihBCyAdEkX3GKaecIm1tbXLNNddIS0uLTJs2Tf2Coz+mn8j2X3Bceumlcthhh8no0aPlkksukcsuu2znH04IIWT3UmSfce+998r8+fNlyZIlMnPmTFm8eLEcf/zxsnz5cmloAMdI53LyqU99ShoaGuT++++X0aNHy5o1a6S6unrnH04IIWT3wr0pQgghYSkBn7HXvhSPGoZEd4hn44IWDX5fLA0aPRbyywlh4x0Ho+5kQsYPh/GrUTlC5BU2Bjg6wj9sHPBdfebuJuzRBrta3rB1ChseIRhDLmyM8TCxCkXCxQ+H8RFBOlQn9EwT6LwdwuLBPgmZLmx5Vf5F1tF4oGw6AopIMGTLUN9rNQSGqBn4HwmPZXi+8RAm7vUIU8dZQbG260HgsCoTxFXyNivZ2zl/bN4eV8ejQzGik6aOnYPie0dhvMziBXCJG7ruqLzBGKOo/WH+IHYoinWK4oKiWO9BUFuguKnY5vpJoeBOABz3GsQxB7GQrIB9tUCgyQwIDIX0FqULHWc8xH0oRhOqO4oPG4xBmwZti2IMh40fjmIiB3UBpcFxzFG835AWOhi3HNj7sLGuRHaPz5g3b57MmzcP/u+pp55Ssv5fcHzYqTSzUrbDC3/UDz2O314jO4Rs3Sh7a6gyIH0OxvwOxhgXwToZR7YO6DMKGBcsR6WB/E+4JSOydQhUhyBonKH44Yhg/PCwoJi6cLyHGHCozZCeIVu0q6D44RkQXx35XmSfLGT/An2M/APySWj8IMrA/Ceoox9tXq3SoNj1KA57GZjXdIK5WWfAL6HxivJHYwDFZkdzkWAdrDDtCOYS/YTxGd3d/viCsVhMYjE837n55pvlnHPOGThNZMmSJfLII4/I0qVL4fG2S5culY6ODnn++efFtrfp+fjx4wct796MOyYjskOIyWRc61FfTyAwaA+KuQw6BIgqVmgjY4A1eLB/61/SepVaV6tkG2rAvBfEMUcxLt2AOv9thA7qbIJY3qieKA54eo0OsOqM8I8FMwPGz2i9rkuCWKS2pcdMR7eOH/rMqwf481qv+7O8DbQZ2JSMb9W+K2g6rQyIU19AAa3DbXSZOZ2fF7jXtUFM3TKgt+CZyLa4IKZ4psYaNk0exMLOVuv881X6oYUarWvldX5dOLihRaX52/8doGT5crBO6tS6hqYiVe/52zvRqnUv26DjzeeqdHtnqvUzC8nh46kXykD7BOrkZt6fzyDhuH/9NImUbfel3X3arhk5fz97KO51RPeX54C9ks3algb7XkSkc7Jf38Y82Krzj+gvubkRoKd1wC44WpbY5LcBMHY1WBakx+n26GvQCjrqWW3r1o31l8PIalsX3aplnoliius6RbtAbHDtRhS9zbryyM/mq3S6bJ3/2ouB/Rnge2NxbbA8sLZxwS97bdvftnFb51UR0/OhpK1ltbG0klXaGSVLBNYoNba+ry7So2RoDdcU6VSyNbl6JUsH1kqjx29RabqeaFKy9CjwnjKj2zGSBm3b678u6KWIGHk9MNq36MDxW6Na+dIj9PpvVbd/PviSjFdpIoF1RSGVFZEndOGkNHwGj08nhJA9hWsM/SGEEEL6oc8ghBASlhA+o7m5WaqqqgY+ixYtglnlcjl5+eWXZc6cOQMy0zRlzpw58sILL8B7Hn74YZk1a5ZceOGF0tjYKIcccogsXLhQHCfMzwAIIYR8oHCdQQghJCwl4DP4UpwQQvYQhjv0Z1dgrD9CCClNdofPIIQQUpqE8Rnr1q2Trq6ugc8VV1wB89qyZYs4jjMQXqOfxsZGaWnRv8IUEXnvvffk/vvvF8dx5NFHH5Wrr75abrrpJrnhhhuKWk9CCCHvH64zCCGEhKUUfMZee3w6IYSUPIz1RwghJCwlELeJEELIB0QIn1FZWSmVlZW75fGu60pDQ4P8/Oc/F8uyZPr06bJhwwa58cYbZcGCBbvlmYQQQnYRrjMIIYSEpQR8Bl+KE0LIHmKob1D1yxnrjxBCiEg4n0EIIYSIFNdn1NXViWVZ0trqjz/a2toqTU06zqKIyMiRI8W2bbGs7XE7DzzwQGlpaZFcLifRqI53SAghZM/AdQYhhJCwlILP2Gtfiuc8T3Le9q8WxA19Hv2O/xcJfxZ8HnxjodbUd/d4uheD5XA8nVk+ZDnCRtOyA1VHuhUHx/VnQD1t9IAiHvUfVu+DdYJp3ldJhn9m2H6CgLbNg4Z0vF1rXKeYnQKyMkEF0PhB6awQRSt2XAakV443fJr3Uw50bxidsQM2InjtY6hYGzvE+tuRBQsWyLXXXquS98f62/HYw52J9ffQQw9JfX29zJ07Vy677DLfBtaHgQmRLVJmb++1CjOn0uQ9f6+OiRRUmhozAXLXrrIALHiLU65kZmA2sCFbo9KUW1kly1v6mUlTp3M8oKlBnwHslQlGjAu0HuWP0tmGvz2gDfPQlEP3Aco/7+p7bcN/rwVmXrB9AClXf9Ek7/nHgIPK5elxErptXSALpHND+pCgnomIVEXSuhygPYLtiOoZFgvUPW4ObzmD+iMi0uPGQ6VDfRembKiesPyGtiVxS9fJAv4y2LZlYAwH65Syh5hJhfAZJBw1VkbKre06EPQPIiK1Vq/vOg10rQzox9R4t5JZwCYu7jhCyUbanb7ridHNKg2yJ5XAj9imtq8ZYLNSnv/FFB4HOi8BeSE63aSSBdsWkQE+I2nosYf6zgY2MZgOpUGYhh7byDYH/R6yV8g3xmGdQs7BAtkhO1RmaR3NgfyxrdZ6FXxG2DULKhsibBsFeTM7Wsm+UP6ekr0E5mHVpvaXwXLkRLcZ8qlofKK2jYJ5ZHCcbQbzymBbDNmuRfQZ0WhUpk+fLsuWLZOTTz55WxauK8uWLZN58+bBe2bPni333HOPuK4r5j/3WlasWCEjR4788L0Q3xwTiW/3AX22Lr/V5+97p0zbmLK1Wo9ylboPOw/S95avAfY7oPZNLwJbDdYxdi/YW6sEshpdtuq3/df5cr1rY2X1M2NdOi8nqp9ZsU7bna7xfv+bGaHvy2b0eOnrrVAy2azL0dCu2zuxxW87re6USmOAvUAjldGyAtj5C8pMXScvrucdbjmQRXV79zXqNW22KqCjoP0LCS1D9fRQeYHrCrraaLfOq2a57nOzoNMVEvoB2Woty5zqf+hLr++n0kQtnX/Fe2BtBrYGRv1Jz/OMDBp7fqyYLmu0XaeLV+s+7mnWstQY/7WT0HpsZv11MvLvb2+KhGPz8nox49vXsmhsxDr8fRNZr/1KvkLraaxd90VqrO772GYw19sSmMNt1OFPEjbwGV3avnZP1IOj41BdtlF/8o+NdIPOP92k70uu0enQ1N0Am1373eO33xuP0muRghaJlQN+sFo/NItsXcCkRzu1PTHAOLK1OZFki36AFXjBE8ki+63bzI3oPZV8OdChZu2nckm/LJPU/r43rfN3CqiBwLsFS7etF2gjuObq1mMluVbXHSyFJbZVl6P9X/xz6+S7Ov9svb4v2qH7uFAG9oTAsjdYtngHGOtbdf6RjC5btBese+1GJeua6M8vMwrMTQJZuX16TrP9n8X3GbfeeqvceOON0tLSIlOnTpVbbrlFZsyYMex9v/3tb+W0006Tz33uc/Lggw+Gfh5jihNCyB7C8Ib+iDDWHyGEkG2E8RmEEEKISPF9xvz58+WOO+6Qu+++W9566y05//zzJZVKDZxQdfrpp/vWKeeff750dHTIJZdcIitWrJBHHnlEFi5cKBdeeGGxqkgIIaRIcJ1BCCEkLMX2Gf3hYBcsWCCvvPKKTJ06VY4//njZvFn/SGBHVq9eLd/5znfk6KOP3uln7rW/FCeEkJJniONG+r+hxVh/hBBCRCSUzyCEEEJEpOg+45RTTpG2tja55pprpKWlRaZNmyaPPfbYwBdy165dO/CLcJFtp109/vjjcumll8phhx0mo0ePlksuuUQuu+yyXagMIYSQ3QrXGYQQQsJSZJ+xs+FgRUQcx5GvfvWrct1118kzzzwjnZ2dO/VMvhQnhJA9hSuDOwvG+iOEELIjRfQZhBBCSpzd4DPmzZs36HHpTz31lJLNmjVLXnzxxV17GCGEkA8OrjMIIYSEJYTP6O72xweIxWISi+lwILsSDlZE5Ac/+IE0NDTI2WefLc8888zO1oDHpxNCyJ6imMeN7Bjrr5/+WH+zZs2C98yePVveeecdcd3tnuxDG+uPEEJKHB5rSAghJCz0GYQQQsJCn0EIISQsYXxGc3OzVFVVDXwWLVoE89qVcLDPPvus/PKXv5Q77rhjl+uw1/5SPG4YEje2B2ZPe9oLB9/o2yCOu6VFUmuG+y5A0tAZ5gPlAGHpxQETBlg2kD/CCTwT1ckGecVD5h8Hsozn/7oHKqsjuqJh64SwZfh7bUP3HSoHwg20ow3uC/aviEge5QVkjgf0JaClLkhjg/MmzJB1CuaPnlFhoBpo4qD5kV4hrEDfhe2TTIhxLSKSD5GdA/Qn5enRYof+mqtOZ4Voji7XX9hed4jCe//8DPa/nWT+/PlyxhlnyOGHHy4zZsyQxYsXq1h/o0ePHnBE559/vvz0pz+VSy65RC666CJZuXKlLFy4UC6++OKdf/gepsNNSsbd3t+dblKlCepIzOhQad7IabfY7pQr2cz4RiWzDW1N2wr+o++zrs4/D/QUYQJbUW2llaw1X+W7TppZlabCyigZUtW0q7/Fhwi2bVOkS6VB9so2CqHyj5s5/UzPn1/eCzelSYE6IVmv4+/PtKu/KBIsg4iIC2xREpQ/Zg5vm61dMQT/BOkV0iEJtBtKYxt6thMFfVdhar1C5ViXr/Vd10d6dLkAPY4eY6jf0TORjwiC6on6wJU+JUN1D5LxbCULlr/PRTPLf1Jkn7Ev826+VpL57XqC9GhDvsZ3XR/pVmn+mp6gZM+AsX1G1d+VbH1fjZK1ZP0+Y2KtjqGFdHmDU6VkNlilNIGx1lHw+7haq1elQSBb0eZUKFkG2E4zMMdqAM90QP55YHPjYNyidMH5NkqDQO3thrgXjfew+SO7kwsxV7BAmyE/9X4IlhfZzThYe0TBvHp1YYTOH/ad39+MjWxVaQ6Nr1Oy5Xnt25ssPY5TIWwz0nfUd6j8PWAeGYYRYFxsDoyxtEOf8UEQbzHFim3v20SbbsC+er8+GJvBmAXtbmXRHoLGAd9XDi4r0g3al0V7gO5GtZ6aCV2OaKeWpf37lVK1CuUP6pQEa/VRWtY9Uc/1Gl/yt4hZ0PUc8aael8ba9dwMvtxzwX5M2j93N/r0esrrA3O/rE7n5kCPBvZZjISut2GBuXwG6BXY37R7dXsYrhW41llZWW1TTCCDNsQENjHmf6aT0GVNjdLKjXSjbKN+aM9YnS77brXv+sDbWlUa2dyuZaMblcgt037EyIH1a6BaRgE0LlIDsHkc6dVtVKFdnPTV+XWmkBh+nuAAezMAfUbRiG3x+wwnoRswqM/xrVpnMjVaF8rXgzlWj7aJW47Wtsgs+PU5c8F0labpBT338IDPKN+o9zf66rUdW3OSX+dGPaXbwgD75GjbNlet7934MV33CZf/xXfdFP2oSpMv0/dt3V/LnJguG7KdQX+M0oApsxQSWoam7m7gpZILlhnIv5nAXMW26oTxdv3QoE1xYtpW5/XSTyzwzMjwWyX/fIb/GrZFVJe/kNSySYtXKNm6b+yvZFV/89fL7gU66oL1INh+jLfrdGgJ5wRkSM+Q3UV5Rfr0veXrwH6v7Ve24JxgW16BcmWH8CshfMa6det84WDRr8R3hZ6eHvn6178ud9xxh9TV1e1yPnvtS3FCCCl1DG/wGBy78m1cxvojhJDSpdg+gxBCSOlCn0EIISQs9BmEEELCEsZnVFZW+l6KD8bOhoN99913ZfXq1XLSSScNyPpPwI1EIrJ8+XKZNGnSsM/lS3FCCNlT7IZv4zLWHyGElCj8BQchhJCw0GcQQggJC30GIYSQsBTRZ+wYDvbkk08Wke3hYNH7jQMOOEBef/11n+yqq66Snp4e+X//7/9Jc3NzqOfypTghhOwhDHeIb1aFPeWdEELIPgF9BiGEkLDQZxBCCAkLfQYhhJCwFNtn7Ew42Hg8Locccojv/urqahERJR8KvhQnhJA9BBcehBBCwkKfQQghJCz0GYQQQsJCn0EIISQsxfYZOxsOthjstS/FVxeiUlbYXtm0q4Ox11pp/7VZUGnqrISSbXV1wHnH07/ttw0drD4T4giAjKc7Kcx9IiJRoDl2oBg2uA/rG6qTLlveQ8/0PzQD2seCz9TEwDNtcLcVeKYp4D4j7FM1TqCefV5OpXHFUbK0q+ve4+pe6Pa0jroBXTBB/8Ylr2RR0KOOaH3Me8O3h+Pq+ypM/Uwb6AvK3QblMFXf6TQWkAlob0Qe6F9wnKU9bc4yQGaBesYNbTtQfkFQnwT7POUM4Q14RFXRWJ1rkHh2e5/ZoE/XZkf4rlvim1WaTflqJbMNracZT9uAFPBTb6VG+a7zwD8g0DNt4ONQuqCd6XHjoe5DfnZTrkrJ3knVK1lZxG9P19m1Kg2iMqL9cZ3do2QVZt+weUVBnVCfoL7LApuedqP+ayeq0iAbnHW17WjzKpQsYWofFCQG+twBvtGCPiNcunIr67uOA/9QH+1WsmozrWQPb/2okpVFskoWfAbUd6ijug/Sju5j1C9h/GXB1W2GdANRbev2CD7T9bTPqIr4dTvTlxeRt/FD6DOKxru5Rp/P6HW0nVyVrvNdj4x3qTQbMtVKVgHs2jPRkUr2jYanleyutqN910/3HqDzt3T+hyTWKZkLbMDGgrbpTsAvoWcellirZD2uXmO9mR6tZEEbIyLSaPvbssMpV2mSJrAdhrZPlaZuDzTfDo4RZBPQHBHN9ZB9zQRshQXKgHwSShfsExHcn8FnIrsZZq4qIpJD7QGCiDoBO1YJ9BH57HvaZinZJ2re0s8Efqo+4vdBaM2F+hO1Rwr4EUSwDzJgLoX6LgPyR2XrdJJK1hWQvZjXsfBSAZ+X682JyJsqnYjQZxQRKydi7aD6SI0KATOWrdP6V/s3MM8YoURS8wYYoyAkYyTlH4/tU9G6E8w9lutUFpiWZkdoRTFz/vw2Hw7KpadEkqsFe05dup61b+hnJjb2+q6TL7aoNEZS+ySx9ZzcqdMNmWnQ49HK+sd8JK39j5nR83SjV9s/s0/7Mwls7HoxPd/0YlrRCtW6nn2N2iZ2jwU2Petv21gXaOstWhbp1XU3cmCNUqbLmxnh74NUky5X71j9TDcG7Pd4nS65Vvdx8x/992Ym6kHWcYKel434h+4nw9HP9Ezd3kG36kXAOiwD1jtb9GCxNrZrGdDlylp/HSIZPca6Jvj1yhlq+UmfUTRMZ9unn+RqnSZX4belWw/StjrWrmXxDq1bhTjYawXrTpnun38bT+l1wbpP6jl5w6vABgB3kx6J7Id/zLfO1Glq3tSy1Ehd/kKZfujke4DNPcy/lrHe0Oske6S2C7EOPbaduB57mz+q0/VO9NtEM6v7ybN1+c0s2DvPAF3o8Ms8C/QvsFdoFx6OZyALznXSo0Cngweg8ttpUM8+4IMCW7Ro+9QG9206Wss2nzxFyTJ1Ol11YDkSAS/wPPB+MOQ2EWzbEK8bYN3dKFibxXVCs6B1NNbl90G5CvC+JDBPMHNDGP+9IBzsjtx11107/by99qU4IYSUOvw2LiGEkLDQZxBCCAkLfQYhhJCw0GcQQggJSyn4DL4UJ4SQPUQpOBFCCCEfDPQZhBBCwkKfQQghJCz0GYQQQsJSCj6DL8UJIWRPwSOqCCGEhIU+gxBCSFjoMwghhISFPoMQQkhYSsBn7FSE8kWLFskRRxwhFRUV0tDQICeffLIsX+4PSJTJZOTCCy+UESNGSHl5uXzxi1+U1tbWohaaEEJKgf5vVg32+bBDn0EIIcWDPoM+gxBCwkKfQZ9BCCFhoc+gzyCEkLCUgs/YqV+K/+lPf5ILL7xQjjjiCCkUCnLllVfKcccdJ//4xz+krKxMREQuvfRSeeSRR+S+++6TqqoqmTdvnnzhC1+Q5557bqcK1lqolGTBGrjucRMqTbcb912vA63eZHUrWdwAQejB1xgqTJ1fhxv1XfcEyoDKJSJigbKlXB34Pm7kh5VVmhmQpqBkOfCdh7xnKRkiWLaMZ6s0UcMJlVfSzCpZNahDWaAOFaah0sRA35Ubuh1d0J+u+Psg7enyd4KB2+YklazdKVeyDiBzPH95y0BbVFh9SlZv9eiCAFJeVMnynn9Y50CfIz1DepU0kT7qdksW0eLlwTeKMp7u93SgnqgtXHCfrhHGEa1/wbbd7FSoNBvzNb7rTKYgImvgMwxv22ew/33Y+SB9xtMd+4md1TqwIxnHb8de8ZpVmnJbj9FRiS6dl6ttogt0JiizQ46VzoK2O90F7Qcjph6PMdNvS+NgHK/L1CqZCZSu3NLt8feNo5QsGvU/MxYJ5x+iEe27krYub2NC28SRcX+/oLKiupsh+yBov1H7dxe0/7FAO6YKyD5pnxHsg7ily19ta5+RdfV0Dtk/G+hLr+OvA2rHGGjH17LjQpUjnwflCPjtLtC2luh+csC8Joee6Wq/53r+sYjGK6LP0WO9M6vLi4hbfv1Gzyy4/joVUrr9+6HPKJ7P+FvPGLF3mNP35sFcMjCGUo4exxUR3V/r09VK9k6sUcneymhbGtTTqkhapUFz/pXZJiXblNPlsMEcLrhGQf7tjT7tL7OeHnvPbp6kZImIth+pvL8tq+Parh1Ro+dOSWTnQXsgm9UQ8a8JbXCfBdYPaD4Y9A8ieo6I7kuDtV8OtGNQD1D+Itqfof5FZEEf9zh6/YrWjUG/uqWgn/lazxgla01XKlmqSrcHWiuty4/wXTugfcrMHCirlqF2RP0Z7D/kUxFozYz6GPkzPe/QfVII+LecM/ichj6jeD4jSF+d1kErsJSueluPn149NCRXq21RvkPrTKJNd9rWgwJpxun5cu5tPfZ69RROop1aZmV0PXNVfp0zmvQeQr5F666RA3k16Lp3HKTr3vbR6kBmVSrNuN9ru987WvttMOWEv2hyYv7y5oC9MnXxxShUK1msUz8guBwBZk3AtF3AtFdyVfrmbA3YDws0R6Zb26FMjW6zqtU6fzOrbU/3OH1vqjnQjvtpf29uBPunGbB+aAs3d9+6v98OA1MqYHolWw7TfQymCoKWEFbW397ovhhobzMLnpkFBfbAfvVK/1q47YgalUYtp4ZYBtNnFM9nRLs9saLbG61rik7jBQxPfLPWD7R93ztaC8ESReIVej6VWevfv4wCfehr1srbauu5TflarRSepWVmITCfAjZs68EgL6B0kXrtb1adXKZkdpd/n8WN6r2vcQ/rPT6zD7xXqQY2cZU2ztEef790TANrrkrtp+woeGYG7G9U+cthb9X6gnx2vAO0ozbD4mhTpPTKqdR1qh2p27E3rW1YLq/3vsr06zpFvkLXqX0a0BdbyzoORem00ndN8fed1afb1o3pvAplQN+zwF+ClxBOwn8vWma4ZWASAMaYa4NBBYxH2aZA3YEvC859HGNwv1sKPmOnXoo/9thjvuu77rpLGhoa5OWXX5aPfexj0tXVJb/85S/lnnvukU984hMiInLnnXfKgQceKC+++KL8y7/8i8ozm81KNrvdWHd3hxgVhBBSCpTAcSNDQZ9BCCFFhD6DPoMQQsJCn0GfQQghYaHPoM8ghJCwlIDP2Knj04N0dW37Rkht7bZv3bz88suSz+dlzpw5A2kOOOAAGTt2rLzwwgswj0WLFklVVdXAp7lZ/xKBEEJKkVI4bmRnoM8ghJBdhz6DPoMQQsJCn0GfQQghYaHPoM8ghJCwlILP2OWX4q7ryre//W2ZPXu2HHLIISIi0tLSItFoVKqrq31pGxsbpaWlBeZzxRVXSFdX18Bn3bp1u1okQgj5UGF4QziRD8k3q8JCn0EIIe8P+gz6DEIICQt9Bn0GIYSEhT6DPoMQQsJSCj5jp45P35ELL7xQ3njjDXn22WffVwFisZjEYiB4ASGElDolcNxIWOgzCCHkfUKfsdPQZxBC9lnoM3Ya+gxCyD4LfcZOQ59BCNlnKQGfsUsvxefNmyf/+7//K08//bSMGTNmQN7U1CS5XE46Ozt9365qbW2VpqamnXrG0z0HSNSzh0zT5/j/n3N1dQ4t36BkVVZaydKudmSNduew6TKuLmMGlLvXiet04F5Eoz18XBITnE3gevoggC4noWRpJ6plrl/WkUvqNAV9X8G1lCxiOkpWYWeVrC7a67seHduq0lSDvquP6PYpM3JKZhsF33WnO0Kl6QHt01aoVLJNuSola83qdEEaY7qsqJ5WVFuQpKnbDNHulPuuOx3ddwhLtA6Z4Os9KJ0V0D9UVqSPKF0wLxGRuJFXsuA4c0D+KK8KM6NktqF11BRDyVKB8Y/sRtoJ2Ahn8AM5hjpW5MNy3EgYPgif8XZrg1hJbWd3xHX9fVqe1Pq3Ll+tZJvLK5RsTHmnzh/oTJBMQdv9qFVQst681i3X0/lXRrU+RwM2d3yyXaWZmGhTst+uOVzJPlq/Xsmyvbps2YCP6ynoslopPRZcYOs8IHs33qBkiQp/3WvK+lSaukRKycqB/0E+NOjPkM/rAf2Uyul0uYL2jR7oz4jlL0dZVPuy3riuUxzoUIWtdQNR8Pxla8+XqTRvdI9Sspyj65TO67rblravkUB7G8DXRMF9CDQu4pb2GUHQnNEEs/cC8C1ZR9/ruDpdcLzHQD915/w65GToMz4In7Fia4NY2e1tXxXX46Un6++bKTWbVZrWjPYPjfEeJXuzd6SSdef0nDPoDx5aP1Wl6ctr/SuLap3fsLlayT42+R0le+n/O9R3jZZfTgzMSzfpsdc7Viti/cHa36SzgXVGr56r5oGNqYnrdUACjPeKiO7Pyoh/3h839X3llr7PCrmqD/qRroKuE5onoHVY3tN1R7KY6deXpKl9BpoLoznzVlDeVEH7uLKI34ciG/xuV52S5Qpab/9r/RFKhkhE/H11TN1KlQbVPbj2Gwy03gnWK+/t8m8JYDmQfgTX35VAj5/YMNl37aQHXx/SZxTPZ2SrRawdhkOhXOuMkwzMbaq1TnoderwjevbTOtOzn04Xr/PPfbMr9H6EmwD6HQPz3pF63uWB+Xyk3e8kXEenqZ3SoWQdK2uVzExpu+aiJjL9dTDAM1d9VjuvSIP2GVUVer0woVqvlYJrsQ1dev+nLwPWdVHdjlvTulJGoE5uG1jHmmCdZGuZkdT6YkXA/mCg3bK2tvFWDsxna0A/Wdomdk9WIsk3+MeBvV7X040AHQVzEZQOLb/dhL/udWM6VZqtXXq94wJ9NyzQ3uCZTo+/wFYP2BdNa1lfnfYF8Q49P7RyoByuXwamCdI7PqBnmcHnNPQZxfMZmRpDrNh2RTFB/+UDfqRvDNhn7dHj0YtoBcyM0nNaMwPmLYFbe8ehMQVkoP/bp2uh3Qn2ewLFGHmQXk9t2FSjH5DVCm1s0GMD7R1JYHnmgnVM3xhtA5Kr9H59vhzsedSB/YHA6wUrDdoCzPm9Bm2/40k9f5CALJ0EdmIrmqvqcsTbw61tsiOGTzetfqOStWXLlWzFct3eFeu13vaM9dvS3rHAPySAQtpaZvSBdwTANheSATsJmtGpAmsK4DME7AmB1wbi1fj70wLlb6rV+lgV02uDlZv0Xmm2T+uHEShb40t6X/G9L/nvczODG/9S8Bk7dXy653kyb948eeCBB+SJJ56QCRMm+P4/ffp0sW1bli1bNiBbvny5rF27VmbNmlWcEhNCSIlQCjE4hoI+gxBCigd9Bn0GIYSEhT6DPoMQQsJCn0GfQQghYSkFn7FTX3m+8MIL5Z577pGHHnpIKioqBuJqVFVVSSKRkKqqKjn77LNl/vz5UltbK5WVlXLRRRfJrFmz5F/+5V92SwUIIeRDSwkcNzIU9BmEEFJE6DPoMwghJCz0GfQZhBASFvoM+gxCCAlLCfiMnXopfvvtt4uIyLHHHuuT33nnnXLmmWeKiMhPfvITMU1TvvjFL0o2m5Xjjz9ebrvttqIUlhBCSolSOG5kKOgzCCGkeNBn0GcQQkhY6DPoMwghJCz0GfQZhBASllLwGTv1Utzzhn/VH4/H5dZbb5Vbb711lwtFCCH7AobrqVhQO/7vww59BiGEFA/6DPoMQggJC30GfQYhhISFPoM+gxBCwlIKPmOnXop/kKQKUckXogPXG9JVKk1HX9J3HTH1VxE2pvR9ZXZWyWqifUoWMUcNW07X02HZ+xxbyXrzMZC/o2Rxq6Bkq2WE7zpm6jSIrKu7F5WjM5tQsu6MP11nZ5l+QI+up5EzlMzMa5lr6wHixvz9Z5TpeiYrdN9VJDJKVhXTsmDbmuCrKynQPql8VMmC7SMiks3q9ohG/c9squxRaXqrdF6IpkiXLpury9bhlPuutxZ037XnypWsM6/1oDOnZb05Xd686x8HlqH7NxHJh5LVxNJKVmXr8Vlu+XWhwgJ6YOm86iO6D1yrV8lsQ+tfxvP3ccbVfb45X+G7zuV1HfsxvG2fwf5HwuMULPEK1sC152q7E5R15a1h04iIbDV1Z7T3JpVsRLnWt61p/xiqBPZq0+ZqJbNjWv9Q2fJ9Wgdr6vw6viGh/SDyl10pPd6fzkzS927Wzwy6M7tHl9XWw0yAm5KCLoYUynVfZWP+cmyMabu2IVmrZGZM+96IrWUh1s3iOrpcbgHoXk6nQ0cKGXF/OXri2n5kK3Re8YjWlw5L62gtsK8Zx98JrekKlSadBX6wW3eUmwEdCuYARt7vMzwbfJU0pmWRhK4n6ruKpB5nSdvflhYYAxEwLzCAMY6C+VsBpMsW/O3RndH+OJPz67GT1u01UBb6jKJxRP1aiZZvb/vXt+o5/9jKrb7rdEGPg4ihdQHx8sZmJRtTred1f1vpT2dEgZ4Cnc9EwZxcL1Fkxc0HK5k92n9tZbQyeYbWy95xOl0kpR+6+e16nV/Ef68X1/VcA3zeip4mJWsc2alk2by2RXXlKd91DKy5ysEaMWFpO+x4umwxYBdUuVxtvwtAlguZLhqoQxyUtTKi6+QK6E+g3xmwpn3q3cm+a2R6nG6dl5HU7d0VjeuyuUBxA7zXNkLJpo3eoGR10ZSSlYH2SJo5JbNC/MTBAfsA6D6UDvVBcO2O1vJlUX8fF7jO+EBw9+8V2UGHnYweG6Oa/D7DtrRN6KwA6+0tev4aadf5o7l1ts8/r/ASoGPBOsZKg3EGZGCpK4XqwFju1Yk6wRzUs4BvSeo2spr0eJSAXSgAG3Pg/uuVrDKq54NH16xUsptfmaNk08ev9V1PHtGm0mzo1Wus4HxTRCST1GM52Ju1o1t0XhHQFgAX+CTEOx11vuutKd13rqX1IFOrZTlddck3gPLmAnN+UC4nqFMiImAOEMxLRMQD+i0Rvx3eslEXdr9Jur1njlgNSqfZkKlWsuC+M9pDbO/R64DOcTqdl9H+3gTj08oG2hYsN52Yv33cIV5U0GcUj75xeTET2zvEAPsPZW/650C5SrBHDmxwZoweZ8hneF1gXR4YLw4YsxbIK7uftqVeFsxVk2AfZIT/Ga0dlSqNndB2s5DS5XeBj4t06bFRKAusM8C+Qq5c37fx6zVKltyk61TQLk5iW4MSYEsbtB7kwH4emCJKstw/fzWjOi83DvY3gRrkqtE7Gp2uUO9/Znm13pdfn6pWsgrge/PlYA5g6XJ07h+wWXFgfNBUp0dXFEy/xY2BcgT0w43ocpWv1A2UHqX1ykP5x3VfBftzVHW3SjMyqfcKppRtVrJq8D7zxcIEJTNcv83p2k8rcnBrw3BKe29qr30pTgghpU4pHDdCCCHkg4E+gxBCSFjoMwghhISFPoMQQkhYSsFn8KU4IYTsIUrBiRBCCPlgoM8ghBASFvoMQgghYaHPIIQQEpZS8Bl8KU4IIXsKT/AZYjKEnBBCyL4JfQYhhJCw0GcQQggJC30GIYSQsJSAz9hrX4qv7KqXSGF77OK2bh1rKdPjj20cietYNCgWZAzE6BxRpuNsonvLbH88DBQrZjMoq+MMHx9NRCQW1XWIRpzANYh1DGLnZfMgFgiIf1VI63sjHf50ia06jkC8HcTz0KEjxE7rr4i4IHaEE/PHvyiAeBj5ch17rqtMxwhqT6J4LIG4IqBLUNwDFEMBhZBEoZxSSX/dV40AscgLuv37HK1XtSAuniW6bdvz/lhFm/p0+7T06pixW7t0jKNCF4if2aX7JdLnrzwIxy0eiFESjHEkIlKoADFdq3Ssm/IKv7IFY0WKiIwu61Sy+qgObDw6ptOheOQ9gRgc76QbVZr3ev1xDgspHbtwAM8TY7C4TmECGpMB3I6YSN/28eWVAyUMxjEGMciMPq3fvXltLFCc1zYQBzPT6deZXk+PMwPER3NADGcUJwyEeZWteX+cpp6tOq62ndI3mqDJgIkRMNRUDNpYt74x2qUfYOaBDUiC+OFVIJZdhb8OTgzFk9J5OdoMq/i2IqC9kRsH7W+F/FYkiq3oBvQvC/zg5qz2GVEw/7FtLWs1tO3vWu/3EShmnQvi/dogfheKd2mBeYGV9dfdRLHZIyDObqUeF7ka3Y5bykEsyzJ/e5RV6YIlonp+GANzLhRPOFfQ5Q3GCy+ANE5A5maHiEtMn1E0Hlt+kJiJ7fZ5TKMKBCdbs349QjGo/7FmpJKNadJ5NVTquQfSLSMQR3LUmA6VZsPqOiUrFPT8GNm13pF63Cba/OO7ezyKHarzinYBAwhEsQ6dn6mGGhgbcT1xrNSh1aStR7cHslm9lf428sA4jif0fBPFBUZrRCswp7BMEFsazBNQXqhsaP1nB54Rj2gbtsXQ69IoiH9eAAujt9bpGO6yxe9EjYIuKwh7Ly6IN+8YIPZ4CFNWALFm/7puipKZo3SMvepKPb+vigN/EGjLkQmtfDb4GUQEtC1ar2VB8Mbugr9tXdAn7b1+u+SkweSwH/qMopHvjYm5wwTyCx99WaV54NkZvmsUQxLFSY7XaT3NgVihBVfb+aDNrX1d55+v0HqUBXOnYAxWEZFIGuzZBOdswO57KVDWKNC5gi5bVbkej5Nqtviu39ysbdNbq7U/rm/QMTpro9oGHNq8Uck+Ufu273p9Tq+njqnV8cnTrrZrSNYbGO8mMH5oT6ja1uWvs3t0/o7ug5aUf43YmdR+JVcF5t86zO7/z96Zh9lRlfn/raq7997ppLOTEHYCCSYkEwGVIRpxZGRwQVFAhlWIoJkZBQWCOhIUZDJoIIogOKigPIM6iiBGAyKbBPITBQKBbJB0Z+39rlX1+yN2J3Xeb3dXum/37b75fp7nPk/X26dOnfX9nnNqOeICnY1t034tXxsM51bq86oa9OQS6WA6DebHoE85xmTMBpOzzTt1pg6v1vvGt+V1OW5o022hPROsz/qU7teT61uULdqg+zpqCx05PYHtzAXLowDWnFtbjTlRoo996qkZRSPxVlSc+L7+kNyu5++7Zwf7nwXaMryxBMJZh+g+5G3V82HTp9eM0b5jT6tu306Tbn/m+rqIiJfUfc2JBG3WxqQKU5io22Vygp47pbfpMW3N8buUbff6YB6sjO4bzQt0+m3QPRI6etkzTttMlwtcsNggHW4FWGQC9yC62o28gzUnJw3WcZD2gkXEzDitB4dOCfrEw4CP7CzotrEjrevJB+nY9M9gDlEb1Dhvu24vlZN023DX1CobGte4YF96MdqopHThpsfrtMZ3gvoE96IKSbAvfVNQjzdM1/pZN1XrPWJuzSZl82boaz6bOzRw3NmlNbVQHWwHHprUdVMGmhHuTi0hhJCiY/l9/wghhJBuqBmEEELCQs0ghBASlqHQjBUrVsi0adMkkUjI/Pnz5bnnngt13v333y+WZcmZZ545sAsTQggZUsphnsGb4oQQUiIst+8fIYQQ0g01gxBCSFioGYQQQsJSbM144IEHZMmSJbJ06VJ54YUXZNasWbJo0SLZvn17n+dt3LhR/v3f/11OOeWUAeaEEELIUFMO8wzeFCeEkBJheX3/CCGEkG6oGYQQQsJCzSCEEBKWMJrR1tYW+GWzvW8VeOutt8rFF18sF1xwgRxzzDGycuVKSaVScvfdd/d6juu68slPflK+8pWvyKGHHtprOEIIIaWlHOYZvClOCCGlwvf7/hFCCCHdDIFm8LOGhBBSpnCeQQghJCwhNGPKlClSU1PT81u2bBmMKpfLyZo1a2ThwoU9Ntu2ZeHChfL000/3moSvfvWrMm7cOLnwwguLmzdCCCHFpQzmGZFSJ6A33t5WL3Yy0XPsxPW791Z7MPlul6PD5PXm8h2VejP5dFdc2VAdWkZ0Xh48VwA+nu+jcJ5OW1abRCLBRyzMNBzINa1OXeWJPTpcsjkYX3KnLv/E7oKyRTrzymZndDgrr+Pzo06fxyIibkKnP1+pbX5EF1IhEbR5UR3GA+d5oJf4oDrdhD43VxXMQzav29lbbp2ytWd0uIp4Tl/T19fsygbbd2dnQoXxd+j44zt1pqp36HYVb9G2WIdRx+CpIN8BdZLUtnxKpyNXnVS2bF3QtrG+RoXZUN+gbFXVaWWrr+hStppYRtnShWjguC2ny7GlIxU4drt0PN309QTVaHmyaqRgFayAv49u1X4+XxUs1Gibbmuob3sZEC6q+0E2ElU2Oxds405Gt3n0aRm7AMJpVyo2sPl2/8+72eCaVZt1o0s36LjqX9FPhLvxYLjEDt3urRzQgk4dLg5ELjGmUqdtfNC3ucCnI3/iapeI9UBXpwK2F6AZKC5XuzXVFjxX66Bf0BfNpPVFMzYYxIBz47uCNtRGnaxOR3wP0IJ23bCiHbpd2XljXIM0A+hxvlKnI1ut85St1eEKKUOPa7SP6KwBY82UbrdORCfYB3rs5kI8e5oJpstL9/6tqWJrRvdnDVeuXCnz58+X5cuXy6JFi2TdunUybty4Xs8rh88axhJ5cZL7yv7t7bUqjGf0K6dCtwU01n57h47Lieh6zXfoNiiRYL9q+quuhyjQBzTuirfocD7wTy1HBMPZergpE57SF8inwDymVpdHarvOe6YuGK6iGYQB/TjeptORatLp2HUcmENkgs7fS+i4OjP6PIQFfIAdNWzAJ9g2OM8BNuC/M46uvIgRX6ej21QsArRXWURsMJf0MvqaZnIjXcD3xXVc0VbdNmLtOh0FoI3x3cH43DgoWzDnLzRXKNuO6XrsvhPMyaUyOKd9PTVWBTlkzG5lizm6LcfAYK3g6bZmG468q6Drc0xlcM5SsHp/S4/zjOKRrEuLk9pXaP/32nEqjN0QrAu0lmTqiohItksPEpG2oOF9pCPYdjsnhptn5Ct14hywEOUDl2j2eTcRbuHT6dIZSExpVbadTdXK5hmXyGZ0mVmOTsfuVu0DHtl5jLJF4rqP5ow+Wg3WBhJVeu1renyHslXZev3hL+mpgeMoqKgu4NM3dI1Rtmd3TVO27e167tTRHtRBNKco1IKxMBp3gGoHsid2OmiMTO1UYVIxXY4eiAzVezSpz01vDebdrwB5Ajr7uzeOULbZk99Wtj3Geo+IyKENuwLHYxMdKkwn8Omb2/RaYALodnVct78Z1TsDxxURrQfJqcHyyXbk5dsq1F7CaMaWLVukunpfH43HtaaKiOzcuVNc15XGxsaAvbGxUV599VV4zpNPPil33XWXrF27tpcUjh4y03JiJ/f1r8oF2tc5LcF26u3UZTn16CZl2/x6o7LVVOr20T4Z3CMw+lUmB3xpvW5H/nZczwpzLCwi8nZwYJevB6IE7uW4m/T6rpPUedrzar2ymctJboVOV7xZ63EhpePf8V5dHkfdpPt38ylB39w2QwWBmPe1RERiE7SfdIwBeFerHjC7wAe7oErQmNwC89zZdW8FjpOO9reHJncq2+PZw5XNTwA/DOrd2xLUbb9Kn1d4sVbHBTQJjU9SW3XeO6ca96KANnpgbpNpBPe1UmhxV5v8ibn+gshftk5Utrc7dL+YUtWibDmwjpiqC45FsvW6/yeajPusYC7YTTnMM/imOCGElAjL8/v8EUIIId0UWzP4WUNCCClfOM8ghBASljCaUV1dHfj1dlP8QGlvb5dzzz1X7rzzTmlo0C+2EEIIGVmUwzxjxL4pTggh5Y7l4yfauv9HCCGEdBNGM9ra2gL2eDwOF6y6P2t4zTXX9NgO9LOGf/zjHw88E4QQQoYFzjMIIYSEpZia0dDQII7jSHNzc8De3Nws48ePV+HfeOMN2bhxo5xxxhk9Ns/b+6phJBKRdevWyYwZIV+9JYQQMuSUwzyDb4oTQkiJ6P7cSG8/QgghpJswmhF2r7++PmvY1KQ/1Sey77OGd955Z1HzRQghpPhwnkEIISQsxdSMWCwmc+bMkVWrVvXYPM+TVatWyYIFC1T4o446Sl566SVZu3Ztz++f//mf5dRTT5W1a9fKlClTBps9QgghRaQc5hl8U5wQQkqF64ugfX+7/0cIIYR0E0Izwu71d6Dws4aEEDLK4DyDEEJIWIqsGUuWLJHzzz9f5s6dK/PmzZPly5dLZ2enXHDBBSIict5558mkSZNk2bJlkkgkZObMmYHza2trRUSUnRBCyAigDOYZI/ameKwpKnZi36bvTjqmwjiZ4LGd1/H4em95cRP6Bfl8Zbii8GLBirUtECYOKh81FHCuoHaTNzLhgxML2hZNa1ukQ9uSO/RFU9uDj3XEWgs6/rasslk5Hc5K57TN1Y+NWF3BdPhRXSd2h667SKuuZN8BtnjQ5kV1GC+q4y+kdLhCCrQhnXXxjQbixfV52WhU2VrylcrWGtFl5ud1fFY6mN7Ybp3+xC6d1uROHX9yp85UbA+o97TR+RzdzvyYrk8votPvJXR63bi25aqDtmy1vmZmTELZ0rX6BsHmZA1IR4hHm0B/jbQb7SzTu2+xpI/PjfR/dbIfds4Se7/+ZgGfGDH8hw++lRLpBCWPvqni6XBuQlemZYRDOoVsqAGgp+0s4HfMNmW5IAywpZq1r/Ztrb2RTp3g2M6gzW7r0hfIoYxq/Kz2MY6tKyGWCPatfJXua05eZ7SQBX5euwoltZ4uCvGiuqJQPaG2ZmfRACJYeWb7ERHx88AGfK4NwkXbgK1DJ8MksVtnKt6qbbEW3YYibRllUxoNyseLg/rM6vq0PK2hlgd022iSdk6XRR74ay+Gxgq6r6Oxn+qLoD7Nvmhlev+IUxjN6N7jrz8O9s8a+v7eXzeWowu2cmxn4LijSY/NBJzn79HOolAFnHVB17WdDtoinSAMcKWRtLZ5YPjhgHMTO4LtEs2d9hyhjVVbwj0Cnq/QeUjtCJ4b360ThmyFCp2pGPBFvq39wu6ZRj4tna6qV3T8bUfruvPRGMAYk1tRUD5g3OuidgDalRfR/s81tDEPFiByBV13NnAkrXsqlC25SZdjxPClLnjuJtquyyfWpq8ZBzYL5MGcT6UbdPxOTp8X3a7TZud0HWfGgnIzfH8O1MmWPbU6HY6u96ij6y4Z1e0qHgnaWtJ6cJJ3g+lyM73PGIZinrFixQq5+eabpampSWbNmiXf/va3Zd68ef2ed//998snPvEJ+dCHPiQ///nPB3j10tFQ2SmRin31My7VrsL8rTmonT5Ys8mgcV0W9NHOcGMPc7Dqg3aK8FLAb8bQ5EObXLNZAs2I1Ouxn/d2Utk6dqf0JWM6bXt2VQWO0RpItFKPQQu7dR9KNHYqW2VSzz3WvR38ko6X0Rlds/tIZUPrA+YaooiIHwnanDE6DbG41sFsVvtlD60J7dJjkVhrMBycg4L0Fyq0MdKlG0fXFLBmWBdsC3VVeo6Yyek8tXfquvNdMN9xdN7NsiyA8b0FfDVaZ127ZbIOBxxrZTR4zbSr87Qnq9t7RUy3W9fTeUoXdHz5WDBcFg38DHJgDaCbYmvG2WefLTt27JDrr79empqaZPbs2fLII4/0fKVq8+bNYoM5fllQsAJr9Lt26zmEOdaLjNOD+R3temzmAx+5G4zhxjW0KZtJHIxPuhK6Te4E2hVN6P5eMO9diIg1KRgOSIa4bdpfZafodAhYxzHXX0VExCiixFbdN9DaHRq/5iI6bW+erR8Qr3s1eNHEdrD2P0Zfs2a9vmZHu24vmUpDM8BajxcH2h4BY9yxut5TlVqD2gpB3c6CdRfkd/JgLSayS59bqNHp8I3xQ+QtsFAHHFK+Gs0fdLiuicD3h5jSWjW6PXqdOk/RHcBXg3yKIQcRcL+nrlLrpQOc9Ka2OmVDupruDE7aLDCuzBvtxUPz2e7zZfTfzxixN8UJIaTcsTxfLA+rSG92QgghByfF1Iz9P2t45plnisi+zxouXrxYhe/+rOH+XHvttdLe3i7//d//zc8aEkLICKPY84wHHnhAlixZIitXrpT58+fL8uXLZdGiRbJu3ToZN25cr+dt3LhR/v3f/11OOeWUA74mIYSQ4WEo1qYWL14M5xUiIqtXr+7z3HvuuWdA1ySEEDL0lMP9DN4UJ4SQElEOIkIIIWR4KLZm8LOGhBBSvoTRjLa24Jtk8Xi81203br31Vrn44ot7NGLlypXy61//Wu6++265+uqr4Tmu68onP/lJ+cpXviJ//OMfpaWlZYC5IYQQMpRwbYoQQkhYykEzyvRbJYQQMgrw+vkRQggh3RRZM84++2y55ZZb5Prrr5fZs2fL2rVr1WcNt23bVqzUE0IIGU5CaMaUKVOkpqam57ds2TIYVS6XkzVr1sjChQt7bLZty8KFC+Xpp5/uNQlf/epXZdy4cXLhhRcWJ0+EEEKGBq5NEUIICUsZaAbfFCeEkBIxFE9WHax7/RFCSLnDzxoSQggJSxjN2LJli1RXV/fYe3tLfOfOneK6bs9DU900NjbKq6++Cs958skn5a677pK1a9cOIPWEEEKGk3J4648QQsjwUA6aMWJvilduEnFi+45rNmRVmEx9MPmVm/Qm9GjT93WX6cnehMd0UeSq9NbwnRONl+vB7vEFF1wUPCWBNp73IvpcqxAMaYG4ou06tmgnCqfjT+5xdbi2oC3anlNh7C5tEwvkKqrL1peCDhfiPAgqW0/nyc4Fr2lHHRXGTUWVzY/oPHlRbbM8ZDPToIJIpEN/sMHPhPuIg627hUQ7gumIt4A636ULLbEzr2yxHboRWR26n4lrxOeAPMV02dqgjr0cCJcAtlzwmpEuXZ/RTp2OXIuup0JKh3Pj2uYZl7BBM3aMOnGzqKfvhXv9FY9IhyVOfl9ZZxq1D0g0BysQ1Z+H3A7wMb5ubhIHbcsykoH8N7K5MW1DII0Tw2bndaBIWp+WrdX9rGKbdlpWFvjXXcHPb/ppfQE/p30MCieOLlxr/FgdLNO/jqB+bOd0eThZ4LMMkxvX9YvqKV+hw9kh5czOB8/1wXk+cCmRjDYmdup8Zsboc802mtwdUh9aQ44LfDCuMcZJfgRoHliX94C2oPaNbJah25GMjh8NzHwn3BgA+QTPCabDdsEFjOJ2QVvsSV4ZTDxGCvl0TFzZ14HrxrSrMHs21gWOk026krNHAx/WqRuv36E7M/LfjjFmQGG8mDbmbd22ImC4BjUuRLqQXuaT+pqpHdp/RDu0ZsRagz4lsr1NhUHEkI606nPtmdOVLTMmFYwLXLLlSJ35yvW60Dqn6jz5xhzOB/3dRd3b0df0gCsteFqjY9XmRECXv2cOXkUkv75K2Wo3gmumtM0x0hbfo9MfTWtbrF2XWaRT27oatbCaemwei+jxt4hI50RdB9l6nbZCjU6H2RG8vL5oxtdpdSKgDuK6A+UK/XdGC3TGTDp4TS/d+6sYYTSjuro6cFO8WLS3t8u5554rd955pzQ0NBQ9/uFmfsNGiVfu64M/WaMfOI5vDfbRaJtuf//2r/+nbP/9yw8qGxzeg3UiL2rYwNjJt0FsceTDwBgF+CfLtIE1kEIWrP/U635gOaD9gvikPRifVaXjKuTB/KFGO9P0nqSy2aCMap5MBI7dhE5X9SZdjk0LtK+oWafPzdUEw/lbtcOF44RKsLYD5qC2lkupejuY3s7xusyQfy1Ualt6AvDpoLwLuWDdNW2tU2EElL8FbI3jWpVtTzsoN7PgUDNL6zYaq9OTAxdoeWGXbkPPv3VE4NhJgzqfrMsnXqFtFUktaImobvPNXUHfPbFCl8/EeNCWjYKG8Xc4zygeyU1RceL7NKHrELBOZKwDO2DtUmbqOrWTui2Mb9DhMnndxsF0QeGDxYZkpW6TEeC/3aj2C7lc0M9YIP5IvZ5PZVsTyiZxfc1CTNtilcF+ldkD4gJNugD6u5UPUWgiUv+HDYFjd9GhKsykP+jJx55j9Bis4S86cbnKYDrylTpdmYZw9xGy43Ub6tpRoWx/8oPzqcPG7lRhEo72KW+8MlHZIlPBxLRTz228tqDNT4KxTxzMPcA9sZrX0do/mBsYsqTGViKSQ8Nt0DSSTaB9g3UAf3tQWC0wj2mbpy8ascE8A/Sp9jatU35XMB2xDjB2MNYihntt6kBe8rvzzjvlhz/8ofz1r38VEZE5c+bIjTfeGOqlwG74+XRCCCkVvt/3T/bu9bf/L5sFavl39t/r75hjjpGVK1dKKpWSu+++u9dz9t/r79BD9cCNEELICCGEZhBCCCEiUlTNaGhoEMdxpLm5OWBvbm6W8ePHq/BvvPGGbNy4Uc444wyJRCISiUTkhz/8ofzyl7+USCQib7zxxqCyRgghpMhwnkEIISQsRdaM7pf8li5dKi+88ILMmjVLFi1aJNu3b4fhV69eLZ/4xCfkD3/4gzz99NMyZcoUed/73idvv/126GvypjghhJQIy/X7/Ilwrz9CCCF7CaMZhBBCiEhxNSMWi8mcOXNk1apVPTbP82TVqlWyYMECFf6oo46Sl156SdauXdvz++d//mc59dRTZe3atTJlypRB548QQkjx4DyDEEJIWMJoxlC+5PejH/1ILr/8cpk9e7YcddRR8v3vf79nbhKWEfv5dEIIKXe41x8hhJCw8LOGhBBCwlJszViyZImcf/75MnfuXJk3b54sX75cOjs75YILLhARkfPOO08mTZoky5Ytk0QiITNnzgycX1tbKyKi7IQQQkoP5xmEEELCEkYzzIdgly5dKjfccIMK3/2S3zXXXNNjC/OS3/50dXVJPp+X+vr6kDngTXFCCCkdfX1WxOdef4QQQvYjhGYQQgghIlJ0zTj77LNlx44dcv3110tTU5PMnj1bHnnkkZ4Hcjdv3iy2zQ8REkLIqITzDEIIIWEJoRlD+ZKfyRe/+EWZOHFi4Ou5/TFib4pXbCtIJFroOY7t6FRhIqv6L5j/eOMlZfvmjONCpcE7ebayWW7SOAbnRbXNjesN7BGWFyIMuKaT1Q0x1qEji3RpW7SroGyOYbPy+qJeXGfUckHiLDA5jumm51v9l5GFOlwBFBq4pO8F4/cdfT3fBmlAT76Eq06x88FznQw40Q8XmQ2KNqK7hUQ7g9dM7NEnxnfn9Xk7O3RkO1uUyesEF/WMOoiCthGPaVsioWy20cf2xq9NZl05OV2O8TZdd9G0DudFwtnMz0ah9hJvCfadQiGnwvTE5/X+KaoDfRp3MHv9deP9vR4jkYisW7dOZsyYcUBpKCV1C5olUrFPYKOObvctr04KHr9D101yk26nuWpdF8hXx1p1e4h2GeeCarV1dxQHqTNyH8DXmfE5OX3ReJvOQKxNJ8Rp02VkZ0CbjjiBQyuVUkGsmNYaL6+v6WUyyub/Veu9XVUVTCvyMUngT6K6cN36SmXLVwcHbPkqR4WxXCQ22oR0u1ChK9TVWVBEgatO7AYXBab4Hm1M7g62BaQPkQ7QDlq0FliobRR0vYtZVzGtGRGg7XZOh/NtPbCOdqGyDdZfaotOV6ZW13FBN2U4foB927iEB/q16etd0F/3D1sszTjY8T0rMC7cvbVGB4oYdZPUZey7ui0kt2u/0DVFt7fGabuVrfWZcTC9gWSBcQzyCy6YcyJfpLQFNKWKZt0fK7foz59Fm9vABcAlu4J+3k+nVRivVcfljNUP8VmGFoiI2E+uVbaJuw4PXjOq+/uYF5RJ3EpdkPmXtC/KVQfj2310uGl2tkFXSrRNtyE7r+s90mWkA9RdtAOMAYAPM+csInpOISKSMHQksUtrBvJHkQ4w2AFUbAX9zBiT5yt0ndjAN1Zv1LaOgi7bjoS2xZqD9Tfpcd3eN/yLFu18pe7rhbxua35G25Kbg/WZPkSXmZ0Ixu+B/HQzFJqxePFiWbx4Mfzf6tWr+zz3nnvuGdA1RwK/eOwfxN5v3mpVaZ+Y3G7MTdO6jG//nzOUrWqXDpcZo/toZlz/8xG3Coy5gE7ZEZ1+C4xBD5ug93HsyAX7Xzqvfd3uJq2pVk63VT+K1l6ALRrMu9Wmr+mkdfwWGpNP0fOMri1aR9JHBk/2EtpXdxyir4nC7Znb/yJfcqOugEi7Dhdt0+VT0NMYyY7Rmc+OCaY31qrPK4CpUwGNfyLaVmjTebCNevGTuiycKjAfBO22adMYZUts1W3BMtqLMwOMMXZqHcmamiqC1/0qwVra9uC6Flqnc1M6/thfdTq6wLimDdRBy2HBcdL6rWNVmNWZowLHXjojIr/RiRPOM4pJ9RZPnOi+tl73mm5IzScGbROe1u3qraR+OeaNT92hbNN/eYmyzTp6k7LlvODYoy2r12NrUrrxtnXqvled0r40FdV56MwF/YLrab+ZBnP8LHLgqBkCbclvN+7boKiAXiZfB35hjPZZXlxH+PqVh/YbZscp2sGm3kBOBqxPG+51zF+11kSADrZPBbFnw61hpXcEF0Je2qEjs4E/tOt7/0R2MGD/NicL5r1AByNdOiq0tlaxDdzbMtYyCylQJ8CE8u4mtSZFQdraZwTT8e65L6swr99yjD5vkp4/dE0C48MY0O2K4DXHvqjbtm9EXwD3AnuuEUIzhuolP5ObbrpJ7r//flm9erUkwH2m3uCjvIQQUio8f+8DBfDHvf4IIYTsRxE1gxBCSJlDzSCEEBIWagYhhJCwFFEzDvQlv/255ZZb5KabbpLf/va3cvzxxx/QdUfsm+KEEFL2eNL7VwdCfDXChHv9EUJIGVNkzSCEEFLGUDMIIYSEhZpBCCEkLEXUjP1f8jvzzDP3RvH3l/x6+0KViMg3v/lN+frXvy6PPvqozJ0798AuKrwpTgghJcPyPLF62TPBMj9JHwLu9UcIIeVLsTWDEEJI+ULNIIQQEhZqBiGEkLAUWzMO5CU/EZFvfOMbcv3118uPf/xjmTZtmjQ1NYmISGVlpVRWgn1lALwpTgghpcLz8ObU3f8bAAfrXn+EEFL2DIFmEEIIKVOoGYQQQsJCzSCEEBKWImvGgb7kd8cdd0gul5OPfOQjgXiWLl0qN9xwQ6hrjtib4vHfviARK9pzPFAJPi2pN4X/ZshzP/y9x5Tt/05/R+DYq6lQYbLj9RMJdl7nwI2Fe2PTCvEpfhS/hWyujsxyQekaDdh39DcR/KijzyvocB4KBz6x4DvB8rDQHgQh9yVA6RUraPOiuvz9CEg/sKG0OTlQtkbROlmdLL+3z02Y8ed1/JG0tkU7gm0+2pbX57Wmlc3q0DbfB23DAgl2jDpGDrBQCGWz8simXZWdN/o26E5WQRttUAdhUf4eFIUbD17T9/qoYH6iqmi0rW4UJ57oOc7V6b5hjw0e174YU2GydTruxC5dSYldOv620zqULfZoUCNgP85oWz6p2240DRoFcIlmO410aR2MZLTNzmob6lcCvjbgJ+NGIpCTBz6yrlrZHKRJAD8a9At+RKfLQ1qAogfptXPB8oikQZgC0gJUd0BbWvW5XjQYzo2qIHBM4GSBFoD24gBbtCPoc532jL5mFvhlVE+o3sOAzsvmtC2mtcABY52C6HFHrDMYLr5ba6OpnyIC+xgaK/i2trmJ/svDPK8A8tMDNaNoWO0RsQr72pMNhijrP7EycHzkDz6j49mldaSQQmNtXXHNG+uVLRYLnusmdVw5MKzuTOm2m9qoHQjKZ3J78BpoPIt8TCGlExKNgqllQafNTwTLzYrouJxUStm8+ipls9v1+NWefYyy5SuD1yxU6vJx47qeCkCPCyCca8hgRLtSKSS0LbFd5x3VU3x3/3MgH2g28lc2qGMbuL/ELjBWMM61C0BrOrT/ztfqzMd26brLN+hwybfbA8dj0rqA3IRue+mxuo5jbah96zIy52xorn301zcqm8S1T9h90iRly9Tqymo7LHiNWLPOU67emM+mQcX1/FOoGUWi7hVfnP38cxy0o0hnsNF0TNJtofYNXfDZal1JXdP1GMVK6Lr2jDFnNKXPSyR1f5xa26Jsr25tVLbX3tI2a3cwX5F2nf5IXJmkUKP7bbxJt3EPyUhlsNzsPJpnaFO+BvgwEG7CkduVbfvu4BzFa9X1Gd+p+7HlaVtEuzrVN+N7dMJqX+tSNqcLtI2cLtuW4/QYw9S4KJojduo2mhmjdSpXqW150JY7p5hrNjqfbqeu9EilzqfXBcr7+Dadjo5gA4xsSupr1um819fruXxrm17vla1ap6LGqTUbdPxOVpdZ63GgPjNgrg3GefarwTZqg3ljrMuYW4I1gB6oGUWj6qd/DtzPkHnHqTCNVnCcm1qn/VDloXr8MPO2y5Vtw5W3K9txz56jbGMqgj5lWvVuFeaYym3KdmyNtm3L1Cjb7qweuyciQf9UG9cOMeZoH/aKpfUnk9Z+2KoAawE7g33erwRrGV1Af05oV7ZUVJ/bsUWvYaXeCvZvqFNbdfpdbZJ4i/aT5j2ObI32E+h+Q+UWbevKa1+E5q+2dk+KPJjjehHg54Hvt9I6HWjNzSTeom3ovkoBvCycBWt1mYZg2qJgXIN8Y+JloC1o/KODiVQHC9cGg5jtH9aTyUn36gbTfoTOU/ItXbaH/HJP0JADFbynNXBY8MCaXDdDoBkH8pLfxo0bB3aR/RixN8UJIaTc4SeqCCGEhIWaQQghJCzUDEIIIWGhZhBCCAlLOWgGb4oTQkipcD3p9RGqkG/LEkIIOUigZhBCCAkLNYMQQkhYqBmEEELCUgaawZvihBBSMnz4Seme/xFCCCE9UDMIIYSEhZpBCCEkLNQMQgghYRn9mjFqboo/unWtsi2aOLvf88KEObD4gxsyOI3jVIjkbr0PhTu2VtkcsKc13KPTwEvozWIsc39l0Xt09xpfTO814EeD10B7ZaK9tlE4tCetj/YPNUwofoQFOiFMh0pDqOjxHgmgf6O9Zc09AeGe7iD9cK+/nG4bNojPNvYJctJ6nwgf7Aks5p7AImKBcrTiYJMME/SpjBjY5Mjci1z0PpO9YWeChQvLFuwxLKhtgD3iw/TFXL3eT6ripeB+OwWvj03MXU8E7dsugsuQ9ErNRlci0X1tv7NR1/0L1wX3Wjrmdr0f04SndX1FVq3RF/yH43Ua3tRtfOfMYHure13vSeTFwJ7le8A+ZGAPMA/oiGP4CrTfpwX8CfKbFvAVXhL0ZdD/VPwgrWH7IwoXys+jMGH3vTaSi3ywVQD7kKK938PqjZl1lHzkr1CZgTpx0L7xpq9Be8bH9XDRTwJfDdoawnKD6fCjWgvcSq01XhyEA7ZYq+5nEXOPW9AOnA6wN25rp7IhnULpLRh7GKP6jDcFNyEsuNSM4aDuZUuc/XxvepyunMNXfzpw7Cd0n/r6Pz2gbPe8/U5lu3jyH5VtjKP3rrxly6LA8WtPT1Nh0L7RtS/rPor2L4t26TyY+7ei/axdoFOpDqBnKdA3wF7P5gS6UAF8TNg5ha33GY+AfVILyaCv8KLABwA/H2vXcUU7gSYZrii1M+Q8BvhvNE9C8wyzLfTyBTsNkAw0FkFzsUiH4b9BPXVN0XuwmueJiLQfpusu1gL2bjwsuG9luh6MTUAzQ3Xsgn3dG5/Taat8I9g/7c16P82Cse+eiIhTqfNe96K+aPvRer/fSDqYL1SfbYcGM+pm+ljOoWYUDS9qibVfe6p4o0WHee3NwHFtAThTANjyUhofnazjr9f9pVAVHHt0TNWOP1Ov2+TGyjplq29C+4kCzTD6Vb5S97Oc3m5VrIJuq04G+NIK4KAM/xpv0j7ABtteWtv1Ne28tnXYutyqssF0oL1VoV/Ww9JQa8NobWr3sXp/XjQGQDbkvz1jCtcJ1gELKR1ZAdSJFwNzSbCPrBi2Q6c3qyA7OnQb7WgBea/VfaoA9saNpYKNIXKk3pc1v11fM/1sg7KlwH7wk363R9n2zAzq1NZ36fKff+IryjazaquyXVCr1x7et+YSZbOMfXDTXWA975VgOfY5TqBmDBlOu26D1a8E26mf0mOFeJsu93G3P6Nss7J6XcsCfmH1v/04cPwPaz+iwjy36RBlO2n6mzrcxmnKdsxkPVY6pDK4b3nS0c50d0739ym1Lco2YUKbsr3aou/JNHvBzHs5sO4c12O/zC6goaDpOzndv819qSPAdzhgv2wbLM8gXbWMcMjHm2FEsCZF9fKGRLr6n9ugsbazDYmeBs0vka7mzUER0EYno7UmXwXWVHfocB1Tdbh8XTBxlZt1nmxXZz5bB9JRoyth4Zy/KduxlW8HjlsLug/EE1OVbcu5umNXP6PbbevxunC7pgUHZxV/bVJhCjt3BY5dv4+N5ctAM0bNTXFCCCk7PFdEwMil53+EEELI36FmEEIICQs1gxBCSFioGYQQQsJSBprBm+KEEFIqPF96fXQcvflJCCHk4IWaQQghJCzUDEIIIWGhZhBCCAlLGWgGb4oTQkip8HwR6e1zI6NDRAghhAwT1AxCCCFhoWYQQggJCzWDEEJIWMpAM8JtOv137rjjDjn++OOlurpaqqurZcGCBfKb3/ym5/+ZTEauuOIKGTNmjFRWVsqHP/xhaW7We8YQQggREdft+zfKoWYQQkgRoWZQMwghJCzUDGoGIYSEhZpBzSCEkLCUgWYc0JvikydPlptuukkOP/xw8X1f7r33XvnQhz4kL774ohx77LHy+c9/Xn7961/Lz372M6mpqZHFixfLWWedJX/6058GndBFE2cr26Nb1/YbZjDxh8FKxJXNTyW0LaqfP7Bc/USFH3V0uEL/G9S7FdF+w4iI+LalbF5E23zDhs7zdVLFc1A4YNMmFZ+PHtkAD5v4NihbHwU0w+ggFipqEM52tRGda+eD4Sx0Xkibk9cXsAr9P33jR0KWT1S7A98BlQDarQKdVwBO0QINAcQPQonkjHCgn4AmikFtDbQrLxEso/jOtD4vYlzV6yMVvr/319v/RjnDqRlVf1wvESvWc/zHv/1BhVnw75cFjqc/s1WFKby5Udlazl2gbDVvdClbvkrX9eSfbwkce7WVKkzr0TXKVrlJty3Ul50M6FeGz4V+Avlg4OfdhPYLSLss80lA8GQgSj8C+ywQzkxv2Mf8gAvzYkijgxdF5YPKUZWFiPggIApn+mZYd2F9A/CvSI8to+Cw9gIjij8G/B0oNyvv9nksIuK0ZbUNJC2K2hUcA/Rfbm6FHtPZGTC+Anl3OnR67VwwX4UqHb94Xt/H+0PNKJpmRDK+OPv3QTAwtd9MBo4TO3WYZbd/QtnQ+PiWbVOUbc/ROr5YqzH+btT1mtqKHI8Ol9itbWj8ao5znZwO48b1NdONuj1HutBYFYzPjDGtkwXtPqROIb+Gxu6mXtp5UOfA58J5DHI7BcMA6gTqCMACGfDBrN2cw6H40ZwiTDsQweWRqw0mxI2Ca4K4CgldaGg+iDQj0hmsu7rt2t+iuXZ0hx6rdR5WrWypt3Q4y1jMyc6ersI4XWali3igvVvpvL7mVj3Os7zgGoIFhnhj1rYHjgtuVt7UwfZCzSiaZox5+LXAPMPds6eYWVEUtrylbNY27QQixxwWOK5/arcK03XkOGUz13pERDzQl30w3vEiwbbjgb4XSevzktuVSVy9bCZ2Doyx0kFh9WIqiBRS4fwm9N+OPtczzvWjQKfAechmxfpfP7HscH7Zh45TY6M8uYZmFMJNnlDaoAtBacsFr7Hh5QkqiJMB/juno3KyKO+6MZjJ6JyqI7MS2sGmJ+vYU5t0I3r9XD1394zpQrJJ52lSskXZfr31WGW752//oGyFvB5cxpNBbZkwplWFeeIz9wSO29o9qfuGCrYXakbRNMNOxMXeXzNeeV2FiYxvDBy3/8MhKky+AvjDI2YoW043SWl8Vo89pv/momBcST2OiUZ139jYXq9s/zB9g7I1xDuUbVI8qJdZs7OISE1Ej4k2to1RNi+py+OYOv3gwvTqoBZ25PWcJV3Q6ejIa3+Szulw2bz2C7lsMFwup/us36VtNvB/SEOdTNDm6KGw2LrKxdJVDMeXcF3LCIfmiHDtCEUPronSW73ZM8KAeQxaI9umTfkKXd6FCpA2Q6faDgf5BJMbr0JnKvmWbi+r1h2pbL+3jggcHztVZ+DIBj1wWvOKno90HALmHnGdti0Lg+17qjtehUka94V8Nyu9TjTKQDMO6Kb4GWecETj++te/LnfccYc888wzMnnyZLnrrrvkxz/+sfzjP/6jiIj84Ac/kKOPPlqeeeYZ+Yd/0MIuIpLNZiWb3deb29raDjQPhBAyOnE9Eb+3z42EePhghEPNIISQIkLNoGYQQkhYqBnUDEIICQs1g5pBCCFhKQPNOKDPp++P67py//33S2dnpyxYsEDWrFkj+XxeFi5c2BPmqKOOkqlTp8rTTz/dazzLli2Tmpqant+UKfpNCkIIKUd8zxXf7eXnjY7PjYSFmkEIIYODmkHNIISQsFAzqBmEEBIWagY1gxBCwlIOmnHAN8VfeuklqayslHg8Lpdddpk89NBDcswxx0hTU5PEYjGpra0NhG9sbJSmpqZe47vmmmuktbW157dly5ZewxJCSFnR/bmR3n5lADWDEEKKBDWDmkEIIWGhZlAzCCEkLNQMagYhhISlDDTjgD6fLiJy5JFHytq1a6W1tVUefPBBOf/88+Xxxx8fcALi8bjE42CPRUIIKXdct5cNXUTEHx1PVvUHNYMQQooENeOAoWYQQg5aqBkHDDWDEHLQQs04YKgZhJCDljLQjAO+KR6LxeSwww4TEZE5c+bIn//8Z/nv//5vOfvssyWXy0lLS0vg6arm5mYZP15v3l4MFk2cPSTxdnPjhueU7UvT5wWOvZoKFWbbe+qVbcLq3crmJaLK5jv65X0vErT5kXAv+PuWBeLXNgs9wFEIGkFUcOsAOwsiQ8lF1zRsfgSk1Qv5tEmY7QtAnix3aJ9mQfHbg7imb4MyMo69OOjmUSdc/KC9OFnt3HxQlgqUVpR3tPcEaIBWIRgO9gsQv4WeWALX9GK6jMxrorJ1zLj62EvDd13xexERf5SISH8Ml2ZYNVVi2fsmJP/wH5epMPWPvRE4LuzYESru2v/Rn8yyq6qULf/BY5UtN7UhcOyk8ypMzSutodJRqE4oG/LfntHXLNAG/diAd08RHzl1MyHIxYC+B30YylNUX9POGz4AOXWUTWCzC7qMLDN+oM9QU5HfscP5ecvQXlQ+ULwQqLyBn7SMukL+HGovGhggwLm+6TuRTiEGIdGWO7B9jdyapLJ5QENNfRARKVQE8xVty+kL7Db6vwfC/B1qRvE0Y/sCT+zkvjob9ycwZv74zsDxmC/qet98hh7zN7xU0HGhoViHNlZvCtZj/cu6XaUb9HnJ3fqa2RrQTkE3iGSCRktHJQ4a3wPcuC5H29Z+p1ARTJtdCOkjQTP3osAPR0LEB1xYAWgNGjeiOZaKDyTBR9oIovKAtqD5glmfHogfzaeQdiHfDzXIMsPoIB4at4ccA3SN1e3bHBd4jXoOjfJuHaIXqlE+MzV6TGfmC45NnJi+ZhGnkrtm6ciO+m6wE1hu776fmlHEtaloVMTW7a4vHt26VtnCrl+FPdfZ3R44fvmGCSrMpN/ozpGrBGtOujmLG9MdxjOKoaCHSTCudCOIPwHm4MAmsaDNjur2G41r8YrHgC2iz01G9fwsGcn3eSwiErF1XBF7YONNG4hGxtX+MOdpW8HT9ZlF57rBtpB3ddtI53Q7LxR0uEJe29yctvmOMbcB3cgFi3dIp0DWxQHDZjsbPDf1hm6QNhjr2LqKYbhYK5iXmhoNNO8PK/S+1GhcVofcM5qK+cF8WS26M37pa8cHjrMdeRF5E1yAmlFMzdh60Wxx4vvWbtA8oGAs7Uz96lMqDHCv8sod85St+lUdbtdxurM5e4INbvKUnSpMa1qvOU2q0OtVtdG0ssVBh9mTr+g3TBS0uzMm/UXZ8sgJALJGgbcCoUq7unyQf92V1fd8OvPap7RlguXWmdFhsg4Yv0b1NfNgzcYz1qI8ML63gT+Mdmob8mtOBswzjHBA8uAcDvk15MPg3Ma4BpormOOQXgF+uPFZMB8x9AbNKVB5o7rzwbpfcruej5hzvU1rDgXX1OmoQHM9tOa5S/fjXG0wbdvfoQsydmhwHOnmMr1JRlloxsBXxf+O53mSzWZlzpw5Eo1GZdWqVT3/W7dunWzevFkWLFgw2MsQQkj54fl9/8oQagYhhAwQagY1gxBCwkLNoGYQQkhYqBnUDEIICUsZaMYBvSl+zTXXyOmnny5Tp06V9vZ2+fGPfyyrV6+WRx99VGpqauTCCy+UJUuWSH19vVRXV8tnP/tZWbBggfzDP+gn4ggh5GDHd70+nqwa2FPmIwlqBiGEFA9qBjWDEELCQs2gZhBCSFioGdQMQggJSzloxgHdFN++fbucd955sm3bNqmpqZHjjz9eHn30UXnve98rIiL/9V//JbZty4c//GHJZrOyaNEiuf3224ck4YQQMurxPen1W/+jRET6gppBCCFFhJpBzSCEkLBQM6gZhBASFmoGNYMQQsJSBppxQDfF77rrrj7/n0gkZMWKFbJixYoBJ8j/+zf/C5If1N6RxaCjHewP6Qc3mrHdrArjZjP6PBDOK6C9mcE+PObebXA/V22Ce4qH3ItU77cG4kJtHNkGuqc42PRtyPcUH+JPPAxmT3GYNrT9dpg8hMwnqgMf7F0XKja0dzDME9owCZiMcoP7O4KHlkLvKQ76p3muVwD7HnrBvl74+/6wPrhu3s2IjxIpf/eBo5xh1QxjH143B/ywGcYfeBnbvt6wx82DaxaC7cF39TXD7nVcAHv/oD7kGfu32khrwB6voRmo3xnMnuIW2LvN2MMZ7n06iGyae0T7YLMeqFPIp4dMR7g9xUOCfB26pqm9w7CneNht0RVDvKc4qmOkGR4Yq8E9xQ2NsArab/iGX6Jm9E4xNcPLBP013Aezy9BzsPcmHPPnwZ7ioO26WbAvaN6o37xuV24Onaev6YL9PuH42LgG2lM8LGhuYOdBHzKcwKD2FAcOBfphFUib0FiyqHuKo/JHe4p74fYUNzXCDelckc4i3w+1y5yXDqYc0RgA7S9oHKPygXuKh8wn2vsw3J7i4a45ULy0jsxcU+g+pmZohnSeEWIO0RZiLWnQ5xrzTi+NNAnt/Qz2KwXpQD7FnDa7wE+AqbV4oHN4wFF6yFGaYzhTK0XEdYEOgjmQC/YUL4D9wk1bPgK0HWyw6hdxT/E8GLvmQeG6wCcWwJqNOY5xgRNzcyD+AmgvoF0hm583zi0Av58HjjlsOLCHrp8LhoM+Hox1UBeD26Ki+NSe4joMGsPAtIXeUzxoLIAx4949xPc77tx7TM3QFFMzzLUoVKeu0ZzD6gPy82hOAZfrM0FroRPcz8jofpbv1B0tB/xmBNgcMyVgQ2sbdAQfhMuHXMfOGe07B9Ztc2C8mQP+NZ/T+y4XwNzGzRpj8oyOy0vrhuCBurPSYE0iY9iyup5QE7J1FWMfk+t/T3F0nvJ9IrjxDXBPcQRc0wegMQz0uYaGomEInHugeQYYE/loSm7uY46KEZV32D3FAZ6xb7zZZkVE3Jx5vNfflKtmWD7KWQl56623ZMqUKaVOBiGEFJUtW7bI5MmTRUQkk8nI9OnTpampqc9zxo8fLxs2bJBEIjEcSRyVUDMIIeUINWNooGYQQsoRasbQQM0ghJQj1IyhgZpBCClHylUzRtxNcc/zZOvWrVJVVSXt7e0yZcoU2bJli1RXV5c6aQdMW1sb019CRnv6RUZ/Hpj+vU9Utbe3y8SJE8Xe7+3cTCYjuRx4tHk/YrHYiBaQkQA1Y+TA9Jee0Z4Hpp+aMdRQM0YOTH/pGe15YPqpGUMNNWPkwPSXntGeB6afmjHUUDNGDkx/6RnteWD6y18zDujz6cOBbds9Tx9Yf//0WnV19ahsgN0w/aVltKdfZPTn4WBPf01NjbIlEokRLxCjAWrGyIPpLz2jPQ8He/qpGUMHNWPkwfSXntGeh4M9/dSMoYOaMfJg+kvPaM/DwZ5+asbQQc0YeTD9pWe05+FgT385a8YgdtskhBBCCCGEEEIIIYQQQgghhBBCRja8KU4IIYQQQgghhBBCCCGEEEIIIaRsGdE3xePxuCxdulTi8XipkzIgmP7SMtrTLzL688D0k+FktNcX019aRnv6RUZ/Hph+MpyM9vpi+kvLaE+/yOjPA9NPhpPRXl9Mf2kZ7ekXGf15YPrJcDLa64vpLy2jPf0ioz8PTH/5Y/m+75c6EYQQQgghhBBCCCGEEEIIIYQQQshQMKLfFCeEEEIIIYQQQgghhBBCCCGEEEIGA2+KE0IIIYQQQgghhBBCCCGEEEIIKVt4U5wQQgghhBBCCCGEEEIIIYQQQkjZwpvihBBCCCGEEEIIIYQQQgghhBBCypYRe1N8xYoVMm3aNEkkEjJ//nx57rnnSp2kXnniiSfkjDPOkIkTJ4plWfLzn/888H/f9+X666+XCRMmSDKZlIULF8rrr79emsQaLFu2TE488USpqqqScePGyZlnninr1q0LhMlkMnLFFVfImDFjpLKyUj784Q9Lc3NziVKsueOOO+T444+X6upqqa6ulgULFshvfvObnv+P9PSb3HTTTWJZlnzuc5/rsY3kPNxwww1iWVbgd9RRR/X8fySnvZu3335bPvWpT8mYMWMkmUzKcccdJ88//3zP/0dyHyZ7oWYMD9SMkQc1Y/ihZox+qBnDAzVj5EHNGH6oGaMfasbwQM0YeVAzhh9qxuiHmjE8UDNGHtSM4YeaMXBG5E3xBx54QJYsWSJLly6VF154QWbNmiWLFi2S7du3lzppkM7OTpk1a5asWLEC/v+b3/ym3HbbbbJy5Up59tlnpaKiQhYtWiSZTGaYU6p5/PHH5YorrpBnnnlGHnvsMcnn8/K+971POjs7e8J8/vOfl//7v/+Tn/3sZ/L444/L1q1b5ayzziphqoNMnjxZbrrpJlmzZo08//zz8o//+I/yoQ99SP72t7+JyMhP//78+c9/lu9+97ty/PHHB+wjPQ/HHnusbNu2ref35JNP9vxvpKd9z549ctJJJ0k0GpXf/OY38vLLL8u3vvUtqaur6wkzkvswoWYMJ9SMkQU1Y/ihZox+qBnDBzVjZEHNGH6oGaMfasbwQc0YWVAzhh9qxuiHmjF8UDNGFtSM4YeaMUj8Eci8efP8K664oufYdV1/4sSJ/rJly0qYqnCIiP/QQw/1HHue548fP96/+eabe2wtLS1+PB73f/KTn5QghX2zfft2X0T8xx9/3Pf9vWmNRqP+z372s54wr7zyii8i/tNPP12qZPZLXV2d//3vf39Upb+9vd0//PDD/ccee8x/97vf7V911VW+74/8Oli6dKk/a9Ys+L+Rnnbf9/0vfvGL/sknn9zr/0dbHz4YoWaUDmpG6aBmlAZqxuiHmlE6qBmlg5pRGqgZox9qRumgZpQOakZpoGaMfqgZpYOaUTqoGaWBmjE4Rtyb4rlcTtasWSMLFy7ssdm2LQsXLpSnn366hCkbGBs2bJCmpqZAfmpqamT+/PkjMj+tra0iIlJfXy8iImvWrJF8Ph9I/1FHHSVTp04dkel3XVfuv/9+6ezslAULFoyq9F9xxRXyT//0T4G0ioyOOnj99ddl4sSJcuihh8onP/lJ2bx5s4iMjrT/8pe/lLlz58pHP/pRGTdunJxwwgly55139vx/tPXhgw1qRmmhZpQOakZpoGaMbqgZpYWaUTqoGaWBmjG6oWaUFmpG6aBmlAZqxuiGmlFaqBmlg5pRGqgZg2PE3RTfuXOnuK4rjY2NAXtjY6M0NTWVKFUDpzvNoyE/nufJ5z73OTnppJNk5syZIrI3/bFYTGprawNhR1r6X3rpJamsrJR4PC6XXXaZPPTQQ3LMMceMmvTff//98sILL8iyZcvU/0Z6HubPny/33HOPPPLII3LHHXfIhg0b5JRTTpH29vYRn3YRkTfffFPuuOMOOfzww+XRRx+Vz3zmM3LllVfKvffeKyKjqw8fjFAzSgc1o3RQM0oHNWN0Q80oHdSM0kHNKB3UjNENNaN0UDNKBzWjdFAzRjfUjNJBzSgd1IzSQc0YHJFSJ4CMHK644gr561//Gtg/YbRw5JFHytq1a6W1tVUefPBBOf/88+Xxxx8vdbJCsWXLFrnqqqvksccek0QiUerkHDCnn356z9/HH3+8zJ8/Xw455BD56U9/KslksoQpC4fneTJ37ly58cYbRUTkhBNOkL/+9a+ycuVKOf/880ucOkJGLtSM0kDNKC3UDEIGBjWjNFAzSgs1g5CBQc0oDdSM0kLNIGRgUDNKAzWjtFAzBseIe1O8oaFBHMeR5ubmgL25uVnGjx9folQNnO40j/T8LF68WH71q1/JH/7wB5k8eXKPffz48ZLL5aSlpSUQfqSlPxaLyWGHHSZz5syRZcuWyaxZs+S///u/R0X616xZI9u3b5d3vOMdEolEJBKJyOOPPy633XabRCIRaWxsHPF52J/a2lo54ogjZP369aOi/CdMmCDHHHNMwHb00Uf3fDJltPThgxVqRmmgZpQOakZpoWaMbqgZpYGaUTqoGaWFmjG6oWaUBmpG6aBmlBZqxuiGmlEaqBmlg5pRWqgZg2PE3RSPxWIyZ84cWbVqVY/N8zxZtWqVLFiwoIQpGxjTp0+X8ePHB/LT1tYmzz777IjIj+/7snjxYnnooYfk97//vUyfPj3w/zlz5kg0Gg2kf926dbJ58+YRkf7e8DxPstnsqEj/aaedJi+99JKsXbu25zd37lz55Cc/2fP3SM/D/nR0dMgbb7whEyZMGBXlf9JJJ8m6desCttdee00OOeQQERn5ffhgh5oxvFAzSg81o7RQM0Y31IzhhZpReqgZpYWaMbqhZgwv1IzSQ80oLdSM0Q01Y3ihZpQeakZpoWYMEn8Ecv/99/vxeNy/5557/Jdfftm/5JJL/NraWr+pqanUSYO0t7f7L774ov/iiy/6IuLfeuut/osvvuhv2rTJ933fv+mmm/za2lr/F7/4hf+Xv/zF/9CHPuRPnz7dT6fTJU6573/mM5/xa2pq/NWrV/vbtm3r+XV1dfWEueyyy/ypU6f6v//97/3nn3/eX7Bggb9gwYISpjrI1Vdf7T/++OP+hg0b/L/85S/+1Vdf7VuW5f/2t7/1fX/kpx/x7ne/27/qqqt6jkdyHv7t3/7NX716tb9hwwb/T3/6k79w4UK/oaHB3759u+/7Izvtvu/7zz33nB+JRPyvf/3r/uuvv+7/6Ec/8lOplH/ffff1hBnJfZhQM4YTasbIhJoxfFAzRj/UjOGDmjEyoWYMH9SM0Q81Y/igZoxMqBnDBzVj9EPNGD6oGSMTasbwQc0YHCPyprjv+/63v/1tf+rUqX4sFvPnzZvnP/PMM6VOUq/84Q9/8EVE/c4//3zf933f8zz/uuuu8xsbG/14PO6fdtpp/rp160qb6L+D0i0i/g9+8IOeMOl02r/88sv9uro6P5VK+f/yL//ib9u2rXSJNvjXf/1X/5BDDvFjsZg/duxY/7TTTusREN8f+elHmCIykvNw9tln+xMmTPBjsZg/adIk/+yzz/bXr1/f8/+RnPZu/u///s+fOXOmH4/H/aOOOsr/3ve+F/j/SO7DZC/UjOGBmjEyoWYML9SM0Q81Y3igZoxMqBnDCzVj9EPNGB6oGSMTasbwQs0Y/VAzhgdqxsiEmjG8UDMGjuX7vl+cd84JIYQQQgghhBBCCCGEEEIIIYSQkcWI21OcEEIIIYQQQgghhBBCCCGEEEIIKRa8KU4IIYQQQgghhBBCCCGEEEIIIaRs4U1xQgghhBBCCCGEEEIIIYQQQgghZQtvihNCCCGEEEIIIYQQQgghhBBCCClbeFOcEEIIIYQQQgghhBBCCCGEEEJI2cKb4oQQQgghhBBCCCGEEEIIIYQQQsoW3hQnhBBCCCGEEEIIIYQQQgghhBBStvCmOCGEEEIIIYQQQgghhBBCCCGEkLKFN8UJIYQQQgghhBBCCCGEEEIIIYSULbwpTgghhBBCCCGEEEIIIYQQQgghpGzhTXFCCCGEEEIIIYQQQgghhBBCCCFlC2+KE0IIIYQQQgghhBBCCCGEEEIIKVt4U5wQQgghhBBCCCGEEEIIIYQQQkjZwpvihBBCCCGEEEIIIYQQQgghhBBCyhbeFCeEEEIIIYQQQgghhBBCCCGEEFK28KY4IYQQQgghhBBCCCGEEEIIIYSQsoU3xQkhhBBCCCGEEEIIIYQQQgghhJQtvClOCCGEEEIIIYQQQgghhBBCCCGkbOFNcUIIIYQQQgghhBBCCCGEEEIIIWULb4oTQgghhBBCCCGEEEIIIYQQQggpW3hTnBBCCCGEEEIIIYQQQgghhBBCSNnCm+KEEEIIIYQQQgghhBBCCCGEEELKFt4UJ4QQQgghhBBCCCGEEEIIIYQQUrbwpjghhBBCCCGEEEIIIYQQQgghhJCyhTfFCSGEEEIIIYQQQgghhBBCCCGElC28KU4IIYQQQgghhBBCCCGEEEIIIaRs4U1xQgghhBBCCCGEEEIIIYQQQgghZQtvihNCCCGEEEIIIYQQQgghhBBCCClbeFOcEEIIIYQQQgghhBBCCCGEEEJI2cKb4oQQQgghhBBCCCGEEEIIIYQQQsoW3hQnhBBCCCGEEEIIIYQQQgghhBBStvCmOCGEEEIIIYQQQgghhBBCCCGEkLKFN8UJIYQQQgghhBBCCCGEEEIIIYSULbwpTgghhBBCCCGEEEIIIYQQQgghpGzhTXFCCCGEEEIIIYQQQgghhBBCCCFlC2+KEzKEvOc975GZM2eWOhmEEELKjBtuuEEsy5KdO3eWOimEEEJGAdOmTZMPfvCDpU4GIYSQUcDGjRvFsiy55ZZbSp0UQgghw4hlWbJ48eJSJ4OQIYU3xUnZMm3aNLEsq9/fPffcM6jrbN26VW644QZZu3ZtUdJNCCEkyLZt2+Tqq6+WU089VaqqqsSyLFm9enWv4Z966ik5+eSTJZVKyfjx4+XKK6+Ujo6OoqXn05/+dCh9+fSnPz3oa914443y85//fNDxEELIwcRvf/tbufDCC2XmzJniOI5Mmzat17Ce58k3v/lNmT59uiQSCTn++OPlJz/5SVHTE0Yz+tO2MLz88styww03yMaNG4uSbkIIGY089dRTcsMNN0hLS0vA3tXVJStWrJD3ve99MmHCBKmqqpITTjhB7rjjDnFdtzSJBaxevTq0bgyWhx9+WG644YbBJ5oQQg4ietOZwdD9MFKY32DH+kORfkJGE5FSJ4CQoWL58uWBmyAPP/yw/OQnP5H/+q//koaGhh77O9/5zkFdZ+vWrfKVr3xFpk2bJrNnzx5UXIQQQjTr1q2Tb3zjG3L44YfLcccdJ08//XSvYdeuXSunnXaaHH300XLrrbfKW2+9Jbfccou8/vrr8pvf/KYo6bn00ktl4cKFPccbNmyQ66+/Xi655BI55ZRTeuwzZswY9LVuvPFG+chHPiJnnnnmoOMihJCDhR//+MfywAMPyDve8Q6ZOHFin2G//OUvy0033SQXX3yxnHjiifKLX/xCzjnnHLEsSz7+8Y8XJT3/8z//Ezj+4Q9/KI899piyH3300YO6zssvvyxf+cpX5D3veU+fDwIQQkg589RTT8lXvvIV+fSnPy21tbU99jfffFM++9nPymmnnSZLliyR6upqefTRR+Xyyy+XZ555Ru69997SJXo/jj76aKUP11xzjVRWVsqXv/zlol7r4YcflhUrVvDGOCGEHAC96cxgGDt2rPL93/rWt+Stt96S//qv/1JhB8NQpJ+Q0QRvipOyxbyB0NTUJD/5yU/kzDPP7HORqLOzUyoqKoY2cYQQQkIzZ84c2bVrl9TX18uDDz4oH/3oR3sN+6UvfUnq6upk9erVUl1dLSJ7vxxy8cUXy29/+1t53/veN+j0LFiwQBYsWNBz/Pzzz8v1118vCxYskE996lO9nkd9IYSQ4eHGG2+UO++8U6LRqHzwgx+Uv/71rzDc22+/Ld/61rfkiiuukO985zsiInLRRRfJu9/9bvmP//gP+ehHPyqO4ww6PaY2PPPMM/LYY4/1qRkie99qTKVSg74+IYQQkfHjx8tLL70kxx57bI/t0ksvlX/913+VH/zgB3LdddfJYYcdVsIU7qWxsVHpw0033SQNDQ196obneZLL5SSRSAx1EgkhhBSZiooK5ePvv/9+2bNnT5++3/d9yWQykkwmhzqJhJQN/Hw6KQn9fRJkuPj0pz8tlZWV8sYbb8gHPvABqaqqkk9+8pMisvcmCvr07Xve8x55z3veIyJ7P2t14okniojIBRdc0Osn2V9++WU59dRTJZVKyaRJk+Sb3/zmUGaLEEJGPA8++KBYliWPP/64+t93v/tdsSyr5yZGVVWV1NfX9xtnW1tbz02G7hviIiLnnXeeVFZWyk9/+tPiZaAf7rnnnp78XX755TJu3DiZPHmyiOzVHvRwVvc+4d1YliWdnZ1y77339vpJ9paWlp6ne2tqauSCCy6Qrq6uocwaIYSUhLC+U0Rk4sSJEo1G+43zF7/4heTzebn88st7bJZlyWc+8xl56623+vwySbF5z3veIzNnzpQ1a9bIu971LkmlUvKlL32pJ03oLb795yv33HNPz0Njp556aq+fZH/yySdl3rx5kkgk5NBDD5Uf/vCHQ5ktQggZVm644Qb5j//4DxERmT59euBTsw0NDYEb4t38y7/8i4iIvPLKKz227rH8k08+KVdeeaWMHTtWamtr5dJLL5VcLictLS1y3nnnSV1dndTV1ckXvvAF8X1/eDL5d7r3ff3Rj34kxx57rMTjcXnkkUd6Pr9u+v/udbju9apPf/rTsmLFip64eluP+973viczZsyQeDwuJ554ovz5z38e6qwRQsiIpS+dGQ6mTZsmH/zgB+XRRx+VuXPnSjKZlO9+97vKx+/P/nOJsOn/+c9/LjNnzpR4PC7HHnusPPLII0OcM0KGD74pTkoC+iRIPp+Xz3/+8xKLxYY1LYVCQRYtWiQnn3yy3HLLLQf0NsbRRx8tX/3qV9Vnc/f/JPuePXvk/e9/v5x11lnysY99TB588EH54he/KMcdd5ycfvrpRc8PIYSMBv7pn/6p50b1u9/97sD/HnjgATn22GNl5syZBxTnSy+9JIVCQebOnRuwx2IxmT17trz44ouDTveBcvnll8vYsWPl+uuvl87OzgM693/+53/koosuknnz5skll1wiIvqT7B/72Mdk+vTpsmzZMnnhhRfk+9//vowbN06+8Y1vFC0PhBBSrrz44otSUVGhPls+b968nv+ffPLJw5aeXbt2yemnny4f//jH5VOf+pQ0NjaGPvdd73qXXHnllXLbbbfJl770pZ487Z+39evXy0c+8hG58MIL5fzzz5e7775bPv3pT8ucOXPgjSJCCBltnHXWWfLaa6+prfP6+tRsU1OTiEhgm71uPvvZz8r48ePlK1/5ijzzzDPyve99T2pra+Wpp56SqVOnyo033igPP/yw3HzzzTJz5kw577zzhiZjvfD73/9efvrTn8rixYuloaFBpk2bFnqP2EsvvVS2bt0Kt/Po5sc//rG0t7fLpZdeKpZlyTe/+U0566yz5M033wz18BkhhJQbA9GZYrNu3Tr5xCc+IZdeeqlcfPHFcuSRR4Y+N0z6n3zySfnf//1fufzyy6Wqqkpuu+02+fCHPyybN2+WMWPGFD0/hAw3vClOSgL6JMgVV1whHR0d8thjjw1rWrLZrHz0ox+VZcuWHfC5jY2Ncvrpp/f52dytW7fKD3/4Qzn33HNFROTCCy+UQw45RO666y7eFCeEHLQkk0k544wz5MEHH5Tbbrut5/O0TU1N8vjjjw9oX7tt27aJiMiECRPU/yZMmCB//OMfB5XmgVBfXy+rVq0a0Od3P/WpT8lll10mhx56aK+fyzrhhBPkrrvu6jnetWuX3HXXXbwpTgghIdi2bZs0NjaqN+O6dWTr1q3Dmp6mpiZZuXKlXHrppQd87qGHHiqnnHKK3HbbbfLe976358tW+7Nu3Tp54okneh7k/djHPiZTpkyRH/zgB3LLLbcMNvmEEFJyjj/+eHnHO94Raus8EZFcLifLly+X6dOn93wFcH8aGxvl4YcfFsuy5PLLL5f169fLzTffLJdeeqnccccdIiJyySWXyLRp0+Tuu+8e9pvi69atk5deekmOOeaYHpv5hnhvLFiwQI444og+t/PYvHmzvP7661JXVyciIkceeaR86EMfkkcffVQ++MEPDjr9hBAy2jhQnRkK1q9fL4888ogsWrSoxxb2TfUw6X/llVfk5Zdf7nkp49RTT5VZs2bJT37yE1m8eHExskBISeHn08mI4Ic//KHcfvvt8s1vflNOPfXUYb/+Zz7zmSGLu7KyMjDBiMViMm/ePHnzzTeH7JqEEDIaOPvss2X79u2BhZsHH3xQPM+Ts88++4DjS6fTIiISj8fV/xKJRM//h5OLL764KPvR9sZll10WOD7llFNk165d0tbWNmTXJISQciGdTveqGd3/H07i8bhccMEFQxb/Mccc03NDXGTvGyFHHnkk5yWEkIOWxYsXy8svvyzf+c53JBLR7w1deOGFgQen5s+fL77vy4UXXthjcxxH5s6dWxJf+u53vztwQ7zYnH322T03xEWkR0OoG4QQUjqmT58euCFebBYuXBj4SuHxxx8v1dXV9P2kbOCb4qTkrF27Vi677DL5xCc+IUuWLOkzbC6Xk927dwdsY8eOHdQNh0gk0rPP61AwefJk9fZJXV2d/OUvfxmyaxJCyGjg/e9/v9TU1MgDDzwgp512mojs/XT67Nmz5Ygjjjjg+JLJpIjs/QKISSaT6fl/b3R/OrGbmpqafs/pj+nTpw/q/P6YOnVq4Lh70WrPnj2BfdUJIYRokslkr5rR/f/eSKfT0traGrCNHz9+UOmZNGnSkG4lZWqGyF7d2LNnz5BdkxBCRio333yz3HnnnfK1r31NPvCBD8Awpt+sqakREZEpU6Yoe3++tLW1NfCwVSwWk/r6+oEkvYdSzjUIIYT0T0dHh3R0dPQcO44z6E+tD7fvF+GcgZQXfFOclJQ9e/bIhz/8YTniiCPk+9//fr/hn3rqKZkwYULgt2XLlkGlIR6Pi23rrmDeyO7Gdd0Dir+3G/a+7x9QPIQQUm7E43E588wz5aGHHpJCoSBvv/22/OlPfxrQW+Ii+z532/0Z9f3Ztm2bTJw4sd/z9/898MADA0rH/qAbKsXSFxFqDCHk4KGYvrObCRMmSFNTk/KZ3TrSl2488MADSjcGy4E+iMV5CSGEDIx77rlHvvjFL8pll10m1157ba/hevObyN6fL73qqqsCmnHWWWcdWKIBnGsQQsjI5pZbbgn4frRVx4FC30/I4OCb4qRkeJ4nn/zkJ6WlpUV+97vfSSqV6vecWbNmqT3HB/tGRm/U1dVJS0uLsm/atEkOPfTQnuPeRIcQQkj/nH322XLvvffKqlWr5JVXXhHf9wd8U3zmzJkSiUTk+eefl4997GM99lwuJ2vXrg3YEKa+HHvssQNKR3/0pS8m1BhCCNnLgfjOsMyePVu+//3vyyuvvBL4/Oyzzz7b8//eWLRokdKNoQLlPZfLqYfAqBmEENK/L/zFL34hF110kZx11lmyYsWKYUqVyBe+8IXA1nr7f5a8mHTHa+oG5xqEEFIcDsR3nnfeeXLyySf3HA/2a4S9Qd9PSHh4U5yUjK985Svy6KOPym9+85vQn/2oq6uThQsXDnHK9jJjxgz54x//KLlcruczhr/61a9ky5YtgZviFRUVIqJFhxBCSP8sXLhQ6uvr5YEHHpBXXnlF5s2bN+BPQdXU1MjChQvlvvvuk+uuu06qqqpEROR//ud/pKOjQz760Y/2m5bhYMaMGdLa2ip/+ctf5PjjjxeRvW8lPvTQQypsRUUF9YUQQuTAfGdYPvShD8nnP/95uf322+U73/mOiOx9A2LlypUyadIkeec739nrucV6OzwMM2bMkCeeeCJg+973vqfe/OC8hBBC+vaFTzzxhHz84x+Xd73rXfKjH/0IfjVwqDjmmGOGdP/vbg455BBxHEeeeOIJOfPMM3vst99+uwq7f1nV1tYOedoIIaQcOJAx96GHHhq4jzBUVFdXS0NDgzzxxBPyuc99rsfen+8n5GCEN8VJSXjppZfka1/7mrzrXe+S7du3y3333Rf4//5Pz5aKiy66SB588EF5//vfLx/72MfkjTfekPvuu09mzJgRCDdjxgypra2VlStXSlVVlVRUVMj8+fOHfH8PQggpB6LRqJx11lly//33S2dnp9xyyy0w3H/+53+KiMjf/vY3Edl7o/vJJ58UEQl88vDrX/+6vPOd75R3v/vdcskll8hbb70l3/rWt+R973ufvP/97x/i3ITj4x//uHzxi1+Uf/mXf5Err7xSurq65I477pAjjjhCXnjhhUDYOXPmyO9+9zu59dZbZeLEiTJ9+nSZP39+iVJOCCGl40B851/+8hf55S9/KSIi69evl9bW1h4dmTVrlpxxxhkiIjJ58mT53Oc+JzfffLPk83k58cQT5ec//7n88Y9/lB/96Ee9fjpwuLnooovksssukw9/+MPy3ve+V/7f//t/8uijj0pDQ0Mg3OzZs8VxHPnGN74hra2tEo/H5R//8R9l3LhxJUo5IYQMP3PmzBERkS9/zZBUVwABAABJREFU+cvy8Y9/XKLRqJxxxhmyc+dO+ed//mexLEs+8pGPyM9+9rPAeccff3zPQ1ejmZqaGvnoRz8q3/72t8WyLJkxY4b86le/ku3bt6uw3WV15ZVXyqJFi8RxHPn4xz8+3EkmhJBRRW86032zuVRcdNFFctNNN8lFF10kc+fOlSeeeEJee+01FW6kpp+Q4YI3xUlJ2LVrl/i+L48//rg8/vjj6v8j4ab4okWL5Fvf+pbceuut8rnPfU7mzp0rv/rVr+Tf/u3fAuGi0ajce++9cs0118hll10mhUJBfvCDH/CmOCGEhOTss8+W73//+2JZVq+fOL/uuusCx3fffXfP3/vfFH/HO94hv/vd7+SLX/yifP7zn5eqqiq58MILZdmyZUOT+AEwZswYeeihh2TJkiXyhS98QaZPny7Lli2T119/Xd3YufXWW+WSSy6Ra6+9VtLptJx//vm8KU4IOSg5EN/5wgsvKN3oPj7//PN7boqLiNx0001SV1cn3/3ud+Wee+6Rww8/XO677z4555xzhj5TIbn44otlw4YNctddd8kjjzwip5xyijz22GNy2mmnBcKNHz9eVq5cKcuWLZMLL7xQXNeVP/zhD7wpTgg5qDjxxBPla1/7mqxcuVIeeeQR8TxPNmzYIBs3bpTW1lYREbniiivUeUuXLi2Lm+IiIt/+9rcln8/LypUrJR6Py8c+9jG5+eabZebMmYFwZ511lnz2s5+V+++/X+677z7xfZ83xQkhpB9605lS31S+/vrrZceOHfLggw/KT3/6Uzn99NPlN7/5jZoLjNT0EzJcWL7v+6VOBCGEEEIIIYQQQgghhBBCCCGEEDIUDN/mOYQQQgghhBBCCCGEEEIIIYQQQsgww5vihBBCCCGEEEIIIYQQQgghhBBCyhbeFCeEEEIIIYQQQgghhBBCCCGEEFK28KY4IYQQQgghhBBCCCGEEEIIIYSQsoU3xQkhhBBCCCGEEEIIIYQQQgghhJQtkaGKeMWKFXLzzTdLU1OTzJo1S7797W/LvHnz+j3P8zzZunWrVFVViWVZQ5U8QggZFnzfl/b2dpk4caLY9r7nkDKZjORyuT7PjcVikkgkhjqJIwJqBiGEUDPCQs0ghBBqRlioGYQQQs0ICzWDEEIOAs3wh4D777/fj8Vi/t133+3/7W9/8y+++GK/trbWb25u7vfcLVu2+CLCH3/88VdWvy1btvT4uXQ67Y8f5/R7zvjx4/10Oj0UbnpEQc3gjz/++Av+qBm9Q83gjz/++Av+qBm9Q83gjz/++Av+qBm9Q83gjz/++Av+ylUzLN/3fSky8+fPlxNPPFG+853viMjep6WmTJkin/3sZ+Xqq6/u89zW1lapra2Vd9d+QiJWrO8LDfTJq8E8sWWNoi/O+94gzh1gswh73mDSNpooZnsp5pOGQ/3UYrHdSjHjG0zbG0B9FvycPN7yE2lpaZGamhoREWlra5OamhpZ//wUqa7Ccba1e3LY3C3S2toq1dXVA0/zKKAYmnHKrM9LxIkPTQJBf4Et0h5gv/J0bKFjQn3DNIEmbxW5j/oD9Smo+YNyRKn1Hds4BmkANh/Fj2xFdJMobZarc2UbNlSuPigzK2x1gnC2azQQ1F5AG0XtNjRmtlD7CdO2RcQy0w+i7zW+MIRs2yj2UGca6Sq4Wfnj//svakYfFEMzTn7HvwU1YzB+OAyh27Nh9ECHhH0UGNE1Ub81w8G0FnlcZ/arsBqCwiHttXU/Uf4UaQbyufCaKG39x+U5IF0xbUPhUIP0IoYOoqQWgI8chGao8UNI54d01kz/3nD9pwOlH45rUL8uZlMOGRdMW4j+H0Z7C25W/rTmFmpGHxRDM4684HpxYvvedkHt1DOWrlwwLfFiuk69qA7nR1E4bfPjwf5tJVwVJhItKJsT0X7BccC4FHQYZDPxQg6iUTjP0zbXDdrcgqPD5LRNstpmZ3XlOWl9TacraHMyOvpIVtvsPPA7ugrEMqrAB9/wzKdAurI6/sRuXZ/5Cp3PfGUwvtr1eRUmtkdnNFenG7Nd0Olwo/qaBSMdKF25Sp3PQoUyQWKtOh3xtqANlRn0rwAPjBW8CBo/9HPcmw0lAy1XwTGA0S+Az3FjRphcRl6996vUjD4ohmYcfe51Ac2wtWsWy+37WETEAf3MQj4GTQNAVfrRYHsoxIG/Re0obDhwC8c3dM+NA30DNh/omQNsiZR+W7U6EfRjNQnt12pjaWWriuhwMVR5ANvouDkg7p2utnWBQit4uvJM7UVaPCnRomyTE3uUrdLW+WzK12hbNmjb2FGvw7Rrf4DSlorpekLhzLxbIEzM0XVSG9f1WR/rUjaEWQddBV0n3iBWBtBYpysfbAt7ulIqTGeH7mRep25DThdoL1l9TdPmgDGMOdZxcxl57a7y1Yyifz49l8vJmjVr5Jprrumx2bYtCxculKefflqFz2azks3uq4n29va9CbNi/d8UH+gNCN4UD3HuAFcMQi+S86b4ATPQ9o4YbTfFB3PzRUc28FMHUZ/o80mVVZZUVuG6GIzojiaKphlOXCLOEH2aBS6Sg3DFvCkeesE6xM0LNG8f0TfF0c0McM3RflM8xELjsNwUN2fRo+2mOFgFgOUx1DfFYT8LcyIORM3AFFUzIvtpxpDfPAt3o9lSd/9QhwQmuBoG7xz2H443xfs+T6SXm+L95wneBAa20DfFoyFuigOnPmJuioMbKAO+KR5Sp0b0TXEjcQeivdQMTLE0w4klxIn3fVNcLV2hZ3XBwr+EvCku4IZ6mJvidizsTXFtG+hNcSvkIBqFs8BNcXENXwduivsOuClug5viYD5vg3Q4xo148MwAtKHyQUsIYW6Ke+BGlAP8SSSq685DD1wZ8UWiunwiKB0RcFMcODEL+HQxbChdDsinH/JZdwf0C8foPxGkBUN8Uxx2gZA3xdGQDp3rGjc4BS2fx3BfpGZgiqoZxbgpjnww6nshb4p7RpvxUftA7Qj0Uahx4FzzhrePdDAR7qa4nQQ3ysHDQ5FkML5oQhdQNKbjigFfGrPBU00Axxxvg5vieXADPA9uwFoDvCkeT+prJhLaqSdtbUvk9bmxaDBtEeCYHfB0BEpbBLQhFM438g7jAjfFo3FQn/FwdWfWQXQYbopH8sFrOKBD2R5Y3wbtyobtBYwfjDyAURMc14iUr2YU/Q7vzp07xXVdaWxsDNgbGxulqalJhV+2bJnU1NT0/KZMmVLsJBFCyIgk77t9/g4GqBmEEBIOagY1gxBCwkLNoGYQQkhYqBnUDEIICUs5aEbR3xQ/UK655hpZsmRJz3FbW5tMmTJFrFhMLHu/JyfQU/rm06CDeHsg9JtAxXxbFwCfWBzoGxzgk6Li6obpo08umuEK+gkbH8Wf159gQuF8kA71Znvxv+wfBNU5eIzXQnWOHvcF4dTTNCHbnoWedEbP8aA3SYxz0RM9qu/0ko7Qb+6YeQ/7BmHotgzarRkO9h30yCR61QY+5q1tJmHKto+yKIgnusfs+x/R9KYZ6XEpiUT3PUmHP6NpHId15yHfSArzOWx4HvgsVuiXuFC3Na8BP9E5iLdrQ5QtKn/4th3qouZT74K1t1AZfEoyXwneGkGfDQur40YeBvN2WVHfOodvuutw6IlxB3xuzfzMI3z7AYHaHuoD6BNvRjg3CT63GA/3puRA39CGbyMC4FP7OfT2Yf9vAqK4zP5ayPde/tSMA6dXzWhMBDTDBp/WdHLmuFTHr7YfEBHLPE9E7JweR9tpUJu5oM3KgjBgTI78Mhyno3GRMaa1Iuhtu5Bzp6ieWvph4kOfFQdvsCGbh8KBN9EKyWDasjUgLjAzbpuOhESb1KPmUHuBLexbYoAwbxkN6kF9pCOG/4vorxVKtFOfGMmEE9F8Cnxat9rq87g3QmsvepsKTcVMQPwR8KVG/OlecK5RlqjMzE8WF3oTBaFmDITeNCMzRmT/D1K54K02tyJYpn4sXBlbIJwP3pa2bPR1EQNwXr5dv32UR2MN1F9A2pI1wW9rJmK6lSXRJ9tt8KYesCHMz4yiuLJ57cA7usCnR8EbVYUC+NS48el1pw28dQ7KsZACb+Fn+h90+qB+vThoG+A1rha4vqHPje8K5mHLQp2naEeVsiFtgZ/zB34z2m68mQY0A71Fiz7DnDlEf363UKW//5ozvoSQy4KxyRb9qVoX1J2VA2/bAZtZRmg8Melx3S/Q5+TRXAy+aWyM/cy3gEVEcjXBYxd8LrcbasaB05tmJPb4EkFf/NgPcy4aawNvuraC9XXQPuBaSUp3yIIx585XoPOADbywWqjS+UP+T6qD7T6S0HmqALb6Cj2gGpPoVLY68Bn0CmNvixowWE3Z2p/szFcqWyX4xvSegvYfbYVk4Li9oPXn2b/OULZojY4fbRFizivQOOHP3jR9Hvz6CvBhYGsR2/R143Va/Wadz2SzjiuzU7eXmg26DlJtwWu4FfrN6LapukHumKTz1DlN+9xDDt2ubP808aVguoBQPfD2XGVDb4Bv2VGnbG4aCALQfBOrHZwHxgXOZN1X/A16D5JoR/DYaLIiIhIJ8XWunvPLQDOKflO8oaFBHMeR5ubmgL25uVnGjx+vwsfjcYnHh2gfWEIIGcF44ovXy+3P3uzlBjWDEELCQc2gZhBCSFioGdQMQggJCzWDmkEIIWEpB80o+ufTY7GYzJkzR1atWtVj8zxPVq1aJQsWLCj25QghZNSS9/0+fwcD1AxCCAkHNYOaQQghYaFmUDMIISQs1AxqBiGEhKUcNGNIPp++ZMkSOf/882Xu3Lkyb948Wb58uXR2dsoFF1wwFJcjhJBRSc73JdeLWPRmL0eoGYQQ0j/UjL1QMwghpH+oGXuhZhBCSP9QM/ZCzSCEkP4pB80YkpviZ599tuzYsUOuv/56aWpqktmzZ8sjjzwijY2NQ3E5QggZlXgCt2vs+d/BAjWDEEL6h5qxF2oGIYT0DzVjL9QMQgjpH2rGXqgZhBDSP+WgGUNyU1xEZPHixbJ48eKBR5CIidj77c1h6y+9+45hs/Qm94MixJMNlguq2gO2ghsufnSu4xgXBfm0gQ2FM+MSEVRqvmekDZ1nhhERP2T8CF8VESiLYj5tAuPS1/Q93fYsG9U7aKNmEw2bNDdce7Fc1BYKwdMssEvCYNpLmHAozGBsoP9bZnnAMgN1AvsdsIEyUnkH5WOmFfWTbgq+JXkft4pCL/ZyZbCakauxxY31vSOIZTQRJ6/rxi4AGwiH3JOFbHnPCIP8pk63FwVtN6LbhAdslhu8hpPTCbMKoE+BtEGNA1gFIxwoRysP+iioMqsDhAM4nbnAcaQ9psLkq6PKlq3VQx8rhLT4wCcMppv6wH2YLstzwDVRM0euFEkoONcsjUIi3DXtPLBBSUJtPhgQteN8StsiGRAXSJsHNc44D9UdsKFwhWS4tgDrysAJNmNxc70PzakZ+xisZth5X+z99rqyXaQHwU7kdOpGb3fllE3yBW0DWCHmBn5C+zWvukZfsjahbIWkdjJ2XjuGaFswX5HWtE5re5dOak7nXY3NpJd5hhMMZ4EyszLgvLj26TaaI0Z13s36czK6bDNjgGbUI3HXpjA+JozW7D0ZmEL4k1irvmi0Q9sSO9FYR8eH6tONBePLVev4OyeAtoekHRRtRDc/lXdXVx0G1EG+SufJiyKbYbBRpQw8HdEWXaGxtmDAyrf1BZyc0Xc4zwjFoDXDDU6x7TbU14x27wE/lNVxR7R7heMdhNn/svXABxd0WmPtOq76l7VPR3OgbF1F4DhfodtyFviF9Fh9zXwVmKOgrBs2LwH6Rr0u3OoqoF2g7SNb3g3Wn1cH8rQzpWwQUMcmTh6MezvRGmgIfyUihbHaqWeNU5PbdBtF7TFbr22prdqG/Lc5lkKMeXyLshXe1hdw6uuUzd21W9l2/2vwM9eRSl22tW/o8tl6kh6DR7rAXBstBRqn2qCvN88FYxigvalmXcexdtBXvGDa4FzV7vt4f6gZ+xisZviRYJsIM4Zrn6zbX+4Y3WY6p4IBVY1uSJG4boTTxwb7yxnj1qkwJ1domwMGPHfteJey/f5Pxylb3ZPBQZsL9mDP6q4tb8/UeU826nzGwACzOVMVON7eWanCtHXpuVN9pXaAOVf7yUREz1uiTjAdUyv3qDAfOfF5ZXt4wzHK5jZpbUlODQp3PAp8fF6XWVeHLu9IhU5/MqHja9tjpCMDxjWNup1lxuv20tmh03bEv76pbMdUbQscv9GlBw97srp8IqAdrN08RdkccC/n5Y6JgeOjKppUmPasLsed68com9TpsVQkCdpLzLhvA3ysl9J1kmvV6fA2VWjbZD2R7qgI9sXal7VjinYEj91cec8zhuymOCGEkL7JiS05dFdQRMAyOyGEkIMYagYhhJCwUDMIIYSEhZpBCCEkLOWgGbwpTgghJcLzLfF6eYKqNzshhJCDE2oGIYSQsFAzCCGEhIWaQQghJCzloBm8KU4IISUiJ04fT1aNDhEhhBAyPFAzCCGEhIWaQQghJCzUDEIIIWEpB83gTXFCCCkRBd+WfC8bDIFteAkhhBzEUDMIIYSEhZpBCCEkLNQMQgghYSkHzRixN8X9dFZ8e18pWrYuaGRTuK6O2we1A8KFwbdCpKHXk71w4QpG2mz9xIVlgacwkC0s6hqODhPXNits2gCwXgYY14DzDtIA0+UBG6pP81x0HgK127Bt2Qumw3dRurTND5knywFtwbChMFYsqs+LABeEzgX1qfJeKOi48trmg3CobBFmvqwQ6ff93nfTcH1b3F5ExB0lIjJSKCQs8WP7tRPgAswvuFjgky69VIfCAs0NPSSX2B3sQ05OV6ydR31P27rG6T7kgW4V6wieG+3Q7dvO6b7tZICPiYBMgf7oVseC6YoALQCuyM6HSwfCNTQoW6f7YyGJ6jisPgTDFfsLQBbq44YNlRmy4QtokxvXxnzK7BjhovdTwAbK1gc+XfUz1F+B1GRB20vs1gXpodGtEcxGzQzUiY3KG9VdCN+B/IsbN477KH9qRvF46zRb7OR+ZYnK3WiEdi6uglgeOBG0mVhbOL2pfzXYMCNdOrJouxYgL6oja56H+p62Vb8ZFJKKppgKE+mqVDanK6/TEQMdF1CoCHZSH2gG0kakXUhHvJguj3RDMJ/ZGh0Gdi8bzA0c0OFC+E6oI4Ppu8a5uTodWa5en9Y5NWT8ME/9J9hHAjcw6Q0PuKTTAfoAqDs/XkQHGlKj87U6YL4ueNx5CDjRaERe2hL5Fb4GNaN4TPxTWiKR/damwGpfrjboYwrJcJOKPAjXsHqLjv+QBmWLvdEUNMS1/84dMkbZNp6eULb2E3SbbHxEx1f32BtBQ22VCiMR3fdy43U4F/jqrkY9ubGMdYrEHjR/0NcspGqVreVQPUjsmKbjq3grmLZYl67zFJjmd4B+W6mrUzJjgn05X6Xjz9cAfUuCASzUJKChY4IJzo/V8ReA5rkFXbZ7JgO9R4NYY5xkZ3Sd7z5Wi1J8ty7IaAcY80fBvFQP1xRbT9HtILFDx9U1UZdR/Uv9z3NdMEdHcxs0r0NjhVylLrdIJlgetW/ocVmkMzhmLBQysl5HLyLUjGKyY46IvZ+b9Sr12N1JBW0Tx7SqMGOj2smse2OivmBB11u0UvuKN/88JXD81uE1KszO6XrMf9mYPyrby3sa9TU7dONNjw3a0hPBmhPwa4eN3aNsUyq07ZDEbmXbng/qTVOH1p9Mp9a3bEKX9/ENW5XtA/V/UbaxkbbA8ar2Y1WYZ3ZO1+lI63QkprQrm+sG63hPu64nv1P7NacL3EvLaCeZBv7JNsbHXhJoUhTocUQ7jPoJun2/1VGrbLuzwUWmrrwun86cdrDprA7nAU3asG6Csm3MBfvUs7uP0/FP0n24/mUdf+xMnc8ukLaurmAdWECz4wnt020010vqevey6H5d8LjtsP7X0by0L3K/CiYi5aEZI/amOCGElDt5sSWPHjgRES1/hBBCDmaoGYQQQsJCzSCEEBIWagYhhJCwlINm8KY4IYSUiLzvSB49Miwi6OVhQgghBy/UDEIIIWGhZhBCCAkLNYMQQkhYykEzeFOcEEJKhCe2uL18c9cb1Pc0CSGElBvUDEIIIWGhZhBCCAkLNYMQQkhYykEzRu5N8Vw2sA+Oj/buNQm5V7if0y/y+2DvYYixXybcXxnseWlFw+2dLGCvP8sxGhk8bxB7m3tgIxvT5oG9i1A+LZDPKNpLGuTBtIEwPsgn2vMW2oxyNI9FBO7XDtsV2ne0APbfzrt9HosI3PfaCmmD7TsfbN8W2kMbxQX6RehNdI19vnxB+UTtJVy79VE4c79zVBYo7zm9T0zYPcXNdFhRfZ65z3hfe4rn/UgfT1YVeQPjMseLiATcDyg+tWcX2q8ZSQGqipD7gmZqjTaD3Alo3mhP5LB7YbvGljUZsNd22OaF9l2G28b4IcKEBOYTuQDDBssMda8i7leKbGH3AbeQrBo2G2zGA/cUD5kOiFGOqO6wDXWykOc6/YdBwD32qrURtQWzPGzQ1y1U3iHqSaSXvm0mLUTZ9iIJIkLNKCZ+1Bc/emCTNReFR1GAqijoLfvguW8b2wRasHPofcngHs5oLAbSttPYbnYnbEu6U1k+2jATnIoItf82iiykswizFzbqyMUEph+Ayjus3pjnFnn9IdTe4MV2PWGuGbJpuNVgD8927UM9tB9vGJC2hxyrDRTf2O8X7m//d6gZxePND0fFTu7zedFWXa6RzmCZwq4N9tm0PB2wa7zeYznSpeNLTp4WPN6u5/Noj+6qTTqu/E69z3jrDB2u5bDDA8duCu3zrG2RDt1JnQxIB9hbO7EzeG7neB1XYhe4ZgbEtQfUAdDajKGNnZNBPsH+qlZK+530sVllq0oGbVVxHQbRkQP74Oa1Rhc8UN6Gf3VBI406Whujjs5nGuzpGgHnhgHF79jalnd1v+vM6PIw9+j1wD67FtiDHs0pbLCGtes03XDV/q2gX0O9BHu4w73ZEea56DxjrOalXZGncHRDoRkrVqyQm2++WZqammTWrFny7W9/W+bNm9dr+OXLl8sdd9whmzdvloaGBvnIRz4iy5Ytk0RC+6eRjJO2xN6vzJyM7i++HbS93azz6NVof2InwRon0pHdKWWzUsF+1bWjQoX5XeRIZfvfl2frtHUAH6C3qhYnHTyO7wR9r0anf/0mvWf5pmS9viZYk03EglrY3pHUCQPp35muVbbHW3UZPfP2IcqWywbjc3frwkA6mGzReXeyOr1mF7RBWcOpB7B5YArnJkOMv1H0wO/4jrZlgGYko7ot571g+0A6lc3ruPI5sOaZA5MDsAe6GHqZq9Vhoi263aYbdNp2bxqjbAg7HbymFweaFwcVBfKE9o1HtwjN9TAL6JttNpg+ZL0c5hmDWLYmhBAyGHK+0+ePEEII6WYoNGPFihUybdo0SSQSMn/+fHnuuef6DL98+XI58sgjJZlMypQpU+Tzn/+8ZDJgVZsQQkhJ4TyDEEJIWIqtGQ888IAsWbJEli5dKi+88ILMmjVLFi1aJNu3b4fhf/zjH8vVV18tS5culVdeeUXuuusueeCBB+RLX/rSYLNGCCGkyJTDPIM3xQkhpER4vt3njxBCCOmm2JrBxSpCCClfOM8ghBASljCa0dbWFvhls71/4eDWW2+Viy++WC644AI55phjZOXKlZJKpeTuu++G4Z966ik56aST5JxzzpFp06bJ+973PvnEJz7R7wO7hBBChp9ymGeMjlQSQkgZkhe716eq8nTPhBBC9qPYmsHFKkIIKV84zyCEEBKWMJoxZcoUqamp6fktW7YMxpXL5WTNmjWycOHCHptt27Jw4UJ5+umn4TnvfOc7Zc2aNT3zijfffFMefvhh+cAHPlDknBJCCBks5TDPGLl7ihNCSJmT9yMS8bEbzhd5X0hCCCGjmzCa0dbWFrDH43GJx/WmY92LVddcc02PLcxi1X333SfPPfeczJs3r2ex6txzzx1gjgghhAwVnGcQQggJSxjN2LJli1RXV/fY0RxDRGTnzp3iuq40Ngb3hW5sbJRXX30VnnPOOefIzp075eSTTxbf96VQKMhll13GL1IRQsgIpBzmGSP3pnjjWBEHC2wPnlHKnt6Y3soXtC2bUzY/n9fxuzo+sY3N4iO6CC1gk1hUXxPZEtrmRYPf4vcj4Z64sFzQCguojFxtM8oIlaP4On4/Gi7vXlLbXMPmpnRcblznPZ/StkLCUjYvZsQV1WHCPsyCytYGTcgxmpqT1ec5OWDL6HpyctpmA5tltFsL1Tlo2yicoDY0UBxQJ/bAnx6yzP4O2iOqJwuEg30dhbNAmzExfITlZkXW4aCeWOIJjrM3O8EUkpb48X1lZqGma9gs7frEAk3BMrXmAPDNNgOavIeUGFS/h7ZlAfGZ1wyfT22DLgCkzTzXR80XpRXZQD5ROLPc0HgMpcO3QaZCpkOFATaUdVQHgoYYBaPuPB0b0hpUd5AQ/QICMwWigu2x/3Ph15VQ/MBWSIFTUXmY/R/2AX2BsH0lTDlCv2TgAq3sJoxmTJkyJWBfunSp3HDDDSr8Qb9YZQlu1wOJZ6AgX2Tgh+qgMrhvf4W5hAPG/AONSyRcuYUtW9SxoOMJEZUDxscg75EomDsZ6fBBGtDQ0vN05fnAF/kusBWMc0EYCGp7IJ8o7yoMKn80ToBaEK7B2Ga4kOchCkkwWEDlZtYfuiSqk7B6XKx220ff5zyjeLzr+FclVrlvMaEqmlFhcsbAtC2fUGGQzQP1rtq8iBSArzBJOHqQWAEGtDFbr+2gdJh5QuFckK6KqP6kMvqUZsTWvjQGbG911AaO4xGd/rp4l7J15PV64s6uCmVLgPKemAzGVx3Tdd4Q61S2oyu2Kluto9PW6QXTVm2nVZgqR9tcUI5dns5ni6sHyK+ng2O+bZkaFSbj6jqvi+l01EZ1niK2doDT4zsCxylbt41xkXZli1q6jqNgQhUTbRsLys2kylzXFZEW4L/XZicq21u5McrWnK8OHKddvd6ZdmPKVgD1mQeT/jjos2HoNK6Z78zJW72EDaMZ1dXVgZvixWT16tVy4403yu233y7z58+X9evXy1VXXSVf+9rX5LrrrhuSaw4VVl7E7mctwTObCBieOLvAWnoMBQRrylkwRjFNwO9nXqxXtgT4Sr6DbNpNKtDc3YvpdOSjugALaJwOxrRdmWC7d3M6LrtL973kDlBmNtDBSFLZ4kYXTW0DdQLuFNoFbcunwNq5kQW0VpWr1ue54LZavhLdy9E2L27cW0DjXtCGnIj2y4WCTvDbGxt04oqIVdBps/JgzJUL2qJt4daE0BpW5XrdXlCbL6SC5W25OjKrDbRbsK4Ydi3NMoaINqhPzwhjZQa3NjXSGbk3xQkhpMzJ+RFxenmyCjwrQQgh5CAmjGaEfYNjIJTTYhUhhJQ7nGcQQggJSzE1o6GhQRzHkebm5oC9ublZxo8fD8+57rrr5Nxzz5WLLrpIRESOO+446ezslEsuuUS+/OUviz2IF2oIIYQUl3KYZ/CmOCGElIiC70i+l9dRC+h1IkIIIQctYTQj7BscXKwihJDyhvMMQgghYSmmZsRiMZkzZ46sWrVKzjzzTBER8TxPVq1aJYsXL4bndHV1qbmE4+xNj0/NIoSQEUU5zDO4ekUIISXC8+0+f4QQQkg3xdSM/RereuL/+2LVggUL4DlcrCKEkNED5xmEEELCUmzNWLJkidx5551y7733yiuvvCKf+cxnpLOzUy644AIRETnvvPPkmmuu6Ql/xhlnyB133CH333+/bNiwQR577DG57rrr5IwzzuiZbxBCCBkZlMM8g2+KE0JIicj7jji9PFmV5w0GQggh+1FszViyZImcf/75MnfuXJk3b54sX75cLVZNmjRJli1bJiJ7F6tuvfVWOeGEE3o+n87FKkIIGZlwnkEIISQsxdaMs88+W3bs2CHXX3+9NDU1yezZs+WRRx6Rxsa9e9tv3rw58LDttddeK5ZlybXXXitvv/22jB07Vs444wz5+te/PrAMEUIIGTLKYZ4xYm+K7zxxjDixRM8x2kzeLvjGMQiT1xXhZL1+4xIRsVxt84294v2IfvrBi+oN5QsJbXNjYFP7CLKZCVNBIBYoMwd82D+MDZUPAqUf5dONgzIybIWUDpOv1NcsVOi0uUlt8xJBmx/R7QB+OwFl3dVps3L6ZCcTDGfn9HlOVp9nudqxoD6A6tgyswXSr8L0YoN5H6Bvs4BT9C1dHjbodzC9bt/He68ZLm1mv957MghnVlWI+N1cRmQd/l/et/sQEVQhpDdy1SLOPskI1TfC9wPQ31G4EL7ZA9UNH6IDNtRUcNsdWCe1PBBZiL6391zToMPAtIJ8Ks0TES8K8mSci8oRnefHgM1B8Rs2VD4I5HOBZqCyNQspdFWiekLngjyocCFdT8jSgITJFupjbiJcgYTJOwwDbKhdwT4QJmGwbQSPvUzveSy2ZhzUi1Vxd++vG9S/zaqATgyAGhfy8yE6uGX6IRHY2CwUV1iXZfgnC/hDx9Hty0PlAWxwTmzYwl4TYYMyQtc0w6Hzoo7u3BGQDlTevum/QRjH1nHlCrpPm3GJiBQ83YjMcB5oxx48T5kkAuZFsF0NIExvoHyGuiawofZYEc8pW9jyzhtzsVxWD07QNf0wvkQEZ8IIh+MytQwI0t/hPKN4fGnC76Sqal9fQqVqyneFDdZAQi4StoB+u9tLgHCpwPGUSIsKA311SHJAvLYW6gLHEyN7VJgKSy/MtXhxZYuCAVUC2Mz4kKQ6IJtrs+OUbUdBbzGzITtW2WoiXYFjlFZEayGlbNvz/W9rk41ElW1Lvl7ZulxdjiknGypthyZ39HksIvDtLhsMyo+Nv61sCSuvbGYbTdk6rWPstLI1Au1NWbqMUnZM2UTAoqHBI126HGfGdilbDNT7rOQmZXOM9cc20F8RKH5URqhsUV8xiRoTu452T37eS9ih0IzFixf3+rn01atXB44jkYgsXbpUli5dOqBrjSSS2y1x9lsHL1ToMK7RRODcFzRvdN9DCmBdFaw9R9uNMQRoQrFWrVO2bn5wGgPnzUa+8lVg7ITmNmD9xO/QPiDfqm3mufB+ErCh8kZ5SuzUttSO/vsjug+CronagjlFyYFy9HRRwPUT896IiIg1VvudylTQ5rq6MNJt2tfl9mibBdpjpAvcQzGG7tAlhb3HgfoFaMumzZD/vWHAPbFsvY4/VwfKG9SLmV47jdKqbbE2HRcYFoiHfIdRtmZZi4hEO4y4QZhuymGeMTreZyeEkDLE9e0+f4QQQkg3Q6EZixcvlk2bNkk2m5Vnn31W5s+f3/O/1atXyz333NNz3L1YtX79ekmn07J582ZZsWKF1NbWDjJnhBBCis1QaMaKFStk2rRpkkgkZP78+fLcc8/1GX758uVy5JFHSjKZlClTpsjnP/95yWQyA7o2IYSQoYNrU4QQQsJSDpoxYt8UJ4SQcqfgO5Lv5cmqwih5sooQQsjwQM0ghBASlmJrxgMPPCBLliyRlStXyvz582X58uWyaNEiWbdunYwbp9/I/fGPfyxXX3213H333fLOd75TXnvtNfn0pz8tlmXJrbfeesDXJ4QQMnRwnkEIISQs5aAZo+PWPSGElCGeb/X5I4QQQrqhZhBCCAlLGM1oa2sL/LLZ3j/DfOutt8rFF18sF1xwgRxzzDGycuVKSaVScvfdd8PwTz31lJx00klyzjnnyLRp0+R973uffOITn+j37XJCCCHDD+cZhBBCwlIOmsGb4oQQUiLyf3+yqrffQOBnDQkhpDwZCs0ghBBSnoTRjClTpkhNTU3Pb9myZTCuXC4na9askYULF/bYbNuWhQsXytNPPw3Peec73ylr1qzpmYu8+eab8vDDD8sHPvCBIueUEELIYOE8gxBCSFjKQTNG7OfTd51YEDtZ2GcATxlYxqbzdhZtTK/v+yOb5YKnGHyQMONUz9GB/Kg+zYvocB4IJxH9iQHfvAa4JkyrB8osB8oI2YwysgoqiFgoGagYQSvzoqA8EsG8+0lXhYkkdUKSKf1EeyqW1+faXp/HIiIWyJTr6faSc3UH78zGdLhcMPNuQZ+XD9n2LFsbLdAWbFQx6sQQYURghaInfnwjDz4KA/odbLcFEA58eUP1WRQXapAo7yhpoLxVOBSXcUkvXRC5D6RN9oqI3YtY5PlZwwMiNz4vdnK/sgT9yipYfR6LiFio2FE1A/+KwplbqSh/LiI+0IcBt8le0hGGsHmCZRuiucJtZUCeUBmZ/UpElBZaMZ0IJ6Z1xAK+3wbpcBxDk4A/cV2dKeTnPeD//BDljYof+X0Eai9I4zwzHch/h/WvsP+EeEoUnQfaWdXEdmXLZvVgCtaBGR8o/9D5DDsACoMRlZfW45duiq0ZBzPJqqw4qX3HHhjrmX1jMO3DCdlvVfTI7YO+jWxh4zN9nQPSbwO/iXxiAZQjIupo36zDAJ+Oxu4h4zfHx1G7/zAieL6ACJP3GLhmzgNzA2BD8ZtzlLBP5Q9wmBCasN4QTl9BHsx6QfG3Z+LKdlLjm8r2l5ZJypbOax2piOYCx1lXT2hRWjMFHa6AxgqgPs32nQfzTXPc4dq9v9kdRjO2bNki1dXVPfZ4XJejiMjOnTvFdV1pbGwM2BsbG+XVV1+F55xzzjmyc+dOOfnkk8X3fSkUCnLZZZfJl770pV7TPFLJ+3t/3bSGWOzrAm7OBa13IvBXUyK6fYz39UPLeUkHjneAtoZ8+i4vqWwtbkrZPDDRSFnBNueBAT7ypbWgrWZAOSYsXR4po9ja4RqFvubh0Z3KNjWyR9mOS2xRthfS0wLHr6cbVZi/tU5QtsqozueuTIWybdtTHTgu5LXvsEHbSCT0ODER1WtkSAcTkWC4uKPPq42llW1cQo+/9xR0nhoiOlx9pCMYv92lwiDyoD47ROd9dyGnbHnj1E2FahXm8ic/pWyHT2lWtqqY7nczKnW7OiG1KXC8y61UYcZHWpUN9bsKW/vhCtB/xjttgeM46DsJY5Ke72PSznlG8SgkRfz9qtFN6DD5qmBDdVNo0VOb7AxYiwHzZlSVpuuMdYAwER1XTjdTccDwI9Kl+22hwphPgeEyapYonx6agqN7C5WGb8uAi3bqAkJpi2vJENsF/mliMD7gIiW5A8wRwftI6QYwxzLiQ2nF623a5FVoX5GM974G0U02A25i5XVanXZdtpEuEA7k3TZlCS6IARtqG6gPgPIwy9YFQ+F8DYpLN9xYCxiHgXzmqoMJRlVnaXmD9xFR/BEgtU4ueE10npn3vqaW5aAZfFOcEEJKRLE/N8LPGhJCSPlSDp+oIoQQMjyE0Yzq6urAr7eb4gNh9erVcuONN8rtt98uL7zwgvzv//6v/PrXv5avfe1rRbsGIYSQ4sB5BiGEkLCUg2aM2DfFCSGk3Cn08WRV4e9PVrW1GU8Ax+Nwwar7s4bXXHNNjy3MZw3vu+8+ee6552TevHk9nzU899xzB5olQgghQ0QYzSCEEEJEiqsZDQ0N4jiONDcH36Rsbm6W8ePHw3Ouu+46Offcc+Wiiy4SEZHjjjtOOjs75ZJLLpEvf/nLYtt8P4MQQkYKnGcQQggJSzloBmcihBBSIgqeLQXP6eW31z2H3euvr88aNjU1wXPOOecc+epXvyonn3yyRKNRmTFjhrznPe8ZlZ81JISQcieMZhBCCCEixdWMWCwmc+bMkVWrVvXYPM+TVatWyYIFC+A5XV1d6sa34+xdPPPhfhSEEEJKBecZhBBCwlIOmsE3xQkhpER4YonXy86L3fawe/0NhP0/azh//nxZv369XHXVVfK1r31NrrvuuqJdhxBCyOAJoxmEEEKISPE1Y8mSJXL++efL3LlzZd68ebJ8+XLp7OyUCy64QEREzjvvPJk0aVLPA7xnnHGG3HrrrXLCCSf0zDOuu+46OeOMM3pujhNCCBkZcJ5BCCEkLOWgGSP2pvg7jt4o0YpYzzF6yiDnBZOfLugd57vy2pYr6ElY3g03MTOrNeK4Kkwsom2JSEHZKqI5ZUtFtC3h5APHUUt/hiDv918+IiJtuYSydeT0Tba0UW6uF65BW5Z+6tuxtS0Gyi0ZDeazKppRYepiaWWriGSVzQbpUO3F1W0D2dBeCKg9ZpL63LwXbFfoPJTWCKjjOGhDlSDvFUYbQuWTsnU7Q+lAec+CdtXhBtsQKscMsIXp1yK4fxaMNo/S6ofcx8KxdXmjOoga7TZm6zqJGHHlO3PyVi/XzXuOWB72Pd1tp3uPv/442D9rmKhLi5Pa14bD1L3rgjYD2qQHwqH2hvyfKRoW8IcOOg+AzkXXjESCbdAG7RuVjgvy7rqgPIAe+IbNA3GhtCK/g4hEtWZEDa2tS2l9QPWEbGa/FRGpjgc1aFc6pcJ0ZWPKhsYYVlKZBL0khcrbxHFAfYIKRfWO2lrEiA/5Q1R3BeCX0VgB+XmzfaA2ha45u/FtZWvL6cLtLOh66cgFbaieUJ7CaguqTzMcOs/MpwvGPt2E0QwSjkk1rRKp2Dd2CTNxS0byyobGAQVQF2iOgq5ptreorX0fakc18YH7P9MP4/NAPwbph/09hB4jH4z0wRbgw0AZxYBtV6YicHxY1U4VJm7rOkagtHW6/T+0iPopGjPnQLiB7s02mD3dUL3bxvgY1lNIbcfXRO2qf//WkOxUNtSuplbsUbYd2Uplq411BY7R/CQH5yfaFrF0e0Rlq8KE6GOFRFbW93J+sTXj7LPPlh07dsj1118vTU1NMnv2bHnkkUd6vlK1efPmwNzh2muvFcuy5Nprr5W3335bxo4dK2eccYZ8/etfP+Brl5qH2o+XhL+vr1Y6Wqur7KAfrgZhbNFt8qWsHsfsKFSFStf2XHCOODW+S4VpdfX4tSHSrmy73Qpl25PXNlOXoqB91zhak6KW1ssE8LlVti63jB81wuj4uzztg13wYcyt+Vpl25KpV7ZVm44IHKOxdgyszzT7uu5Scb32UtgU9DugGCXSAfRYX1I6QHcuVIGxezJo86t1+dtRMH8A65s1lb2PV/fHHG+jtdIJVW3KNiaufXpVRF9zTFSHSznB9a+mbI0KY8dAnsBY6s+vHKps6xrGKdumccE2hNaSOvK6jZqaKiIyOdWibGHGYWiMZK4FZjvyIvILFU6E84xikq/2xU3sqw9PD/WkUBVsg1YKdG6AGwfjhzwYr6XBWmt9sI0UKsC6SBrMFcDwGA1j3AQYtxgSh8oCTcPcSt03/CRwlIh8MEKrEG4s7MV1H8qO0efmK8G6nFF9qWYw7wfJzzTocO1H6gKvfD1YcOnx4PPUqByTOlyiNpz/Lhj+2y+gSgftBTRlR9+WgG0obwzJUdsL3R6Tup7QdK1QE6yYSDXQ7Da9vmTlw/UVT58qsVbjXDR1AnlCZQtkRAp66Kf6O5wiGjY323vfKQfNGD13PAghpMxwxZaCj39oEt8X/KwhIYSUN8XUDEIIIeXNUGjG4sWLZdOmTZLNZuXZZ5+V+fPn9/xv9erVcs899/QcRyIRWbp0qaxfv17S6bRs3rxZVqxYIbW1tYPMGSGEkGLDeQYhhJCwDIVmrFixQqZNmyaJRELmz58vzz33XJ/hW1pa5IorrpAJEyZIPB6XI444Qh5++OHQ1xuxb4oTQki54/lWr2/wDOTNHn7WkBBCypdiawYhhJDyhZpBCCEkLNQMQgghYSm2ZjzwwAOyZMkSWblypcyfP1+WL18uixYtknXr1sm4cfoLLblcTt773vfKuHHj5MEHH5RJkybJpk2bDujhW94UJ4SQElHo43MjYT4VaXIwf9aQEELKnWJrBiGEkPKFmkEIISQs1AxCCCFhCaMZbW3B7VHi8bjE43jLsVtvvVUuvvjinpf6Vq5cKb/+9a/l7rvvlquvvlqFv/vuu2X37t3y1FNPSTS6d3uBadOmHVAeeFOcEEJKRMG3xeplT0Fzz/SwLF68WBYvXgz/t3r16sBx92cNly5dOqBrEUIIGT6GQjMIIYSUJ9QMQgghYaFmEEIICUsYzZgyZUrAvnTpUrnhhhtU+FwuJ2vWrJFrrrmmx2bbtixcuFCefvppeI1f/vKXsmDBArniiivkF7/4hYwdO1bOOecc+eIXvxj6y7cj9qb4+8b8TZKV+5LngoLO+NHAcdaL6jDA1gV2uc96uijyvi5ER4I72EdsvaN9ys4pW5WTUbZKYKuws8qWsPKBY9fc+V5E8r5OP8p7u5dQttZCStk63OCTG6gsEGb5iIjYlt6bOGq52mYHbah8qmxtQ5htQ0Skww3mfXuuSoXJg6dcCmAvhJij05+K5JXNtoLlEbV0+SQdfV5FRLcD1IZQWzPLbYzToeOy08oWA3WC2hoq23Y3GTju9PSTP2HbEGzLwGb29y5X9+uw14zbBWVL2LpezPJOgf5qtu10vCC/7OW6/ERV8ahJZcSp2Odr4qCPOoa/Rr4JlbvraR+QB7ZcQbdTL8TW7Dao6gjyMVHdJuOObrs6DfoCWVenNWw+UbmZV4iC9COQhqI8JYCfnJxqCRyjfLbkk8q2O1uhbHlX+4p0Qfs6EwuURSKm0+rYOhwqx7xrG2H0NVHbiIJyRHUQtmzDUAD+tQDaCypHM+8RoI2TK1qU7biqt5Rte65a2XbldR3vyFQGjvdk9dgH9QEEqjsEKg8TM6aCpXWlG2pG8ZhetUtilfvGDWj8Z47haqN67ITGCiguNB6BehNi/62wdY3G2sjnmmN3NCdC6cqBcEhbEKYvslVPEPHAGBSBzq2NdinbrOqg/4iDunNAXGHGoCIidZHgNVtdrT/IJ6Axf9g69nxTM3T9mmEGi5leNBdBbQ/VJxq7o3l62g2WN2p7LiizE1KblK0zoeco2/K1ymbOo8OuFaC6Q3WA6srUljBx5aR3DadmFI+frJ8rTgq/2dJNoRBsD4VtepwBXIx4KTCuqwZz/AqwFmCMObtyR6swaH6Sz+u2i8a0CMfx+jwWEYlHwLqCp9tcDISriOm87+wIjuva27R/9V0dvx0BPrEL6FRMh7M6g+Hi23WZtdXo8wTksy0KNK42qINOKxhXgyYUa9XxO7rIRNr77+P5OPBhIJyX1eF27dR14HQBvTEibKvRdb4rValsdhT4SDCG8cAczjeK2y/odNXW6zWytU8eoWxORNedu7FW2dZU1wSOCyCfAtqjgHa7FqTXAj7BMeLzgV+3jDLzujIi8gudDqFmFJNClS92Yr+1qcm6vZ044e3AMZpfvrhtkrJ1tek1fXH1uWgp1DeuYeV1vRZSIC7QtZFkRDpAGzSaPYrfrdNjGQv45XhCh6ur0mP+GTW7AsfPbZ6qwhR2aB/m1gM9TulrRkFfzuwKxpfv1Fpj53T55GqUSaykTkf6HcF0oPZiAz22XV152S49j7HAepWXCzaiyC59npPWefJBXPlqYKvTa1PxpuA1Us2gvcTAPTEtI+KCYVuhWpdt3cTWwPFZ0/6fCrO2dbKyrXl9mrLlanV5J3aA8bxRlDCtoK+AZS6xkdyAvmgCprjixo25CKjLnv+F0IwtW7ZIdfW+9bre3hLfuXOnuK7b85XbbhobG+XVV1+F57z55pvy+9//Xj75yU/Kww8/LOvXr5fLL79c8vl86Bf/RuxNcUIIKXcKni1WLzdNwtxMIYQQcvBAzSCEEBIWagYhhJCwUDMIIYSEJYxmVFdXB26KFxPP82TcuHHyve99TxzHkTlz5sjbb78tN998M2+KE0LISMf1rV4/N4LeeCGEEHLwQs0ghBASFmoGIYSQsFAzCCGEhKWYmtHQ0CCO40hzc3PA3tzcLOPHj4fnTJgwQaLRaOBT6UcffbQ0NTVJLpeTWEx/fcyEj3sRQkiJ6P7cSG8/QgghpBtqBiGEkLBQMwghhISFmkEIISQsxdSMWCwmc+bMkVWrVu2L3/Nk1apVsmDBAnjOSSedJOvXrxfP27dtwGuvvSYTJkwIdUNcZABvij/xxBNy8803y5o1a2Tbtm3y0EMPyZlnntnzf9/3ZenSpXLnnXdKS0uLnHTSSXLHHXfI4YcffkDXOSq2TSrifd+zN/fUQvsfI9CeyMiG9jE3ccB+YOYe4L3b9P4JaM9pszGhve1Q+nNgE5G8AFtMx2eei/bYi4L0o/JA5egV8XkMFD/aO13lAfQRtLd0Gux3h/aBQ3uDm/uA1zh6P8oqYEN7VaM6SIB9/CqsoA3tw14Veh9F3V46QfszrxF277yY6M0vcqCNmvv67U1btM/j3s5DoD1BK0AdmP0Y9WuzHDtzaGeuvRQ8W6TMP1E1XJoxJtUl0dS+/lsb03sLTUi0BY6PSDSpMLMTm5XtqS6dlle6Jijb9ozeyKZg7C2L9tRMRUA/BpvDRcBmMaivmXtvZsAer3A/a7APbhQMZsYm9Z5YU5J7AsdT47tVGKRdKP1oL9L6iL6m6Xe25OtVmLftOmVDZFztK8w+iPatjYF9u+GevcD/oXBh9qpG6UBtCO0fjvbuzhr1brZZEZEcsKF9MfG+zDpgRTSY3jHxThUG7R/+juRGZdvoNChba0xvtrQzHuyfTVn9GafOgt7jCO7pGnK82VUIjh9QXGY7K+R731OcmlE8zaiJpCUe2ddH0H7BZh9FYdCexQ/vPk7Z3lnzhrKhfbrNvaqRP0TznbDjdBSfuS8yGk+hvZPRvtrtLtjnEGCOj+emNqgwKB0trt5gdWykTdmmRfYomzl+/U3bLBXmyMQ2ZWvz9J6DWVC2psbVW9qv4TkR2AcXOViAqS1h5q7oPBHcNrBN590kD/QetSGUT6QZ5r736ZBjGNRejorrOh4Dxhjmue1gj3i0/zma7yBQnzVBfd3sd5l873uKUzOKpxmRp6vFie/zb2CaKBGjOXROBvtUg/2mpVO33cRrehyTd7StzeiicA9ZYHOAzU2ANomaqbF/qAf2sgRbjKp9ZUVEXLAXdgbEZ3blKNibNMz+mSIimbFg/L0HzFGMfTVzdfq86jd0P3KyaD9eMJaMGBoHko/2SEVTBVS2DmijZvtz0jrfFhjrOHo5CbarbB3Y+zVl7EEP9h23W/T428kAbQTtBe2nbkohKscWbZJYVl8zGnIP9+pNwby7MTBuqgXruOPAPr6gbK1OPb6KmPvGg7ZhLm+62d5vAVAziqcZVkNGrP381oLJG1WYyxt/Hzj+W3aiCvPsxmk67jawHzTwAX4U+KKKYINwwVgb+Rg/oo3xHbqhgmGj5GqMvlGhAzkpPbasqtCO57hxW5Xt/HF/UrbTjD25Z7x5gQrjg32741XaccZjOm2xiD43I8FxYhZoRnyP7keZ8WC+Bq5ZUxksDwtUVFVcp78rr+u4aateI/MLYD/4bDC9SGdztTqfsO05YC1tu/ZHY/4WjA9IkqD7rcjPF2p0PVWPb1e2OY3Bdad3Veo9rD9R87yynf7H/wDX1OXR3gjmU8be6Q7Ybx7cpoD9E+/rrsOpcR4oRzOM18d8pdiasWTJEjn//PNl7ty5Mm/ePFm+fLl0dnbKBRfs7b/nnXeeTJo0SZYtWyYiIp/5zGfkO9/5jlx11VXy2c9+Vl5//XW58cYb5corrwx9zQNOZWdnp8yaNUtWrFgB///Nb35TbrvtNlm5cqU8++yzUlFRIYsWLZJMBoykCCHkIMb17T5/5QA1gxBCigM1g5pBCCFhoWZQMwghJCzUDGoGIYSEpdiacfbZZ8stt9wi119/vcyePVvWrl0rjzzyiDQ2NoqIyObNm2Xbtn0PN0+ZMkUeffRR+fOf/yzHH3+8XHnllXLVVVfJ1VdfHfqaB/ym+Omnny6nn346/J/v+7J8+XK59tpr5UMf+pCIiPzwhz+UxsZG+fnPfy4f//jHD/RyhBBStvT1WZFy+UQVNYMQQooDNYOaQQghYaFmUDMIISQs1AxqBiGEhGUoNGPx4sWyePFi+L/Vq1cr24IFC+SZZ54Z0LVEiryn+IYNG6SpqUkWLlzYY6upqZH58+fL008/Dc/JZrPS1tYW+BFCyMGA69l9/sodagYhhISHmkHNIISQsFAzqBmEEBIWagY1gxBCwlIOmlHUVDY17d2ftfvV9m4aGxt7/meybNkyqamp6flNmTKlmEkihJARi9eHgHijREQGAzWDEELCQ82gZhBCSFioGdQMQggJCzWDmkEIIWEpB8044M+nF5trrrlGlixZ0nPc1tYmU6ZMkTF2VqrsvgvRMd7Gj+JgCr3FvUge7B2fR7vOm2kAm86jVJtp7Q0XpCMT4tmFqKVzhdKGsC0dzjw3anmh4kLXdEE55sH+AhnfMY5188wDW844T0QkahWULWVnA8c1TqcK48V0utBeCCj+CjunbFV22giTVWEGU3dhSISMH7UyVO8VovNeZeWD8YdoU71dE7W0HPAFWaPeUTtDNpQOlM8oSEnMCBcN0a/bI733HV9E/F6qungtoLzoTTPGxdslloj12A9LbVfnzkxuCRwvTLarMBEgi66sBzZd+YcldTs1/U67l1Bh8h7yddqvZUE4ZLONdoo+XZMBfRRRFdU+67CKHcr2rspXA8ez4y0qTLsX7pqdwM9XAZ+7w4sHjrfk61WYuK3PGxvrULa0F1O2vBesg7pYWoXxQDuIgWtWOrocIzbwO4a/doBvsoF3QO0l7epRUdbrf6SUAed15OPKlnVBu3WBHjtag1KRoF6Oi+u+ODaibR7Q4xkx3dd3uZX6mkZfbIjodtCcr1a2sP2uAOrAbAttuaQKYwqhb/feT6gZB05vmhG3CxK39/VfNBY2+9X7a15SYcY6up1+aIxu82jc6IFRUMIYT6ExLgL5CkQG+TrjGuiaGV+f1wXGl3E7r2xo3NUQCb5Jc3KiVYVp93SZ7fZ02sY6Ou/jnApl21wI9vn5FW+oMBlf+z9TU0VE8nb/U2jklxFonoHqE80XXEPf0fgbtb2w8zXkc832geZmLhrhD6Itm1qO4o+A8nlkx0xlO+vQ15Tt9bwuo6Z8TeB4YnSPCoPKLBNCZ3vDLEvUHk18MObo+Z9QMw6U3jSjarMrkei+Noa2Smw7xJibVuq2bGXBukI7GEu2gBoC18zWBc9NNuvzgNsX9FXLeKs+N9YBVs5CyI2TBYHANfOV2i90NWhbxyHG2lQb8AFgy19UT/HdYL0nCdYujHqxPJ2B5E7gqzu0zY2D9bBUMB1gaCm+DXxMg05rIaVtyOWa5Rbp0mEcvaQlQHph2cZadHoTb/Z/zeRuoFMZ0IZAt/DAgkyuOpjgrrE6A/kqME+aoMcwyU06XGaMTkimIZiOGi01UvWWzlNiN2ob2lZI6fjMuop06nTF24O2Qp5rU8WkN804bPwOiVTsmz/Prtqizh1rVODSp85UYWLbwLgUVEZuku64h03Vc+TNO+sCx9YWLRBIM5y07vBg2CWZsbp9edXB/u2kdH+vrtTrLEeN0el/Z40eux8T1XOIVsNpeVntA+yEzkB2p56rxyfqud6eVj3PiDcHnTjSlZbjtY+JVum6SyR0uIixplIA6y6ZArhfUgAOHDSiSItua04m6Iu8GNDKDBD3LFiHB+GS23V8XWPNxREdfdckbcyNA/cpxur1ngnV+msO1ZFg+9uYa1BhXvYmKRvcNrug82m163oxp08uqJP4LqAFYDkJjX+QbhvLp3DsIBOMyLpA5H+nHDSjqDfFx48fLyIizc3NMmHChB57c3OzzJ49G54Tj8clHteLrYQQUu64vt2LkuJF0nKDmkEIIeGhZlAzCCEkLNQMagYhhISFmkHNIISQsJSDZhQ1ldOnT5fx48fLqlWremxtbW3y7LPPyoIFC4p5KUIIGfW4ntXnr9yhZhBCSHioGdQMQggJCzWDmkEIIWGhZlAzCCEkLOWgGQf8pnhHR4esX7/vU7IbNmyQtWvXSn19vUydOlU+97nPyX/+53/K4YcfLtOnT5frrrtOJk6cKGeeeWYx000IIaMe37fER9+v+/v/ygFqBiGEFAdqBjWDEELCQs2gZhBCSFioGdQMQggJSzloxgHfFH/++efl1FNP7Tnu3j/j/PPPl3vuuUe+8IUvSGdnp1xyySXS0tIiJ598sjzyyCOSSOh9VAkh5GDG9WwRr5fPjfRiH21QMwghpDhQM6gZhBASFmoGNYMQQsJCzaBmEEJIWMpBMw74pvh73vMe8XvbSV1ELMuSr371q/LVr351UAmrsC2psPc9WRCmOB0J9ySCC7Z8j4V8iME18u5YxX36IQfS5okXOI5ZngqDiIKkDXWzdIAtD/LkiBsiLn2eaxW0DdS7N8D9C2xQtigdURAuYek8meeGTVUe5MkFT9rkQYzeAJ/ICdeqcD7NtobyidoGIg9ssLz9/lPshOwDKG2ob9shfcz+uH34CM8TsXr5rIgXtkJGOMOlGUdWNEuiYp+kHZN4W4U5JLLHsERVmO1ul7LFQLhaR4dL2VllUz7A1WXRZcWULe/rVumE9P2u0crD+kM7ptNWH+tUtsmx3co2zukIxgXib3B0nqKg93X52gtkQBNqKtSAqxjXjLZro67OUGUUBb4Ph9M6ZVtIB3V9mnWHwmR8nYEuT7ehqKX3N+sCjsUrBH1QBLSzmKPzjvYJQk+EJiO6Putjwf4zI7FdhUlY+jyk0ag8YqCupkR39Xseir/D1YsgqLx35SqVzWxXHtCQ+niwj+ULORWm53xqRtE0I2K5ErX67vfmeAr17XYv3CIZ0owYGAsnjGtkfD1VywG/iTQD9VEH6FTOODcKrumAOklYuq16wPujvlxvaMabumjl+JjuUw2+LrOwY7MmN+gTkWZHfZCQkJjlHVqzUT2FnI+YPgtpRh61IdBe8qBdobTZpu8E2TS1TETEA/3NtcEcDvg41zg3but6Stm6PW6PVCnb1oIuD7Pfiehx5G5Xt8cK0Lar7YyyIb3p9LRG540qHuh8tud8akbRNMMu+IGxXLYGtHFjaBDbpfsUmqY7aW1snw587i4drnNK0CemmnS6Uju030w26XbqtKZ14gpg7Bsx8mWDskjqNu8iW0z3x2y9zqc/PpjeExZsVGGe/svhyhbfruP3I7psC0ltq3grmK+617SfcHJgXB0BawhAWiyj3RZSuhwLKX0eGJaKF9fp9x2wlpYMpg3Ig+RAG0VToHiLDhjfra9ptr9Yi/bVThqtAGk80F4soGeRTLAso11APwugnWV0HWTH6jq28miRKXiNtsN0mMR2HT+YIkpFk75mJK1ttikaAMsz1q8LvY9zqBnF04zGRLvEkvvGmWiO+VjnYYFjuw34KzAM8MCajdWlz23P6nGG+1bQqVTs0fXdcQgYS9ZqJ2AlgC2iz43Hgm1uXE2HCtOQ1LZJyRZlS9jaV7xe0OOzZ7tmBNOVBmtrzWANIQPKI6fXnGzgKM0qjh6q16HGVer54LamOmWLVmo9rksEbZt26/N276lQNq9D59PpAH4TjEXMYS7ymx5YW0OaEQPLcj5YsM9VG8e1QLPH6HYQr9LzulRMh4vYuo22FZKB4+c7pqswTZlqZXMrdFzRlnB+3piW4vGhzpJUbAX9swKNI/W5+YZgX4yCMptY3xY4LnRmZZOOSkTKQzMO+KY4IYSQ4lAOnxshhBAyPFAzCCGEhIWaQQghJCzUDEIIIWEpB83gTXFCCCkRnm+J1YtYDPSNf0IIIeUJNYMQQkhYqBmEEELCQs0ghBASlnLQDN4UJ4SQUuFZ4vfyuRHpzU4IIeTghJpBCCEkLNQMQgghYaFmEEIICUsZaAZvihNCSInw/b2/3v5HCCGEdEPNIIQQEhZqBiGEkLBQMwghhISlHDRj1NwUD7NHe97XoRyruE8nDDQ+N2SLcIAtblxyMPvVD/Ve92Hjt4EtanlGmHBl5omuE9vS5zqGzQkZP05rqFNDlYdbZGeB8q6uCcosCsoD1YED8o7KSF9TM5j2iNJhEoXnhau8MHlCmHnqK4++Z4vv4Sv1ZieYBanXpaJiX5klLN3idnuJwPHWjG4hjsSVzbZ0LdY7HcqW8XV8eT/o1R3QIqrsjLJBgEBkIv1fM+v1H6Y3W6Wj0xYFZdvkVgeOW7w8OK+gbAlgi1o6HVsLNcpmlvf4SKsKg3BD9m6U3oHi+fqaKB1mHeR8PUxDcSEc0G5R+4vZwXzWxtIqTNLR9ZmP6XRURnLKVh/tVLbp8R2B47Bl3eKmdDpAGaH4TN9fYWd1GFA+qA/EbV0eDRHtE8x01DpdKozpSzoTrvxUhdoLNaN4JOyCJPYrspSl20PCCtbzC+lpKszYSLuyuaCP7nIrle3Y+Fv6XGN8hsZrMTCiQv0dnAoxfRHSApQnBEqHWY4iWkeioO+9kUd9Sl+zytJpawKDTseY9lZY2l+hMkN1gAg7rxho/GF9v4of1EkMBUTJB0kz258Hyt8T7YM9MP5Gec+D5QmzvdREtC/92aZ3KNttR9+vbLu8cDpitls75KwFtXeEC8rNHBagcYJrvHnhgjFZN9SM4pGvtMWL7iuz9Fhdfp1HGjqS1r7USYP+AoZATkb3jUyD7qR+NGjL1urzEnt0/F4UpK2g25Ll6nbvpYJzJbdSz53cuI6/UKFt2WpQHmCF0msLjvmffnWGTmuFLsjcdG2LvKXT68d02VY0BfPePgVoYwIkNqQUpMcHA/pgDcdNAr8Duq6VA2s7bTpgpCMYLqplViJpsP4D5DLepss2vkf7P6crGM7KgQYP9MEPuWazf7/sJp8K2goJFUTclC5b3wGVB1ysb+twlmsZYfR5OT2dlcbndZnFWnSBWx7o/3bwml4MzO8rg20Uak93fNSMolEX65J4bF/ddri6Ef6x7fCgAXR3JwvGTvFw7XTHugZls412WrmoSUfVlVS2yoSeJ3VkgC8FSYs4wYxFHZ3YhDPwtYDfth2nbE80HxY4tmp1n3ILOv2RLl3ekXZQB0md0fEnbw2eZ+sKXb+pUdkkq/tWa1uFsu3eHlxvi+7Qa3zxDqAFwM97aCIA2p+5jBjRy0QChglQx7O1KB26HHM1hjZW6bYRq9D1GY/pcMmo9q8JsK4VN9bDqiN6/efhZ+bodID2gjD1QUTEMjqLC8qic7KOy7d1e+mYqivBHq/z0FAdnD9VxnW/npBqCxznBQwAutNSBpoxam6KE0JIueF7e3+9/Y8QQgjphppBCCEkLNQMQgghYaFmEEIICUs5aAZvihNCSInwfUt8Hz9d1pudEELIwQk1gxBCSFioGYQQQsJCzSCEEBKWctCM0fE+OyGElCG+b4nv9fIbJSJCCCFkeKBmEEIICQs1gxBCSFiGQjNWrFgh06ZNk0QiIfPnz5fnnnuuz/AtLS1yxRVXyIQJEyQej8sRRxwhDz/88ICuTQghZOgoh3kG3xQnhJBS4Vt7f739jxBCCOmGmkEIISQs1AxCCCFhKbJmPPDAA7JkyRJZuXKlzJ8/X5YvXy6LFi2SdevWybhx41T4XC4n733ve2XcuHHy4IMPyqRJk2TTpk1SW1t7wNcmhBAyxJTBPGPE3hTP+b7kfL3R/AFTjDiKgFuKa46MrIfGkWCCHWvgGRjoJxCckP027PYIZh2g81wZuLMwywxhgzBhzusN2K5K4O/MdKC6y8PzwuXdCZOGEGH69GP+33+9/Y+Ept2Li+ftq7V2ECbvByUvahVCxZ33dWuYFGlRtjfyY5XNNbyRY4XzHlFLty4XDCxQfK4fvGat09VvmN5A8aO0tbipfsOg8o6hfAKHgtKrNeP/s3ffYVZU9//A3zO3bt+FBZYOdgWBiEKwYRTlmxgTuzFG0RhNDCQqaRq/EY1RTDCGFCLRxBJLsOSnYoyFLwKxYEOxi4ggiOxSlu1728z5/WH2wszns8vsctnm+/U8+zzcw5mZMzNnzuecmXvnyHUFOT6t5fNLKfVAa0u1+qIt66+PgKwv7m50JrW2P2LLY1RoJb15wrK+xG3ZmubbKSUtKdK0ZUO+aLjNKRR5tHqrHzPlGCn5XN82I0oLvm+sUqRtSPeV2wxYh4rshHebAducVu2BmDFv3jzMmTMHlZWVGDt2LP74xz9iwoQJreavqanBVVddhf/3//4fqqurMXz4cMydOxdf+cpXOlaALtI/Uou8yI56ErdkPe0XrvN8rnfjIs+2jKy7CRMRaUWhhEhT2/kAHaqg/UatfdKW9V+PartsyeW08rsBY4v/GGnHrMBS2hjlGqrPYSdUOz7RwL3+XbOVsU0oYDufUrL5j3fQurE7Yw8/W2n7bGUiOa2+aEKhXTdmCaWN/80B/xRpjSYaaJta/fOrCNeKtBFhWUdXp/MCbVMbrIZ8x81/bQJAgS/ONofbiCscZ+RMcx8bodiOk9Y0QDmAvmvZUq5trYlMF8p1ReuUNrdeprlhb/+y6fAGkafpcLnNWEzWm7Atr5ea6gKRVvJ6zLuu7bL8qSJZ1ky+SELDfjL2jhi5WaTVNnvj7/aqYpHHUtpXNyn739o5CDXIxKov+q7HhDI+yZftmgnQhn22Qt84pl6WNa9SpkXkKUYoKbepDW1itb59SspM4UZlPKXkCyVkHbLSSjvvuxdibO0EKHU7ooydimVb3Vwu40FzuXcbdQcqZc3IbVppJc1R0pRlYfv2U7mZFN8q07RjayeVdl05nybPu++ZPOWYFXqPRSbdRj8tQMyoq/P2jWOxGGKxmLIAcPPNN+Oiiy7CBRdcAACYP38+Hn/8cdx+++244oorRP7bb78d1dXVeOGFFxCJfHauR4wY0Xp5u7GYnUHM3lFPqjOyLa1Leds1E5YHP1kmT7yJyTStnpa9KdMS5d60Tz/pI9fVJOtRY4Hs20Q/ldejVu+b+nrrc35M9p0+zpSJtOqkDBr/2jZapKU+kcfWRHbdDhul/U4XKm1RVIktefIcfPzuQM/ncP9mkSe8VR4zNbZvlu1azDeUjG+T5bLTweKPFgddpcvsjyNOVGn7tPGJsq5MnjIuLVQatmJvv6CwSI6hS/JkWnleo0gbkFcn0gpD8n5VzPbW0X+uHifyRJR+gqvUM1d7kKD0TxxfHbKV+ONUyLLWlcgNFA2QHYNS5RhVFHiPx6A8ObbxS2W0pyr/1QvGGXx9OhFRV3Gttv+IiIha5DhmtPyCY9asWXjttdcwduxYTJ06FZs3yxvSwI5fcKxbtw4PPfQQVq1ahdtuuw2DBw/e3T0jIqJc4ziDiIiCChAzhg4dipKSkuzf7Nmz1VWlUimsWLECU6ZMyabZto0pU6Zg+fLl6jILFy7EpEmTMH36dAwYMACjR4/GDTfcAMfpip+YERFRm3rBOKPb/lKciKi3M+5nf639HxERUYsgMYO/4CAiIoDjDCIiCi5IzNiwYQOKi3e8qaG1McbWrVvhOA4GDBjgSR8wYADef/99dZmPPvoIzzzzDM455xz8+9//xocffojvf//7SKfTmDVrVvt3iIiI9pjeMM7gL8WJiLpKyxwcrf0RERG1CBAz+AsOIiICwHEGEREFFyBmFBcXe/5aeyjeEa7ron///rj11lsxfvx4nHXWWbjqqqswf/78nG2DiIhypBeMM/hLcSKiLmK5+nxfLf9HRETUIkjM4C84iIgI4DiDiIiCy2XMKC8vRygUQlVVlSe9qqoKFRUV6jIDBw5EJBJBKLRjztwDDzwQlZWVSKVSiEaViYqJiKhL9IZxRrd9KO7+968tQX7mrv12RVtvb/zJfEj5YobTBZPd5/J47+nzlOvjE6QdCEFu1IE8eSFL5tOWDZJnd46juk++TWh1b0/Tzp1WjqBtc67a8DbX09Y3qHrIN6u6i02ZUuSl2w5pcTvt+VxsJ0QeJ+CZz7cyIi1qyYjjT6tx8uU2jbwitbRQwJ6FP5+t7JMdcF2u0lqEAqxPa8MiyvoTRqZGlGOr8efTyqrRjqO2Tf85KLCDlctx5UPIkHYcrbRIS5uQ53PQ4+NfDgCglNcNMLePq5w7bf1NrrwxoeWrd7Xj7b0uiuxmpRzKdaGUTaPlS/jOi2MFW796fWplU/Jtcwo9n6PKufNfn81OG/UsQMxo+eXGnrDzLzhCoRDGjx+PjRs3Ys6cOT3uoXhfux4FO91088cHAKh3457PBVZKrijcIJKaUsF+NaP1zxImvMs8u3MdBFlf0OVcpc6nINuAqDIa86f524TP0mTbUau0r0W2PC/1SvsUDxBb1D6z0v8OwlWu1YTSRmrHUW2LlHbH3+Zq7ZAWR9T2NWA50r466qj7KY+/djw0Wozz78OSLfuLPIcMWyfS4mqclX3FPiF5HZf6+oha3fgoHRdpQQXp68RDsvx+TXYbb+rgOCNnmgcZ2PEddSBTKo+71ei7vrVuacDDnokrbVFSLlz+hjdfVWGeyFM2bLtIK44n5fqVts62ZTvsHtfo+TywVK5fG2fEQ7LOv7xuhEiraZL7UN/gSwspMalRtnVWntxmpo9y7Sn946PGer8gWJOSY7jVm/uJtNTGApEW2yorQ3yrdx/ytsljFk4o/caUlk+pj2llrOc7bHZaaT8seSzciCy/G5XxzOTL9jXtS3Ojcv2ZuBJ/YjJfqliJSYUiCcky745GtitlDXpDTOkCKF1GhFLeFUZkWEHFc9UiLVMs40i6RKY5MaUPUOhNSxXJPMlSXx8v2caO5zBmRKNRjB8/HosXL8bJJ58M4LNxxOLFizFjxgx1mSOOOAL33XcfXNeFbX9Wzg8++AADBw7scQ/EB0RrkRfdUffXJcpFnvfeH+L5bCnDeROS17GVCXYuqsfIZYc+7a3Q/V+Ty9UNl3WkuZ/Sf1Vu7hgjLxjLd21Uru0rFwx4PRatlm2MrVSN5grvvpsC2ZZazXKfnLxgff5QoyxwwQZvWr0lr+OiTVr7J9ev1YWw79ZlJl9pq5XboUq3V72ctduD/nZSW1dGOWaZAplmimXDGYnL85KX5x3XFSr9lVhYLlcYkfk02v2qtOO7B7dV9kPC+Uq/I6LUFy1JOZ+xam9iSN4OQ2ORXLB8cK1I65vfKNL659WLtH5Rb2AaGpcxyS+R2b17U91db3wWTETUM7i7+CMiImqRw5jR0V9w7Lfffq3+goOIiLoRjjOIiCioHMeMmTNn4rbbbsNdd92F9957D5dccgkaGxtxwQUXAADOO+88XHnlldn8l1xyCaqrq3HppZfigw8+wOOPP44bbrgB06dP3+1dIyKiHOsF44xu+0txIqJez7XUb6Zn/4+IiKhFDmPG5/0XHEREvR7HGUREFFSOY8ZZZ52FLVu24Oqrr0ZlZSXGjRuHJ598Mjt10/r167PjCQAYOnQonnrqKVx++eUYM2YMBg8ejEsvvRQ/+9nPOrQ7RES0B/WCcQZ/KU5E1EUs0/YfERFRi1zHDP6Cg4io99oT44x58+ZhxIgRiMfjmDhxIl5++eU289fU1GD69OkYOHAgYrEY9ttvP/z73//u2MaJiGiP2RMxY8aMGfj444+RTCbx0ksvYeLEidn/W7p0Ke68805P/kmTJuHFF19EIpHAmjVr8POf/9zzhioiIuoeesPzjF71S/Hd+XU+5xlv/3IabV1Bj2NvPN69UVecp47O9R50ua6YAx3AZ/ONtFbGHhJEuouI5Xrm79bmJy2wvfPMaHNIRpVoUK7M6Vhqy/BZb+ScehvS3jmTEsrkS9o8odo8mBptvnC/kNIj0eYF18oRUuaCDTKfsv9Yt0YrvzavqSboHOJBaHOMijzaJE0BBZ0n3U+by1Gbq1Wr72llTl0tnx2gsYkEnE89KP8cuto+FRj5Om5tTldtXqiUNm+vr75o8+Dq85gr8ygGnCMpyFzE/utTu16zchwzPs+/4Ci2kyiw225D/G2RNl+2Ns/40Oi2Xa6rNQW+tr/OBJ2fXLYV2vWuzV+t5ZPrD3btafOHa/xzVSvTEqrXWb4ykWda2acg84drtGs26PUeZG72oPOwa/nUbfryaW1Y0FipxiktNvryhdT1y+siqsx9rc47rPR/Qr58Bw/bEGg5rf3OV/onNY6cK7g+QP9Kjw8d75v4z4F23fnzaP2ErBzHjPvvvx8zZ87E/PnzMXHiRMydOxdTp07FqlWr0L9/f5E/lUrh+OOPR//+/fHQQw9h8ODB+Pjjj1FaWtr+jXcxZ1ACZqdqkp8n63hTnXdiUKtBXj8mHGz+yaIPZPthucpcrb7T3+9FWf8a18u5bDeWyXU5yjzm4UalHfM1WW/0LRJ5QgXyejSOsq4meYya1pXJsvX1th+WMi9ybJCcUzNfmYs0oszRW10r5wF/9vUDvOv6RJa1YIs8ZiVNMi2+XR4P/7kLafOCZ2RZTVhpY5SbHrYyp7g/nLkRGX8yBUq9VW6WaN1VNyLzJUq923DD2rzgyvzhJXL9/rnCAcApVeaWLffWhVH9K0WeNxYdINLShfKYRbcr42NlmFuy1nv+8jbLNiLZX9azdLE83olSuU1t/uCU79LLFCpz++Z798lNdF7M+Dx7aMMhCBfs6MNvb5RzFPvn2jZFsi7bEaUvn5H1I1wlx9KOUp9r9vXWtyEPfyLXHxog0oxy76u5vxanZFrepyFfHpFFnXO5aZg8Hs39ZUUc9Jw8RuuH+cqVVMYs1cpGlYZN69ZF65S5wWVXUmgYJndei7PpIlmOZLk3zcTkuuw8ecxicSUea/dBlHMXDnuPbUFMrqsgKtu6wqhsJIsjCZHWJ9ok0mK+sV7fiIztJWG5nDYWrojI+bc3pOSc9vWOt/82eORWkadmiZxernmgcp81oRxH5RxHfLeiM7KJgJWWlW/bVtnn2h5VxjF95D2Ej+DtD75gRoo8/ntRmcYkgKWycECviBm96qE4EVFPYhlL7Ti2/B8REVGLPREzZsyY0err0pcuXSrSWn7BQURE3VuQmFFXV+dJj8ViiMX0L+PcfPPNuOiii7JvE5k/fz4ef/xx3H777bjiiitE/ttvvx3V1dV44YUXEIl89tWXESNGdHR3iIhoD+K9KSIiCqo3xAz+OJeIqKuYXfx1AF9rSETUS+2BmEFERL1UgJgxdOhQlJSUZP9mz56triqVSmHFihWYMmVKNs22bUyZMgXLly9Xl1m4cCEmTZqE6dOnY8CAARg9ejRuuOEGOE6wN0oQEVEn4jiDiIiC6gUxg78UJyLqIparvz6o5f/a6/P8WkMiot4u1zGDiIh6ryAxY8OGDSguLs6mt/Yr8a1bt8JxnOz0Gi0GDBiA999/X13mo48+wjPPPINzzjkH//73v/Hhhx/i+9//PtLpNGbNmtX+HSIioj2G4wwiIgqqN8QMPhQnIuoq7n//Wvu/duJrDYmIerEcxwwiIurFAsSM4uJiz0PxnG7eddG/f3/ceuutCIVCGD9+PDZu3Ig5c+bwoTgRUXfDcQYREQXVC2JGt30obmPX73YP7eJzV9nTLwTL+TvvO/iqfyfg6xBCAdff0f3SzntHz0HQ61bbd21Zx3dwnYDzKoQsuYGgywrKYrbyLoug50nT0XMX9HhrZQta/3KpI/vZ1jKW+eyvtf8Dgs/11/JawyuvvHLHttvxWsNHH30U/fr1wze/+U387Gc/QyjUXVrUYIaHt6EgsuNox62MyBPxfV2txJYtxZBwobJ2/VczfpWZfJEW8m1zU7pU5NGux5Jwk0jLt1MiLarsZ8p4Q3sUMo+/bWpNwkRFWki5cuN22lcGWX9CytXgPz7tYQdoQdyAV61WXtfYu86zGxHZf540rtLuO8o2beU4FoYSHSuYQjvnWjm0fPl2cpfrj1ryWmx0Zd3z1zMAqHHkdWcrDau/bCmlC6zWbUu57pS2I6LsQ9x3zYaUa73IbvZ8bgy3Xq+DxAwKpthOotBu+/r1n9NtToHIE7dknRwdrRdp2lm9YfNkkbZvXpX3c7RS5NGuF60t9bdhgN5WpH1tm1ZPta97a+vSttmoxBH/cdNiUkJpIwsgj7e2rLoPHaS1J1rbHGxd8phFlaI6Sj6tzY378mnHP6L0EzSOsqxWv3cnbot1KedJO5/+cmjLvZkcKtLOKPxQpL2Q7CPSSm3Z5+oo7Rxr4spoNUibE/R8ArmNGeXl5QiFQqiq8rZRVVVVqKioUJcZOHAgIpGIZ0xx4IEHorKyEqlUCtGobBu6rW0xoHHHeKA5LMsebvZeQ06BrAsFH8u+ZKpEnoyaA2Ra4XqlfR3kvV4qXpLXrHbLL9Ior7NUkZJWJstR6nsxQLowIvLYKbnNWK3SloblNos2yH5j7V7esViij1wu2SzHcM2NRSINVbIc/bfJc5W32deHa2gUeSxXLmc1yfJbaeW6dXzL2nKfTFyOQd2iuEyLyHrVPCBPpCVLvHXIjchtpvNlmmXkMTPKDRoljIhXqMZq5LrKVsu+tp2SxzaTJ/czWSrTEt/wLvvSW/uIPNGw0if/SO6AIw8jBi2rE2lWwneOlWMRUs5TdJvMFyuV571+iExr8IU9J1/pqyW8BbEyrfdfOM7InarV/WDH5bW6s1i199yEN8i4klFuTUVrZFrjEHnuY1tkfcvb4s3nbKoSefJjshzRWtkfqRspL47qUbJ+DXrWe2009ZfxoalCLpf/scyndbEsV1bOfe71xsLKw+X9AuXWHeAobWKp3Kh2G8c/PIvWyEZAm385IpsTFGyUaaGkd9lQSq4rkycL5oZkPUwr8V6rQ6l8704l82UsqwvL9TuOsu8iBbBDyrH1nU7XVdrlOllH89fLfbeV0BvbLuvLtkne+pK3Rq4/VS6Xi22TZUsXynwR2X2A/7ZWLKGsX6lD4WZZtkijMiaPyDfF1o70rq95sDxAlu8acJtbv6fYG2IG5xQnIuoqrtX2H4LP9dfWaw0rK+VNdeCz1xo+9NBDcBwH//73v/GLX/wCv/3tb/GrX/0qt/tJRES7L0DMICIiApDTmBGNRjF+/HgsXrx4x+pdF4sXL8akSZPUZY444gh8+OGHcHd6cPjBBx9g4MCBPeuBOBHR5wHHGUREFNQeiBnz5s3DiBEjEI/HMXHiRLz88suBlluwYAEsy8LJJ5/cru3xoTgRURdpmYOjtT/gs7n+amtrs387/xJ8d+38WsPx48fjrLPOwlVXXYX58+fnbBtERJQbQWIGERERkPuYMXPmTNx2222466678N577+GSSy5BY2Njdtqm8847zzNOueSSS1BdXY1LL70UH3zwAR5//HHccMMNmD59eq52kYiIcoTjDCIiCirXMeP+++/HzJkzMWvWLLz22msYO3Yspk6dis2bN7e53Lp16/DjH/8YRx11VLu32W1fn05E1Ou18bqRlleNBZ3r73P/WkMiot4uQMwgIiICkPOYcdZZZ2HLli24+uqrUVlZiXHjxuHJJ5/MvqVq/fr1sHealmLo0KF46qmncPnll2PMmDEYPHgwLr30UvzsZz/rwM4QEdEexXEGEREFleOYcfPNN+Oiiy7Kftl2/vz5ePzxx3H77bfjiiuuUJdxHAfnnHMOrr32Wjz77LOoqalp1zb5UJyIqKu4aH1i9XZ+s2rn1xq2vDKk5bWGM2bMUJc54ogjcN9998F13exNLL7WkIiom8phzCAiol5uD8SMGTNmtDquWLp0qUibNGkSXnzxxY5tjIiIOg/HGUREFFSAmFFXV+dJjsViiMViInsqlcKKFSs8b5yybRtTpkzB8uXLWy3CL3/5S/Tv3x8XXnghnn322fbuQfd9KO4/tkHe8x6x5DvrQ+j43CdOgK82pI3ME7S/oO1TSEtT9ivIujTa8Qiyn+o+BTy0+n7Khe0A+xn0fGr75PrOlZpHWVdKOcdpJV/CyLPnBChvSDv+SpKrnrsg63dkmrJYRFs2wDkBgtU/7dg6yrGVpQXSyvFI+/bdMbKs2jGzlYOrnQMtrSPzTWhlb2G18c2qVr9x1YaZM2di2rRpOPTQQzFhwgTMnTtXvNZw8ODB2XnJL7nkEvzpT3/CpZdeih/84AdYvXo1brjhBvzwhz9s/8a7WK2bh7S74xqMWBmRJ218IS9cI/J8nJA1tcbNF2kTY9tEWtySV9HqlPdX+glXu9KkkCPLEbdkyxOx5RVTnSn0fC4MJUSeqHJ8NGmlXUsrkao+E/d87heuV5aT4kpqSHnXjq20IK7vinSMvEK1NlLUAwCNruyYNTjefWpy5RdFtG1q7U5MOd5xW+677dt3NT4ogsSC1vi3oV07ttIg5VvNIq3ATok0rQ5tSPfxfNbqi3Zs6zN5Is3VzrsWD5R8QUQseY3l20mRVhpq6tD6613vPjW5WhT8TK5jxufZhkwp8jM76qbWBmzJFHk+9wk3iDxvJIaJtJW2bHOnFqwSadUpGVtWZrzrG9F3i8ijxZGoUk+1a7lUuW43ZEo9n/uF60SeqFK/tGujxo2LtISR5XUs7/XYN9Qo8mgxQ2vrgraTQaQDxhF9WW9b5yi9Rq0dSiltZE+i1QOtvxJV4vi6TF+RprX9/n7BoPB2kWdMbINIW5WW13WpLdvqIOcqeD2QfQztHPv7MBqtrNscbx+vyWHM6Ax5n9oIxXacs/g2eQCTfbx1xFjyvGvHPZRQ+qpF8npxlO8rm7B3hY0DZP2L1WnrkvXPjstyRGtkWtMA7+fidUofPazsU76yrkEyrW4vGUcGLvf2L+20jCt931H62ltlPFZllLFHk7evZyVk388oaUjKNDe963GXlS/7BJatnKdmWa+MLY9jpEFu0/K1RUrzjVBCJtpp5U6Oq1RmpRxOzBcb8+Q+NQyU57NpoFxXwUa5zfrhMl/qw1LP5wNvqRJ5TNVWkWYNkW/XcwtkHLFSyvn07ZblaI2scsCVQxtukDG06BOZr7mf91rJ5Cttjm/9TrL1WMaYkTuxLd6Y4chqhPxK70GNb5eVIdFHXi9F62X9i9bKtn/r4bIehZu911rzJeNFnoHPy3G5CclyFGyS62/uJ3d0/Ve9y1b8R7nPmpH1Uhu6J8vksp8eJfd95BWveD73j8r91K6X7fvJdblRWTbt1dCiq6fcj9DaXK1uGOWGvT9fSAk/Wrm0tNh2eRzjW+U2M/ne+uLEZFudLhJJCCn7GVZulWjn2H/LTc2j3FLNFMh92nvuhyJtwwX7irSS170bjTTKdVnaHNlKknYctX3w39aylPqiPipSho1hpR9ZuEHpi4S9McNyZX0P+24VOMnWx6lBYsbQoUM96bNmzcI111wj8m/duhWO42TfPtViwIABeP/999VtPPfcc/jb3/6GlStXtlrGXem2D8WJiHo9g9ZfK8LXGhIR0c5yHDOIiKgXY8wgIqKgGDOIiCioADFjw4YNnulgtV+Jd0R9fT3OPfdc3HbbbSgvL+/wevhQnIioi1hG/+Zey/91BF9rSETUO+2JmEFERL0TYwYREQXFmEFEREEFiRnFxcWeh+KtKS8vRygUQlWV980uVVVVqKiQb3FZs2YN1q1bh5NOOimb5rqfFSYcDmPVqlXYe++9d7ndjr1LkoiIdpvltv1HRETUgjGDiIiCYswgIqKgGDOIiCioXMaMaDSK8ePHY/Hixdk013WxePFiTJo0SeQ/4IAD8NZbb2HlypXZv6997Wv40pe+hJUrV4rXtreGvxQnIuoqfEUVEREFxZhBRERBMWYQEVFQjBlERBRUjmPGzJkzMW3aNBx66KGYMGEC5s6di8bGRlxwwQUAgPPOOw+DBw/G7NmzEY/HMXr0aM/ypaWlACDS29JtH4pvzOSjILPjh+xxKyPyFNlpz+d85ajHlbnqI1bAH8grJzFhvF93aFLzyInoHciChNTyOiIt5sunTXNvW9r6lZ0PqKNfBNSOrFYO7RxELO+e2cra/HnaI22cNj8DQINJi7Skco7r3YhIazQyLeFLi1ty/Vrdjihfq0kbeTzSSl3zc7W6YctyRJTqop1P7dwFqWuOUt9dJc1/jQGALK08Htp1F+T4AEBEue6069N/HWvrT/mu0EbHBbBZ3W5b36Dit3Hb563EEMTDO643WzmASd91WxmtFnmq0iUiLaZcLwk3KtLq3bhIe69xkF7gXXCN1m7KehpXyubf9wZHlqsk1CTSmlw5v8umlDweHzb2E2lF4aTnc2lErt9W3rtWGEqKtPJIvVy/3SzSQr71OcoxSxvZzfG3y5/lk9dywldfmhx5zrXlmpV8rtJG5tkpkeYXs2V8cNTYKOuGVraQEt3zQ95y5CvlGhTZLtL6hetE2qPbDxFpeSFZR/3bqFba28JQQqRpddl/ngB934PEg4wrj62/3WhNcVjW0Y6UIdmQBvC++n+MGbmzsnkY4qEd57bBke1fXSbP81lr16rTBSKtQGnX3o1uE2kzKhaLtFuqjvV8fr5+P5GnPNIg0oZHt4q0uC3bnQ2+fQKARl88e6/hAJFndN4nIk279t5skt/K1tr5PmHvPlQrMSnflstp/eioFhuVfP7Y6Cr9ao02htPaYe14iDxq/JFxStumxr8PWt9Ho25TiaH+OKvlK1baai1m37PlcJE2pexdkRZRxkV9bW/90I7jbp1PbYzl689reVylHqTU+COPd70Sz/z9yMqk7IM1+67XVEMKwDsiH8CYkUt2GrB3Ot3KKUWy1Hu9pPrKtqnvClk/kqVyXWXvKHWyUClYg7c+Vx+snFjlXkDJBzKb0lQj2Ve2AaGkd31bxsvlIg1ym8lyeTwiNXI/+74ttxmr9MaM+CtVIo+VL+Mbwsp9uT7yNZ7J/nJZO+29HsNNSlxJyPbKrpPxzEoqfX5f2UxEVio3LvsmTrFMa66Q7UndUGUckPIe21itPNZ5W2VauFHuu5VSxigFcgyU6Ovdr8YKWa6GYcp9opisL40jZL789fK4DVnsXTaxV1+RZ/v/DBRpfd6VF4Hlym2akDwHJuSt80bpg4USyjWwVdaX0KeyzxhS6nJxH+99hnBCXv+1I7zx0mlj+MmYkTt2BrB3OmV5VbIeZfK9daTycFln4ltkWkGVTNNiEkLKNg/z3meJPVsk8mw4Xqb1e0NpAzJy/U2DlJhR723nN0+Qefq8JdMaBsv9zBTKirjf3UqbO9o7lom9tV7kiQ6S8xDHt8r+ZSZfpm0eL9uAxuHe69tKa+dJ7qedUtqKZpkWrfGm2RmZR2uv1KkPAj6w9N/ybBoUrCHQ9inSKNPCjbIgEV94126fRJQHcZuOlGmbv7aPSEuUy3yl7/nKldCOozJ+UKbHVrKpxH4pwxhjK7FRhllk4nJhOy0zxmq9dTRVKBsO/747qdYrS65jxllnnYUtW7bg6quvRmVlJcaNG4cnn3wSAwYMAACsX78etp3bF55324fiRES9HQceREQUFGMGEREFxZhBRERBMWYQEVFQeyJmzJgxAzNmzFD/b+nSpW0ue+edd7Z7e3woTkTUVfiKKiIiCooxg4iIgmLMICKioBgziIgoqF4QM/hQnIioi/DbuEREFBRjBhERBcWYQUREQTFmEBFRUL0hZrTrZeyzZ8/GYYcdhqKiIvTv3x8nn3wyVq1a5cmTSCQwffp09O3bF4WFhTjttNNQVSXn/CEi+rxrCSKt/fV0jBlERLnDmMGYQUQUFGMGYwYRUVCMGYwZRERB9YaY0a5fii9btgzTp0/HYYcdhkwmg5///Oc44YQT8O6776KgoAAAcPnll+Pxxx/Hgw8+iJKSEsyYMQOnnnoqnn/++XYV7J3kYORFdhQvYjkiT1Go2fO5b6hB5Cm1m0VakZ0WaSHlt/0OLJHW5EY8n+tMTMkj0xImItI0cUuWzZ+WbydFHq38mrQJiTRtP/3lTZtgVcWGrPlx5XgXW3Ifinz7lS+LhXxblj9uBSubY7zHqMnIctUoF261kyfStrkFIm1LplikpXzHrVipj6WhJplmyzSNVq8aTXSXy8Vdue9FdkKkFVgZuaxyLUZ856pd37bxSStVOWHkGtO+tIRSR7Xjo7Ul2uUjj5Bc3zanUOTxpzWnMgA+UtaGXvG6kbZ0Zsx4cfteiKR21P2wcp5rU95rOS+stE0ReR2Ux2RsaXJkOx9SIr+/nkaUPK7SBtdl4iKt2ZHX9qZUiUjz13Gtzq9P9pHlMLIchSHZVr+1aZBIC4e924iGZduhCdmyopfFZTvZL0+eg4HxWs9nrazavkdsWTYthvrjZYNyzuuV8+QoxzHhyLYo4xaJtGjIdxyVshaGU4G26SrtZsSWx8O/X8VheQ3YSr19s2moSNPqqFaOZst7PLZn8kWekNKfcJTo0qwcW22b/vqtXXdpV/YxMsq66lLyvGviIW8bk1H6YM0Zb/kzjbIeZzFm5CxmrG7qj4i1o77WpWRfL+WrD/lhec3aljzwDSF5HbzdLK+XJlfm89fBmNIGaO3ax6lykbY1LfsoWhvQ5LtutX1anRwg0mqV6/aFLXvJbYaUbaa99b4oKuv9keVr5LqUfY8p44x8W7aTpaFGz+cCZTwVlDYuSvmub60d0vqlCVemBR13+UWUfrvWbmrjQa1fo+Xz93W0Y/1q7XCRtiUh62O6VFm/0vZvzJR5PmsxrzikxC5lXa5yPPznDpDnL+hxdJTzrlHX53rPe01aXmNJX550po3tMWbkLGbA+u/ffzX1l3Uw7Bu+x7fK67hhuDzwqXLZrqW3y2Xj2+Sy2w/0fs4fWSfX/64cKzTIkISoXBShhNzPVInvuuov29LUZtmeWGllXRWy/d4G2SZuPrTUm2DKRJ7hT8q2qGGwEmflZaXevHVi3vKmSuQ+KSEaVqZUpMVq5bnzhzOlWYMSsqGEDKSK5cLJMrlN/7KJOiVOlcljVrJOrt9WbtrUDZOFaxjqXTa9txzn2Z/KfnUooYxjtshyaMdt+/6+vrXSbfdfrwCwdaxSb5VzoAklvcdD6ZogphxvO6lsU+mTwsjjXbTaOxbeMkFeF+3CmJGzmBGtNQhFdxy0mgN33QbENyvjV+XWbt0wGR+UIThihUrbvMF7Hzuu1O+mYbJhqwrLa7vgU7lPajvm++wqeaoPVtal3CcK95ftx0enKeOdWt+4PyrvfY14tEakWUl5QNxSeY2WfCTzRX3Xd/U4mSdULBuGSFQe71RSnuN0jfckR2pkfQk1y4Mb2y6SEGmSx1ZrJ/31yimW+9RnYK1Ia2yWxyyVkuPoAqXfYfnaulSh3M9tY5X6EpVp1eNkcDcRma92P29/PpRUrsWwcswKZJqt9HW0IaeT711WGz64BcoFGlJie1jWF0sZoxRscn15lD6prz/hJJULtkUviBntGnE/+eSTns933nkn+vfvjxUrVuDoo49GbW0t/va3v+G+++7DscceCwC44447cOCBB+LFF1/EF7/4xdyVnIioh7PMZ3+t/V9Px5hBRJQ7jBmMGUREQTFmMGYQEQXFmMGYQUQUVG+IGbvzg07U1n72jZA+fT771s2KFSuQTqcxZcqUbJ4DDjgAw4YNw/Lly9V1JJNJ1NXVef6IiD4PesPrRtqDMYOIqOMYMxgziIiCYsxgzCAiCooxgzGDiCio3hAzOvxQ3HVdXHbZZTjiiCMwevRoAEBlZSWi0ShKS0s9eQcMGIDKykp1PbNnz0ZJSUn2b+hQ5V1ORES9kdnFXy/CmEFEtJsYMxgziIiCYsxgzCAiCooxgzGDiCioXhAzOvxQfPr06Xj77bexYMGC3SrAlVdeidra2uzfhg0bdmt9REQ9hWXa+GZVDwkiQTFmEBHtHsaM9mPMIKLPK8aM9mPMIKLPK8aM9mPMIKLPq94QM9o1p3iLGTNm4F//+hf+85//YMiQIdn0iooKpFIp1NTUeL5dVVVVhYqKCnVdsVgMsVhMpK9N9EMsHMl+bnaiuyxXQVjOXl8SapZp4SaRVmTLfFFLTmqf8k1W3+TKsmtptU6eSHOM/E5CRNmmv7xaHk3aV1YAaHK08spjW5eJez7X+z4DQH1arivjym2GbWWfIgmRVh5r8HweEJGvnhkQqRVppaFGkVZgpUSaX8IUibRtTqFIq87ItE2pEpG2JSXX59cnKss6LLZNpO0d3SzS8m1Zvx1YIq3eV9e0fXKVumcr77fQ6lrUyshl4fryyOW0dUWUdYWUcoSUrxn59127noKuy7bTIk3jGO82EyYi8viv9YQj97FFW68V6SmvGwmiM2LGR1v7INS0o51ynF1/5ysSkXUyLyrrwqZYsUirzJNpthL5w74TmXBk2HWNch2nZZtrK3W3KCrb0vywt/0bEq8ReQbHtou0/7dhnEgb0/dTkZZoUOJxynu8rYzSxjTL/XRjcp+2RGTa6jx5HeUVetvE0gIZx/vmyXhfrMQfrf3zx7OmjNzvRiWtPinrZyojY6NRznsk7K2TBVEZy8picp/yw7Lean0irQ5lfH2FLSkZM96sHSzSkhlZl5szsk2MhJS239cvCNu5bfDioV236Vp/xVViasaVdbkpLc+7dh03wFsXYmFZjxtS3nU5qdZHEIwZORxn1PZFOLMjXTs3acdbRwoj8prS5Cn178Om/iKtKiH7jf72+/FPRslyKe1JUVyW7dNtsq96+MiPRNqrj4/2fFaGD3CUtjq/UqnzQ2W+fqNln7Yp6a33NY1ynPSs2UekBYl5AFCstH/+NrEoJNel9VXzbbl+rf/tp4251LGZMg5LusH6Cv7yFir7re1TwpVtdU06P1A58kLe4+GPIQDwUW1fkZZW+mV3bZgk0jQFEe82J5d/IPLUOgUiTYvtIQRrLB3fbwe0sZO+nNLXUZbV6oI/rTQi4/0Lm0d6t9fYervEmJG7mJEqBkI7dc3TRfIAunFv+2eVyLbD1Cl9aKU5qd9fxpGGtKxHsQHeOpJYJdt9N1+2y25clj89ULnH5MjChau97YerdFtK960WaTVr+og0u0Fp62TzBPjaP+1m69qvyQXDA+S9l5JC2faPLJX3Y/z9/g01pSJPc7M8n9GY7E/UKPn8O+FukWM/2HJHjTJOspRxUiii1FHXexyTUVnXQylZzxKlSp85LOtG3d4iCekB3usgskHup6vskxtV0pRtatdPc55338uH1Ig82+tkzHNTcj+tkDyOlhIOnHpv/QvVy3VFGpU+QD9Zjrxtsk9kZ5R7WGlvmham6kf46lmC44zOiBnJUguh2I7KqXQ5kfG1zc2D5EG2k1p/U7lGB8uYYTUrjWnYu82G4Up9UJJs5ZbmtvHKveJqpa3wFaPv/rK93VIlYxeScj+tjfJ6gdJ+ZHxdQidP5mkaKu955K+VzyBSxcp1W66cg3Lv51CTzOMo94/D5fLgxvOU5xm+tKYC5X7hdi2AKvflgj1SQrLPrp9sjiqXb1GoScnz9MH78ngXfSLrbd0I7z5oY0s3T2mQlJhnNcs+RqhZOS+++qEMf+AUKxeB0kfSGmJlSAiUePc9HJfHol+xHAf0Ue5vfljZT6QlE/Ic2L5+ZP9XZB9pzRne+OMmWm/8e0PMaNcvxY0xmDFjBh5++GE888wzGDnSOygbP348IpEIFi9enE1btWoV1q9fj0mTgg16iYg+N3rB60bawphBRJRDjBmMGUREQTFmMGYQEQXFmMGYQUQUVC+IGe36pfj06dNx33334dFHH0VRUVF2Xo2SkhLk5eWhpKQEF154IWbOnIk+ffqguLgYP/jBDzBp0iR88Ytf3CM7QETUU/WGb1a1hTGDiCh3GDMYM4iIgmLMYMwgIgqKMYMxg4goqN4QM9r1UPyWW24BABxzzDGe9DvuuAPnn38+AOB3v/sdbNvGaaedhmQyialTp+LPf/5zTgpLRNSb9IYg0hbGDCKi3GHMYMwgIgqKMYMxg4goKMYMxgwioqB6Q8xo10NxY3b9+/d4PI558+Zh3rx5HS4UEdHnQluvFekhrxtpC2MGEVEOMWYwZhARBcWYwZhBRBQUYwZjBhFRUL0gZrTroXhn2pQoQSQUzX7eligQeZozkV2uJxJyRFpBJCXSSqNysvricFKk2b6vO2RMSOSpSeWJtKZMVKRp4uG0SAv7tpkXknlcY4m0pCtPb0M6JtJqkrK81U3etMb6uNxmozz+dkJOU29lRBKUwwY37t1Pq0AumF8oz0lJfrNIK4rKfPlhed79tPNUn5LHrD4h01IpebyjUe8+DCyqF3mSpXK5EOTXavqF5bJp5UBuyRR7Pleli0We6rS8nrR6W5eW570xLY9R2vGWw7ZkC5in1O2CiHI+owmRVhSWacW+tKKQzFMSkte1dhw1/msdABLGW+frHXl8tqaLPJ+TabnfLSzXwHL1aNFaOukyqTDc8I5rKZNQwpuvXqYg27BETKY1ROT1vq0xX6QVxGQbU9/srSP5Sp7qWnk9hkKy/mnjuHSTvB779KvzfN6YVyry+OMKAGyvl/v0QmKkSAtvkdv0t/ORehmTosql5yih0ZGHG5l82dalYt6Mm/ILRZ5P82UfIBRT0sIyzfLVF6PEWceRMc/NyDSTVIKecj4tX9lq4zIONhfJOpoXke2M1ub2ick2scnXl9rcVCTzpOQ2a2pkvTUJZT8z8rhZvjQTUQ5GRNbRUL48Hv44CwAFcXmd+Y9RLKx0ThQh5VqJheSySUe2OWnXWxe2Nshjlkx7l3Oa5PFqwZiROyNLtiFSsKMB+mB7f5Fnn9Ktns8xW7keHXltuEa2ASuqhoi0kjzZb3l73SDPZ+MqbWm+vLb9/TBA74t9OPcgkRap8H4OJYPVpfqRMl+kQZZ3yzv9RJob87WvcXmdrXflcUzUywDhj3kAkMrI67Gi2BuEtDZSGysUhGSaC7mfWlz10+pLRqkvKeV8avUqbHtjhlb+QiXNUeJZY0YZ27iyHEvW7CvSxPprlX6CMq5riMl+tBZX/T7cXC7Sxgz6VKSVxxpFmjamiFhKv6CDP3GwA94F0sZw/vrR7MrjGPfFrkwbsYwxI3fMgfUwO7W9brO8lodUbPd81u5D1ZTKOl+9WY7VQ9tlGxapV9oK35je5GmdS5kWalSusyalT6sMpzLFvv1S7gnVheSYwtiyHEbpp0cGyfsDGV9fKdMgC3bgfhtFWrFyX+GostUi7eYVU0Ta+JHrPZ/37btF5NnUKM9dVDnvzXF5jMK2t43pM7hS5Ikr9/202K7R7g9+WO1tO7fXy3KZkBJ7+yp1r0RuMz1Aud+W8i6rhDI4ZUo7powfoJRNq1f+McTWTbKw++wlj/fEvuvkuhQbE6Ui7dNG7za0e2Zb6+Q4YPtwmW+7Mp6ym+W+h5K7Prb+MZZxWq8/jBm507RXGnbeTudRqc95G31jwLg8xlobnBggr5ewEjOMraT56kOmn7xm7e3KPbID5T13KPfbMgVKO1/m3UZNvbzvHFPu86eU+23+8QMAhBtkxc8UevO5ebJdThXJ62zjtDKRVvCJPHcZeSkjXu39bCkXZEIpfyqx6+daAJBX4D1Gdli5X6is3yirT5Yq97W0e3D9vdvML5YxtapJxsEiJfamC5Wy2bIctfv6zp1yXWjsOu0akPncqHKd+Z9FhWW5Cj+UB7JpsDwH/nUBAJR7XflF3mNUUSJvlg4r3C7S9s6XfZHSqLw+X0zL+7h2xtsHrdlPXmO2v0lI9e57U932oTgRUW/XG143QkREnYMxg4iIgmLMICKioBgziIgoqN4QM/hQnIioi1hG/fJ+9v+IiIhaMGYQEVFQjBlERBQUYwYREQXVG2IGH4oTEXWR3vDNKiIi6hyMGUREFBRjBhERBcWYQUREQfWGmNFtH4p/XFeG0E6TGtQ2ynknUknvO/2NctAtZQ6BiDL/ZJ4yz2uhkuafwyepzPnWlJTzwqTSyvwGyjcn8pV5MKO+uU7985wCgKPNu6dsM9GszAetzAMVqvEuG6+W649Vy3JEG5Q5q5S5CbW5I5yY91imlTlkMwVyHq7t+XKOoC35yjwR/vl0lKlPtWngbG0+VDklCZTpBdHgm9drTb9g8/U1Fst8/ZQJeR3I81KT9l4rm5rl8alskHPGblfmNc4ocwKGa2V5w83endeOj3a8HWXes0yhvJCtEmWOR98cHP0K5byBA/LkMRsYrxVpFTGZVmTLeVCaXN8cxspkWmsb+3o+pxvbmMvetBEsesg3q7qLTDIMe+d5kxzlgvTPU6kcY1OrzKFUICt0MirTtPnI043ea6jRyFhmNSsXR1KWX5sTx1IWrc5462VtbV+RJ1or169MYw6jXMvKpYZwo/dgRprkyqJ1ypydKZlPnT+8SJnLrsi7D5m4FjNkmjZfkj8+AIBomrXpdLTp7gJ2AE1Ii1PejaZiMo5vScl9iipzj0cjMm2TJed8qv3EW19sbU7JmHI+a5T5bRuVeXaVKcj8/QKtHXRDSj0oUfpXJfI4bitWVljoPR7anFh5UWVudmW+dm0+3mZl3nX/vMbptLJPvjbCVY5XFmNGzry0bgTs/B19yn0q5PxcNSlve63Nz/n+xwNF2pCB1SKtX4FsOEO20t/Z7q0PffeR66r+sI9Iyyjn382T628cKK/b+DbvwvUj5H6mlTkCtetdo7UV/n6irVwbTp68poqa5Pprhsrjoc3n9lGjt/G3lHlI43F5vWvnKazMGesf2mjLaXVIo80Zq60v5MsXU+aX1ubG9s9l29o23/54kEiztnrrqKX0t8LaOECJLRlLtukaf9Eyrtzma+vlXOf2EFlhSotkI1sSV+YZ983XPrRAzusXUY6tHfAuUEYZGCUdb8xIKpOJbtrujeNOkyx7FmNGzqQaY7B3Ggd+fexKkWfhi+M9n7V+nnbc8/vL+JCIyWsj48j7IP6xQZ+35bWRKpbXXrJUGYNr7XyTcs/Gf99JadZMoywrlLlOkZFlK8qXc8vuXeadL/zdLQNEnvfWyXhc3k/eCyiPyuM9atgmkTal73uezxtTcq7ZI/t8KNKSrjKWVNK2Z7xzeYYgL1ZtXaUR2a71Cct9anJlHar0zYFeUyBjRrJUaZvkrsNR+hiRSlneTIk3n6Pc6ynqqwwuFU1NchBnlHgQEvdP5bo+3iJ3at9i2RdsyMjj+FFtuUirT3jLVpYvY83AsjqRltdP9ju0vkIio8xnm/amJZX7v/V13r6saea9qc4Q2RqGHd9xPkpXyTxbJnmvP60/ZaWVyqvct7CGy2vI2SrvO/nn3+6/f43IU9Uk67ddJa89W5uvWel/R3z3zawP5X3n1FDZ7hcMke1346fyPnafMfK63fK+dx/sJtmuVR0uj6Mti4G8rTLf9nLteYY3TZsjXpv/PJ2nHUjlXkN9oS+PXCzUrNzHCSsXb55cf7Kf7LyPGLLV83mf4q0iT7Mj26YtzYUizT+fPQB8/HVZjkipN8a5W2Q9LlD6Te4KeW/eP7c8oD+DgG9+dku5/9s0SF4DsS3yeDv5yhz02nOPKm88Xr+XPI5lMWXgq5hQulakuXvLcryU3svzOdyk9DVLve2S26w9/PqvXhAzuu1DcSKiXs8Y/dsxLf9HRETUgjGDiIiCYswgIqKgGDOIiCioXhAzlK+kEBFRZ2h53Uhrf0RERC0YM4iIKCjGDCIiCmpPxIx58+ZhxIgRiMfjmDhxIl5++eVAyy1YsACWZeHkk0/u2IaJiGiP6g3jDD4UJyLqIpbT9h8REVELxgwiIgqKMYOIiILKdcy4//77MXPmTMyaNQuvvfYaxo4di6lTp2Lz5s1tLrdu3Tr8+Mc/xlFHHdXBPSEioj2tN4wz+FCciKiLWKbtPyIiohaMGUREFBRjBhERBRUkZtTV1Xn+kkllEub/uvnmm3HRRRfhggsuwEEHHYT58+cjPz8ft99+e6vLOI6Dc845B9deey322muvVvMREVHX6g3jjG47p/jmbcWwm+LZz6ZJFtXKWG1+BvTX2CfDcjL5RCRPpG2Pyt/725Fdf93BGKUcGeX7B47Ml2yIiTQrFOC9A9o203KbVlNIpMW3y3xx35f38rfI/Y5tz4i0cKNMsxMyzXKU42h7y+FGZVndPFkPMnElX1TuUybPm+bIagA3pBxHuXq4ypXjRuWyqSJvWjITF3k+zvQVabXNMl9hLCW3qZz3xqR3x+obZN12t8mdj2+WO1q0VV5A8e0yLdrgO8dKlTVhWVYnphyzAnnuUsVyH1Kl3rR1fUtEnrV95DErLG4WaaV5CZkv2noHv0VDSl6v25u85XKaWl+P5RpYrh4tWksnndUQguXsqMOhZqX9C3BIrbRMc9LygjcReb04kNeV7bsWQglZ5+20kiabTfXbdtpraYzl3Xd1v5W0wk/kyhJ95XEs+0Bpi3zXd3yzvM7sZuXgpmRazFZiUlm+LFu5t53M5Mvl0nny2GZkc6K238Zu+3OraVp8UNIc2czD8p13re/gKsErkZAbkK0a1H5HbJt3G3ZS5gknZH2P1shKFKuTlTTSoPSl0t4025HrciPK+SyQ5UiUKvmKZD7/sslSeRwbS5U+TL6SFpLl1S4z19cPM648tkjZbX/eyZ6IGfPmzcOcOXNQWVmJsWPH4o9//CMmTJiwy+UWLFiAs88+G1//+tfxyCOPdGjbXSkayyAU29H+fLipv8jj1Ec8n+1C2V7ZEVm/N24uFWnhqLw2Miml/xr3rm/7e7KPGJHNKyzl2o58IuuSI7stqNvbP56SeQa8IvczE5fb1GJGQaVcNlniXVYbZ2j9wUizrOcFn8pyVI+WxzaT8I0DYrJcjY0RkWYp51gbm/lCLywl+KpptkyzlbRwWB6jsK+T0RyS5Q/5OyKtpCmtE0xSHkd/0cJNckk3JssfqVHOZ73cphajY75440bkNrX7AJmqQpG2ZaQMvlu1trnIe72vzZfX4vC+1SItHlLGx3awn0tkXO/xTrny+A/tW+NdJp7ER62sj+OM3MkrTiCUv+OYPfbewSKP8fUXtPjgNir9tSbZH3GVa89S+h6RBm/dbRqgXclSpkCuK5TadV8YkNe8E1fqknJNhZS2Im+/GpG2tbJYrs8n0SyPmdaWbq8tEGn/3jZKpEXzZHx/xB3n+VwclT3r0UWfirSRsS0irTTUKNJebxrh+RxRBnr1ymBhY6JUpC1vGCnStjTIfW+s965Pu1+YKZXl0NpXreOrjTntZu+y4WHyWMQjSsdDkbDkeY8o5655k7ftN/nKPilx/P/W7CfSDhn6iUirbpDj0hG+eDAgTwa4xows//q6MpEWD8vjod2bGlm8zfO5KCzrqOu7iFMNafxN5PpMkJgxdOhQT/qsWbNwzTXXiPypVAorVqzAlVdemU2zbRtTpkzB8uXLWykB8Mtf/hL9+/fHhRdeiGeffbbVfN2dMyQBs1M1aT5Qnj/bd+/f2iw76eGh8npJbpUdpXhcXgfOQFnvQ756X6/cd7aK5b0eJJQBhDZkVfrW7kbf/dF+yvOBBtl/Ta+TZQvlyfq59Z1+cn2+xsgpkOWKbZbx2FHWv+V/5HW1/2x5XqqO8vYT6/YWWeAqx8yul+UID2yS+XwxLtEg2xPHUvZJueceqZPl8PdhAOCQPhs8n2PKTcr8kKwv1Un5hRaTp7TDSp/IrPfGLlMsl3NWynv/Wu/HUcYjeZvkSWga6u1zuco9OCsq15UYIMumxRvtWnErvMdSGyO+W1Uh0jY3FYm0igJ5QrVnRfml3psIiT7yuot/6j0nTrL1x8a9YZzRbR+KExH1egb6kxS0kU5ERJ9POY4ZLa81nD9/PiZOnIi5c+di6tSpWLVqFfr3lw+JW/C1hkREPQDHGUREFFSAmLFhwwYUF+/44kospjwsBbB161Y4joMBAwZ40gcMGID3339fXea5557D3/72N6xcubKdBSciok7XC8YZfH06EVEXaflmVWt/RERELYLEDL7WkIiIAI4ziIgouCAxo7i42PPX2kPx9qqvr8e5556L2267DeXl5TlZJxER7Tm9YZzBh+JERF3Ectv+IyIiahEkZgwdOhQlJSXZv9mzZ6vranmt4ZQpU7Jp7X2tIRERdV8cZxARUVC5jBnl5eUIhUKoqqrypFdVVaGiQr4SeM2aNVi3bh1OOukkhMNhhMNh/P3vf8fChQsRDoexZs2a3dk1IiLKsd4wzuDr04mIuohlWp/nOsj810RE9PkRJGbwtYZERARwnEFERMHlMmZEo1GMHz8eixcvxsknnwwAcF0XixcvxowZM0T+Aw44AG+99ZYn7X//939RX1+P3//+92IucyIi6lq9YZzRbR+KhypjsOM7buTZKZnHTlu+zzKPdiKU+eZhlCPhxOXCTsyb0Y3KPCasbDTob/IzsnDGeBe2tB1wZFK4WW40UieXzdsqy5u/xfu1jth2eXDD9fKk2M3KSUjKfFY6I/NZ3rJZEXlSQg0yLRyW+2kiIZHmRr3Lmqhczg3JtEy+XFcmX+ZL54skGNu7T25MLpeMRETa9kyRSKsJya/amIxcn9XkLW90u8wT36rVA7n+vK3yPEWrE3KbCV8+pb6bqDx3blQe23hMOd5xmZYu9G4kWSo3mugTl2ml8gHBJwXKsVWubdi+NFceRyvlO+fN8vxmOUauc+f/o8Asx4K1c/upNJP+dtJS2k1/XAGAkPb2YUtpq2U1BXxVS4tTtlIOo8UMrUpqTal/P5Vv6Wn7XlCpFA6y/obrZb6Qr+23GpV2IqUFaeU4NjbL9dsyXyTua9NDso1Rz2dKpmVkUyHOgXZ+3ahclxOV+VRJpf3wvWbIVeK9pfUTQjJN23etDxBpaLOUAID4dlmJYjVK2nYZ70P1si7A9h5cNWYrsT2UlBXXcmUdtR25Pv91rB2fjNJeO3EldkXkxail+dshrU9qOb5MiTY6jAFiRsvrDHOtt73W0MnYMJkd9SSeJ+tu0nfCnLTSb1T6AVD6ZqkmZaChnGp/H0Id2yjbjNbJfFpbFFLGU+Em72etravdSyYWbpRtgBZb0vlKn3Obb5yxVRYsvlnW9XSx3KlYtdyoCcl81aP85ZAnoOg9mVZ7kAy0Jqyc95C3vJZyrVoh5fpVjllI6fNrdS3j24Zty+W0dWldpPraPJGW97FsEyONvjIo8TNSL7cQq5X7HmmQaaG0Mrb29QGayuX6wwm5XFh2JxBKymuxqUKpa77xSDoij+P66jK5zbA8oTElLRqW9Srkb3OUPkAq4y2/s5sxg4IpL2xEuGDHOdt78Mciz5tbB3k+h5TrsTYmr7Nko2yvrGbZ5qr3q3yn0WhtjMItUNpN5d6IJsg2I2Vy8OR+Im+W1FcXiDQ7Jsu2bav33ohR4nG4QAZMp1qO++MVjSKtME+Wd9Un3i8Nuil5TlZs3V+kaWM47f6gP83uK8sQjcp2IpVS+qXK8bCqZb3y3xfSxpHaeDNToLSvTbJ9ahqm3Dsq844DyoqaRJ5ESsaa+kYZXIy/zwzAVu7f2X28x9JJyGOmxWjtRvFrG4bIfIo+Me9+NTtyn2qT8voviMr+jzb+a84oYxRfvqQr9zMv5L0uXO3Gw44V5jRmzJw5E9OmTcOhhx6KCRMmYO7cuWhsbMQFF1wAADjvvPMwePBgzJ49G/F4HKNHj/YsX1paCgAivSdwm8PY+XGLrPVAPN977jND5LnR+n5WgbzOkklZP/oUy7auOOq9NrR6VR+S5ahV6mQ0LtvctNLHsgd78xlHGU81yHIkByuDFqWtCzXIttk/vo5XaveilXatUel3bZFx5KNvyPap7H3v+vKqlPvT5XKbRavlNhvqZWxMFnuXDSv3kpy4dl9bpqX6iSQUlMh7NjW+hxz+9gTQ2520K89JeJs8x5kSpS9S7t1GeIMy0FAGMumiYM/mmgYpN0cD/KrZKlba6kZlnLRZpqVLlXtY+d6NRiIyT2mBMpBRbGwoEWmJtDwvzQ3eumwp10DK9xzUTbRxcHrBOIOvTyci6iKWaWMODtMzgggREXWOXMYMvtaQiKh32xPjjHnz5mHEiBGIx+OYOHEiXn755UDLLViwAJZlZX8xSERE3UuuY8ZZZ52Fm266CVdffTXGjRuHlStX4sknn8y+pWr9+vXYtGlTrneDiIg6QW94ntFtfylORNTb9YbXjRARUefgaw2JiCioXI8z7r//fsycORPz58/HxIkTMXfuXEydOhWrVq1C//79W11u3bp1+PGPf4yjjjqq/RslIqJOsSfuTc2YMUMdVwDA0qVL21z2zjvv7NhGiYhoj+sNzzP4UJyIqIu0fIuqtf8jIiJqkeuY8Xl+rSERUW8XJGbU1XnnXojFYojF5KtCAeDmm2/GRRddlI0R8+fPx+OPP47bb78dV1xxhbqM4zg455xzcO211+LZZ59FTU1NB/eGiIj2JN6bIiKioHpDzODr04mIuojlmDb/iIiIWuQ6ZvC1hkREvVeQmDF06FCUlJRk/2bPnq2uK5VKYcWKFZgyZUo2zbZtTJkyBcuXL2+1DL/85S/Rv39/XHjhhbndOSIiyinemyIioqB6Q8zotr8UL9hoIRS1sp/tlDygdsabZmnzvyvnwShfBXCjMi0Tt0Sa40tzlC9SO3G5UaMcaaNNSG/kNi3H+9nOyMVCzXK5SKPMF62T24xvlwcuWufdSKhZbtTKyOWMpZQ/Infe2AG+jxFS8ijrh3LeraQj0kJpb5pJhORyeUpZw3KbbkTZTzfAuUuJLAg3yP10kzJNe/2EnZRp0XpvOaI12jmXxydvS1qkRbY2yW3WNsiNZnz1IxoRWYxSD+yYvPDcqJJPOS+hpDct3CzPZ0Q5tqnt8jxl8uWy2rXtbzu0Nsf2HUYnKde9Y4VQ26js/3XAvHnzMGfOHFRWVmLs2LH44x//iAkTJuxyuQULFuDss8/G17/+dTzyyCMd23gXCiUs2NgpZqTlefa3uXZG5tHaV+08+6/t1vL5z6O6nHKuHSUmaYKUzU7LDYSb5XKpYnmdFVTKdiHULNPsLTWezyYl85i0bADdRqUgCmtAX1mOJu82InawtjqUlGnhhGwrjO/SdZR1ZeIiST3HaaVsWjgLOd5Ef3sCqN0EhBMyMb5VnveEPIyiLxXfrsSManlhRKsTcl1NSpDT5hNSLxafiHJOtOOoHG+tzlu+8xdS4ieUY2sr9UXtF2j9vJC3HFr/UxRBOZc7VoCcx4zP62sNY7EMQrEdF1hzQvZbMv747SjtSZ6sgE5aOdHKspZSt0TsUqqD1gakimSaVse1Omh8u672bZTYmInJguRtU8YUtfIYRWu9bUV4u+xvWkmlX7pRrstsrxVpxamRIq25b4G3DHUiC2oOkBdS4Yfy4m4cqoyBIt40//XfWprGhdJ3VOpQtNR/kpV2U6kwmQ9lhSlbKzeZyZdp/rFMvjLOiDTJtGi9PHfhJpnW3E8ZQ/h2SxtXa/GycbDc92QfWbZMsRZIvPm0/nxSGftlQsqYIiovoGRG5gt14F2DjtNGYAkQMzZs2IDi4uJscmu/Et+6dSscx8l+aarFgAED8P7776vLPPfcc/jb3/6GlStXtl7GHmJSv7WIFe6om/etlGMrf3fHUq73eL7sJxllPAKtrdC6U77T78jLByas9IliShum5PNfB4CyX0rblEkq91T6yovUCinl0Op0vXd9VrFcl6vF2VJ5vJu35Yk0u5/cz5IXvJ18JyrXX7xeth2bDpflL31PaYvKvPnMp0qDqwgVyLLm1Wj9CblskS+GNgxs4z7FTjKFMq15oNKmF8vjnU54z11lQ5lcmVbPlLQB/WW8r64rEGm2ry3V7s27zbKORsvk2MbR6rdSh1785ADPZ+3+bGqIPD6xAplWkCc7cNGwUteaSjyfB+bL49PXd1M4pFWMFntgnPF5lbc+glBsR4PctJc8gM3+GwlKH2DggBqRlnbkdav19TTVzd52Ji8i60MsIvss+YWyToZs2X5r9zf815B2bUdK5PrT9coNMSU2OmVyH6JF3usqsV25aaOwUkpbqtyb1/RZss5brqlyLDJ4sbxGt48uFmn93pD7mSz2liNdIMva3D9YWVMV8hw3Vsq2dLk7wvN5v35bRB5bGTiueW+QSAsPk2M9NMpOi1PrTbPy5LFwlWdukVq57yWrZVpGOW5JX1hylf5QSrt9pdT3vCrlXmOj0ifa7Ausyri9boLyHDTgWKG+VsYp4xvLhLX7ok2+Z55t1f9eEDP4S3Eioi7S8rqR1v7aq2Wuv1mzZuG1117D2LFjMXXqVGzevLnN5TjXHxFR95frmEFERL1XkJhRXFzs+WvtoXh71dfX49xzz8Vtt92G8vLynKyTiIj2HI4ziIgoqD0RM+bNm4cRI0YgHo9j4sSJePnll1vNe9ttt+Goo45CWVkZysrKMGXKlDbza/hQnIioiwR53UhdXZ3nL5nUft74mZ3n+jvooIMwf/585Ofn4/bbb291mZ3n+ttrr71yvo9ERJQbveEVVURE1DlyGTPKy8sRCoVQVVXlSa+qqkJFRYXIv2bNGqxbtw4nnXQSwuEwwuEw/v73v2PhwoUIh8NYs2bNbu0bERHlFscZREQUVK5jRnt/5Ld06VKcffbZWLJkCZYvX46hQ4fihBNOwMaNGwNvkw/FiYi6ijFt/4Fz/RER0X8FiBlEREQAchozotEoxo8fj8WLF2fTXNfF4sWLMWnSJJH/gAMOwFtvvYWVK1dm/772ta/hS1/6ElauXImhQ4fu9u4REVEOcZxBRERBBYgZe/JHfvfeey++//3vY9y4cTjggAPw17/+NTs2CarbzilORNTbtfVakZZ0zvVHRERAsJhBREQE5D5mzJw5E9OmTcOhhx6KCRMmYO7cuWhsbMQFF1wAADjvvPMwePBgzJ49G/F4HKNHj/YsX1paCgAinYiIuh7HGUREFFSQmOH/EuysWbNwzTXXiPwtP/K78sors2lBfuS3s6amJqTTafTp0yfgHnTjh+JFH2cQjmSyn+20nNXezngPvnYyjC0njnfDSlpE/mjeicl8TtQKkEckwY3IfCakpMkk2I7vc0ruZzgh0yJN8piFG2VapCkj0uxmb5rlyOVU2j6FlGpmafvuS9PeY6AUw3KVRO2bjP71K2XVyhWUVv9s36ENJZT1ayddqwfyNCHcJNMi9d5yxGsdkSe2LS2X29Yoi1FdK9Lc+ga5Ud/xtsLKOY9G5Prz8kSanac99I3LTYa8FSSUkgctWi/XFGmWaWqboKT5T5X/2gRku5RJK5myGzFAa68V8c31l2u9ba6/SL2l1oGdWa73/215GejLKdeepZ02pSmy/edXa5qU5UKtf4Ful/z7FVJiRqxWbjRaIw9IqFmmWUnlwPmub0u53uHK69hKy4PrNsq2yLwtv9hhFxV5Psfiyvq1cihpTt8ikZYu9rZFmYKQXL8rA5W/ngFQY5KTJ/Nl/Lug1JeIPDyIbwt2oyJWLfPFa7x1IV4tz2+4RlZIu14GICuRkht1lDbQfw4ysuMUUmK7nZKxxVVieVQ5BU7Me/6K18u6lyiV59iR1Urv0yl9Fv91nMnXYo1ve8k22rEAMYOCSSTCsO0d9dC2lePnu5ajlbLtSPWT5ytUF2x45UblNq2ML04pcU2LP1HZXVPrrlZPLaftzwCQv1lejwWfynYhslnpeCntn5XytTMJuS63Ru6UXd5XrqtE9pGsF94QaYNr9vMWKyrPU/lrSltdINundLFMS5Z424+a/eTB1rr8yX7ygEe3y2XttFLXVvtirxYzGrTxscyn9YnCTUrM2O5Ni9Uo48iUMgZtkPFBG6fnb5LLGt84PVUgxwraOKx4nUxrSMmdb4zKtEiVt34MXibr6NpTlLpRKI+H48j1u42y/sV8bUx6LzlosULefXKVseCO/8xtzDjrrLOwZcsWXH311aisrMS4cePw5JNPZr+Qu379eth273wR4f97fxzs/B2Nan5xQuRp2uDtS5qorMvNTfK8hxqU+1D5ynWQJ9P896agbBMZeZ3ZIaWPpcSk/QbKV1Y2pb31vj4pr4PqyhKRZinXnlH6U/7YCwAI++411Cr3FZJauylXhWG7PncA0Ly/71qLyra6YYRybcdlvuqJynnx7WfeOrlP2r2MaJ0yfiiQ+ZJ95flM9vHGqWidXC6Tr6TlKXEkLNMy9XIf7CbvNrV6HCpW4oMj97PyY9kHiH+qjA0i3rKF9pZtqbtVxpFkkzJu1BQp99K2eDtdYWW85lTL9UfekeVoLJL1sS4uj/f2kd77ch+uGyDy2L566zYlADwmCwdwnJFDxR+7CEV21HU3ItvJvPHbPJ+3fyrbzW118uLWbllrt7+njPxApL1ZPcjzORJS7hWHZT+mpkHety0pltdVfpG8V9yQ8u57xpHj7aakvDbSRnmwElJ2NC3b4fRmb3m1/rHWFuVVyvYk2SdA7AXw4YyRns9au7nlCHk+8z8K1m/yj+vK35LnLqTE2YYhcl1WQtmmcoyat3oDwhtbhst1FSjjgD6yz2wCPvfwPwcKNSv3epQ4GFbu82v32wo2yePmhr11MlMQ8BmNEgvcmKxDEaWvXr+3txyHH7JK5Fl38/5yucHy+mkcooy1I0rcLvKeq36vKX1B3+HZ3ecZe/JHfn4/+9nPMGjQIM/bc3el2z4UJyLq7SxjYLXyKqrW0luzO3P9tXD/+xAqHA5j1apV2HvvvdtVBiIi2nNyGTOIiKh32xMxY8aMGZgxY4b6f0uXLm1z2TvvvLND2yQioj2P4wwiIgoqSMzYUz/y87vxxhuxYMECLF26FHHlx1Kt6Z1f5SUi6glcA7huK3+c64+IiHaSw5hBRES9HGMGEREFxZhBRERB5TBmtPdHfju76aabcOONN+Lpp5/GmDFj2rVd/lKciKiLWI6Bpb2n5r//116c64+IqPfKdcwgIqLeizGDiIiCYswgIqKgchkzdv6R38knnwxgx4/8WntDFQD85je/wfXXX4+nnnoKhx56aLu2CfChOBFR1zFGnwCo5f/a6fM81x8RUa+X45hBRES9GGMGEREFxZhBRERB5ThmtOdHfgDw61//GldffTXuu+8+jBgxApWVlQCAwsJCFBYWBtpmt30onrelGeHQjoNoaZO7u75J4eUc8foL4i1LJJmwzGhCSlrEO6m9G9GWk+t3w8o2bZkGJUlkUb5xYae1NHlALEdJyyhp/tUpD9LckEgClH0yyvHW8gUS9MJSz7E3zdXOr3Ke3EiwstppmRZp9JY3lJR5jFZHld0MKec4nJBpkQbv+YzUpeRytQmRZjU2y2JkMrIg2kNVx3t9Gkder1ZGOY5pedCsiGyWLHV93v20U3L92rViOlj1AMBy/Z93fd1lMspJb+G6cqU7/18HfF7n+ovWGoSiO86H1r4arc3y0U6Hdp61a1S0m9r6lPXbymVmKdXGVuqzpYRGO+PNF26WG400yo1q8UGlta/RiK8QwS40a8hAkRbS6r7S7hhfW6HFcVeJxxqtvtgp78FVQoZaN0IpmTHSrMSWkFzWifriVERkUetoKBUsPoSbZIWJNHjrQqhBVj6rWcYRtb5oBylI3FbOubZNrR5o14VyWSBW591GrFrGn0iDtqSk9vOUvkImbvs+yzxOzJvmKOdyx0ZyHzM+rzKpMOzQjjbEspV4HvPWh8xwWT/Cn8o5qyJ18jw3j1A6ico2M/neZTOO0rdR+jvpIpkWrZHXi6XEm/xt3nL4+66AjCuflVUG1bDWh0vKa9nffmsttV1WKtLcvnJOMruuSaaNO0ikZQqins/pItnAauO1dIESW5S2OZPnXVbr82eUKc5iW+RxDCnNX6x6122p1s/R9kmLGVpfJK9a1nk76R/byDyhBrkDTmFU5lPGKJk+8iDFP6n3fO7bKK8np0CelERfmRZpUPY9qRwj3/nz9wkA4MDrPxFpiMdE0rYjBsmy9ZH1qn6ktx1365WKFvXlaW7jdg5jRu58kgfsNEdhyuSJLFaer27lKX2KBnm+3Jisk0P33iLSapvltZHOeC96SxmMxCLy4h5UXCfSPtjUX6S9v0F5ZeV277WsxZqw/1gAyJTIcsQq5fHQ2rF0sW/cn1b67Uobli5T2jClKR20/2aRtnl7keezWyOv7dhWJc66yvGQt1lE4IttlwUr/UDGN7WfrtzPqzm4j0hzfM1wOCGXi9bLtOZyeVJShTItXSzPS+MQ3zlQToDTINu6cKFyn6hROd5jZF1O1XvPVeRjeb06St0o79sg0rbXFog0aH0/b5hCyUdKbEzIY1Z7sLKfyljS5Cv3w9Z4b/qHIsq9r5DvGlPGh1mMGTlT8n4dwjt1JOLb5QOami19vQmHyj5RWZFsA6rrZJ0cNXCTSPu0WfaZRxRv83zeO3+ryNPgyLbu8PKPRNr6ZtnGbGqS28yPeOt4SYG8ZsO2rF/v2QNEWnOz7EvayrKZrd5r3hQo10+TvB6dcfUiLRZS2sRKeT5jG7zri25XxnBKfHDlLiFaq13L3vUli+S6QvI2Pwo3yLSmtNz3TL5yX8u/D0ofI6WMVZ1wsJulltImhnz3zbQmyd/etrJ6ddyVLJbHLdHPNxZWxvLaYDX+jowtGZkE5Ctpxd7rImLLOrr5NHlCB/1dVpj6feU+5W2Ux3bYHY2ez1aj8lwo4W2HMm7nPc9o74/8brnlFqRSKZx++ume9cyaNQvXXHNNoG1224fiRES9HV9RRUREQTFmEBFRUIwZREQUFGMGEREFtSdiRnt+5Ldu3boObWNnfChORNRV+IoqIiIKijGDiIiCYswgIqKgGDOIiCioXhAz+FCciKirOC70eR9a/o+IiOi/GDOIiCgoxgwiIgqKMYOIiILqBTGj2z4Ut5ozsEI73rGvzSksDvJuzHNiafNea2m++TJNSJn0SJ2fPNj61XnGA8zNqs7brVGyuVFlH3zlDTr/dtB5fNV89q7zBJlzHdi9eaM7uk1tXlP/3FaW8m0ZdY54Ze5Gdd74lKzztm9uPzulTLClEHMCA7AsOV+NFVcm5vBfn9q841qaMvekiSnz5ynstHeb2rHV61DAE6rMFezfhjZ/F3zn03baOP7Gbb3dMj0jiHQXsVqD8E7zaGnzNfvnmdHaCbXOKNRrWZuP3PHnCfaNOXXe6GSwZf3th9ZOWEob08qbbwQ3X5n4KAgt5imxscPxTFu/tipt/dpU2L58arus9E20ebvVmNfBOKXOLa+1V0p5rbQyn5a/L6Udx7hsl02eUg8CvibJ8rV72nzwTqGcz8yJy5jhxJQ53OtluxtW5qD1CzXIayVUI+cXNPkyDjoF8niEfXPcpgtk+f3zEGfSbbT9jBk5Y5rCMGbH+TDaJKNiIeXaKFTmsStR1qWdnozW8Hg/Whll3tSkXK50lVyVo3TXtDjijzdOdNd9dAAoqFXmCteu0RKlIL4Y6uQp8+xGlLGHdsiGFom0UEK2da6vrQgaa6INwdr0qK+pyNsacBwTMPaq7XyQyz7g+o1yV0CbjzysHA+/pqFy/BCpl8s17aPMybhdtt8N+5Z4Pjf3VeZkVMqvjVW1+QX7r5AHsmi1d85Le0OlyOPU1Io0u1Due5/XZDyrP0DOxRlu8u5XulDuVONQb1mtRBu3cxgzcsaEDcxOc2RGlHm08zd561u4MVh/WbuOE6/KubzjyrUc81VxVxlGpwvkdbAur69IK96izCeqdJ1c372dtLyMkVLmlrYcWVdDCeUaVeYjF/NvK3N5a3OK522W29TKUW/LSUALfXMv28qx0MYGWpwK0vZr44LqUXIiUmPJNK2t1u7B+euHE1Pm1M2TO6DNkermyYqr9qV881zvNbJKZNnaINvN+lq5UdNHnoSM0peKFXj7J+H95byp6c1ym40vlYu0PGWO3iFPbxdp20d749TGY+Q5Ofywd0XaAYUytlxY+qpIO2HFxSLNlHq3kVbmCU7Ve9shN8x7U53C9wtK7XrM3+o9puHnZAPeVCzn1U7vI/tT79kyZuTH5VzARTHvtbGxsVTkqW6UbczB/T8VaS9+PEKkHTRQXt/DC6s9n4vD8qKqTsnrcXiZvM76VDSKtI9q5XW7KeM94CYlT4CJyTqd3KY0dkqzZifl9Z3o680YbpJ5Qsr0zP77hQCQydduVHo/au2+2ufXbsPL2xtqef31Vhv/xDfJY6veF1XStLid8VU/LTaGm+WOpguVsm2T+RqGynzpEm9BCj+W+2QrfYdkmVKOEnlCjz1Etv3jiryTvVdn5DXwSmyYSNvwTXlCi1+W9bZ2rBynN47wjpmLVioVoT16Qczotg/FiYh6Pdeg1buVAR+eEhHR5wRjBhERBcWYQUREQTFmEBFRUL0gZvChOBFRV3EdAK384sbd9S9xiIjoc4Qxg4iIgmLMICKioBgziIgoqF4QM/hQnIioqzhu668V2Y3pIIiIqBdizCAioqAYM4iIKCjGDCIiCqoXxAw+FCci6ioGYl5Nz/8RERG1YMwgIqKgGDOIiCgoxgwiIgqqF8SMdj0Uv+WWW3DLLbdg3bp1AIBRo0bh6quvxpe//GUAQCKRwI9+9CMsWLAAyWQSU6dOxZ///GcMGDCg3QWzHAfWzj/DD/I+eltOOK+v3JJpIWVZJZ/xp4WUdWm0L0mEgi3q36bRtqmkGVumuVq+sNx34yubuq6wsi5ln7RljXK4/WlGO7RKmjgnAKzWLkzPgsrqlfOkp8mFbeXtEJbjzWdn5HKWtlxGblRdVskn9l2rxxHlRGnXRURpIhzlgAQ53hptm1o2bT+dtLcI2roCNgmBBfiyk+X7RpTtK6eH4wCmZ79upC2dGTPi2zMIhzPZz1q746e2MZqAbZG+Ed9iWh1Srh/1etfioNaO+dcX9Et6WjsfDRioOipgG6DHA19iwOtdbSuCns8g61IzyiTbUc6x/9ypebT1B6sb2jk2/gOnHsfc1gPjayettGzvQnVJkRauTch1KX0Ylf9cKcfMKYjKxdJ5cl3KJu2EbOstx7+f8mKMNHpXlskwZnRGzChaHUYotqOPk1FOc7rIW0e0vqsmlFLSmpW+aoA+p52ReUKJYNd7rCZY4+/fptYvdSMyrWmQPGjRWlngILHL318GgFDAOh20HQ41+3dUW5eyoNZuag2xf7xma3mUbQYeJykxOrLrPNqx1eKPRuuLpEq9YwNtPKjJxDreKQ83eetCnyoZC7T+SnRLo0hr2KdEpOWvl/n88ThxyEhZrkZZ312lvoeaZbuep+yDZWKez01K7C1a5z2OTqqN48qYkbOYEdtmI7RTHY5Vy/NcuMl7TKM18ryH65TznlLqUVz2R0xM6Yv56pt2HaSLZQOuXdtOVNYlN6LdA/KVS7mPZpR2oeBTkYRMgUyzU3LZcLN3v9yYyIJMgdwn/d6UkhaWy/r3E0oeE1La0ohy30LL54sRltrBl0zgAay2sO+jG/BejBa7tOJq60t7D/ja9waKLHZCnpSIHAYglNDipawM/n5N43Cljx6XbWDzYLnN/I/l/bDV58o44q9reZVynwbGa0Xak58eJNL+/s5EkZZJy8ocz/d2OEf22ybyfLB5iOez1dzGmI4xI2cxA7bteT4RrZaDg1SRtx+d6CPrd/MApd1R2qJkk2zntWs0EvJeHEUxeaHlx2RZ61Kyzz9h2HqRVh5rEGkDo956n1AGFfl5cptr6spFWr+4XP+BZVUiba+SrZ7PDWnZTjQqaQ1pGXubU7K8ybRsF5LJiO+zcq0lZFqoUbYVWlvnH0tq401buY1gKWPJwLdBfW1pKKXUx6C3YpQmRCtvtNF3Pz0d7F6pNhZOF8jCZfJlmpXxHpG6fZX9VGK0WyB3Km+DrC9LCvcTaUutfT2fDxxaKfIc1F+mvfL23iKtYbjy/EiJcZ8c761/+2zpI/L477cZJwJsFNk+0wtiRrtGp0OGDMGNN96IFStW4NVXX8Wxxx6Lr3/963jnnXcAAJdffjkee+wxPPjgg1i2bBk+/fRTnHrqqXuk4EREPZ7rtv3XwzFmEBHlEGMGYwYRUVCMGYwZRERBMWYwZhARBdULYka7fil+0kkneT5ff/31uOWWW/Diiy9iyJAh+Nvf/ob77rsPxx57LADgjjvuwIEHHogXX3wRX/ziF9V1JpNJJJM7vp1UV1fX3n0gIuqZXINW3ysS5O0Y3RxjBhFRDjFmMGYQEQXFmMGYQUQUFGMGYwYRUVC9IGZ0+D1mjuNgwYIFaGxsxKRJk7BixQqk02lMmTIlm+eAAw7AsGHDsHz58lbXM3v2bJSUlGT/hg4d2tEiERH1KMZ1YJxW/nrI60aCYswgIto9jBmMGUREQTFmMGYQEQXFmMGYQUQUVG+IGe1+KP7WW2+hsLAQsVgM3/ve9/Dwww/joIMOQmVlJaLRKEpLSz35BwwYgMpK+R78FldeeSVqa2uzfxs2bGj3ThAR9UiO0/ZfL8CYQUSUI4wZjBlEREExZjBmEBEFxZjBmEFEFFQviBnten06AOy///5YuXIlamtr8dBDD2HatGlYtmxZhwsQi8UQi8VEupsfhRuKtm9lltXhfCbgorC9GY22TW1dQbeprc/31QVja+sKWA5tk9prDXxJ6uozynJBj2PQfLkU5O0N2i5py5lgr4KwHG8+fV3aNgO+aiIkD6RrhbzrCinffYnu+py3qxxugP3UBF1/0Hx+uZ7GIrTrLMb2ZnKdNgph2njdSEf3uZvprJgRbswgHM60a12B231N0HjTTWlxRNdN9rPD77WR1Hatg5eb1dEFgWDtk9Z3UBtrmU9rh412Pm3jy9MZfCc01u7uaKdximV701Faf8tKegcMdqaNAQRjRru1FjOidQahnfpC0Vq5rLUpwAYCHvag7Y6/jgTuNyrtibasWgfdXedBItg2nXjAxrqDffLdEiScBQx56rirg2Uw2iELPIbzryzYQXPDwcalal/BlxR8DK2sX1m2aUBkl9ts6r/rPABghivtt5IvUVa867Kp+6mUI4e0azjc5GsjUm2cc8aMdmstZpR9kEE4smOcYWmh2ndI08Wyb5MuLuxw2dTr1ncvQMvjKl0sJ6LkU6qzE5X5nLjvs3KZuUpa8wBt/TKQuHEluPjy2TF5AsJRmRaPpWVaRI4X48oYMi/sXTam5InaMi1sy/LbynXo+hoVV2kQM65sOFPKCdXyJR2ZL+V471Mk0kqejFJv0/ImSEZZ1k3Lchjf/TCj1EdXuzeltenKvRg7pYyBfKegYI2s3JZy20A5neq1Hq2V+2n77/s5cp+W/km+gltbf5nWp1P6ZrbjvV9e78r2ZbDvnm0mDbT62JYxo91aixlrziyGHd/RWLoxeVKNv/1T2quw0tYVxGW+vKhMK4imRFrc167lh2WegflyUFSo5CsIJUVafkjm8ysKyUGFrQwqvjboTZGWVhoBLS3hC2hJ5V5DoxK86tJxkdaQlvlqU0q+pDdfY0I+z0pGZFvkhJW2NCbbGDfiTXMTsu1TDi2U06SnKf1J21ettPbKVp4LaWNEfxvZWj51HOqj9XXU21xKmz7gVeVa9I13XCXW6P0ree5MSJY/vlXWIeNb34ZXRyrrkuUo1GKoks/U5Cnb9H7++Mv5Sh5vmptIANfI9X+WuefHjHbfhYxGo9hnn30AAOPHj8crr7yC3//+9zjrrLOQSqVQU1Pj+XZVVVUVKioqclZgIqLewjgOjHpXBTCmZ3yzalcYM4iIcoMxgzGDiCgoxgzGDCKioBgzGDOIiILqDTFjt3975boukskkxo8fj0gkgsWLF2f/b9WqVVi/fj0mTZq0u5shIup9HLeN143k+mfu3QNjBhFRBzFmMGYQEQXFmMGYQUQUFGMGYwYRUVC9IGa065fiV155Jb785S9j2LBhqK+vx3333YelS5fiqaeeQklJCS688ELMnDkTffr0QXFxMX7wgx9g0qRJ+OIX5WtiiIg+74xrYFp517zpIa8baQtjBhFR7jBmMGYQEQXFmMGYQUQUFGMGYwYRUVC9IWa066H45s2bcd5552HTpk0oKSnBmDFj8NRTT+H4448HAPzud7+Dbds47bTTkEwmMXXqVPz5z39uV4FaDlzGUSY52JXAU6Rqc3QG1MPnFFe3qa5v1yvTy6+kaXr6nOIBa0xO5xQPmi/I3JC7s36Nfw7MYEu1Y07xoCv06YovJ/nK2tKWaUEh7SRgoL9WJAM5L1BP06kxI9P+mPF5nlO8x8nhnOLdRkfbp4DtZiv90w6vjzpHS1vGmLFnY4aTUiZd8wl0DQWeUzzYsmJO8aDb3NNzigedxzzAPHCtra9Dedqjh88pHmSsF7hfE3CMGGRO8d0Z+wU+xWI/d2PcG/gcd2y5ztbSljFm7OFxRtobM4LMKR54HB2QNue0cXc9L7VWDEfJpzXfjlLxHcufR9LW5WrzaisZXaMEF/8vjhy5VTcj0xxHTijqKHOKZ0JKmm/uXTss128pE5aaPTyneNqV63dcZVnlGGV8c4o72jHT0pS5wl1tTvGMMqd4ypfmr0AALH8eAFDmgkVaqd/KnOIm0/ZnQJ9TXM2nVHCt/+OfOz3ocoHzadeK6L/J5fzz/TppxozOiBluwhszXOW69dcZuEq7plzHjqu0axl5fjJpOb+3v11Lh+Vy6YhcLqXMFR4OyWVDSprtm5jaUSqqNqe4q3SQ00p80NKSrv+zrO8ppS1KK21YWml3Mkq74/gOkZNQ9imhnOOEbEuthCyHSFPmFId2S1RL06Z+V+YUNwHmFBf1GFDHpR2eUzxoVyrg2EnrYvjHO1oeV4mzymWtzgOu5vP335QwqK1Leyu5Oqe4luaPjcr85/55x1vast4aMyzTzR7ff/LJJxg6dGhXF4OIKKc2bNiAIUOGAAASiQRGjhyJysrKNpepqKjA2rVrEY/HO6OIPRJjBhH1RowZewZjBhH1RowZewZjBhH1RowZewZjBhH1Rr01ZnS7h+Ku6+LTTz9FUVER6uvrMXToUGzYsAHFxcVdXbR2q6urY/m7UE8vP9Dz94Hl/+wbVfX19Rg0aBBse8fXvxKJBFIp7St6O0Sj0W4dQLoDxozug+Xvej19H1h+xow9jTGj+2D5u15P3weWnzFjT2PM6D5Y/q7X0/eB5WfM2NMYM7oPlr/r9fR9YPl7f8xo1+vTO4Nt29lvH1j/fdVBcXFxj6yALVj+rtXTyw/0/H34vJe/pKREpMXj8W4fIHoCxozuh+Xvej19Hz7v5WfM2HMYM7oflr/r9fR9+LyXnzFjz2HM6H5Y/q7X0/fh815+xow9hzGj+2H5u15P34fPe/l7c8zojbN0EhERERERERERERERERERAeBDcSIiIiIiIiIiIiIiIiIi6sW69UPxWCyGWbNmIRaLdXVROoTl71o9vfxAz98Hlp86U08/Xyx/1+rp5Qd6/j6w/NSZevr5Yvm7Vk8vP9Dz94Hlp87U088Xy9+1enr5gZ6/Dyw/daaefr5Y/q7V08sP9Px9YPl7P8sYY7q6EERERERERERERERERERERHtCt/6lOBERERERERERERERERER0e7gQ3EiIiIiIiIiIiIiIiIiIuq1+FCciIiIiIiIiIiIiIiIiIh6LT4UJyIiIiIiIiIiIiIiIiKiXosPxYmIiIiIiIiIiIiIiIiIqNfqtg/F582bhxEjRiAej2PixIl4+eWXu7pIrfrPf/6Dk046CYMGDYJlWXjkkUc8/2+MwdVXX42BAwciLy8PU6ZMwerVq7umsD6zZ8/GYYcdhqKiIvTv3x8nn3wyVq1a5cmTSCQwffp09O3bF4WFhTjttNNQVVXVRSWWbrnlFowZMwbFxcUoLi7GpEmT8MQTT2T/v7uX3+/GG2+EZVm47LLLsmndeR+uueYaWJbl+TvggAOy/9+dy95i48aN+Na3voW+ffsiLy8PBx98MF599dXs/3fna5g+w5jRORgzuh/GjM7HmNHzMWZ0DsaM7ocxo/MxZvR8jBmdgzGj+2HM6HyMGT0fY0bnYMzofhgzOh9jRsd1y4fi999/P2bOnIlZs2bhtddew9ixYzF16lRs3ry5q4umamxsxNixYzFv3jz1/3/zm9/gD3/4A+bPn4+XXnoJBQUFmDp1KhKJRCeXVFq2bBmmT5+OF198EYsWLUI6ncYJJ5yAxsbGbJ7LL78cjz32GB588EEsW7YMn376KU499dQuLLXXkCFDcOONN2LFihV49dVXceyxx+LrX/863nnnHQDdv/w7e+WVV/CXv/wFY8aM8aR3930YNWoUNm3alP177rnnsv/X3cu+fft2HHHEEYhEInjiiSfw7rvv4re//S3KysqyebrzNUyMGZ2JMaN7YczofIwZPR9jRudhzOheGDM6H2NGz8eY0XkYM7oXxozOx5jR8zFmdB7GjO6FMaPzMWbsJtMNTZgwwUyfPj372XEcM2jQIDN79uwuLFUwAMzDDz+c/ey6rqmoqDBz5szJptXU1JhYLGb+8Y9/dEEJ27Z582YDwCxbtswY81lZI5GIefDBB7N53nvvPQPALF++vKuKuUtlZWXmr3/9a48qf319vdl3333NokWLzOTJk82ll15qjOn+52DWrFlm7Nix6v9197IbY8zPfvYzc+SRR7b6/z3tGv48YszoOowZXYcxo2swZvR8jBldhzGj6zBmdA3GjJ6PMaPrMGZ0HcaMrsGY0fMxZnQdxoyuw5jRNRgzdk+3+6V4KpXCihUrMGXKlGyabduYMmUKli9f3oUl65i1a9eisrLSsz8lJSWYOHFit9yf2tpaAECfPn0AACtWrEA6nfaU/4ADDsCwYcO6Zfkdx8GCBQvQ2NiISZMm9ajyT58+HSeeeKKnrEDPOAerV6/GoEGDsNdee+Gcc87B+vXrAfSMsi9cuBCHHnoozjjjDPTv3x9f+MIXcNttt2X/v6ddw583jBldizGj6zBmdA3GjJ6NMaNrMWZ0HcaMrsGY0bMxZnQtxoyuw5jRNRgzejbGjK7FmNF1GDO6BmPG7ul2D8W3bt0Kx3EwYMAAT/qAAQNQWVnZRaXquJYy94T9cV0Xl112GY444giMHj0awGflj0ajKC0t9eTtbuV/6623UFhYiFgshu9973t4+OGHcdBBB/WY8i9YsACvvfYaZs+eLf6vu+/DxIkTceedd+LJJ5/ELbfcgrVr1+Koo45CfX19ty87AHz00Ue45ZZbsO++++Kpp57CJZdcgh/+8Ie46667APSsa/jziDGj6zBmdB3GjK7DmNGzMWZ0HcaMrsOY0XUYM3o2xoyuw5jRdRgzug5jRs/GmNF1GDO6DmNG12HM2D3hri4AdR/Tp0/H22+/7Zk/oafYf//9sXLlStTW1uKhhx7CtGnTsGzZsq4uViAbNmzApZdeikWLFiEej3d1cdrty1/+cvbfY8aMwcSJEzF8+HA88MADyMvL68KSBeO6Lg499FDccMMNAIAvfOELePvttzF//nxMmzati0tH1H0xZnQNxoyuxZhB1DGMGV2DMaNrMWYQdQxjRtdgzOhajBlEHcOY0TUYM7oWY8bu6Xa/FC8vL0coFEJVVZUnvaqqChUVFV1Uqo5rKXN3358ZM2bgX//6F5YsWYIhQ4Zk0ysqKpBKpVBTU+PJ393KH41Gsc8++2D8+PGYPXs2xo4di9///vc9ovwrVqzA5s2bccghhyAcDiMcDmPZsmX4wx/+gHA4jAEDBnT7fdhZaWkp9ttvP3z44Yc94vgPHDgQBx10kCftwAMPzL4ypadcw59XjBldgzGj6zBmdC3GjJ6NMaNrMGZ0HcaMrsWY0bMxZnQNxoyuw5jRtRgzejbGjK7BmNF1GDO6FmPG7ul2D8Wj0SjGjx+PxYsXZ9Nc18XixYsxadKkLixZx4wcORIVFRWe/amrq8NLL73ULfbHGIMZM2bg4YcfxjPPPIORI0d6/n/8+PGIRCKe8q9atQrr16/vFuVvjeu6SCaTPaL8xx13HN566y2sXLky+3fooYfinHPOyf67u+/DzhoaGrBmzRoMHDiwRxz/I444AqtWrfKkffDBBxg+fDiA7n8Nf94xZnQuxoyux5jRtRgzejbGjM7FmNH1GDO6FmNGz8aY0bkYM7oeY0bXYszo2RgzOhdjRtdjzOhajBm7yXRDCxYsMLFYzNx5553m3XffNRdffLEpLS01lZWVXV00VX19vXn99dfN66+/bgCYm2++2bz++uvm448/NsYYc+ONN5rS0lLz6KOPmjfffNN8/etfNyNHjjTNzc1dXHJjLrnkElNSUmKWLl1qNm3alP1ramrK5vne975nhg0bZp555hnz6quvmkmTJplJkyZ1Yam9rrjiCrNs2TKzdu1a8+abb5orrrjCWJZlnn76aWNM9y+/ZvLkyebSSy/Nfu7O+/CjH/3ILF261Kxdu9Y8//zzZsqUKaa8vNxs3rzZGNO9y26MMS+//LIJh8Pm+uuvN6tXrzb33nuvyc/PN/fcc082T3e+hokxozMxZnRPjBmdhzGj52PM6DyMGd0TY0bnYczo+RgzOg9jRvfEmNF5GDN6PsaMzsOY0T0xZnQexozd0y0fihtjzB//+EczbNgwE41GzYQJE8yLL77Y1UVq1ZIlSwwA8Tdt2jRjjDGu65pf/OIXZsCAASYWi5njjjvOrFq1qmsL/V9auQGYO+64I5unubnZfP/73zdlZWUmPz/fnHLKKWbTpk1dV2ifb3/722b48OEmGo2afv36meOOOy4bQIzp/uXX+INId96Hs846ywwcONBEo1EzePBgc9ZZZ5kPP/ww+//duewtHnvsMTN69GgTi8XMAQccYG699VbP/3fna5g+w5jRORgzuifGjM7FmNHzMWZ0DsaM7okxo3MxZvR8jBmdgzGje2LM6FyMGT0fY0bnYMzonhgzOhdjRsdZxhiTm9+cExERERERERERERERERERdS/dbk5xIiIiIiIiIiIiIiIiIiKiXOFDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhQnIiIiIiIiIiIiIiIiIqJeiw/FiYiIiIiIiIiIiIiIiIio1+JDcSIiIiIiIiIiIiIiIiIi6rX4UJyIiIiIiIiIiIiIiIiIiHotPhSnHuvOO++EZVlYt26dJ33OnDnYa6+9EAqFMG7cOABAJpPBT3/6UwwdOhS2bePkk0/u9PISEVHPsG7dOliWhTvvvNOT/uSTT2LcuHGIx+OwLAs1NTUAgLvvvhsHHHAAIpEISktLO728RETUta655hpYluVJa2380dDQgO985zuoqKiAZVm47LLLOr/ARETUpY455hgcc8wxnrSqqiqcfvrp6Nu3LyzLwty5cwEAq1evxgknnICSkhJYloVHHnmk08tLRES77/zzz8eIESM8aa2NDVqLCUS0+/hQnLqFpUuXwrKs7F8sFsOAAQNwzDHH4IYbbsCWLVsCrefpp5/GT3/6UxxxxBG44447cMMNNwAAbr/9dsyZMwenn3467rrrLlx++eV7cneIiD6XNm3ahCuuuAJf+tKXUFRUBMuysHTp0lbzv/DCCzjyyCORn5+PiooK/PCHP0RDQ0POy7VzfAmHw+jTpw/Gjx+PSy+9FO+++26gdWzbtg1nnnkm8vLyMG/ePNx9990oKCjA+++/j/PPPx977703brvtNtx66605Lz8RUW/19NNP48ILL8To0aMRCoXETaKdua6L3/zmNxg5ciTi8TjGjBmDf/zjHzkvU8sXb1v+4vE4Bg0ahKlTp+IPf/gD6uvrA62ntfHHDTfcgDvvvBOXXHIJ7r77bpx77rk53wciop7ghRdewDXXXJP9ommLpqYmzJs3DyeccAIGDhyIoqIifOELX8Att9wCx3G6prBtOP/88z1xo7CwEHvttRdOP/10/POf/4TruoHWc/nll+Opp57ClVdeibvvvhv/8z//AwCYNm0a3nrrLVx//fW4++67ceihh+7J3SEi6jVaizO50PKl2Ja//Px8DBs2DCeddBLuuOMOJJPJQOtpbWzQWkwgot1nGWNMVxeCaOnSpfjSl76EH/7whzjssMPgOA62bNmCF154AY899hhKSkrwwAMP4Nhjj80u4zgO0uk0YrFY9pcZV1xxBebMmYPm5mZEo9Fs3m984xt47rnn8Mknn3T6vhERfV60tOX77rsvysvLsXz5cixZskT8CgIAVq5ciUmTJuHAAw/ExRdfjE8++QQ33XQTvvSlL+GJJ57Iabksy8Lxxx+P8847D8YY1NbW4o033sCDDz6IxsZG/PrXv8bMmTOz+Y0xSCaTiEQiCIVCAD77lfiXv/xlLFq0CFOmTMnmnT9/Pi655BKsXr0a++yzT07LTUTU251//vm4//77ccghh2D9+vUIhULiLVAtrrzyStx444246KKLcNhhh+HRRx/F448/jn/84x/4xje+kbMy3Xnnnbjgggvwy1/+EiNHjkQ6nUZlZSWWLl2KRYsWYdiwYVi4cCHGjBmTXSaTySCTySAej2fTWht/fPGLX0Q4HMZzzz2XszITEfVEN910E37yk59g7dq1ni9Fvf322xgzZgyOO+44nHDCCSguLsZTTz2Fhx9+GOeddx7uuuuuriu04vzzz8eCBQvw17/+FQDQ3NyMjz/+GI899hjefPNNHHPMMXj00UdRXFycXSaVSgGA575VRUUFpkyZgnvuuSeb1tzcjPz8fFx11VX41a9+1Ul7RETUO7QWZ3LhmmuuwbXXXotbbrkFhYWFSCaT2LhxI5566im88MILGDNmDP71r39h6NCh2WXS6TRc10UsFsumtTY20GICEeVGuKsLQLSzo446Cqeffron7Y033sAJJ5yA0047De+++y4GDhwIAAiFQtmHFS02b96MvLw8z8CiJT2Xr7Q1xiCRSCAvLy9n6yQi6unGjx+Pbdu2oU+fPnjooYdwxhlntJr35z//OcrKyrB06dLsDaIRI0bgoosuwtNPP40TTjghp2Xbb7/98K1vfcuTduONN+Kkk07Cj370IxxwwAH4yle+AgDZXwbubPPmzQAgYklr6bujsbERBQUFOVsfEVF3dcMNN+C2225DJBLBV7/6Vbz99ttqvo0bN+K3v/0tpk+fjj/96U8AgO985zuYPHkyfvKTn+CMM84Q44Ld9eUvf9nza7wrr7wSzzzzDL761a/ia1/7Gt57773sWCAcDiMc9g6tWxt/bN68GQcddFDOyum6LlKplIhbREQ9VUVFBd566y2MGjUqm/bd734X3/72t3HHHXfgF7/4Rbf7Mmo4HBZjjV/96le48cYbceWVV+Kiiy7C/fffn/0//z0rQI8bLW9NzOVYI5FIIBqNwrb58lAiot11+umno7y8PPv56quvxr333ovzzjsPZ5xxBl588cXs/0UiEbF8a2ODXD/LyGQycF1XjT9EnzfsAVGbWuZVbe2vM4wdOxZz585FTU1N9iYYIOcUtywLd9xxBxobG7Pla8mzZMkSvPPOO9n0ltf5uq6LuXPnYtSoUYjH4xgwYAC++93vYvv27Z4yjBgxAl/96lfx1FNP4dBDD0VeXh7+8pe/AABqampw2WWXYejQoYjFYthnn33w61//2vOKrJbjeNNNN+HWW2/F3nvvjVgshsMOOwyvvPKK2Of3338fZ555Jvr164e8vDzsv//+uOqqqzx5Nm7ciG9/+9sYMGAAYrEYRo0ahdtvvz0Xh5yIKOuhhx6CZVlYtmyZ+L+//OUvsCwr+xCjqKgIffr02eU66+rqsGjRInzrW9/y/GLivPPOQ2FhIR544IHc7UAb+vbtiwULFiAcDuP666/PpvvnFD/mmGMwbdo0AMBhhx0Gy7Kyc0HNmjULANCvXz9YloVrrrkmu54nnngCRx11FAoKClBUVIQTTzwR77zzjqcM559/PgoLC7FmzRp85StfQVFREc455xwA7Y9Rzz33HCZMmIB4PI699toLf//738U+19TU4PLLL8eIESMQi8UwZMgQnHfeedi6dWs2TzKZxKxZs7DPPvsgFoth6NCh+OlPfxr49V9E9PmmzZUH6PNuDxo0SL055Pfoo48inU7j+9//fjbNsixccskl+OSTT7B8+fLdLncQxx57LH7xi1/g448/9vxqY+d9a4kh2vjDsiysXbsWjz/+eDa9ZSwTtO21LAszZszAvffei1GjRiEWi+HJJ58EEGx80FKOBx54ANdffz2GDBmCeDyO4447Dh9++KHY55deeglf+cpXUFZWhoKCAowZMwa///3vPXnef/99nH766ejTpw/i8TgOPfRQLFy4cLePNxH1Xtdccw1+8pOfAABGjhzpaRPLy8s9D8RbnHLKKQCA9957L5vWcs/nueeeww9/+EP069cPpaWl+O53v4tUKoWamhqcd955KCsrQ1lZGX7605+iM1+YecUVV+CEE07Agw8+iA8++CCbvvOc4i37YIzBvHnzssfimmuuwfDhwwEAP/nJT2BZlie+tqfNX7BgAf73f/8XgwcPRn5+Purq6gB81sb/z//8D0pKSpCfn4/Jkyfj+eef96yjJcZ9+OGHOP/881FaWoqSkhJccMEFaGpqEvt8zz33YMKECcjPz0dZWRmOPvpoPP300548QcZJRES7o604s6edc845+M53voOXXnoJixYtyqbvPE5qbWzQWkxo0d7nEHPnzs0+h2iZPjBI372lHM8//zxmzpyJfv36oaCgAKeccoo6ze0TTzyByZMno6ioCMXFxTjssMNw3333efIEiTlEnYG/FKc29evXD3fffbcnLZ1O4/LLL+/UbxadfvrpuPDCC/H00097Hlzs7O6778att96Kl19+Ofvaqi984Qu4++67cf3116OhoQGzZ88GABx44IEAPvu2ccsrEn/4wx9i7dq1+NOf/oTXX38dzz//vOcm3apVq3D22Wfju9/9Li666CLsv//+aGpqwuTJk7Fx40Z897vfxbBhw/DCCy/gyiuvxKZNmzB37lxPGe+77z7U19fju9/9LizLwm9+8xuceuqp+Oijj7LbevPNN3HUUUchEong4osvxogRI7BmzRo89thj2X2vqqrCF7/4xexNsX79+uGJJ57AhRdeiLq6Olx22WW5PPxE9Dl24oknZh9UT5482fN/999/P0aNGoXRo0e3a51vvfUWMpmMmA8vGo1i3LhxeP3113e73EENGzYMkydPxpIlS1BXV+d5SN/iqquuwv77749bb701+zrdvffeGyeffDL+/ve/4+GHH86+Mqvldbp33303pk2bhqlTp+LXv/41mpqacMstt+DII4/E66+/7rmhlclkMHXqVBx55JG46aabkJ+fD6B9MerDDz/Mxspp06bh9ttvx/nnn4/x48dnbyo2NDTgqKOOwnvvvYdvf/vbOOSQQ7B161YsXLgQn3zyCcrLy+G6Lr72ta/hueeew8UXX4wDDzwQb731Fn73u9/hgw8+wCOPPLLnTgYRUStef/11FBQUZPvwLSZMmJD9/yOPPLJTynLuuefi5z//OZ5++mlcdNFF4v9bxk/a+OPuu+/G5ZdfjiFDhuBHP/pRNn97295nnnkGDzzwAGbMmIHy8nKMGDGi3eODG2+8EbZt48c//jFqa2vxm9/8Bueccw5eeumlbJ5Fixbhq1/9KgYOHIhLL70UFRUVeO+99/Cvf/0Ll156KQDgnXfewRFHHIHBgwfjiiuuQEFBAR544AGcfPLJ+Oc//5l9iEVEtLNTTz0VH3zwAf7xj3/gd7/7XfZXdv369Wt1mcrKSgDw/CKvxQ9+8ANUVFTg2muvxYsvvohbb70VpaWleOGFFzBs2DDccMMN+Pe//405c+Zg9OjROO+88/bMjinOPfdcPP3001i0aBH2228/8f9HH310dh7ZlimfAGDMmDEoLS3F5ZdfjrPPPhtf+cpXUFhYCKD994Suu+46RKNR/PjHP0YymUQ0GsUzzzyDL3/5yxg/fjxmzZoF27Zxxx134Nhjj8Wzzz6bjbEtzjzzTIwcORKzZ8/Ga6+9hr/+9a/o378/fv3rX2fzXHvttbjmmmtw+OGH45e//CWi0SheeuklPPPMM9k3gbVnnERE1FEdiTO5dO655+LWW2/F008/jeOPP178f2tjg5ZnGf6YAKDdzyHuuOMOJBIJXHzxxYjFYujTp0+7++4/+MEPUFZWhlmzZmHdunWYO3cuZsyY4Xn7yZ133olvf/vbGDVqFK688kqUlpbi9ddfx5NPPolvfvObANDumEO0Rxmidvr+979vQqGQeeaZZ3K2ziVLlhgA5sEHH2w1z9ixY01ZWVn28x133GEAmLVr12bTpk2bZgoKCsSykydPNqNGjfKkPfvsswaAuffeez3pTz75pEgfPny4AWCefPJJT97rrrvOFBQUmA8++MCTfsUVV5hQKGTWr19vjDFm7dq1BoDp27evqa6uzuZ79NFHDQDz2GOPZdOOPvpoU1RUZD7++GPPOl3Xzf77wgsvNAMHDjRbt2715PnGN75hSkpKTFNTkzgGREQddfbZZ5v+/fubTCaTTdu0aZOxbdv88pe/VJd58MEHDQCzZMmSVv/vP//5j/i/M844w1RUVOSs7MYYA8BMnz691f+/9NJLDQDzxhtvGGN2tNl33HFHNk9LzHnllVc8y86aNcsAMFu2bMmm1dfXm9LSUnPRRRd58lZWVpqSkhJP+rRp0wwAc8UVV3jydiRG7Xw8N2/ebGKxmPnRj36UTbv66qsNAPP//t//E8egJcbcfffdxrZt8+yzz3r+f/78+QaAef7558WyREQ7mzZtmhk+fLhIb2kvW3PiiSeqy7X831577SXSGxsb1TZ0d7TW3u+spKTEfOELX8h+1vZNG38Y81mbfeKJJ3rS2tP2AjC2bZt33nnHkzfo+KBl3HXggQeaZDKZzff73//eADBvvfWWMcaYTCZjRo4caYYPH262b9/uWefO45LjjjvOHHzwwSaRSHj+//DDDzf77ruv2H8iohZz5swR93Rak0wmzUEHHWRGjhxp0ul0Nr2lzZ46daqnbZo0aZKxLMt873vfy6ZlMhkzZMgQM3ny5FzuRqv3oVq8/vrrBoC5/PLLs2mTJ08W5dDGLC3jkjlz5njS29vm77XXXp77RK7rmn333Vcct6amJjNy5Ehz/PHHZ9NaYty3v/1tz7ZOOeUU07dv3+zn1atXG9u2zSmnnGIcx/HkbdlGe8ZJRES7qz1xpr20e0E72759uwFgTjnllGyaNk7SxgbG6DGhvc8hiouLzebNmz15g/bdW+LrlClTPHHi8ssvN6FQyNTU1BhjjKmpqTFFRUVm4sSJprm52bOtluXaE3OIOgNfn07t8ve//x1//vOf8Zvf/AZf+tKXOnXbhYWFqK+vz9n6HnzwQZSUlOD444/H1q1bs3/jx49HYWEhlixZ4sk/cuRITJ06VazjqKOOQllZmWcdU6ZMgeM4+M9//uPJf9ZZZ6GsrCz7+aijjgIAfPTRRwA+my/qP//5D7797W9j2LBhnmVbXpVijME///lPnHTSSTDGeLY7depU1NbW4rXXXsvNQSIiwmdt1+bNm7NTTwCfvVbddV2cddZZ7V5fc3MzACAWi4n/i8fj2f/vLC2/uMhVjFm0aBFqampw9tlne9roUCiEiRMnivgCAJdcconnc3tj1EEHHZSNKcBn337ef//9s/EFAP75z39i7Nix6q/2WmLMgw8+iAMPPBAHHHCAZ7vHHnssAKhlJyLa05qbm1uNGS3/35n2xLikPW3v5MmTPXMPdmR8cMEFF3je/OUfl7z++utYu3YtLrvsMjGfYUvMqK6uxjPPPIMzzzwT9fX12W1u27YNU6dOxerVq7Fx48bcHCQi+lybMWMG3n33XfzpT39COCxfennhhRd6Xi87ceJEGGNw4YUXZtNCoRAOPfRQT/+4M+R6rNGRNn/atGnIy8vLfl65ciVWr16Nb37zm9i2bVt2+cbGRhx33HH4z3/+43kVLwB873vf83w+6qijsG3btuyr2B955BG4rourr75azFfecm46Mk4iIuqJct32A+1/DnHaaad5fhnfkb77xRdf7ImvRx11FBzHwccffwzgs3a9vr4eV1xxRXZs1qJluY7EHKI9ia9Pp8BWrlyJ733vezj77LMxc+bMNvOmUilUV1d70vr164dQKNTh7Tc0NKCoqKjDy/utXr0atbW16N+/v/r/mzdv9nweOXKkuo4333yz1Vev+Nfhf9Dd8oC8ZX7YlsFZW68i3rJlC2pqanDrrbfi1ltvDbRdIqLd0TLnz/3334/jjjsOwGevTh83bpz6CsBdabkho81RnUgkPDdsNC2vTmxRUlKyy2Xa0tDQAAA5izGrV68GgOzDDD//K9rD4TCGDBki1tGeGOWPL8BnMWbn+cfXrFmD0047bZdlf++99wLHNSKizpCXl9dqzGj5/9Y0NzejtrbWk1ZRUbFb5WloaGi1fe6I9ra9/nFJR8YHuxqXrFmzBkDb45IPP/wQxhj84he/wC9+8YtWtzt48OBW10FEtCtz5szBbbfdhuuuuw5f+cpX1Dz+Nq2kpAQAMHToUJG+c/9YU1tb6/myVTQaRZ8+fTpSdAC5H2t0pM33x42W8cq0adNa3U5tba3nRx1txY3i4mKsWbMGtm17vrTl195xEhFRZ2loaMi218BnX6TanVet57rtB9r/HMLf9nek756LMUNHYg7RnsSH4hTI9u3bcdppp2G//fbLztfdlhdeeEH8knzt2rUdnhsonU7jgw8+aPe8tW1xXRf9+/fHvffeq/6/P8BoN9tc18Xxxx+Pn/70p+o6/A+LWvtSgDEmSJGz2wSAb33rW60Gk5Y5bYmIciEWi+Hkk0/Gww8/jD//+c+oqqrC888/jxtuuKFD6xs4cCAAYNOmTeL/Nm3ahEGDBgVavsUdd9yB888/v0NlAYC3334boVBI/fJTR7S003fffbf64MX/65ZYLCZ+TdHeGJWL+NKy3YMPPhg333yz+v/+G4tERH47/5JgZ47jdHidAwcOxJIlS2CM8ay/JY60FTfuv/9+XHDBBZ609raNO/vkk09QW1uLffbZp8Pr8Gtv2+sfl3RkfJDLccmPf/xj8UatFrk8TkT0+XPnnXfiZz/7Gb73ve/hf//3f1vN11qbpqXvqp279NJLcdddd2U/T5482fPGrPZ6++23AeSuPexIm99a3JgzZw7GjRunrqPlV44tchk3go6TiIg6y0033YRrr702+3n48OFYt25dh9eX67YfaP9ziNba/vb03XPZ9rcn5hDtSext0C65rotzzjkHNTU1+L//+z/k5+fvcpmxY8di0aJFnrTd+UXGQw89hObm5lYb7I7Ye++98X//93844ogjOvwLw7333hsNDQ2YMmVKTsq01157AdgRODX9+vVDUVERHMfJ2XaJiHblrLPOwl133YXFixfjvffegzGmQ69OBz77Bmk4HMarr76KM888M5ueSqWwcuVKT5rGH19GjRrVoXIAwPr167Fs2TJMmjQpZ9/g3XvvvQEA/fv373A7nYsYpa2zrfjSkueNN97Acccd1+qDLSKitpSVlaGmpkakt7xiryPGjRuHv/71r3jvvfc8v0B76aWXsv/fmqlTp4q4sTvuvvvu7HpzZXfb3j0xPmiJZW+//Xar62wZu0QiEY5LiKjddtXePfroo/jOd76DU089FfPmzeukUgE//elP8a1vfSv7eXd/uXb33XfDsiwcf/zxu1s0ALlp81va+OLi4pzGDdd18e6777Yal3MxTiIiCqo9/erzzjsPRx55ZPbz7t6H2VNjht15DrEn+u47jxla+wLAnog5RLuDc4rTLl177bV46qmn8I9//CPwr+jKysowZcoUz59/Xomg3njjDVx22WUoKyvD9OnTO7QOzZlnngnHcXDdddeJ/8tkMurNPG0dy5cvx1NPPSX+r6amBplMpl1l6tevH44++mjcfvvtWL9+vef/Wr6BFQqFcNppp+Gf//yn+nBjy5Yt7domEVEQU6ZMQZ8+fXD//ffj/vvvx4QJEzr8y+qSkhJMmTIF99xzj2d+pbvvvhsNDQ0444wzdlmWnf/8vxwPqrq6GmeffTYcx8FVV13VoXVopk6diuLiYtxwww1Ip9Pi/4O007mIUX6nnXYa3njjDTz88MPi/1pizJlnnomNGzfitttuE3mam5vR2NjY7u0S0efL3nvvjdraWrz55pvZtE2bNqltT1Bf//rXEYlE8Oc//zmbZozB/PnzMXjwYBx++OGtLjtw4EARNzrqmWeewXXXXYeRI0finHPO6fB6/Ha37d0T44NDDjkEI0eOxNy5c0XMaYkZ/fv3xzHHHIO//OUv6ttfOC4horYUFBQAgNqv/c9//oNvfOMbOProo3HvvfeKtyrtSQcddJAnZowfP77D67rxxhvx9NNP46yzzsK+++6bk/Llos0fP3489t57b9x0002e1wW3Zx1+J598Mmzbxi9/+UsxN2xL3MjFOImIKKi24ozfXnvt5Wn7jzjiiA5v97777sNf//pXTJo0KTsFYS7s7nOIPdF3P+GEE1BUVITZs2dnp7Zq0dL274mYQ7Q7+EtxatNbb72F6667DkcffTQ2b96Me+65x/P/O397NheeffZZJBIJOI6Dbdu24fnnn8fChQtRUlKChx9+eLfn/9vZ5MmT8d3vfhezZ8/GypUrccIJJyASiWD16tV48MEH8fvf/x6nn356m+v4yU9+goULF+KrX/0qzj//fIwfPx6NjY1466238NBDD2HdunUoLy9vV7n+8Ic/4Mgjj8QhhxyCiy++GCNHjsS6devw+OOPY+XKlQA+G1gtWbIEEydOxEUXXYSDDjoI1dXVeO211/B///d/Yj53IqLdFYlEcOqpp2LBggVobGzETTfdpOb71a9+BQB45513AHz2oPu5554DAM8rD6+//nocfvjhmDx5Mi6++GJ88skn+O1vf4sTTjgB//M//5Pz8n/wwQe45557YIxBXV0d3njjDTz44INoaGjAzTffnNNtFhcX45ZbbsG5556LQw45BN/4xjfQr18/rF+/Ho8//jiOOOII/OlPf2pzHbmIUX4/+clP8NBDD+GMM87At7/9bYwfPx7V1dVYuHAh5s+fj7Fjx+Lcc8/FAw88gO9973tYsmQJjjjiCDiOg/fffx8PPPAAnnrqKRx66KG7c3iIqJf7xje+gZ/97Gc45ZRT8MMf/hBNTU245ZZbsN9+++G1117z5H3zzTexcOFCAJ/NcVdbW5uNI2PHjsVJJ50EABgyZAguu+wyzJkzB+l0GocddhgeeeQRPPvss7j33ntbfa3f7njiiSfw/vvvI5PJoKqqCs888wwWLVqE4cOHY+HChR3+wq8mF21vrscHtm3jlltuwUknnYRx48bhggsuwMCBA/H+++/jnXfeyd6MmzdvHo488kgcfPDBuOiii7DXXnuhqqoKy5cvxyeffII33nijw8eFiHq3lofNV111Fb7xjW8gEongpJNOwtatW/G1r30NlmXh9NNPx4MPPuhZbsyYMd1uyrhMJpO9X5ZIJPDxxx9j4cKFePPNN/GlL32p1bm/O2p323zbtvHXv/4VX/7ylzFq1ChccMEFGDx4MDZu3IglS5aguLgYjz32WLvKtM8+++Cqq67Cddddh6OOOgqnnnoqYrEYXnnlFQwaNAizZ8/OyTiJiCio1uJMy8PyXHjooYdQWFiIVCqFjRs34qmnnsLzzz+PsWPHivi1u3LxHCLXfffi4mL87ne/w3e+8x0cdthh+OY3v4mysjK88cYbaGpqwl133bVHYg7RbjFEbViyZIkB0OrfntpOJBIx/fr1M0cffbS5/vrrzebNm8Uyd9xxhwFg1q5dm02bNm2aKSgoEHknT55sRo0apW771ltvNePHjzd5eXmmqKjIHHzwweanP/2p+fTTT7N5hg8fbk488UR1+fr6enPllVeaffbZx0SjUVNeXm4OP/xwc9NNN5lUKmWMMWbt2rUGgJkzZ45YHoCZNWuWJ+3tt982p5xyiiktLTXxeNzsv//+5he/+IUnT1VVlZk+fboZOnSoiUQipqKiwhx33HHm1ltvVctJRLS7Fi1aZAAYy7LMhg0b1DztiRnPPvusOfzww008Hjf9+vUz06dPN3V1dTkv985lsG3blJaWmi984Qvm0ksvNe+8847I39Jm33HHHdm0lpjzyiuvePLOmjXLADBbtmwR61myZImZOnWqKSkpMfF43Oy9997m/PPPN6+++mo2T2txq8XuxKjJkyebyZMne9K2bdtmZsyYYQYPHmyi0agZMmSImTZtmtm6dWs2TyqVMr/+9a/NqFGjTCwWM2VlZWb8+PHm2muvNbW1ta2WlYioxdNPP21Gjx5totGo2X///c0999yTbS931tK2an/Tpk3z5HUcx9xwww1m+PDhJhqNmlGjRpl77rkn52X3lykajZqKigpz/PHHm9///vdqnNL2rbXxR2ttdtC2F4CZPn26WvYg44OWcdeDDz7oWVaLfcYY89xzz5njjz/eFBUVmYKCAjNmzBjzxz/+0ZNnzZo15rzzzjMVFRUmEomYwYMHm69+9avmoYceUstJRNTiuuuuM4MHDza2bWfv7+zqPtTO90/a20ffVd+7I6ZNm+YpX35+vhkxYoQ57bTTzEMPPWQcxxHLaP10rX1v617S7rT5LV5//XVz6qmnmr59+5pYLGaGDx9uzjzzTLN48eJsntaOpXZPzhhjbr/9dvOFL3whG8smT55sFi1a5MkTZJxERJQLWpzJhZa2seUvHo+bIUOGmK9+9avm9ttvN4lEQiwzbdo0M3z4cE9aa2OD1vr8u/scwphgfffW4mtLXFmyZIknfeHChebwww83eXl5pri42EyYMMH84x//8OQJEnOIOoNlzH/fY0BERERERERERERERERERNTLcE5xIiIiIiIiIiIiIiIiIiLqtfhQnIiIiIiIiIiIiIiIiIiIei0+FCci6gVuueUWjBkzBsXFxSguLsakSZPwxBNPdHWxiIiom9q4cSO+9a1voW/fvsjLy8PBBx+MV199tauLRURE3dyNN94Iy7Jw2WWXdXVRiIiom5k9ezYOO+wwFBUVoX///jj55JOxatWqXS734IMP4oADDkA8HsfBBx+Mf//7351QWiIi6kpd9TyDD8WJiHqBIUOG4MYbb8SKFSvw6quv4thjj8XXv/51vPPOO11dNCIi6ma2b9+OI444ApFIBE888QTeffdd/Pa3v0VZWVlXF42IiLqxV155BX/5y18wZsyYri4KERF1Q8uWLcP06dPx4osvYtGiRUin0zjhhBPQ2NjY6jIvvPACzj77bFx44YV4/fXXcfLJJ+Pkk0/G22+/3YklJyKiztZVzzMsY4zZo1sgIqIu0adPH8yZMwcXXnhhVxeFiIi6kSuuuALPP/88nn322a4uChER9RANDQ045JBD8Oc//xm/+tWvMG7cOMydO7eri0VERN3Yli1b0L9/fyxbtgxHH320muess85CY2Mj/vWvf2XTvvjFL2LcuHGYP39+ZxWViIi6gc54nhHeUyueN28e5syZg8rKSowdOxZ//OMfMWHChF0u57ouPv30UxQVFcGyrD1VPCKiTmGMQX19PQYNGgTb3vFyjkQigVQqtctl/e1gLBZDLBZrcznHcfDggw+isbERkyZN6njhOxFjBhFR58WMhQsXYurUqTjjjDOwbNkyDB48GN///vdx0UUX5WZH9jDGDCKizh9nTJ8+HSeeeCKmTJmCX/3qV7tX+E7EmEFE1DX3pgCgtrYWwGcPOVqzfPlyzJw505M2depUPPLII7tcf64xZhARfQ6eZ5g9YMGCBSYajZrbb7/dvPPOO+aiiy4ypaWlpqqqapfLbtiwwQDgH//4x79e9bdhw4ZsO9fc3Gwq+od2uUxhYaFImzVrVqvt55tvvmkKCgpMKBQyJSUl5vHHH89Fk77HMWbwj3/845/3b0/HjFgsZmKxmLnyyivNa6+9Zv7yl7+YeDxu7rzzzlw17XsMYwb/+Mc//nn/OmOc8Y9//MOMHj3aNDc3G2OMmTx5srn00kt3t0nf4xgz+Mc//vHP+9cZMaOF4zjmxBNPNEcccUSb+SKRiLnvvvs8afPmzTP9+/ff5TZyiTGDf/zjH/+8f731ecYeeX36xIkTcdhhh+FPf/oTgM++LTV06FD84Ac/wBVXXOHJm0wmkUwms59ra2sxbNgwTC46E2Ermk13m5Pws8tKPJ+dof1EnvDWOpGWrigVaZFN20Wau3mrSBNlKO8r0kwqLddVK8uhMelMgEyukpbz00gttG/49aTjHbT8u5NP5LGVpGDflDSOI9JCRYUizU22/a0kALBjUZHm1DfIjEp57ah8kYY1fIg3YbtyXbveY5ZxU1hWfTdqampQUvJZm1VXV4eSkhJ8+OpQFBfJbQNAXb2LfQ7dgA0bNqC4uDib3tY3q1KpFNavX4/a2lo89NBD+Otf/4ply5bhoIMOUvN3F7mIGUfiRIStyI6MyjkND+zv+ezW1Ys8Vl6eSFs7R8aWYee/J3fkkANl2mvefHa+XL9dKOt3ZvMWua6OXrc9vQ0jaosdEklavDG+ttkKyeWgLZeU/c9QcZFIc5T2RKy+IF+kuY1Nns8ZpPEc/r3HY0Y0GsWhhx6KF154IZv2wx/+EK+88gqWL1++y33pSjkZZ+z1fYRDO46L8+FasZ1QqXecgb5yvnUTj4g0a5McP6z65XCRtt/8JpFWt4+3bhV+0izLtU3px9TJORpNk0yzd6obLTJVmz2frbDcJ5PedZ8r53bnFzaMcZ8/SiyAK8cUasxQ4oEV8tU/JY/Wf3Oqa2S+EnndOdu2ibTwsMG+BWV7b3zlyLhJLPvolj0eMzZs2IBDDz0UixYtys4lfswxx/SI16fnJGb0ORdhe6d7U/XKGCLsPTduk9J+++MKAFcZm9qlpTJfTY1IMxnfvaOAfX4rrIxz8+KBykbULv462RXxuQvuh2ljfpNRYpIr128y8n6yFqf829Bu7ft/pZcxKSyrf6BT7k21uOSSS/DE/2/v/qPkqAq8/3+qe36FJDMhgcwkJIH4BfkVEyRAGNjVINE8eZAHHj0uu1+eLxFd/eImLBC/j2v2uOC6q+HoUX48QsAVia5PNi56Ao+oxGwwyWHlVwJZQdcIGk2EzASU/BqS+dF1v3+EmaT63p7c6a7uqq55v86pc6bv3Kq+3V1Vn6q63XV/9CM98cQTmjZtWsl6TU1N+uY3v6m/+Iu/GCq799579fd///fq7u4e9jniFEdmzO+4PpIZ5tBh+4kao8fbvWefYlUxDfa61vzvv7CXdbp9nqHf7LKKwsPR43nXNU9TcKyTrvMA13GXi6v/wqrDcTvSzXXc5Oq78D3msuZ1zddo92e4ntN17Ss/yb4rR/E5v/PadNFx6kDYp41dD2a2PyP226f39fVp69atWr58+VBZLpfTggULnBfZVqxYob//+7+3GxY0RTvFA3tHmstFV5CgwT6Qb8jZFzONs579oYSBvQLabbDnM44VMgzsC0wuxusCkCtUCJGqcX4mdfR++7a/onrFdRyd4p4XN41j3rxjWwyD438GOcd8gWtbdHWKO+oF+aLtPefaR7jb5Xr9J4w3OmG8u/7AW8tpbW2NhMhwmpqadPrpp0uS5s6dq2effVZ33XWX7r//fq/5kxBfZjQev1O8aH8dBnY+BI7PNH+CIzNc65EjW1RUz7VOFmeZa74jytxu630fBgwncHRmONZ5U5QZgWM+17ZiHMefrkxyZksR1/YfBkUXvszg8qqbGVOmTLFOMM4++2x973vfO+68SYotM/LNkU5x1+dnfc7FxwCSTN5xrODYp+ccnQ0NefuktqExWq+hwf6883n7YqlydpkpXrfklzeu96J4+6mJim47ScaNOs59uuNCjTMzXGVF8zrquLYn1/bjW8+6HuHRKX50edXNjK1bt2rPnj06//zzh8oKhYI2b96sr371q+rt7VW+RNuSFFtm5JqineKu88QgemktDOwfO7jPaf3WGVc969qR5zF/cVuPlPk9JzAi1jqZluOJ6l4Pcx3zG8c+QY7jK+NYvCunip/DOLd1d1trcW1KkpYuXapHH31UmzdvHrZDXJI6Ojqszu/u7m51dHR4P1+l4s2Mo5nuOp9ULrp/LTiuJbk6xRtc/RSOcxR5XEN1XfN0tdV5HuA67nLy6BTnuB0p5zpucvVd+B5z2fO65nNtn359LXmPc373tWl3h3VW+zPcXfoVeP3111UoFNTe3h4pb29vV1dXl1V/+fLl2rdv39C0a5f9bSYAyKKCMcNOlQrDMPLN1TQiMwDAT5yZcemll2r79u2Rsl/96lc69VTHLw1ShMwAAD9xZsbll1+uF154Qdu2bRuaLrjgAl177bXatm1bKjvEJTIDAHzFmRnGGC1dulRr167V448/rpkzZx53ns7OTm3YsCFStn79+uqPKXsMMgMA/GShPyP2X4qPVKmfzvfPmhn5RXfDtpftmYtuG3B4sv3T/3Gv27e2yj+33SobcNzyPD9urFVWfEsa4xhY3hy0b1fo/NWS6/YIrlt3Ft8WyyULt8dNw62VXNLSDl/lvo9x1jP2r59cd81x3UbEpbDfvk15bmx0+wzftG9Dapocv8xwXbRxfNvKtd2F238TbYPjl1+5k6K3KQnC0t8kHlAox++9hv43EsuXL9eiRYs0Y8YMHThwQKtXr9bGjRu1bt26ES0n7Xxu0SVJ+ZPtoS1++f9Njzye+B/2ZzP53+wTmdP+n19ZZa6tIP+rnXZh0W1rCn/4o70sR/44f93kWie8tkfPbTsLOYLRxxEujgiy6wz4bU+5FsftRB23wHPeirQog1zzWbeeM2HJL9THmRm33HKLLrnkEn3hC1/Qn/3Zn+mZZ57R1772NX3ta18b0XLSrlRmhDt2Rn4Blz/dvlj3xgXRITfavvucvZwLHMNmvN3+JcxpD9n7112L7NuxT36u6LaGhx3nAI6hYUyP4/bpjnOUgd32BT1rPtet4FyqnRmVZBfSJ871xbmske0Djyc8HN1fO2996DinyJ9i/6Kt8MrxtzvJHrrNOaRUR3S/FISlhzaIMzPGjx+vWbNmRcrGjh2rSZMmWeX1rFRmmENvRn7l6b7lfrTMNRRFwXdIvcOO4wXX7ZOL2ho6ssB5fOLKxYPcKn3UqcW5XxrOJeO+HuazKMf5vesW1K7b4+Ycw7k5P6vifcIhe8gGU3SL7uFGT40zM5YsWaLVq1frkUce0fjx44c6lNva2jTmrdd33XXX6ZRTTtGKFSskSTfddJPe/e5368tf/rKuuOIKrVmzRlu2bEn1uUnJzBjTInPMr7fNJPuXkl1/Gr1OtPed9vpx9pf3WWWh4zpl/g17/73rE+dZZdO+9VK0na7zB1f+uHgOUWNd13LNB6SJq//O1S/nezdc1y3Pi/sqHENpBI1+/SW5sXZmOIcKLToedF4reHV39LEplQrZ6M+IvVP8pJNOUj6fT/y2JwCQdv0mVH+J85L+EV7c27Nnj6677jrt3r1bbW1tmj17ttatW6f3vve9MbS0esgMAPATZ2ZceOGFWrt2rZYvX67Pfe5zmjlzpu68805de+21MbS0esgMAPATZ2bUKzIDAPzEmRkrV66UJM2fPz9S/uCDD+rDH/6wJGnnzp3KHTNMyCWXXKLVq1frM5/5jP72b/9WZ5xxhh5++OGafvGKzAAAP1noz4i9U7ypqUlz587Vhg0bdPXVV0s68pP3DRs2aOnSpXE/HQDUrVClR9gZ6aWqBx54oMLWJIPMAAA/cWaGJL3//e/X+9///gpaVHtkBgD4iTszim3cuDGGpVQXmQEAfuLMjOF+kT7IlSEf+tCH9KEPfWiEzxYfMgMA/GShP6Mqt09ftmyZFi9erAsuuEAXXXSR7rzzTvX09Oj666+vxtMBQF3qM0Z9JU4YSpVnEZkBAMdHZhxBZgDA8ZEZR5AZAHB8ZMYRZAYAHF8WMqMqneLXXHONXnvtNd16663q6urSeeedp8cee0zt7e3VeDoAqEsDCtQv9zgkAyXKs4jMAIDjIzOOIDMA4PjIjCPIDAA4PjLjCDIDAI4vC5lRlU5xSVq6dGlltxfJvTW9JTx40KpSmPvOyOOxT75s1TGTJ1llwbix9vP9ca9dr3W8Xa+oHYXuPXadXN5eVs5eIUzo+OaE6777gcfKlOZvYbja72pvml9DXHzfi0qk4X30fZ15x7biWJxrWwl7eo7bDHO41y4rFOyKgd+2mJ8wIdqGN9+0ZzsYbZcJ+0q2LzRHplL/G00qzYz8xBOVzzUdLXB8zqff/FTkcdDcbNUxE9rssv4Bqyw3ZozdiJPtvDGvRsejcs1n+kqvI9GKVV4p0rDvAEaqzPU2aLAPgZ1Zc/iw3/Iam6wyr227OGuGGX+JzDiq0swI3v42BfmjGbDvbHvfP/Gnr0QeDwz0W3Xyb9qfcWGcnS2/+7/tTGp41V4Hx7zw+8jjcNIEq472/NEuC+31xnme4XN85jsGWLUzg/OHbEnic3KuQ47129j1io8RXfvzwoED9rIO2ucngeN8x/V+OM9RigzsejX62Nj7pUFkxlGVZoYZKMgER88HgiY789XUGJ3HsS64uNaPIJ+zypzHKIeKjlEc67wZsM9jXGXO4yJHPetal3ObGmUrWL2qt8+pFtfSPJ4zaGi0ykx/NCOM45jR67qupPDQIa96x47HLblvWZ4res5gmI4KMuOoSjOj+7LJyje1DD3uWPNLq07HpujjyStfsuoEU+1xzPMzplllpse+Ljn1rmfsekXHNrmJJ1p1XNc4netuYOeUwuMfx3hnDZAUV6749vM5juVdx4ymN9pX4douXGWu7bOwz84b57lHcRscO/biPg9j+qQ33PNnITOq1ikOABhen3Lqk+NgUpJnNykAYJQgMwAAvsgMAIAvMgMA4CsLmUGnOAAkpN/k1G/cIdJfJ9+sAgDUBpkBAPBFZgAAfJEZAABfWcgMOsUBICEF5VQo8c2q4994CAAwmpAZAABfZAYAwBeZAQDwlYXMoFMcABIyMMw3qwbq5JtVAIDaIDMAAL7IDACALzIDAOArC5mR2k7x3E9/rlzQOPS4oX2yVSf/HzuiBSa0F9T1ulVkBgbs55t1hlU28LNfHpAlSPcAAElJSURBVLedQXOzXRYEVll4+LBjZruejMea45rPV7nPWYlqL7+ejJb3wvN1mj7PkSYcywsaoruv3Pjx9my9vVZZbkyLVRY66il0vIaifYzpt/cl5lB0Wzem9GvsN3n1m3yJ/9XLd6tSYuIEKX/M/njPH6wquRNOiDx2rn+udcEhPOxYt7pes8vaT448HtjxO6tO0NRkP0GhXkZhAVLI4zjJdSzovXjHsZ8rD4J80f49dGzXueIMyEklIpTMiE/uD3uVyx3d945f+7JVZ/+VcyOPx/1xr1XHNNgngvmf/doqO/Pj/XYbTpxgL68oW4JXuq06QYu9/oWu9Tksc51Iy7FqWtqB9Il53XDlgZUivs/pOtUecGz/Lfb5iCl6juJzHUnW+UlgQslxCUQiM6opPHjQKstpXPSxY18dTJ9qlRV+ZWeG6bM/e9NvH0MUryOujzVodJxnOLjWU6dyswWoVEqOC5zbis81Wkf7fbZrSTKOa1PF1zJc84VF16ZCU3o7JzPi0/rbATU0Hj22CA8csCs1RvMgaLQ/v4FXXrXKrPNLSQN/Otsqa3hir1UWvvlmdFnjxtrLdx17BPb5jmvdrfY5OJAYx7GPMX79a65+ieJrQKZgL7+wf79V5rwO5biu7brG7NP/UnjjjejjjGdGajvFASDrCgpUcF09e+t/AAAMIjMAAL7IDACALzIDAOArC5lBpzgAJGT4b1bVuDEAgFQjMwAAvsgMAIAvMgMA4CsLmUGnOAAkpN80qK9kiNTHN6sAALVBZgAAfJEZAABfZAYAwFcWMiO1neK5pobImOLGNSZ30b30zT77fvthn2MMr9ln2mX7eqwy4xrLpeg+/8578jvG+HCOC+MaS8NnzO9KxtLxHg+tqB0pGb8nk5IY5z0tPF+nc1ybIoW9e72Wnz/5ZLte0dg6knv7tMZ3c40rUjSurBlmLI1QOYWyx+c58r9Rsg7EJPzt7xUekxnK2dtV8Vguzv2yIzNc8q3jrLLiMbskaeC3u467LJ+xXQCMQLkZao3vLfd+3rXNusaPKprXNa6nNS6hKTE4rMiMOJnewzLB0fc6d8IJVp1x616IPA5O6bDq5F79g73sKZPtJ3zDPkdxrkdF5xmuXNFB+5zFNdYfAAefcV/lOA/wnM/9nI6xOF3nGUVjxuYcY31a4xKSGbURGik45j1z7XPDcPjHkoJ9jnFlXccsjnnzJ55oVysap9Y11qxz3FeXStZxIKtc26fP9TvXOYVK768ji3KMLes8z+gr3ufYdYJ8dF8VmEAqcbmDzIhPvq+g/DHnga7P9PAp0Yxved4edzg3frxVZg4dssoan/yFVRYWn2NK1npUeM0+j3EeVxjPvgtgNPHMB9exWXG/oXPccceyivsbJCloaLTKQsd+Ijcueg07cOyX7PkCldr9ZyEzUtspDgBZ12fyaijxzaq++sgQAECNkBkAAF9kBgDAF5kBAPCVhcygUxwAEtJvGoYZg4NvXgIAjiIzAAC+yAwAgC8yAwDgKwuZQac4ACQklFQoERZ+N9cCAIwWZAYAwBeZAQDwRWYAAHxlITPoFAeAhPSbBjUY9264v05uNwIAqA0yAwDgi8wAAPgiMwAAvrKQGantFA/7BhQeO6h8X7+j1sHIo9yYFqtG/uRWq6zwH/9pL6q52Soyof0p5orqhY52Gcdg9UHefUsBe+aUrDlpacdowHsdFdjfNDIDA8etFzQ02vP191ll4Rtv+C0/Z2+zpj9aL9/q2L8c7InOY+z9waD+Ycbg6Ge9GBEz0K/Il9SCnF2paJ1xfe75aVOtsnDnK3ZZzyG7DY71DUAdCR37a0cmOXPbkRky0e/ImgH7mLH4+DAwoeSIJInMiJM53CsTDP+eBePGRufpes2uM+lEe9mvdNn1Gu1TLuM6hyg6ziheh6QSxywuvusuMNq5tovi7cd323Ed97u2xcA+b5Gi8xb+aJ+z2Oc7juPdt5AZMcoF0c/R8TGb3t5ogeNcpPD6HxzLtj8j1zUm1/Ww4myx2jASrBOAH59txXVO4ct1TuH43Z11jdlxzBj2RdsRmtLHkGRGfJr29Kghf/S9Dh3XKses2xZ57HyH+x3nCo5+ChUcv8t0ZFCQP/71MPcxC+cUVcN7W788PztXH6GKtr1ci92fGTqO6Vx9i87r0K5+lUPRa9i5E06w6hSX5Uyf1GNVk5SNzEhtpzgAZF1BORVKXMwqVQ4AGJ3IDACALzIDAOCLzAAA+MpCZtApDgAJGTB59Zf4ZtVAnXyzCgBQG2QGAMAXmQEA8EVmAAB8ZSEz6BQHgIT0m7zydX67EQBAbZAZAABfZAYAwBeZAQDwlYXMoFMcABISmpxC476tSKlyAMDoRGYAAHyRGQAAX2QGAMBXFjKjbjrFcy3NVllYNEh8eOiwVSdwDEyvwP5wwr5+u54Jj1/PUUeOb0SY0PEtCcfA9655gVHFdxsoqmf6+/xmKxT8lh866uWj34IqHDjg8YSOfcRb+k1umG9WlZ4P1TPw2512oWNfbfod64drn+6D/T6QXr7bpyszfBY/MFD0dAMlapIZcTIDBZng6HsdNNinROGBg5HHgWMfH3btsZftWGfCN9+0yoK8/VlaxyiOcxYnzikAP67totrbj+vagOt8xCNHzED0WoQxjmsYbyEz4mP6+mSCo59j0NBo17E+U8fn6XntKGhssqs5znVNb3nHHgCqzPe6gCtrPM8pis8hvLLMlF42mRGfwi9fVhAczYnc+PFWHVPUn6Gc/d6Hjv4MV/4o5/jsXRFUtM74Zo3rPMla/yoxms9jRsvr9FW8LiTx/viujzGeUzj7JB3XAXz7PVyKj1NdfajF+xKT8cyom05xAMia4cfgqI8QAQDUBpkBAPBFZgAAfJEZAABfWcgMOsUBICEFk1OhxG1FSpUDAEYnMgMA4IvMAAD4IjMAAL6ykBl0igNAQvpNXrk6v90IAKA2yAwAgC8yAwDgi8wAAPjKQmbQKQ4ACRkw+ZJjcNTL7UYAALVBZgAAfJEZAABfZAYAwFcWMiO9neJhITKovOkfsKoEDY2Rx64B52P/HIoHmQ8Cd71iYenB6QHUkDHlz1q8j/Hd/ksomEAF415GqXKUYIykYz5bn7fP9fm51g/fdcZVr/g5Klj/AIxuZEZ8TKEgc+x5RsE+Ts+PHx95XNi/316Q73GAY9/vek6rXvF5xwiW742cwmhX7XXeebxZ5kUKax9Ruu1kRoyCXPTalHP/Hf1Mg7x9odB7l+64rgWgjiRxLOVzLUJB5JLJsciM+OROGKNc0DT02Bw6ZNWxciSwbzfszBFXPjiOM5zzFh16+GaNGbD7Y7z5nGdw7oFB5a4Lvtd2featZH0sd17XeYHXPr1EPY92mIH+49cxpetkITPS2ykOABlXCPMaCN3frCqE9fHNKgBAbZAZAABfZAYAwBeZAQDwlYXMqI+RzwEggwoKhp0AABhEZgAAfMWZGStXrtTs2bPV2tqq1tZWdXZ26kc/+lGVWg4AqLW4zzM2b96sK6+8UlOnTlUQBHr44YeHrb9x40YFQWBNXV1dZb4iAEC1ZOHaFL8UB4CEDIQ55Up8s2qAIRcAAMcgMwAAvuLMjGnTpun222/XGWecIWOMvvnNb+qqq67S888/r3PPPTeO5gIAEhT3eUZPT4/mzJmjj3zkI/rABz7gPd/27dvV2to69Hjy5Mkjfm4AQHVl4doUneIAkJB+k1Ng3Dfs6C9RDgAYncgMAICvODPjyiuvjDz+/Oc/r5UrV+qpp56iUxwAMiDu84xFixZp0aJFI55v8uTJmjBhwojnAwDUThauTdVNp7jp77MLXQPM15rH4PUAMspn+x+mTmhyCkuERalyeErLN9PICAAxITNiZIyk4ffPhQMHKlh2jPWqLS3tALIqoW3MJzP2798fKW9ublZzc/Owyy0UCnrooYfU09Ojzs7OeBqbdiaUdJzxEYs+ZzMwUL32AICP4vyp8NpUOZkxUuedd556e3s1a9Ysffazn9Wll14a6/JrwRRCmaBwzOPyrk05cyTn+GWmsfMpNRnEeQZqoZL1LA3raNzXD8rtLw2KMyBX8pJJFq5N1UcrASCDCsppwLinArtnAMAxyAwAgC+fzJg+fbra2tqGphUrVpRc3gsvvKBx48apublZN9xwg9auXatzzjmnVi8HAFBFcWfGSE2ZMkX33Xefvve97+l73/uepk+frvnz5+u5556L7TkAAPHIwrWpuvmlOABkzUCYV1ByDA53OQBgdCIzAAC+fDJj165dkbFbh/vF35lnnqlt27Zp3759+u53v6vFixdr06ZNdIwDQAbEnRkjdeaZZ+rMM88cenzJJZfo17/+te644w798z//c2zPAwCoXBauTdEpDgAJCU2g0Lhva1KqHAAwOpEZAABfPpnR2toa6eAYTlNTk04//XRJ0ty5c/Xss8/qrrvu0v333x9PgwEAiYk7M+Jw0UUX6YknnqjZ8wEA/GTh2hSd4gCQkAGTU1BirI2BOhmDAwBQG2QGAMBXtTMjDEP19vZWvBwAQPLSeJ6xbds2TZkyJZHnBgCUlsbMGKn67hT3HWAeAFJoIMwpCEuESIlyAMDoRGbEKAiOTINc5xQ+5xmciwCopaD4lxeBVGI3FGdmLF++XIsWLdKMGTN04MABrV69Whs3btS6detGtJy6ZYxKvtGDij+bSvLB+pwrXB4AHEfc5xkHDx7Uyy+/PPR4x44d2rZtmyZOnKgZM2Zo+fLleuWVV/Stb31LknTnnXdq5syZOvfcc3X48GF9/etf1+OPP64f//jH5b2gBJneXpkgHL5Sruj2wuY49QeFBbvMlRkA0iGJY7pylz+CXUkWrk3Vd6c4ANSxLNxuBABQG2QGAMBXnJmxZ88eXXfdddq9e7fa2to0e/ZsrVu3Tu9973vjaCoAIGFxn2ds2bJFl1122dDjZcuWSZIWL16sVatWaffu3dq5c+fQ//v6+vTJT35Sr7zyik444QTNnj1b//Zv/xZZBgAgHbJwbYpOcQBISMEEJW83UqiTEAEA1AaZAQDwFWdmPPDAA3E0CQCQUnGfZ8yfP19mmF8rrlq1KvL4U5/6lD71qU+N+HkAALWXhWtTdIoDQEIGwpxU57cbAQDUBpkBAPBFZgAAfJEZAABfWciMEbdy8+bNuvLKKzV16lQFQaCHH3448n9jjG699VZNmTJFY8aM0YIFC/TSSy/F1V4AyIzB242UmrKAzACAeJAZZAaAUc4YeyqBzKhxZnh+LmUti/HEAVQZmVHjzAgL0Yn9PpBN9bRtm9CeSshCZoy4U7ynp0dz5szRPffc4/z/F7/4Rd19992677779PTTT2vs2LFauHChDh8+XHFjASBLCiY37DQSK1as0IUXXqjx48dr8uTJuvrqq7V9+/YqtdwfmQEA8YgzM4rdfvvtCoJAN998czyNLROZAQDxqGZmpAWZAQDxIDPIDADwlYX+jBHfPn3RokVatGiR83/GGN155536zGc+o6uuukqS9K1vfUvt7e16+OGH9ed//ueVtRYAMqQQ5hSUuK1IYYS3G9m0aZOWLFmiCy+8UAMDA/rbv/1bve9979MvfvELjR07No7mloXMAIB4xJkZx3r22Wd1//33a/bs2WUvIy5kBgDEo1qZkSZkBgDEg8wgMwDAVxb6M2IdU3zHjh3q6urSggULhsra2to0b948Pfnkk84Q6e3tVW9v79Dj/fv3x9kkAEit4W4rMlhevE9sbm5Wc3OzVf+xxx6LPF61apUmT56srVu36l3veldMLY4XmQEA/uLMjEEHDx7Utddeq3/6p3/SP/7jP8bX2CogMwDAn09mZBmZAQD+yAwyAwB8ZaE/I9ave3V1dUmS2tvbI+Xt7e1D/yu2YsUKtbW1DU3Tp0+Ps0kAkFphmFOhxBS+9c2q6dOnR/aRK1as8Fr2vn37JEkTJ06sWvsrRWYAgL9qZMaSJUt0xRVXRC4ApRWZAQD+fDIjy8gMAPBHZpAZAOArC/0Zsf5SvBzLly/XsmXLhh7v37//SJAEwZFpUJoHogeAMhQUSCW+WVXQkfJdu3aptbV1qHy4X/wNCsNQN998sy699FLNmjUrnsamRMnMAICMizsz1qxZo+eee07PPvtsvA1NkZKZYYwkzi0AZJdPZiCK8wwAdStw7NdHcB2dzBi5RDKjws8ZAOKQhf6MWDvFOzo6JEnd3d2aMmXKUHl3d7fOO+885zzHu60jAGSVMYFMiRAZLG9tbY2EiI8lS5boxRdf1BNPPFFxG6uJzAAAf3Fmxq5du3TTTTdp/fr1amlpibWd1UJmAIA/n8zIMjIDAPyRGWQGAPjKQn9GrPdAmTlzpjo6OrRhw4ahsv379+vpp59WZ2dnnE8FAHWvEAbDTuVYunSpHn30Uf3kJz/RtGnTYm5xvMgMAPAXZ2Zs3bpVe/bs0fnnn6+GhgY1NDRo06ZNuvvuu9XQ0KBCoVClV1E+MgMA/FXjPKOekBkA4I/MIDMAwFcW+jNG/EvxgwcP6uWXXx56vGPHDm3btk0TJ07UjBkzdPPNN+sf//EfdcYZZ2jmzJn6u7/7O02dOlVXX311nO0GgLoXhjkFJcZnGum4TcYY3XjjjVq7dq02btyomTNnxtHEipEZABCPODPj8ssv1wsvvBApu/7663XWWWfpb/7mb5TP58tuZyXIDACIR5yZkVZkBgDEg8wgMwDAVxb6M0bcKb5lyxZddtllQ48Hx89YvHixVq1apU996lPq6enRxz/+ce3du1d/8id/oscee6xubs0IALUSmkBBiduNhCO8RdWSJUu0evVqPfLIIxo/fry6urokSW1tbRozZkzFbS0XmQEA8YgzM8aPH2+N0TR27FhNmjSp6mM3DYfMAIB4xJkZaUVmAEA8yAwyAwB8ZaE/Y8Sd4vPnz5cxpuT/gyDQ5z73OX3uc5+rqGEyRlLp5wGAeheGUlDitiJhOLJlrVy5UtKRffSxHnzwQX34wx8uo3XxqFlmAEDGxZkZaUVmAEA8yAwyA8AoMsy+0AeZQWaMeoFj/a9wuwLqSvH6Psz6n4X+jBF3igMA4hHnN6uGO7gHANS/av+CY+PGjRUvAwCQDqPhV38AgHiQGQAAX1noz6BTHAASYkwgUyIsSpUDAEYnMgMA4IvMAAD4IjMAAL6ykBl0igNAUsJApsTtRlSqHAAwOpEZAABfZAYAwBeZAQDwlYHMoFMcABIShkHJsAjrJEQAALVBZgAAfJEZAABfZAYAwFcWMoNOcQBIigmOTKX+BwDAIDIDAOCLzACA0S0o3tcHUqmhW8mM+pDQ2LujAu8t4C8DmUGnOAAkxIRHplL/AwBgEJkBAPBFZgAAfJEZAABfWcgMOsUBICHGlB6Dw9TJN6sAALVBZgAAfJEZAABfZAYAwFcWMoNOcQBIiDFBybColxABANQGmQEA8EVmAAB8kRkAAF9ZyAw6xQEgKRkYgwMAUCNkBgDAF5kBAPBFZgAAfGUgM+gUB4CkhG9Npf4HAMAgMgMA4IvMAIDRzZjhHx+LzMi+oKijarj1AQCGk4HMoFMcAJKSgW9WAQBqhMwAAPgiMwAAvsgMAICvDGQGneIAkBATHplK/Q8AgEFkBgDAF5kBAPBFZgAAfGUhM+gUB4CEBGGgIHR/g6pUOQBgdCIzAAC+yAwAgC8yAwDgKwuZQac4ACTFvDWV+h8AAIPIDACALzIDAOCLzAAA+MpAZtApDgBJCYMjU6n/AQAwiMwAAPgiMwAgu4Ki/bipsBeCzEhW8ecpVf6ZVnt5SL9arFcYnTKQGXSKA0BSwremUv8DAGAQmQEA8EVmAAB8kRkAAF8ZyIxc0g0AgFHLBMNPAAAMIjMAAL5izIwVK1bowgsv1Pjx4zV58mRdffXV2r59e5UaDgCouZjPMzZv3qwrr7xSU6dOVRAEevjhh487z8aNG3X++eerublZp59+ulatWjXy1wEAqL4MXJuiUxwAEhKEw08AAAwiMwAAvuLMjE2bNmnJkiV66qmntH79evX39+t973ufenp6qtN4AEBNxX2e0dPTozlz5uiee+7xqr9jxw5dccUVuuyyy7Rt2zbdfPPN+su//EutW7du5E8OAKiqLFyb4vbpAAAAAAAAo8j+/fsjj5ubm9Xc3GzVe+yxxyKPV61apcmTJ2vr1q1617veVdU2AgAcEhgX2DczJGnRokVatGiR97Lvu+8+zZw5U1/+8pclSWeffbaeeOIJ3XHHHVq4cGH5jU4Dn3GdGecZ1cB6BZTEL8UBICGBCRSEJaY6ud0IAKA2yAwAgC+fzJg+fbra2tqGphUrVngte9++fZKkiRMnVq39AIDaqWZm+HjyySe1YMGCSNnChQv15JNPxvYcAIB4ZOHaFL8UB4CkhG9Npf4HAMAgMgMA4MsjM3bt2qXW1tah4lK/+IvMGoa6+eabdemll2rWrFmVtxMAkLwqZYavrq4utbe3R8ra29u1f/9+HTp0SGPGjIntuQAAFcrAtSk6xQEgIYE5MpX6HwAAg8gMAIAvn8xobW2NdHD4WLJkiV588UU98cQTFbYQAJAW1coMAED2ZOHaFJ3iAJCUDHyzCgBQI2QGAMBXFTJj6dKlevTRR7V582ZNmzat3JYBANIm4fOMjo4OdXd3R8q6u7vV2trKr8QBIG0ycG2KTnEASMjgeBul/gcAwCAyI2MCx2dm6uRr1QBSL87MMMboxhtv1Nq1a7Vx40bNnDkzjiYC9Y0cR4YkfZ7R2dmpH/7wh5Gy9evXq7Ozs+rPXXVp3S+49mG+0vqasoBsgZT69SDpzIhDLukGAMCoZY4zAQAwiMwAAPiKMTOWLFmib3/721q9erXGjx+vrq4udXV16dChQzE3GgCQiJjPMw4ePKht27Zp27ZtkqQdO3Zo27Zt2rlzpyRp+fLluu6664bq33DDDfrNb36jT33qU/rlL3+pe++9V//6r/+qW265paKXBQCoggxcm+KX4gCQkCA8MpX6HwAAg8gMAICvODNj5cqVkqT58+dHyh988EF9+MMfHnnjAACpEvd5xpYtW3TZZZcNPV62bJkkafHixVq1apV279491EEuSTNnztQPfvAD3XLLLbrrrrs0bdo0ff3rX9fChQtH/uQAgKrKwrUpOsUBICnDhEi9jMEBAKgRMgMA4CvGzDApul0jAKAKYj7PmD9//rDZsWrVKuc8zz///MifDABQWxm4NkWnOAAkZbjbinDtCQBwLDIDAOCLzAAA+CIzAAC+MpAZdIoDQEICc2Qq9T8AAAaRGXUiCOwy1y9l+OUlgCoiM6rIdz+PeBS/30m810l85qxnqCEyI4Uq2Qfk8sevExb8ntN3+a7l+Sw/yJW3rFJP0RDt6jIDA2Uvq+rYz1eP73tb7jFGtT+7SpZVg/UqC5lBpzgAJCQLY3AAAGqDzAAA+CIzAAC+yAwAgK8sZAad4gCQpDr5BhUAIAXIDACALzIDAOCLzAAA+KrzzKBTHAASkoVvVgEAaoPMAAD4IjMAAL7IDACAryxkBp3iAJCQLIQIAKA2yAwAgC8yAwDgi8wAAPjKQmbUT6d4uYPE12BweQAoi1Hp242wmwIAHIvMqA+cZwBIAzKjelz7edd1pziX71L8nGnJn0quwcX5Prrk8naZcVy9LW5vEu9tWj5PjA5kRm2Vu/927MOCnL3fNAMDx39OF992mIJfPa/XWUEPmuP9MAWPtrneiyBnl4Wer7Nc7Oerx3tdLvMzSPNnV27brO0iGD4X6jwz6qdTHAAyJgvfrAIA1AaZAQDwRWYAAHyRGQAAX1nIDDrFASApoUp/KbJOQgQAUCNkBgDAF5kBAPBFZgAAfGUgM+gUB4CEBObIVOp/AAAMIjMAAL7IDACALzIDAOArC5nhGDChtBUrVujCCy/U+PHjNXnyZF199dXavn17pM7hw4e1ZMkSTZo0SePGjdMHP/hBdXd3x9poAMiCwduNlJrqHZkBAPEhM8gMAPBFZpAZAOCLzCAzAMBXFjJjRJ3imzZt0pIlS/TUU09p/fr16u/v1/ve9z719PQM1bnlllv0/e9/Xw899JA2bdqkV199VR/4wAfKaFk+OhljTz7KnU86MsB88QQAcQmPM9W5mmYGAGQdmZG+zHCdK3D+ACANyIzqZUbxtapc3m8+17WpSq5XlTtftcX5mipZlktYsKe4nwOoR2RGbTOj3H2OYx9mBgasySnOvhHf8x2f56xkH+yzT69kWZzX1S8+u+E53p8gn7emkjKQGSO6ffpjjz0Webxq1SpNnjxZW7du1bve9S7t27dPDzzwgFavXq33vOc9kqQHH3xQZ599tp566ildfPHF8bUcAOpc8NZU6n/1jswAgPiQGWQGAPgiM8gMAPBFZpAZAOArC5kxol+KF9u3b58kaeLEiZKkrVu3qr+/XwsWLBiqc9ZZZ2nGjBl68sknncvo7e3V/v37IxMAjAZZuN3ISJAZAFA+MoPMAABfZAaZAQC+yAwyAwB8ZSEzyu4UD8NQN998sy699FLNmjVLktTV1aWmpiZNmDAhUre9vV1dXV3O5axYsUJtbW1D0/Tp08ttEgDUF6PStxop465tmzdv1pVXXqmpU6cqCAI9/PDD8bW1QmQGAFQoxszwGVcvSWQGAFQo5vOMNCMzAKBCZAaZAQC+MtCfUXan+JIlS/Tiiy9qzZo1FTVg+fLl2rdv39C0a9euipYHAPUiMMNPI9XT06M5c+bonnvuib+xFSIzAKAycWaGz7h6SSIzAKAycZ9npBmZAQCVITNGjswAMFploT9jRGOKD1q6dKkeffRRbd68WdOmTRsq7+joUF9fn/bu3Rv5dlV3d7c6Ojqcy2publZzc7P9j7AgBSPss885BoAPC37zBo473puMJT+AVBnutiLl3G5k0aJFWrRoUWWNqoKaZEZcyAIAKRVnZhxvXL0k1SQzgiC6vy93P1/tfCCTAJQp7vOMtErkPMM43kCffbNrn+6s57gOVu5zAsim4v1JhfsDMiPGzMjlpeCY/gmffglXPjizoMp9HN7tKHOlSMu5je9zkrP1i88uyiMzzMBAUZUBq87Q4jLQnzGiXmdjjJYuXaq1a9fq8ccf18yZMyP/nzt3rhobG7Vhw4ahsu3bt2vnzp3q7OyMp8UAkBE+Y3AUj1HU29ubbKNHgMwAgPhUMzOKx9VLApkBAPHJwlh/wyEzACA+ZAaZAQC+stCfMaJfii9ZskSrV6/WI488ovHjxw+Nq9HW1qYxY8aora1NH/3oR7Vs2TJNnDhRra2tuvHGG9XZ2amLL764Ki8AAOqWUemxNt4qLx6X6LbbbtNnP/vZarYqNmQGAMSoSpnhGlcvCWQGAMTIIzPqGZkBADEiM8gMAPCVgf6MEXWKr1y5UpI0f/78SPmDDz6oD3/4w5KkO+64Q7lcTh/84AfV29urhQsX6t57742lsQCQJT63G9m1a5daW1uHyqt66/CYkRkAEJ9qZcbguHpPPPFEHM0sG5kBAPHJ+q1wyQwAiA+ZQWYAgK8s9GeMqFPceNyPv6WlRffcc09NB0YHgHoUhEZB6N6vDpa3trZGQqSekBkAEJ9qZEapcfWSQGYAQHx8MqOekRkAEB8yg8wAAF9Z6M8YUad4TQVBdBB4V4AVDxIfFsp/Po+ABIBYZfwWVYkqzgfJbz9PFgBIqxgzwxijG2+8UWvXrtXGjRutcfVGHZ/MKDdXRjJvcT0yCUC5OM+onnL3zd7zOX56M5rzgGwEbHFvB2RGfMKCFOSGHgauX0cWdSaZ/j67jrH7OIIGuxvHDAw45i3zPMNZVkFfi8/yK1HJ+ZnPslzIINQjn2sblmD4XKjzzEhvpzgAZFzct6g6ePCgXn755aHHO3bs0LZt2zRx4kTNmDGjzFYCANIgzsw43rh6AID6lvVb4QIA4kNmAAB8ZaE/g05xAEhI3CGyZcsWXXbZZUOPly1bJklavHixVq1aVUYLAQBpEWdm+IyrBwCoX3RwAAB8kRkAAF9Z6M+gUxwAEhKYI1Op/43U/PnzvcZKAgDUnzgzg6wAgGyL+zwDAJBdZAYAwFcW+jPS2yluhrs5/VuOGaPjCM+xl1z3zbeWJfcY5b5jTPi0A0B9i2EMjpLfoGKXMTK5vBTkjz527b8BIG2sHCEzUquSY/k4x7ZznbOYOh9/Ns7xAAEcQWbEJwii+ynX/imXjz6u5FzE9xpWufPV2/613toLuPhui0lts2RGbILmZgVB47B1zEC/x4LsdcE5frhLcSY5VXALgDjPbeI+x/IZO73cZZUq81mW6zPhuiWS4rNdDFcnA5mR3k5xAMi4IDQKQndalCoHAIxOZAYAwBeZAQDwRWYAAHxlITMcPzUAANSEOc4EAMAgMgMA4CvmzNi8ebOuvPJKTZ06VUEQ6OGHH46vrQCAZFXhPOOee+7RaaedppaWFs2bN0/PPPNMybqrVq1SEASRqaWlpbwnBgBUVwauTdEpDgAJCQrDTwAADCIzAAC+4s6Mnp4ezZkzR/fcc0/8jQUAJCruzPjOd76jZcuW6bbbbtNzzz2nOXPmaOHChdqzZ0/JeVpbW7V79+6h6Xe/+10FrwgAUC1ZuDbF7dMBICFZuN0IAKA2yAwAgK+4M2PRokVatGhRpc0CAKRQ3Jnxla98RR/72Md0/fXXS5Luu+8+/eAHP9A3vvENffrTn3Y/TxCoo6NjxM8FAKitLFybSm+neBAcmQYfNjRaVUx/nz2Pz6LzeUeh/aN5YxwjxvsMRI/acn3ufE6oBdd65rkfkqTAHJlK/Q8jEBai+3HX51C8nw/r5OtrALKrOEeGOX4hM2JkYrqvV85xTuGbLWUfvzrOTxznMTJ1lHG+x1Mc3wPefDJj//79kfLm5mY1NzdXuWV1qDgzHPv+IBfdZ7kuJXmfJ5a7r2O/CaSX77YY5zZbvK8yofMwUoo3M/r6+rR161YtX778aFNyOS1YsEBPPvlkyeYePHhQp556qsIw1Pnnn68vfOELOvfcc0vWTyvTPyBzbH9GzmPf78iV3Bj79vFhT4/XvM4QKj5fSCIfavGccT5HnMvi+iMGFR+v1eGxWhauTXH7dABISBAOPwEAMIjMAAD48smM6dOnq62tbWhasWJFso0GACQizsx4/fXXVSgU1N7eHilvb29XV1eXc54zzzxT3/jGN/TII4/o29/+tsIw1CWXXKLf//73sb5OAEDlsnBtKr2/FAeArDOm9DfC6vCbYgCAKiIzAAC+PDJj165dam1tHSrmV+IAMEolnBmdnZ3q7OwcenzJJZfo7LPP1v33369/+Id/iO15AAAxyMC1KTrFASAhw32Dql6+WQUAqA0yAwDgyyczWltbIx0cAIDRKc7MOOmkk5TP59Xd3R0p7+7u9h4zvLGxUe985zv18ssve9UHANROFq5Ncft0AEhIUDDDTgAADCIzAAC+yAwAgK84M6OpqUlz587Vhg0bhsrCMNSGDRsivwYfTqFQ0AsvvKApU6aM6LkBANWXhfOMuvmluCkUrLKgsams+czAgL2sBsdb4fNz/yDwm8+33mgR5/sxmt9HpE9Q/F2jnFRqFTUa/n8on/U5SArtPEDCyEbAH5lRPbl8efMZx9egXcty1Sv3PMPFN9/Y5wL1w2v7D2p2nnHw4MHIL/Z27Nihbdu2aeLEiZoxY8bIF5g1RecegedthU2/fW1K8swMn3WE/T4wehUfH5phjhdjzoxly5Zp8eLFuuCCC3TRRRfpzjvvVE9Pj66//npJ0nXXXadTTjllaFzyz33uc7r44ot1+umna+/evfrSl76k3/3ud/rLv/zLkT950sJCJBNcpwHW+YLjWD48dNgqc/VdmNDxATn280E+mgfOdqUlM2gHsiyt65C1vtfuPCMJddMpDgBZExijwHUA+9b/AAAYRGYAAHzFnRlbtmzRZZddNvR42bJlkqTFixdr1apVZbURAJAOcWfGNddco9dee0233nqrurq6dN555+mxxx5Te3u7JGnnzp3K5Y52HL/xxhv62Mc+pq6uLp144omaO3eufvrTn+qcc84p7wUBAKomC9em6BQHgIRkYQwOAEBtkBkAAF9xZ8b8+fNl6uQiFwBgZKpxnrF06VItXbrU+b+NGzdGHt9xxx264447ynsiAEBNZeHaFJ3iAJCQwJiS36Cql29WAQBqg8wAAPgiMwAAvsgMAICvLGQGneIAkJCgYBQEJUKkUB8hAgCoDTIDAOCLzAAA+CIzAAC+spAZqe0UD/J5BUF+6HFuQptVJzxwMPLYVSewBomXTOs4u2znK+U0U2ZgwC50PKdc35LwrZcGrrYGObssLPgtz+d11tP7M5qM5s+l+LW7XnfxNmCG2SZCc2Qq9T/4y+WlYzIjyDn2/cax7tqV/J+vmO/+D0eNln0HILnz064kldosyIzYBI1NCoLGowXm+Pf4CsaMscrCAwfsenlH/gx4fj4+60gl+8207nO9to0Kl5fW1w6U4lpnrePPHJmRErmJEyKPg7EnWHVMg+P84bU/2vV6e+2yvj67zHUtqljc+1eMHJmEpFjrHucZNREE0ffece08P25stGBMi1Un3LvPXnSD3Y1j3nzT3YbiesWZkZbj7yT6UKqdjY7lB3n7GKCiPiUMj/dxeK5r2h7XRIZkIDNS2ykOAFkXmCNTqf8BADCIzAAA+CIzAAC+yAwAgK8sZAad4gCQkCzcbgQAUBtkBgDAF5kBAPBFZgAAfGUhM+gUB4CkZOB2IwCAGiEzAAC+yAwAgC8yAwDgKwOZQac4ACQkMEZBiTFNSpUDAEYnMgMA4IvMAAD4IjMAAL6ykBmp7RQ3hYJMkDta0Ndv1XnlxrmRx207Cladg6fYA8dP2fhHqyz4v061G7HzVbtdh3tdzS2qVMGH7zPQfVpWrtB+v2Pl+zqDoPx5MTze26hj90mSZOxtIGhsij42gWTvvo4IjVTqtiJ18s2qtMif2KZ87pj3vvizklT4Q3Tfnx831q5zsMcqy7U0208YhnbR4TL3iaNlOxstrxMYCWtflZNKbRZkRmyKzzOCvH38bQai4W0OHLAX5DpudyzLsfdTzpFBYVEGmYEBe1kN9umbKVRwTJ7Efrg4DxyZbZ3/jATZgnrks10Un387zkWO1iUz4tLQ0a6GY84zTL99chcUnS+Y5ka7zn77PEMTWu2yNw/ZZfv228sryoPw8GF7vtG8P0zLucdo/gxQPUXrt+tYtjhHAiOuTdWCMTr2hC5/ut3fELwZ3V8PnDLJqpN3vO/73/U2q2z8Yy/aTXD0oVjnC65j7Ur2V+XO65ovif13nMt3LMt1XhdrO9KSeXHyfU2uekmoo88gyNltHe60wpKBzEhtpzgAZF0QGgWB+6JvUCchAgCoDTIDAOCLzAAA+CIzAAC+spAZdIoDQFKMKf2tsZR+mwwAkBAyAwDgi8wAAPgiMwAAvjKQGXSKA0BCgoJRUOI+uUGp25AAAEYlMgMA4IvMAAD4IjMAAL6ykBl0igNAUsJQKnG7EdeY1QCAUYzMAAD4IjMAAL7IDACArwxkRmo7xfsuP09hQ8vQ456ORqtOw+Ho48Mn5qw62z59r1V2+Ysftcqa/njIKnvl4++wyqZ94z8jj02v/RaGb75plSmXt8uMYyVxlaWB760PgqD8ectVJ7dlqEu8t1FhIfrYsb4H+eh+KDCB1F9ieRm43UhaFN7YpyA4mhO5Jjszgnx0P1zYv9+u02Dv001fn102MGCV5U8+2W7X66+7Gxx5Uju7ZAqOegnsX+NUT20FqsGxrQe56HYdmEAqdShIZsQmyOcVBMdkQs7ev+bPeXvksfnt7606rmN+02/ng0th/0GrzJVd1vId+RM0NjnqOQ4+fNaTWmRN8fJ8My8t6j2Pk8B7FuXxfgRNrmPS4oAIVOJHGmRGjMy4E2TyzUcfj7H3uQPjmiOPc2/a++Cg2XF+0nPYKnPKO64nFfO+5hTj55/mbTst7YhTmt9v1FbR524K9rFUbtyYyGOuTdVG78LzVWg82p9xwtO/seqYovPC3pNarDpjDk+yytqe67LK3ni/3XfR9sOfW2VBf/TDN47P1XkeU+0ccUnL8utpn5vWdtVCWl57WtrhUHyt23lNu7U1Wsf0SfZl88F/1n1mpLZTHACyLgu3GwEA1AaZAQDwRWYAAHyRGQAAX1nIDDrFASAphVAlfxJYSOldIwAAySAzAAC+yAwAgC8yAwDgKwOZQac4ACQlA7cbAQDUCJkBAPBFZgAAfJEZAABfGciM1HaKHzylUfljxtXb8rmVVp23rf9I5HFDkz2Gyqy7/soqG3Oa/eE8++0H7OX/2B57fODsGdHnfHGHVSc3frxVFh44YJV5j5VnjUFZg/E8itvmu/wkVvx6GuMjCVl9f3y2n0pep8f7lhs3zq8NpYaKM6EUlvgGlWvcHpRmot9Sc42P8vriCyOPT17zM3sxjvlyM06x673abZWFjjHKc83R8QXDPtcYr47PupKxVMvdfwOouWBM8Vh/eclxyCiJzIhRrnWccrmjY8KaQ3ZQh9t/fdzlNJw63Soze/dZZUFbq1XmGh/WvBGd17jOHxxMf59XPadyMyPO48skjlUrec4s5mq1P4MsvmeVKPP9CIr2G4EJJcfwn0eeg8yITdGFv/4Tx1hV+tqil9Z62+w6LW/Y16t6Otqssokv2teTGvbbORW8WVTW/ZpVJzx0yCpzcu0DrOtQstedal+Hcj1HVq9v+BgtrxPH53G9IDf2hOjjsIHzjBooNOcUNB6z/zzR3s+rMZoZLbvftKoEr+yxyn6z5CyrbOYa+9qUprZbRQOTxkaX/9SL9nO6zk/67ewaNfvheurjqPZn4rv8ONuRxXUqISY8/ntpTpsafVzolezdxOAC6z4zUtspDgCZVyhIxnGAKUlhiXIAwOhEZgAAfJEZAABfZAYAwFcGMoNOcQBISgZuNwIAqBEyAwDgi8wAAPgiMwAAvjKQGXSKA0BSCmHp24qUug0JAGB0IjMAAL7IDACALzIDAOArA5lBpzgAJCU0OnYcbPt/AAC8hcwAAPgiMwAAvsgMAICvDGTGiDrFV65cqZUrV+q3v/2tJOncc8/VrbfeqkWLFkmSDh8+rE9+8pNas2aNent7tXDhQt17771qb28fccMmvXhQDfmBocez7v4rq85v/vreyOOZ6z5qL6jN8UEEgVV06c03WGUnN9v1Gl57I/LYDAxYdUxfn9dzet9OoHjWWtyGoE5udSCpvtqahCTen0rWd18JvK6guTnahMO9x53HmP7S/wxDlQ6R+vhm1XBqmRn5tvHKB01Dj/cvOMuqc9K3tkYLmhrt5UztsMoKv/29VxvMgP1ZB+PGRR+71lvHAYMpOMZg8R2XhX0iEB/fPPOp59iGi48ZyYzaZIbp6ZEJjr7X4eHDx50nd8IJVlm453V72f32uUHOsZ8PD9nPafqj60Nu7Fi7jusk0/Utbd8sKK7nu877bgc+z1lJbtXimHM04D2LR7nbgGtR+bxVlpt4YnQxYZ/UVWIBZEZsmfHm6RPV0Ngy9PiNt9vnEMVvdfM++zPubc1ZZSe85jg2aLLrhU32pbvg192Rx7mTT7Lne2W3PV+DvW5VdO7ho5JsKadOmiSRU2Rj9hV9nkGDvY/4w4KZkceFvsPSd0osj8yILTPG7uqJ9Gf0nTLBqpPrj76nja/80V7QmDFWUcfTjv6GfQesooHTp1plDT/fES0YZ59nhL2Oa5w5OzPqZczguuRzjlVuflai3HPLNKn3bKzk2lSR/EmT7EW9vLOowLG/GZSBzLCPtocxbdo03X777dq6dau2bNmi97znPbrqqqv085//XJJ0yy236Pvf/74eeughbdq0Sa+++qo+8IEPVKXhAFD3wnD4qc6RGQAQIzKDzAAAX2QGmQEAvsgMMgMAfGUgM0b0S/Err7wy8vjzn/+8Vq5cqaeeekrTpk3TAw88oNWrV+s973mPJOnBBx/U2WefraeeekoXX3yxc5m9vb3qPeabSPv37x/pawCAumQKBRnj/oalycA3L8kMAIgPmUFmAIAvMoPMAABfZAaZAQC+spAZI/ql+LEKhYLWrFmjnp4edXZ2auvWrerv79eCBQuG6px11lmaMWOGnnzyyZLLWbFihdra2oam6dOnl9skAKgvxhy5dbZrqqdbuHggMwCgQmQGmQEAvsgMMgMAfJEZZAYA+MpAZoy4U/yFF17QuHHj1NzcrBtuuEFr167VOeeco66uLjU1NWnChAmR+u3t7erqKjXQlbR8+XLt27dvaNq1a9eIXwQA1KVCYfgpA8gMAIgJmUFmAIAvMoPMAABfZAaZAQC+MpAZI7p9uiSdeeaZ2rZtm/bt26fvfve7Wrx4sTZt2lR2A5qbm9Xc3GyV5/YfVi5/9JsFL/71P1t13vnsn0cen3TyAatOfsNEq2zC9oNWWfe88VZZ7wS7vW2/jtbL/fb3Vp3cafa3w8Jdr1plpn/AfgLjuO++qywNgsAuS8u3QcptW5pfUxLKfT9cddL83rraFtjfGTJ9fdEqDY32bC3R/VlgAqnf/bSmUJAJStxupMRtSOpNrTLDHO6VCY6uT+N/9KI9c1P08xq44O1ez5mfOM4qy/3a3vebPjtSwwNFuZTLW3WCnL3+ucrKjoI0b3tpVvy+8Z6NTr6fu6te8fbuuI1U0BDdbwQmlHqtakeegswYsZLnGZMmKpc7Wh6+Yh+n588+I1rwh71WHfPmIasscIyhFR46bNdrcWRZ0QmkGfA7V3Adj5iBEgcfVsUq79t8jgmT2L+meZ8e53mMS5yvvZLnzOLxSYztz7Xa1yes5Q/zfGTGyJXKjJY9h9SQP7rv7ZvbZtUJm6KfxX/8zUq7vQ98wiqb/ri9n1//Lw9aZW9b+/9aZW//+v8Vedwz9QSrTsue16wy01viQMNHuftv33rsF+rjOevpc0pzW33b5qgX5KPnGcXnFJK07/ToNa3C4dK/iyMzRq7kecbObuVyTUOPm/L2NSAVHePv/rMzrSqNdteFJn3/F3bh1HarqOGPPVZZ0NYaeVx4tduqkxvTYpUVXH0Xvsd/xddVfS9qpWUbTYs0nAfEfczvM28l+2/HtVfX9ZiyVbD/Lnu+CraLXNH18PCAvYPJTYge3wZhn/Sme3lZyIwRd4o3NTXp9NNPlyTNnTtXzz77rO666y5dc8016uvr0969eyPfruru7lZHR0dsDQaAzDBGUolQy8hBIJkBADEhM8gMAPBFZpAZAOCLzCAzAMBXBjKj7DHFB4VhqN7eXs2dO1eNjY3asGHD0P+2b9+unTt3qrOzs9KnAYDsKYTD3G6kvJ8F33PPPTrttNPU0tKiefPm6Zlnnom50ZUhMwCgTGQGmQEAvsgMMgMAfKUgMx566CGdddZZamlp0Tve8Q798Ic/LOt5fZEZAFCmFGRGpUb0S/Hly5dr0aJFmjFjhg4cOKDVq1dr48aNWrdundra2vTRj35Uy5Yt08SJE9Xa2qobb7xRnZ2duvjii6vVfgCoW0duN+L+blI5txv5zne+o2XLlum+++7TvHnzdOedd2rhwoXavn27Jk+eXGlzR4zMAID4kBlkBgD4IjPIDADwlXRm/PSnP9Vf/MVfaMWKFXr/+9+v1atX6+qrr9Zzzz2nWbNmjfj5i5EZABCfpDMjDiPqFN+zZ4+uu+467d69W21tbZo9e7bWrVun9773vZKkO+64Q7lcTh/84AfV29urhQsX6t577x1Rg8xbP7EfKETHOdp/wP6WQeHNaJ0g7/gmQp89ht9AwS4r9Nlj8RUcQy0NDETnzRl7vL6cY8bQUc8YzzHF7RmPX6cmUjw2T9ltS/NrSkKc70ea31vXGB+OnXvR9hm4hv0w0WUNvLXtG8dr7Q/7ZErcbmTgrYHI9+/fHykvNW6RJH3lK1/Rxz72MV1//fWSpPvuu08/+MEP9I1vfEOf/vSnnfNUU00zo3gfaxzjxxQp3p+XfI6CYz9v+ux6jv28te93jQVr/MbXcWaGlzRve2mWgjFvUd+Kt3fHyUHxvoTMqFFmhO73PVK3+Hg+dO337TI5j/ntz8217y/OjMAxnytrXMcjrnpO1nNUkhm+88a5f81ixsV4HuMS6/tTyXNm8bOLkWOfU2xwX0Zm1PbaVKHXPocwYfS9dl2/Cg87rk0N2NeOnPMecl3Xis470G+fvzrzzTcfnKp9fMx+oT7U0+eU5raWedwkKbCuTTmumRftc8K39l1pzIy77rpL/+W//Bf9z//5PyVJ//AP/6D169frq1/9qu677z7nc4xEkucZClxjG0ev7RQcfRc5x2HAgOvcw9V54Tr3CIuyzNWfYewccdXzHhu8+LoqY4pXUbXPA2I+5veat4L9t2tdi3Ws6/L332XPV8E5ea7o2oNrUbniayRZP88wKbNr167Bm9IzMTExZWbatWvX0H7u0KFDpqOj47jzjBs3ziq77bbbnPvO3t5ek8/nzdq1ayPl1113nflv/+2/VXGvnSwyg4mJKYsTmVEdZAYTE1MWJzKjOsgMJiamLE5pzIzp06ebO+64I1J26623mtmzZ1eyG68pMoOJiSmLUxozIw4j+qV4LUydOlW7du3S+PHjdeDAAU2fPl27du1Sa2tr0k0bsf3799P+BNV7+6X6fw20XzLG6MCBA5o6depQWUtLi3bs2KG+vuF/AWKMURBEv81V6ltVr7/+ugqFgtrb2yPl7e3t+uUvf1lW2+sBmZEetD959f4aaD+ZUW1kRnrQ/uTV+2ug/WRGtZEZ6UH7k1fvr4H2pzszurq6nPW7urqGbVeakBnpQfuTV++vgfanOzPikLpO8Vwup2nTpknS0JvX2tpalyvgINqfrHpvv1T/r2G0t7+trc0qa2lpUUtLSyXNgsiMNKL9yav31zDa209mVA+ZkT60P3n1/hpGe/vJjOohM9KH9iev3l/DaG8/mVE9ZEb60P7k1ftrGO3tz3JmuEdEBwDUlZNOOkn5fF7d3d2R8u7ubnV0dCTUKgBAGpEZAABfZAYAwFc5mdHR0UHGAMAolNR5Bp3iAJABTU1Nmjt3rjZs2DBUFoahNmzYoM7OzgRbBgBIGzIDAOCLzAAA+ConMzo7OyP1JWn9+vVkDABkXFLnGam7ffqxmpubddttt5W853za0f5k1Xv7pfp/DbS/tpYtW6bFixfrggsu0EUXXaQ777xTPT09uv7665NuWk3U2+dVjPYnq97bL9X/a6D9tUVm1NfnVYz2J6ve2y/V/2ug/bVFZtTX51WM9ier3tsv1f9roP21dbzMuO6663TKKadoxYoVkqSbbrpJ7373u/XlL39ZV1xxhdasWaMtW7boa1/7WpIvo2z19nkVo/3Jqvf2S/X/Gmh/bSVxnhEYY0zVlg4AqKmvfvWr+tKXvqSuri6dd955uvvuuzVv3rykmwUASCEyAwDgi8wAAPgaLjPmz5+v0047TatWrRqq/9BDD+kzn/mMfvvb3+qMM87QF7/4Rf3X//pfE2o9AKCWan2eQac4AAAAAAAAAAAAACCzGFMcAAAAAAAAAAAAAJBZdIoDAAAAAAAAAAAAADKLTnEAAAAAAAAAAAAAQGbRKQ4AAAAAAAAAAAAAyKzUdorfc889Ou2009TS0qJ58+bpmWeeSbpJJW3evFlXXnmlpk6dqiAI9PDDD0f+b4zRrbfeqilTpmjMmDFasGCBXnrppWQaW2TFihW68MILNX78eE2ePFlXX321tm/fHqlz+PBhLVmyRJMmTdK4ceP0wQ9+UN3d3Qm12LZy5UrNnj1bra2tam1tVWdnp370ox8N/T/t7S92++23KwgC3XzzzUNlaX4Nn/3sZxUEQWQ666yzhv6f5rYPeuWVV/Q//sf/0KRJkzRmzBi94x3v0JYtW4b+n+ZtGEeQGbVBZqQPmVF7ZEb9IzNqg8xIHzKj9siM+kdm1AaZkT5kRu2RGfWPzKgNMiN9yIzaIzPKl8pO8e985ztatmyZbrvtNj333HOaM2eOFi5cqD179iTdNKeenh7NmTNH99xzj/P/X/ziF3X33Xfrvvvu09NPP62xY8dq4cKFOnz4cI1batu0aZOWLFmip556SuvXr1d/f7/e9773qaenZ6jOLbfcou9///t66KGHtGnTJr366qv6wAc+kGCro6ZNm6bbb79dW7du1ZYtW/Se97xHV111lX7+859LSn/7j/Xss8/q/vvv1+zZsyPlaX8N5557rnbv3j00PfHEE0P/S3vb33jjDV166aVqbGzUj370I/3iF7/Ql7/8ZZ144olDddK8DYPMqCUyI13IjNojM+ofmVE7ZEa6kBm1R2bUPzKjdsiMdCEzao/MqH9kRu2QGelCZtQemVEhk0IXXXSRWbJkydDjQqFgpk6dalasWJFgq/xIMmvXrh16HIah6ejoMF/60peGyvbu3Wuam5vNv/zLvyTQwuHt2bPHSDKbNm0yxhxpa2Njo3nooYeG6vznf/6nkWSefPLJpJp5XCeeeKL5+te/XlftP3DggDnjjDPM+vXrzbvf/W5z0003GWPS/xncdtttZs6cOc7/pb3txhjzN3/zN+ZP/uRPSv6/3rbh0YjMSA6ZkRwyIxlkRv0jM5JDZiSHzEgGmVH/yIzkkBnJITOSQWbUPzIjOWRGcsiMZJAZlUndL8X7+vq0detWLViwYKgsl8tpwYIFevLJJxNsWXl27Nihrq6uyOtpa2vTvHnzUvl69u3bJ0maOHGiJGnr1q3q7++PtP+ss87SjBkzUtn+QqGgNWvWqKenR52dnXXV/iVLluiKK66ItFWqj8/gpZde0tSpU/W2t71N1157rXbu3CmpPtr+f/7P/9EFF1ygD33oQ5o8ebLe+c536p/+6Z+G/l9v2/BoQ2Yki8xIDpmRDDKjvpEZySIzkkNmJIPMqG9kRrLIjOSQGckgM+obmZEsMiM5ZEYyyIzKpK5T/PXXX1ehUFB7e3ukvL29XV1dXQm1qnyDba6H1xOGoW6++WZdeumlmjVrlqQj7W9qatKECRMiddPW/hdeeEHjxo1Tc3OzbrjhBq1du1bnnHNO3bR/zZo1eu6557RixQrrf2l/DfPmzdOqVav02GOPaeXKldqxY4f+9E//VAcOHEh92yXpN7/5jVauXKkzzjhD69at0yc+8Qn99V//tb75zW9Kqq9teDQiM5JDZiSHzEgOmVHfyIzkkBnJITOSQ2bUNzIjOWRGcsiM5JAZ9Y3MSA6ZkRwyIzlkRmUakm4A0mPJkiV68cUXI+Mn1IszzzxT27Zt0759+/Td735Xixcv1qZNm5Julpddu3bppptu0vr169XS0pJ0c0Zs0aJFQ3/Pnj1b8+bN06mnnqp//dd/1ZgxYxJsmZ8wDHXBBRfoC1/4giTpne98p1588UXdd999Wrx4ccKtA9KLzEgGmZEsMgMoD5mRDDIjWWQGUB4yIxlkRrLIDKA8ZEYyyIxkkRmVSd0vxU866STl83l1d3dHyru7u9XR0ZFQq8o32Oa0v56lS5fq0Ucf1U9+8hNNmzZtqLyjo0N9fX3au3dvpH7a2t/U1KTTTz9dc+fO1YoVKzRnzhzdddddddH+rVu3as+ePTr//PPV0NCghoYGbdq0SXfffbcaGhrU3t6e+tdwrAkTJujtb3+7Xn755bp4/6dMmaJzzjknUnb22WcP3TKlXrbh0YrMSAaZkRwyI1lkRn0jM5JBZiSHzEgWmVHfyIxkkBnJITOSRWbUNzIjGWRGcsiMZJEZlUldp3hTU5Pmzp2rDRs2DJWFYagNGzaos7MzwZaVZ+bMmero6Ii8nv379+vpp59Oxesxxmjp0qVau3atHn/8cc2cOTPy/7lz56qxsTHS/u3bt2vnzp2paH8pYRiqt7e3Ltp/+eWX64UXXtC2bduGpgsuuEDXXnvt0N9pfw3HOnjwoH79619rypQpdfH+X3rppdq+fXuk7Fe/+pVOPfVUSenfhkc7MqO2yIzkkRnJIjPqG5lRW2RG8siMZJEZ9Y3MqC0yI3lkRrLIjPpGZtQWmZE8MiNZZEaFTAqtWbPGNDc3m1WrVplf/OIX5uMf/7iZMGGC6erqSrppTgcOHDDPP/+8ef75540k85WvfMU8//zz5ne/+50xxpjbb7/dTJgwwTzyyCPmZz/7mbnqqqvMzJkzzaFDhxJuuTGf+MQnTFtbm9m4caPZvXv30PTmm28O1bnhhhvMjBkzzOOPP262bNliOjs7TWdnZ4Ktjvr0pz9tNm3aZHbs2GF+9rOfmU9/+tMmCALz4x//2BiT/va7vPvd7zY33XTT0OM0v4ZPfvKTZuPGjWbHjh3m3//9382CBQvMSSedZPbs2WOMSXfbjTHmmWeeMQ0NDebzn/+8eemll8z//t//25xwwgnm29/+9lCdNG/DIDNqicxIJzKjdsiM+kdm1A6ZkU5kRu2QGfWPzKgdMiOdyIzaITPqH5lRO2RGOpEZtUNmVCaVneLGGPO//tf/MjNmzDBNTU3moosuMk899VTSTSrpJz/5iZFkTYsXLzbGGBOGofm7v/s7097ebpqbm83ll19utm/fnmyj3+JqtyTz4IMPDtU5dOiQ+au/+itz4oknmhNOOMH89//+383u3buTa3SRj3zkI+bUU081TU1N5uSTTzaXX375UIAYk/72uxSHSJpfwzXXXGOmTJlimpqazCmnnGKuueYa8/LLLw/9P81tH/T973/fzJo1yzQ3N5uzzjrLfO1rX4v8P83bMI4gM2qDzEgnMqO2yIz6R2bUBpmRTmRGbZEZ9Y/MqA0yI53IjNoiM+ofmVEbZEY6kRm1RWaULzDGmHh+cw4AAAAAAAAAAAAAQLqkbkxxAAAAAAAAAAAAAADiQqc4AAAAAAAAAAAAACCz6BQHAAAAAAAAAAAAAGQWneIAAAAAAAAAAAAAgMyiUxwAAAAAAAAAAAAAkFl0igMAAAAAAAAAAAAAMotOcQAAAAAAAAAAAABAZtEpDgAAAAAAAAAAAADILDrFAQAAAAAAAAAAAACZRac4AAAAAAAAAAAAACCz6BQHAAAAAAAAAAAAAGTW/w+0WbB8PbIaCgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2000x1200 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "mean_pred_np = mean_pred[0][0].cpu().detach().numpy()\n",
    "batch_np = batch[0][0].cpu().detach().numpy()\n",
    "\n",
    "titles = [\"z\", \"v10\", \"u10\", \"t2m\", \"t\"]\n",
    "\n",
    "# Create a figure with 5 columns and 3 rows of subplots (Prediction, Truth, and Difference)\n",
    "fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "\n",
    "# Reduce spacing between images, with smaller hspace to shorten the vertical space between rows\n",
    "plt.subplots_adjust(wspace=0.2, hspace=-0.8)\n",
    "\n",
    "# Plot each variable (z, v10, u10, t2m, t)\n",
    "vmax_list = [3,6,4,2,3]\n",
    "for i in range(5):\n",
    "    vmin = batch_np[i].min()\n",
    "    vmax = batch_np[i].max()\n",
    "    \n",
    "    # Top row (Prediction)\n",
    "    ax1 = axes[0, i]\n",
    "    im1 = ax1.imshow(mean_pred_np[i], cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    ax1.set_title(f\"{titles[i]} - Pred\")\n",
    "    \n",
    "    # Add a small colorbar next to the top row images\n",
    "    divider1 = make_axes_locatable(ax1)\n",
    "    cax1 = divider1.append_axes(\"right\", size=\"3%\", pad=0.05)  # Make the colorbar smaller\n",
    "    fig.colorbar(im1, cax=cax1, orientation='vertical')\n",
    "\n",
    "    # Middle row (Truth)\n",
    "    ax2 = axes[1, i]\n",
    "    im2 = ax2.imshow(batch_np[i], cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    ax2.set_title(f\"{titles[i]} - Truth\")\n",
    "    \n",
    "    # Add a small colorbar next to the middle row images\n",
    "    divider2 = make_axes_locatable(ax2)\n",
    "    cax2 = divider2.append_axes(\"right\", size=\"3%\", pad=0.05)  # Make the colorbar smaller\n",
    "    fig.colorbar(im2, cax=cax2, orientation='vertical')\n",
    "\n",
    "    # Bottom row (Difference)\n",
    "    difference = (mean_pred_np[i] - batch_np[i])/batch_np[i]\n",
    "    ax3 = axes[2, i]\n",
    "    im3 = ax3.imshow(np.abs(difference), cmap='viridis', vmin=0, vmax=vmax_list[i])#\n",
    "    ax3.set_title(f\"{titles[i]} - Difference\")\n",
    "\n",
    "    # Add a small colorbar next to the bottom row images\n",
    "    divider3 = make_axes_locatable(ax3)\n",
    "    cax3 = divider3.append_axes(\"right\", size=\"3%\", pad=0.05)  # Make the colorbar smaller\n",
    "    fig.colorbar(im3, cax=cax3, orientation='vertical')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7d568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f058221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05d5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6b35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec554e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_ode",
   "language": "python",
   "name": "clim_ode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
